I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:04:00.0
Total memory: 7.92GiB
Free memory: 4.41GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x5e9ab50
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Quadro K620
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:03:00.0
Total memory: 1.95GiB
Free memory: 1.85GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Ignoring gpu device (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0) with Cuda multiprocessor count: 3. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
Connecting to port  15690
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-04 21:48:52.958846: step 0, loss = 4.67 (94.2 examples/sec; 1.359 sec/batch)
2017-05-04 21:48:53.924357: step 10, loss = 4.67 (1325.7 examples/sec; 0.097 sec/batch)
2017-05-04 21:48:55.096719: step 20, loss = 4.63 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 21:48:56.268128: step 30, loss = 4.61 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 21:48:57.426430: step 40, loss = 4.59 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-04 21:48:58.599841: step 50, loss = 4.51 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 21:48:59.800176: step 60, loss = 4.44 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:00.965325: step 70, loss = 4.36 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:02.122299: step 80, loss = 4.35 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-04 21:49:03.324608: step 90, loss = 4.45 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:04.518355: step 100, loss = 4.31 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-04 21:49:05.699019: step 110, loss = 4.31 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:06.894603: step 120, loss = 4.34 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:08.095638: step 130, loss = 4.38 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:09.274846: step 140, loss = 4.35 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:10.441731: step 150, loss = 4.19 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:11.610115: step 160, loss = 4.27 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:12.786637: step 170, loss = 4.19 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:13.942686: step 180, loss = 4.24 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-04 21:49:15.115726: step 190, loss = 4.25 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:16.326354: step 200, loss = 4.12 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 21:49:17.479555: step 210, loss = 4.27 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-04 21:49:18.657748: step 220, loss = 4.19 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:19.832835: step 230, loss = 4.25 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:21.014807: step 240, loss = 4.23 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:22.226956: step 250, loss = 4.25 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-04 21:49:23.437470: step 260, loss = 4.19 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 21:49:24.630409: step 270, loss = 4.17 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:49:25.833691: step 280, loss = 4.08 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:27.029420: step 290, loss = 4.03 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:28.224908: step 300, loss = 4.07 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:29.430885: step 310, loss = 4.21 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 21:49:30.628142: step 320, loss = 4.09 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:31.801439: step 330, loss = 4.02 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:32.997364: step 340, loss = 4.31 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:34.193219: step 350, loss = 4.00 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:35.404213: step 360, loss = 4.14 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 21:49:36.605104: step 370, loss = 4.03 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:37.791629: step 380, loss = 4.13 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:49:38.968366: step 390, loss = 4.10 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:40.169992: step 400, loss = 3.92 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:41.355792: step 410, loss = 4.21 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 21:49:42.544811: step 420, loss = 3.97 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-04 21:49:43.709472: step 430, loss = 4.16 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-04 21:49:44.891441: step 440, loss = 3.95 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:46.058049: step 450, loss = 4.03 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:47.244448: step 460, loss = 3.93 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:49:48.413667: step 470, loss = 3.86 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:49.584365: step 480, loss = 4.00 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:50.759420: step 490, loss = 4.04 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:51.937357: step 500, loss = 3.93 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:53.139575: step 510, loss = 3.90 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:54.313406: step 520, loss = 3.91 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:55.497824: step 530, loss = 3.96 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:56.680345: step 540, loss = 3.97 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:57.855306: step 550, loss = 3.97 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:59.045147: step 560, loss = 3.90 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:00.218827: step 570, loss = 3.80 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:01.369949: step 580, loss = 3.89 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-04 21:50:02.560430: step 590, loss = 3.85 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:03.733412: step 600, loss = 3.91 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:04.909654: step 610, loss = 3.77 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:06.084327: step 620, loss = 3.81 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:07.270148: step 630, loss = 3.82 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:08.445698: step 640, loss = 3.76 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:09.620970: step 650, loss = 3.79 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:10.791080: step 660, loss = 3.87 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:11.972818: step 670, loss = 3.84 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:13.153422: step 680, loss = 3.79 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:14.319548: step 690, loss = 3.87 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:15.499482: step 700, loss = 3.72 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:16.685890: step 710, loss = 3.88 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:17.845530: step 720, loss = 3.83 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-04 21:50:19.027491: step 730, loss = 3.74 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:20.200271: step 740, loss = 3.69 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:21.377072: step 750, loss = 3.72 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:22.569086: step 760, loss = 3.86 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:23.725954: step 770, loss = 4.02 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-04 21:50:24.903845: step 780, loss = 3.69 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:26.074939: step 790, loss = 3.82 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:27.258349: step 800, loss = 3.64 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:28.443316: step 810, loss = 3.70 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:29.611015: step 820, loss = 3.83 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:30.785161: step 830, loss = 3.79 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:31.953574: step 840, loss = 3.81 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:33.128839: step 850, loss = 3.63 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:34.287341: step 860, loss = 3.66 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-04 21:50:35.478899: step 870, loss = 3.73 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:36.641831: step 880, loss = 4.00 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 21:50:37.827978: step 890, loss = 3.79 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:39.009342: step 900, loss = 3.63 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:40.192301: step 910, loss = 3.70 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:41.358581: step 920, loss = 3.51 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:42.535034: step 930, loss = 3.70 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:43.702773: step 940, loss = 3.47 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:44.878963: step 950, loss = 3.49 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:46.036558: step 960, loss = 3.78 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-04 21:50:47.231119: step 970, loss = 3.63 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:48.400368: step 980, loss = 3.67 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:49.705406: step 990, loss = 3.53 (980.8 examples/sec; 0.131 sec/batch)
2017-05-04 21:50:50.768042: step 1000, loss = 3.50 (1204.6 examples/sec; 0.106 sec/batch)
2017-05-04 21:50:51.924315: step 1010, loss = 3.61 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-04 21:50:53.106640: step 1020, loss = 3.69 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:54.278739: step 1030, loss = 3.60 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:55.452109: step 1040, loss = 3.72 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:56.645714: step 1050, loss = 3.58 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:57.806296: step 1060, loss = 3.76 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-04 21:50:58.977956: step 1070, loss = 3.75 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:00.154389: step 1080, loss = 3.47 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:01.324529: step 1090, loss = 3.51 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:02.495172: step 1100, loss = 3.55 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:03.681891: step 1110, loss = 3.63 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-04 21:51:04.873397: step 1120, loss = 3.51 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-04 21:51:06.055302: step 1130, loss = 3.63 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:07.246140: step 1140, loss = 3.61 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:51:08.410084: step 1150, loss = 3.61 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-04 21:51:09.579523: step 1160, loss = 3.45 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:10.754574: step 1170, loss = 3.58 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:11.916276: step 1180, loss = 3.45 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-04 21:51:13.113282: step 1190, loss = 3.53 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-04 21:51:14.281948: step 1200, loss = 3.44 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:15.461807: step 1210, loss = 3.44 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:16.667047: step 1220, loss = 3.62 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-04 21:51:17.831342: step 1230, loss = 3.44 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-04 21:51:19.013517: step 1240, loss = 3.58 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:20.192947: step 1250, loss = 3.46 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:21.373353: step 1260, loss = 3.63 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:22.545923: step 1270, loss = 3.53 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:23.703936: step 1280, loss = 3.45 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-04 21:51:24.881872: step 1290, loss = 3.77 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:26.044732: step 1300, loss = 3.43 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 21:51:27.212904: step 1310, loss = 3.48 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:28.389427: step 1320, loss = 3.42 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:29.567808: step 1330, loss = 3.50 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:30.761975: step 1340, loss = 3.33 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:51:31.952815: step 1350, loss = 3.37 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:51:33.128339: step 1360, loss = 3.52 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:34.302422: step 1370, loss = 3.34 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:35.475434: step 1380, loss = 3.58 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:36.631076: step 1390, loss = 3.44 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-04 21:51:37.784957: step 1400, loss = 3.41 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-04 21:51:38.973053: step 1410, loss = 3.43 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-04 21:51:40.175881: step 1420, loss = 3.34 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-04 21:51:41.346709: step 1430, loss = 3.36 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:42.483699: step 1440, loss = 3.48 (1125.8 examples/sec; 0.114 sec/batch)
2017-05-04 21:51:43.666150: step 1450, loss = 3.55 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:44.837869: step 1460, loss = 3.51 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:46.011015: step 1470, loss = 3.42 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:47.213391: step 1480, loss = 3.39 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-04 21:51:48.400496: step 1490, loss = 3.45 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-04 21:51:49.565388: step 1500, loss = 3.23 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-04 21:51:50.723585: step 1510, loss = 3.33 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-04 21:51:51.899554: step 1520, loss = 3.53 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:53.085920: step 1530, loss = 3.36 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:51:54.253989: step 1540, loss = 3.17 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:55.425220: step 1550, loss = 3.56 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:56.596490: step 1560, loss = 3.36 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:57.758039: step 1570, loss = 3.23 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-04 21:51:58.948624: step 1580, loss = 3.58 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:00.126433: step 1590, loss = 3.27 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:01.291081: step 1600, loss = 3.15 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-04 21:52:02.460471: step 1610, loss = 3.32 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:03.649652: step 1620, loss = 3.26 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:04.829963: step 1630, loss = 3.22 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:05.999745: step 1640, loss = 3.34 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:07.178688: step 1650, loss = 3.32 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:08.338767: step 1660, loss = 3.15 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-04 21:52:09.512493: step 1670, loss = 3.40 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:10.689154: step 1680, loss = 3.31 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:11.867461: step 1690, loss = 3.25 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:13.031830: step 1700, loss = 3.40 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-04 21:52:14.216258: step 1710, loss = 3.47 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:15.390089: step 1720, loss = 3.22 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:16.563991: step 1730, loss = 3.19 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:17.722298: step 1740, loss = 3.42 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-04 21:52:18.912530: step 1750, loss = 3.43 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:20.102403: step 1760, loss = 3.29 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:21.284709: step 1770, loss = 3.17 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:22.460835: step 1780, loss = 3.33 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:23.629501: step 1790, loss = 3.36 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:24.835190: step 1800, loss = 3.32 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-04 21:52:26.025854: step 1810, loss = 3.29 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:27.203185: step 1820, loss = 3.36 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:28.385092: step 1830, loss = 3.29 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:29.579922: step 1840, loss = 3.20 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:30.769199: step 1850, loss = 3.05 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:31.957794: step 1860, loss = 3.05 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:33.172674: step 1870, loss = 3.19 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 21:52:34.361584: step 1880, loss = 3.41 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:35.561196: step 1890, loss = 3.15 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-04 21:52:36.760189: step 1900, loss = 3.06 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-04 21:52:37.942768: step 1910, loss = 3.28 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:39.128288: step 1920, loss = 3.27 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:40.324354: step 1930, loss = 3.41 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-04 21:52:41.491047: step 1940, loss = 3.14 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:42.661546: step 1950, loss = 3.12 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:43.843076: step 1960, loss = 3.16 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:45.041318: step 1970, loss = 3.24 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-04 21:52:46.344787: step 1980, loss = 3.22 (982.0 examples/sec; 0.130 sec/batch)
2017-05-04 21:52:47.403419: step 1990, loss = 2.97 (1209.1 examples/sec; 0.106 sec/batch)
2017-05-04 21:52:48.582295: step 2000, loss = 3.06 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:49.770022: step 2010, loss = 3.25 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:50.944639: step 2020, loss = 3.13 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:52.134825: step 2030, loss = 3.34 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:53.322288: step 2040, loss = 3.13 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:54.509922: step 2050, loss = 3.09 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:55.695060: step 2060, loss = 3.11 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:56.887591: step 2070, loss = 3.20 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:58.064938: step 2080, loss = 2.98 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:59.239160: step 2090, loss = 3.35 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:00.406693: step 2100, loss = 3.07 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:01.578665: step 2110, loss = 3.26 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:02.779189: step 2120, loss = 3.12 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-04 21:53:03.948545: step 2130, loss = 3.04 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:05.125572: step 2140, loss = 3.08 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:06.298892: step 2150, loss = 3.04 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:07.498078: step 2160, loss = 3.21 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:53:08.670645: step 2170, loss = 3.08 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:09.874220: step 2180, loss = 3.02 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 21:53:11.011960: step 2190, loss = 3.15 (1125.0 examples/sec; 0.114 sec/batch)
2017-05-04 21:53:12.196638: step 2200, loss = 3.22 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:13.364595: step 2210, loss = 2.95 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:14.538599: step 2220, loss = 3.20 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:15.730060: step 2230, loss = 2.94 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:16.922936: step 2240, loss = 3.21 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:18.098324: step 2250, loss = 3.10 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:19.271843: step 2260, loss = 2.98 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:20.461635: step 2270, loss = 3.11 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:21.625297: step 2280, loss = 3.12 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 21:53:22.825311: step 2290, loss = 2.97 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-04 21:53:24.017106: step 2300, loss = 3.37 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:25.211353: step 2310, loss = 3.06 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:26.353131: step 2320, loss = 3.23 (1121.1 examples/sec; 0.114 sec/batch)
2017-05-04 21:53:27.539605: step 2330, loss = 2.98 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:28.700817: step 2340, loss = 3.04 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-04 21:53:29.867694: step 2350, loss = 3.30 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:31.066188: step 2360, loss = 2.95 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-04 21:53:32.230475: step 2370, loss = 3.16 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-04 21:53:33.396271: step 2380, loss = 2.91 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:34.579658: step 2390, loss = 3.00 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:35.763275: step 2400, loss = 3.00 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:36.950395: step 2410, loss = 3.03 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:38.120032: step 2420, loss = 3.10 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:39.306968: step 2430, loss = 2.94 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:40.493155: step 2440, loss = 3.01 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:41.637643: step 2450, loss = 2.92 (1118.4 examples/sec; 0.114 sec/batch)
2017-05-04 21:53:42.822172: step 2460, loss = 3.09 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:44.022678: step 2470, loss = 2.78 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-04 21:53:45.198389: step 2480, loss = 3.03 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:46.384326: step 2490, loss = 2.96 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:47.571676: step 2500, loss = 2.82 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:48.751687: step 2510, loss = 3.40 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:49.919337: step 2520, loss = 2.92 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:51.088455: step 2530, loss = 3.03 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:52.259034: step 2540, loss = 3.01 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:53.439602: step 2550, loss = 3.21 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:54.618508: step 2560, loss = 3.01 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:55.802556: step 2570, loss = 2.96 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:56.977998: step 2580, loss = 3.02 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:58.180580: step 2590, loss = 3.09 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:53:59.371478: step 2600, loss = 2.99 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:54:00.572401: step 2610, loss = 2.95 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-04 21:54:01.781011: step 2620, loss = 3.07 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:54:02.977000: step 2630, loss = 2.87 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-04 21:54:04.183068: step 2640, loss = 3.11 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-04 21:54:05.402388: step 2650, loss = 3.04 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-04 21:54:06.587949: step 2660, loss = 2.91 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 21:54:07.789467: step 2670, loss = 3.09 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-04 21:54:08.977667: step 2680, loss = 2.86 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-04 21:54:10.166226: step 2690, loss = 3.06 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:54:11.356503: step 2700, loss = 2.78 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-04 21:54:12.541164: step 2710, loss = 3.08 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:54:13.697802: step 2720, loss = 2.88 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-04 21:54:14.880308: step 2730, loss = 2.82 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-04 21:54:16.064600: step 2740, loss = 2.89 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:54:17.246941: step 2750, loss = 3.07 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:54:18.432754: step 2760, loss = 2.82 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 21:54:19.630514: step 2770, loss = 2.80 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-04 21:54:20.830645: step 2780, loss = 3.21 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 21:54:22.056295: step 2790, loss = 2.89 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:23.263447: step 2800, loss = 2.94 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-04 21:54:24.461352: step 2810, loss = 2.78 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-04 21:54:25.661758: step 2820, loss = 2.95 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-04 21:54:26.874849: step 2830, loss = 3.03 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:54:28.093670: step 2840, loss = 2.87 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-04 21:54:29.335259: step 2850, loss = 2.83 (1030.9 examples/sec; 0.124 sec/batch)
2017-05-04 21:54:30.544375: step 2860, loss = 2.88 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-04 21:54:31.782837: step 2870, loss = 2.76 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-04 21:54:33.002001: step 2880, loss = 2.84 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 21:54:34.226664: step 2890, loss = 2.83 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 21:54:35.455683: step 2900, loss = 2.79 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:36.704589: step 2910, loss = 2.99 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-04 21:54:37.902013: step 2920, loss = 2.93 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-04 21:54:39.127738: step 2930, loss = 2.74 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:40.360519: step 2940, loss = 2.87 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:41.590450: step 2950, loss = 2.91 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:42.824046: step 2960, loss = 2.68 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:44.198879: step 2970, loss = 2.91 (931.0 examples/sec; 0.137 sec/batch)
2017-05-04 21:54:45.283076: step 2980, loss = 3.01 (1180.6 examples/sec; 0.108 sec/batch)
2017-05-04 21:54:46.511535: step 2990, loss = 2.76 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:47.745537: step 3000, loss = 2.75 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:48.986492: step 3010, loss = 2.90 (1031.5 examples/sec; 0.124 sec/batch)
2017-05-04 21:54:50.233223: step 3020, loss = 2.80 (1026.7 examples/sec; 0.125 sec/batch)
2017-05-04 21:54:51.444722: step 3030, loss = 2.80 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-04 21:54:52.672257: step 3040, loss = 3.01 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:53.913309: step 3050, loss = 2.75 (1031.4 examples/sec; 0.124 sec/batch)
2017-05-04 21:54:55.138212: step 3060, loss = 2.85 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 21:54:56.395124: step 3070, loss = 2.83 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-04 21:54:57.612145: step 3080, loss = 2.76 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 21:54:58.852776: step 3090, loss = 2.85 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-04 21:55:00.073757: step 3100, loss = 2.76 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:01.298997: step 3110, loss = 3.02 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-04 21:55:02.542857: step 3120, loss = 2.94 (1029.1 examples/sec; 0.124 sec/batch)
2017-05-04 21:55:03.790565: step 3130, loss = 3.14 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-04 21:55:04.999131: step 3140, loss = 2.90 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:06.253276: step 3150, loss = 2.85 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-04 21:55:07.489568: step 3160, loss = 2.84 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-04 21:55:08.846159: step 3170, loss = 2.78 (943.5 examples/sec; 0.136 sec/batch)
2017-05-04 21:55:09.925526: step 3180, loss = 2.73 (1185.9 examples/sec; 0.108 sec/batch)
2017-05-04 21:55:11.137029: step 3190, loss = 2.68 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:12.345217: step 3200, loss = 2.97 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:13.540973: step 3210, loss = 2.67 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:14.747551: step 3220, loss = 3.05 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:15.962047: step 3230, loss = 2.98 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:17.155611: step 3240, loss = 2.78 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-04 21:55:18.368917: step 3250, loss = 2.78 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:19.580666: step 3260, loss = 2.88 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:20.808097: step 3270, loss = 2.82 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-04 21:55:22.005815: step 3280, loss = 2.74 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:23.221361: step 3290, loss = 2.70 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:24.431753: step 3300, loss = 3.23 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:25.639453: step 3310, loss = 2.82 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:26.847070: step 3320, loss = 2.88 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:28.057897: step 3330, loss = 2.52 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:29.251903: step 3340, loss = 3.00 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:55:30.426900: step 3350, loss = 2.70 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:55:31.627060: step 3360, loss = 2.91 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:32.829171: step 3370, loss = 2.79 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:34.042942: step 3380, loss = 2.89 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:35.273412: step 3390, loss = 2.94 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-04 21:55:36.488340: step 3400, loss = 2.76 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:37.702012: step 3410, loss = 2.65 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:38.890529: step 3420, loss = 2.74 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:55:40.072577: step 3430, loss = 2.74 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:55:41.272531: step 3440, loss = 2.88 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:42.473499: step 3450, loss = 2.96 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:43.647275: step 3460, loss = 2.70 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 21:55:44.845055: step 3470, loss = 2.82 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:46.031543: step 3480, loss = 2.80 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:55:47.232898: step 3490, loss = 2.72 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:48.441857: step 3500, loss = 2.89 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:49.645322: step 3510, loss = 2.72 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:50.868405: step 3520, loss = 2.88 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:52.109196: step 3530, loss = 2.67 (1031.6 examples/sec; 0.124 sec/batch)
2017-05-04 21:55:53.341042: step 3540, loss = 2.75 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-04 21:55:54.558780: step 3550, loss = 2.77 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:55.782429: step 3560, loss = 2.93 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:56.987436: step 3570, loss = 2.90 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:58.207872: step 3580, loss = 2.69 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:59.437187: step 3590, loss = 2.69 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:00.652366: step 3600, loss = 2.73 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:01.876417: step 3610, loss = 2.81 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:03.122098: step 3620, loss = 2.82 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-04 21:56:04.338725: step 3630, loss = 3.04 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:05.573516: step 3640, loss = 2.76 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:06.813604: step 3650, loss = 2.85 (1032.2 examples/sec; 0.124 sec/batch)
2017-05-04 21:56:08.045673: step 3660, loss = 2.77 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:09.269347: step 3670, loss = 2.75 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:10.495858: step 3680, loss = 2.71 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:11.739566: step 3690, loss = 2.77 (1029.2 examples/sec; 0.124 sec/batch)
2017-05-04 21:56:12.960758: step 3700, loss = 2.83 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:14.197101: step 3710, loss = 2.69 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-04 21:56:15.428695: step 3720, loss = 2.73 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:16.651338: step 3730, loss = 2.84 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:17.874929: step 3740, loss = 2.63 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:19.106021: step 3750, loss = 2.75 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:20.339212: step 3760, loss = 2.82 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:21.531489: step 3770, loss = 2.85 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-04 21:56:22.775507: step 3780, loss = 2.68 (1028.9 examples/sec; 0.124 sec/batch)
2017-05-04 21:56:24.001935: step 3790, loss = 2.86 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:25.229193: step 3800, loss = 2.69 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:26.439074: step 3810, loss = 2.77 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 21:56:27.669375: step 3820, loss = 2.84 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:28.903220: step 3830, loss = 2.82 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:30.113749: step 3840, loss = 2.64 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 21:56:31.335202: step 3850, loss = 2.63 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:32.558268: step 3860, loss = 2.69 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:33.753679: step 3870, loss = 2.81 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-04 21:56:34.947033: step 3880, loss = 2.69 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-04 21:56:36.154433: step 3890, loss = 2.73 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:56:37.369775: step 3900, loss = 2.56 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:38.574986: step 3910, loss = 2.80 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:56:39.772931: step 3920, loss = 2.70 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-04 21:56:40.989942: step 3930, loss = 2.62 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:42.187441: step 3940, loss = 2.62 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-04 21:56:43.403015: step 3950, loss = 2.59 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:44.731494: step 3960, loss = 2.63 (963.5 examples/sec; 0.133 sec/batch)
2017-05-04 21:56:45.813457: step 3970, loss = 2.50 (1183.0 examples/sec; 0.108 sec/batch)
2017-05-04 21:56:47.022170: step 3980, loss = 2.71 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-04 21:56:48.227182: step 3990, loss = 2.59 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-04 21:56:49.438860: step 4000, loss = 2.63 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-04 21:56:50.636194: step 4010, loss = 2.58 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-04 21:56:51.854064: step 4020, loss = 2.65 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:53.047964: step 4030, loss = 2.75 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-04 21:56:54.244104: step 4040, loss = 2.74 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-04 21:56:55.422078: step 4050, loss = 2.58 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:56:56.589952: step 4060, loss = 2.70 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 21:56:57.787311: step 4070, loss = 2.63 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-04 21:56:58.974048: step 4080, loss = 2.64 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:00.159081: step 4090, loss = 2.72 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:01.351315: step 4100, loss = 2.90 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:02.535482: step 4110, loss = 2.63 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:03.705126: step 4120, loss = 2.85 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 21:57:04.874868: step 4130, loss = 2.67 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 21:57:06.035946: step 4140, loss = 2.65 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-04 21:57:07.215881: step 4150, loss = 2.68 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:08.398812: step 4160, loss = 2.52 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:09.571552: step 4170, loss = 2.55 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-04 21:57:10.742218: step 4180, loss = 2.83 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:57:11.919044: step 4190, loss = 2.86 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:13.102312: step 4200, loss = 2.59 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:14.287969: step 4210, loss = 2.59 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:15.466215: step 4220, loss = 2.74 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:16.633431: step 4230, loss = 2.63 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-04 21:57:17.787863: step 4240, loss = 2.67 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-04 21:57:18.970824: step 4250, loss = 2.76 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:20.175388: step 4260, loss = 2.68 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 21:57:21.360153: step 4270, loss = 2.79 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:22.533583: step 4280, loss = 2.86 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 21:57:23.737263: step 4290, loss = 2.52 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:57:24.923665: step 4300, loss = 2.54 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:26.117631: step 4310, loss = 2.82 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:27.312253: step 4320, loss = 2.56 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:28.491629: step 4330, loss = 2.61 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:29.686218: step 4340, loss = 2.78 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:30.888280: step 4350, loss = 2.67 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-04 21:57:32.062797: step 4360, loss = 2.51 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 21:57:33.257949: step 4370, loss = 2.76 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-04 21:57:34.456049: step 4380, loss = 2.52 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:57:35.654544: step 4390, loss = 2.68 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-04 21:57:36.875912: step 4400, loss = 2.67 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-04 21:57:38.077062: step 4410, loss = 2.76 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 21:57:39.290406: step 4420, loss = 2.61 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 21:57:40.497713: step 4430, loss = 2.38 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-04 21:57:41.707367: step 4440, loss = 2.60 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 21:57:42.922889: step 4450, loss = 2.74 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 21:57:44.151624: step 4460, loss = 2.63 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-04 21:57:45.374985: step 4470, loss = 2.79 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 21:57:46.584042: step 4480, loss = 2.85 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 21:57:47.785701: step 4490, loss = 2.61 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 21:57:48.992684: step 4500, loss = 2.51 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-04 21:57:50.188417: step 4510, loss = 2.69 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-04 21:57:51.398845: step 4520, loss = 2.70 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-04 21:57:52.591832: step 4530, loss = 2.87 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:53.801525: step 4540, loss = 2.70 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:57:55.032198: step 4550, loss = 2.55 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 21:57:56.220512: step 4560, loss = 2.55 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:57.426200: step 4570, loss = 2.68 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-04 21:57:58.645855: step 4580, loss = 2.67 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 21:57:59.857863: step 4590, loss = 2.59 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:58:01.086282: step 4600, loss = 2.49 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-04 21:58:02.334185: step 4610, loss = 2.45 (1025.7 examples/sec; 0.125 sec/batch)
2017-05-04 21:58:03.559300: step 4620, loss = 2.60 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 21:58:04.776265: step 4630, loss = 2.53 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 21:58:05.972464: step 4640, loss = 2.51 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-04 21:58:07.196894: step 4650, loss = 2.71 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-04 21:58:08.424100: step 4660, loss = 2.56 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 21:58:09.655019: step 4670, loss = 2.58 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-04 21:58:10.865477: step 4680, loss = 2.61 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-04 21:58:12.073264: step 4690, loss = 2.73 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 21:58:13.304053: step 4700, loss = 2.43 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-04 21:58:14.536698: step 4710, loss = 2.49 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-04 21:58:15.757417: step 4720, loss = 2.53 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 21:58:17.012869: step 4730, loss = 2.76 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-04 21:58:18.231273: step 4740, loss = 2.69 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 21:58:19.492705: step 4750, loss = 2.44 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-04 21:58:20.695298: step 4760, loss = 2.40 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:58:21.912455: step 4770, loss = 2.45 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 21:58:23.126939: step 4780, loss = 2.67 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 21:58:24.341928: step 4790, loss = 2.48 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-04 21:58:25.547701: step 4800, loss = 2.62 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-04 21:58:26.734476: step 4810, loss = 2.32 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-04 21:58:27.936911: step 4820, loss = 2.50 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-04 21:58:29.145570: step 4830, loss = 2.59 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-04 21:58:30.336104: step 4840, loss = 2.51 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-04 21:58:31.507930: step 4850, loss = 2.85 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 21:58:32.697599: step 4860, loss = 2.52 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:58:33.882196: step 4870, loss = 2.50 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:58:35.097384: step 4880, loss = 2.50 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 21:58:36.294956: step 4890, loss = 2.40 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-04 21:58:37.502545: step 4900, loss = 2.56 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 21:58:38.711680: step 4910, loss = 2.67 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-04 21:58:39.908782: step 4920, loss = 2.49 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-04 21:58:41.103655: step 4930, loss = 2.49 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-04 21:58:42.280824: step 4940, loss = 2.68 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-04 21:58:43.591404: step 4950, loss = 2.59 (976.7 examples/sec; 0.131 sec/batch)
2017-05-04 21:58:44.670448: step 4960, loss = 2.51 (1186.2 examples/sec; 0.108 sec/batch)
2017-05-04 21:58:45.820324: step 4970, loss = 2.53 (1113.2 examples/sec; 0.115 sec/batch)
2017-05-04 21:58:47.004415: step 4980, loss = 2.55 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-04 21:58:48.198688: step 4990, loss = 2.51 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:58:49.359987: step 5000, loss = 2.58 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-04 21:58:50.544405: step 5010, loss = 2.41 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:58:51.749804: step 5020, loss = 2.50 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-04 21:58:53.016079: step 5030, loss = 2.49 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-04 21:58:54.193724: step 5040, loss = 2.58 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:58:55.414429: step 5050, loss = 2.56 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 21:58:56.641577: step 5060, loss = 2.41 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 21:58:57.864596: step 5070, loss = 2.48 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 21:58:59.090893: step 5080, loss = 2.70 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 21:59:00.332918: step 5090, loss = 2.44 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-04 21:59:01.562153: step 5100, loss = 2.61 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 21:59:02.787850: step 5110, loss = 2.65 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 21:59:04.030209: step 5120, loss = 2.67 (1030.3 examples/sec; 0.124 sec/batch)
2017-05-04 21:59:05.263996: step 5130, loss = 2.47 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-04 21:59:06.486541: step 5140, loss = 2.61 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:07.696691: step 5150, loss = 2.50 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:08.957375: step 5160, loss = 2.43 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-04 21:59:10.174406: step 5170, loss = 2.49 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:11.401285: step 5180, loss = 2.49 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-04 21:59:12.620279: step 5190, loss = 2.48 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:13.822336: step 5200, loss = 2.38 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-04 21:59:15.064273: step 5210, loss = 2.43 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-04 21:59:16.299786: step 5220, loss = 2.59 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-04 21:59:17.519167: step 5230, loss = 2.61 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:18.756391: step 5240, loss = 2.68 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-04 21:59:19.984520: step 5250, loss = 2.75 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 21:59:21.208346: step 5260, loss = 2.61 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:22.428905: step 5270, loss = 2.68 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:23.650415: step 5280, loss = 2.48 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:24.872661: step 5290, loss = 2.52 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:26.087959: step 5300, loss = 2.68 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:27.324610: step 5310, loss = 2.40 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-04 21:59:28.560550: step 5320, loss = 2.55 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-04 21:59:29.777265: step 5330, loss = 2.47 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:31.006789: step 5340, loss = 2.56 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-04 21:59:32.192075: step 5350, loss = 2.52 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:59:33.402181: step 5360, loss = 2.53 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:34.631438: step 5370, loss = 2.54 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 21:59:35.854904: step 5380, loss = 2.54 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:37.060321: step 5390, loss = 2.50 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:38.251178: step 5400, loss = 2.45 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:59:39.480414: step 5410, loss = 2.44 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 21:59:40.688965: step 5420, loss = 2.46 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:41.892700: step 5430, loss = 2.48 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:59:43.104585: step 5440, loss = 2.55 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:44.308652: step 5450, loss = 2.35 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 21:59:45.537139: step 5460, loss = 2.50 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 21:59:46.754186: step 5470, loss = 2.53 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:47.970476: step 5480, loss = 2.45 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:49.189594: step 5490, loss = 2.41 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:50.389746: step 5500, loss = 2.49 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 21:59:51.618907: step 5510, loss = 2.45 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-04 21:59:52.848428: step 5520, loss = 2.61 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-04 21:59:54.064477: step 5530, loss = 2.42 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:55.289091: step 5540, loss = 2.41 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:56.507790: step 5550, loss = 2.46 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:57.740335: step 5560, loss = 2.84 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-04 21:59:58.978957: step 5570, loss = 2.45 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:00.226946: step 5580, loss = 2.63 (1025.7 examples/sec; 0.125 sec/batch)
2017-05-04 22:00:01.469926: step 5590, loss = 2.53 (1029.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:02.701438: step 5600, loss = 2.38 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:03.936792: step 5610, loss = 2.56 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:05.167488: step 5620, loss = 2.28 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:06.413744: step 5630, loss = 2.33 (1027.1 examples/sec; 0.125 sec/batch)
2017-05-04 22:00:07.644802: step 5640, loss = 2.37 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:08.883491: step 5650, loss = 2.45 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:10.100330: step 5660, loss = 2.46 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:00:11.332583: step 5670, loss = 2.64 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:12.574058: step 5680, loss = 2.46 (1031.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:13.788566: step 5690, loss = 2.51 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:00:15.026429: step 5700, loss = 2.38 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:16.250714: step 5710, loss = 2.45 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:00:17.493829: step 5720, loss = 2.34 (1029.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:18.726863: step 5730, loss = 2.46 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:19.964512: step 5740, loss = 2.60 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:21.160391: step 5750, loss = 2.67 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:00:22.372920: step 5760, loss = 2.41 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:00:23.601902: step 5770, loss = 2.55 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:24.842224: step 5780, loss = 2.69 (1032.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:26.075944: step 5790, loss = 2.35 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:27.303384: step 5800, loss = 2.66 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:28.538182: step 5810, loss = 2.34 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:29.775176: step 5820, loss = 2.56 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:31.007789: step 5830, loss = 2.42 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:32.220516: step 5840, loss = 2.58 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:00:33.447191: step 5850, loss = 2.57 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:34.674048: step 5860, loss = 2.47 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:35.912170: step 5870, loss = 2.30 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:37.159818: step 5880, loss = 2.47 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-04 22:00:38.361507: step 5890, loss = 2.34 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:00:39.582131: step 5900, loss = 2.51 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:00:40.810454: step 5910, loss = 2.59 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:42.027523: step 5920, loss = 2.47 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:00:43.269873: step 5930, loss = 2.48 (1030.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:44.602991: step 5940, loss = 2.55 (960.1 examples/sec; 0.133 sec/batch)
2017-05-04 22:00:45.703669: step 5950, loss = 3.24 (1162.9 examples/sec; 0.110 sec/batch)
2017-05-04 22:00:46.941617: step 5960, loss = 2.43 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:48.176657: step 5970, loss = 2.35 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:49.406910: step 5980, loss = 2.39 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:50.619158: step 5990, loss = 2.47 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:00:51.840947: step 6000, loss = 2.46 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:00:53.083939: step 6010, loss = 2.44 (1029.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:54.310522: step 6020, loss = 2.48 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:55.544379: step 6030, loss = 2.61 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:56.781484: step 6040, loss = 2.34 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:58.031857: step 6050, loss = 2.36 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-04 22:00:59.267463: step 6060, loss = 2.44 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:00.482391: step 6070, loss = 2.44 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:01:01.713687: step 6080, loss = 2.29 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:02.946509: step 6090, loss = 2.51 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:04.184447: step 6100, loss = 2.51 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:05.400681: step 6110, loss = 2.44 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:06.628861: step 6120, loss = 2.30 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:07.877415: step 6130, loss = 2.39 (1025.2 examples/sec; 0.125 sec/batch)
2017-05-04 22:01:09.068817: step 6140, loss = 2.40 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:01:10.297896: step 6150, loss = 2.42 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:11.531362: step 6160, loss = 2.44 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:12.781954: step 6170, loss = 2.33 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-04 22:01:13.997344: step 6180, loss = 2.43 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:15.235344: step 6190, loss = 2.50 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:16.468980: step 6200, loss = 2.42 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:17.701503: step 6210, loss = 2.79 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:18.940420: step 6220, loss = 2.45 (1033.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:20.165983: step 6230, loss = 2.43 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:21.398636: step 6240, loss = 2.25 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:22.640701: step 6250, loss = 2.43 (1030.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:23.850730: step 6260, loss = 2.36 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:01:25.079804: step 6270, loss = 2.52 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:26.306363: step 6280, loss = 2.38 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:27.523534: step 6290, loss = 2.39 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:28.752375: step 6300, loss = 2.36 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:29.964631: step 6310, loss = 2.55 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:01:31.196013: step 6320, loss = 2.41 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:32.414539: step 6330, loss = 2.30 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:33.619988: step 6340, loss = 2.40 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:01:34.859178: step 6350, loss = 2.32 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:36.097030: step 6360, loss = 2.36 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:37.327786: step 6370, loss = 2.63 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:38.555059: step 6380, loss = 2.46 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:39.777614: step 6390, loss = 2.62 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:41.003042: step 6400, loss = 2.37 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:42.216867: step 6410, loss = 2.48 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:01:43.434756: step 6420, loss = 2.68 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:44.658521: step 6430, loss = 2.36 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:45.878898: step 6440, loss = 2.31 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:47.103043: step 6450, loss = 2.50 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:48.325532: step 6460, loss = 2.59 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:49.550513: step 6470, loss = 2.24 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:50.781677: step 6480, loss = 2.29 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:52.018461: step 6490, loss = 2.37 (1034.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:53.263388: step 6500, loss = 2.29 (1028.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:54.462509: step 6510, loss = 2.37 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:01:55.696719: step 6520, loss = 2.29 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:56.938342: step 6530, loss = 2.27 (1030.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:58.124109: step 6540, loss = 2.27 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:01:59.363050: step 6550, loss = 2.37 (1033.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:00.584769: step 6560, loss = 2.44 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:01.816833: step 6570, loss = 2.41 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:03.058438: step 6580, loss = 2.43 (1030.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:04.284526: step 6590, loss = 2.42 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:05.489029: step 6600, loss = 2.53 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:02:06.722312: step 6610, loss = 2.36 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:07.938820: step 6620, loss = 2.36 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:09.200000: step 6630, loss = 2.38 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-04 22:02:10.425946: step 6640, loss = 2.31 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:11.645186: step 6650, loss = 2.35 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:12.896507: step 6660, loss = 2.32 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-04 22:02:14.111475: step 6670, loss = 2.39 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:02:15.334735: step 6680, loss = 2.52 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:16.556528: step 6690, loss = 2.43 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:17.780511: step 6700, loss = 2.38 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:19.017453: step 6710, loss = 2.43 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:20.257880: step 6720, loss = 2.56 (1031.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:21.477104: step 6730, loss = 2.29 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:22.655138: step 6740, loss = 2.45 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:02:23.864210: step 6750, loss = 2.42 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:02:25.088203: step 6760, loss = 2.37 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:26.298873: step 6770, loss = 2.57 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:02:27.501979: step 6780, loss = 2.25 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:02:28.717750: step 6790, loss = 2.23 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:29.927453: step 6800, loss = 2.65 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:02:31.157999: step 6810, loss = 2.50 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:32.376733: step 6820, loss = 2.20 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:33.606921: step 6830, loss = 2.27 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:34.837250: step 6840, loss = 2.37 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:36.068415: step 6850, loss = 2.48 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:37.320574: step 6860, loss = 2.33 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-04 22:02:38.531516: step 6870, loss = 2.35 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:02:39.755665: step 6880, loss = 2.28 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:40.990811: step 6890, loss = 2.44 (1036.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:42.216758: step 6900, loss = 2.31 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:43.454188: step 6910, loss = 2.45 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:44.689729: step 6920, loss = 2.22 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:46.004536: step 6930, loss = 2.39 (973.5 examples/sec; 0.131 sec/batch)
2017-05-04 22:02:47.128138: step 6940, loss = 2.43 (1139.2 examples/sec; 0.112 sec/batch)
2017-05-04 22:02:48.380072: step 6950, loss = 2.34 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-04 22:02:49.617173: step 6960, loss = 2.20 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:50.845322: step 6970, loss = 2.02 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:52.091121: step 6980, loss = 2.39 (1027.5 examples/sec; 0.125 sec/batch)
2017-05-04 22:02:53.327199: step 6990, loss = 2.27 (1035.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:54.550873: step 7000, loss = 2.52 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:55.771167: step 7010, loss = 2.50 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:56.988356: step 7020, loss = 2.40 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:58.226033: step 7030, loss = 2.44 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:59.453055: step 7040, loss = 2.34 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:00.686989: step 7050, loss = 2.46 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:01.913736: step 7060, loss = 2.33 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:03.133534: step 7070, loss = 2.35 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:04.361213: step 7080, loss = 2.34 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:05.576948: step 7090, loss = 2.51 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:06.800757: step 7100, loss = 2.36 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:08.038051: step 7110, loss = 2.51 (1034.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:09.257070: step 7120, loss = 2.39 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:10.443344: step 7130, loss = 2.40 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:03:11.652258: step 7140, loss = 2.29 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:03:12.896170: step 7150, loss = 2.21 (1029.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:14.119385: step 7160, loss = 2.33 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:15.334820: step 7170, loss = 2.29 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:16.547211: step 7180, loss = 2.42 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:03:17.797597: step 7190, loss = 2.46 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-04 22:03:19.025679: step 7200, loss = 2.36 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:20.253790: step 7210, loss = 2.42 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:21.485986: step 7220, loss = 2.27 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:22.710094: step 7230, loss = 2.14 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:23.938282: step 7240, loss = 2.47 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:25.175383: step 7250, loss = 2.31 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:26.386271: step 7260, loss = 2.24 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:03:27.613291: step 7270, loss = 2.39 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:28.852285: step 7280, loss = 2.44 (1033.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:30.068076: step 7290, loss = 2.46 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:31.318165: step 7300, loss = 2.33 (1023.9 examples/sec; 0.125 sec/batch)
2017-05-04 22:03:32.538171: step 7310, loss = 2.48 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:33.759623: step 7320, loss = 2.28 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:34.956263: step 7330, loss = 2.47 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:03:36.188696: step 7340, loss = 2.44 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:37.409941: step 7350, loss = 2.10 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:38.633334: step 7360, loss = 2.44 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:39.852513: step 7370, loss = 2.39 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:41.090607: step 7380, loss = 2.14 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:42.326649: step 7390, loss = 2.50 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:43.548649: step 7400, loss = 2.51 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:44.772192: step 7410, loss = 2.40 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:46.008437: step 7420, loss = 2.23 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:47.245952: step 7430, loss = 2.21 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:48.480208: step 7440, loss = 2.27 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:49.715880: step 7450, loss = 2.48 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:50.938414: step 7460, loss = 2.37 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:52.167812: step 7470, loss = 2.22 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:53.386474: step 7480, loss = 2.27 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:54.604818: step 7490, loss = 2.46 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:55.826602: step 7500, loss = 2.30 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:57.044094: step 7510, loss = 2.49 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:58.254897: step 7520, loss = 2.56 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:03:59.466046: step 7530, loss = 2.22 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:04:00.695263: step 7540, loss = 2.37 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:01.931019: step 7550, loss = 2.26 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:04:03.172340: step 7560, loss = 2.38 (1031.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:04:04.406286: step 7570, loss = 2.47 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:05.617290: step 7580, loss = 2.36 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:04:06.846878: step 7590, loss = 2.51 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:08.075041: step 7600, loss = 2.52 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:09.298005: step 7610, loss = 2.55 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:10.536132: step 7620, loss = 2.45 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:04:11.768144: step 7630, loss = 2.26 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:13.003462: step 7640, loss = 2.31 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:04:14.222963: step 7650, loss = 2.28 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:15.447673: step 7660, loss = 2.28 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:16.669701: step 7670, loss = 2.43 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:17.884352: step 7680, loss = 2.30 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:04:19.116065: step 7690, loss = 2.27 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:20.348375: step 7700, loss = 2.41 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:21.579390: step 7710, loss = 2.28 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:22.824334: step 7720, loss = 2.57 (1028.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:04:24.026023: step 7730, loss = 2.38 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:04:25.242502: step 7740, loss = 2.26 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:26.475145: step 7750, loss = 2.18 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:27.680710: step 7760, loss = 2.38 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:04:28.917643: step 7770, loss = 2.23 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:04:30.135599: step 7780, loss = 2.28 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:31.371459: step 7790, loss = 2.41 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:04:32.608479: step 7800, loss = 2.39 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:04:33.840992: step 7810, loss = 2.38 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:35.063848: step 7820, loss = 2.11 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:36.289609: step 7830, loss = 2.26 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:37.515442: step 7840, loss = 2.40 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:38.734349: step 7850, loss = 2.40 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:39.958045: step 7860, loss = 2.37 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:41.185816: step 7870, loss = 2.13 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:42.407489: step 7880, loss = 2.26 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:43.641454: step 7890, loss = 2.53 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:44.878546: step 7900, loss = 2.22 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:04:46.093664: step 7910, loss = 2.23 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:47.393547: step 7920, loss = 2.28 (984.7 examples/sec; 0.130 sec/batch)
2017-05-04 22:04:48.503913: step 7930, loss = 2.34 (1152.8 examples/sec; 0.111 sec/batch)
2017-05-04 22:04:49.738513: step 7940, loss = 2.42 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:50.972960: step 7950, loss = 2.32 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:52.202956: step 7960, loss = 2.46 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:53.415653: step 7970, loss = 2.18 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:04:54.635829: step 7980, loss = 2.21 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:55.867777: step 7990, loss = 2.41 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:57.092506: step 8000, loss = 2.37 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:58.314461: step 8010, loss = 2.18 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:59.527855: step 8020, loss = 2.25 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:05:00.756380: step 8030, loss = 2.40 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:02.001451: step 8040, loss = 2.46 (1028.1 examples/sec; 0.125 sec/batch)
2017-05-04 22:05:03.233563: step 8050, loss = 2.24 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:04.474041: step 8060, loss = 2.60 (1031.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:05:05.695999: step 8070, loss = 2.15 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:06.928399: step 8080, loss = 2.16 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:08.161373: step 8090, loss = 2.38 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:09.384861: step 8100, loss = 2.29 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:10.601548: step 8110, loss = 2.26 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:11.794702: step 8120, loss = 2.13 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:05:13.018306: step 8130, loss = 2.31 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:14.255473: step 8140, loss = 2.51 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:05:15.480838: step 8150, loss = 2.33 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:16.716776: step 8160, loss = 2.21 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:05:17.940365: step 8170, loss = 2.39 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:19.163424: step 8180, loss = 2.19 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:20.382146: step 8190, loss = 2.16 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:21.594008: step 8200, loss = 2.26 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:05:22.834328: step 8210, loss = 2.26 (1032.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:05:24.074604: step 8220, loss = 2.18 (1032.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:05:25.298607: step 8230, loss = 2.24 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:26.525529: step 8240, loss = 2.51 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:27.773387: step 8250, loss = 2.32 (1025.8 examples/sec; 0.125 sec/batch)
2017-05-04 22:05:29.007465: step 8260, loss = 2.32 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:30.226507: step 8270, loss = 2.27 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:31.464424: step 8280, loss = 2.34 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:05:32.688618: step 8290, loss = 2.27 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:33.918375: step 8300, loss = 2.25 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:35.140805: step 8310, loss = 2.33 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:36.324086: step 8320, loss = 2.40 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:05:37.524453: step 8330, loss = 2.21 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:05:38.747581: step 8340, loss = 2.18 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:39.974111: step 8350, loss = 2.33 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:41.183140: step 8360, loss = 2.34 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:05:42.367839: step 8370, loss = 2.09 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:05:43.568010: step 8380, loss = 2.42 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:05:44.786562: step 8390, loss = 2.17 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:45.972429: step 8400, loss = 2.33 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:05:47.185547: step 8410, loss = 2.22 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:05:48.416606: step 8420, loss = 2.51 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:49.654166: step 8430, loss = 2.26 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:05:50.893277: step 8440, loss = 2.25 (1033.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:05:52.120964: step 8450, loss = 2.30 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:53.343953: step 8460, loss = 2.29 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:54.577215: step 8470, loss = 2.28 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:55.811889: step 8480, loss = 2.20 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:57.045788: step 8490, loss = 2.35 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:58.258641: step 8500, loss = 2.17 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:05:59.500021: step 8510, loss = 2.31 (1031.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:00.698984: step 8520, loss = 2.24 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:06:01.921342: step 8530, loss = 2.45 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:03.156673: step 8540, loss = 2.39 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:04.397841: step 8550, loss = 2.35 (1031.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:05.623690: step 8560, loss = 2.45 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:06.854022: step 8570, loss = 2.30 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:08.091313: step 8580, loss = 2.14 (1034.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:09.312582: step 8590, loss = 2.26 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:10.535543: step 8600, loss = 2.28 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:11.784117: step 8610, loss = 2.35 (1025.2 examples/sec; 0.125 sec/batch)
2017-05-04 22:06:13.023708: step 8620, loss = 2.37 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:14.245179: step 8630, loss = 2.56 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:15.476376: step 8640, loss = 2.32 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:16.697412: step 8650, loss = 2.30 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:17.906209: step 8660, loss = 2.43 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:06:19.142628: step 8670, loss = 2.21 (1035.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:20.382432: step 8680, loss = 2.14 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:21.602054: step 8690, loss = 2.15 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:22.825720: step 8700, loss = 2.50 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:24.053442: step 8710, loss = 2.27 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:25.254535: step 8720, loss = 2.32 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:06:26.484191: step 8730, loss = 2.40 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:27.716707: step 8740, loss = 2.13 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:28.960105: step 8750, loss = 2.39 (1029.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:30.191861: step 8760, loss = 2.23 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:31.419143: step 8770, loss = 2.26 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:32.674599: step 8780, loss = 2.45 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-04 22:06:33.891299: step 8790, loss = 2.24 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:35.118894: step 8800, loss = 2.52 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:36.353082: step 8810, loss = 2.17 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:37.587305: step 8820, loss = 2.20 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:38.833456: step 8830, loss = 2.26 (1027.2 examples/sec; 0.125 sec/batch)
2017-05-04 22:06:40.055962: step 8840, loss = 2.29 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:41.278895: step 8850, loss = 2.22 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:42.502850: step 8860, loss = 2.33 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:43.730759: step 8870, loss = 2.35 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:44.965816: step 8880, loss = 2.33 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:46.180883: step 8890, loss = 2.39 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:47.408225: step 8900, loss = 2.19 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:48.738936: step 8910, loss = 2.33 (961.9 examples/sec; 0.133 sec/batch)
2017-05-04 22:06:49.825884: step 8920, loss = 2.15 (1177.6 examples/sec; 0.109 sec/batch)
2017-05-04 22:06:51.061108: step 8930, loss = 2.32 (1036.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:52.300164: step 8940, loss = 2.39 (1033.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:53.537961: step 8950, loss = 2.22 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:54.782681: step 8960, loss = 2.61 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:56.022797: step 8970, loss = 2.23 (1032.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:57.255932: step 8980, loss = 2.16 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:58.479325: step 8990, loss = 2.21 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:59.714145: step 9000, loss = 2.29 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:00.926363: step 9010, loss = 2.29 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:07:02.146069: step 9020, loss = 2.20 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:03.378218: step 9030, loss = 2.21 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:04.601020: step 9040, loss = 2.44 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:05.822043: step 9050, loss = 2.57 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:07.042402: step 9060, loss = 2.26 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:08.287041: step 9070, loss = 2.31 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:07:09.504257: step 9080, loss = 2.41 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:10.735013: step 9090, loss = 2.23 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:11.955772: step 9100, loss = 2.15 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:13.147368: step 9110, loss = 2.38 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:07:14.372448: step 9120, loss = 2.29 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:15.594054: step 9130, loss = 2.46 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:16.817805: step 9140, loss = 2.45 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:18.036156: step 9150, loss = 2.19 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:19.287576: step 9160, loss = 2.19 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-04 22:07:20.509504: step 9170, loss = 2.20 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:21.746683: step 9180, loss = 2.12 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:07:22.989198: step 9190, loss = 2.19 (1030.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:07:24.224056: step 9200, loss = 2.40 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:25.455208: step 9210, loss = 2.11 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:26.680098: step 9220, loss = 2.15 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:27.915164: step 9230, loss = 2.26 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:07:29.155078: step 9240, loss = 2.23 (1032.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:07:30.387463: step 9250, loss = 2.29 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:31.601551: step 9260, loss = 2.21 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:07:32.851425: step 9270, loss = 2.28 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-04 22:07:34.071694: step 9280, loss = 2.29 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:35.307876: step 9290, loss = 2.13 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:07:36.537369: step 9300, loss = 2.36 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:37.741746: step 9310, loss = 2.15 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:07:38.947444: step 9320, loss = 2.27 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:07:40.203287: step 9330, loss = 2.38 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-04 22:07:41.441925: step 9340, loss = 2.35 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:07:42.659364: step 9350, loss = 2.28 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:43.880698: step 9360, loss = 2.29 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:45.119965: step 9370, loss = 2.16 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:07:46.344633: step 9380, loss = 2.29 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:47.589165: step 9390, loss = 2.21 (1028.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:07:48.817441: step 9400, loss = 2.23 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:50.034422: step 9410, loss = 2.37 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:51.261107: step 9420, loss = 2.25 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:52.480350: step 9430, loss = 2.26 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:53.691549: step 9440, loss = 2.45 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:07:54.924488: step 9450, loss = 2.32 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:56.161615: step 9460, loss = 2.17 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:07:57.395663: step 9470, loss = 2.19 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:58.609580: step 9480, loss = 2.22 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:07:59.848326: step 9490, loss = 2.37 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:01.061529: step 9500, loss = 2.23 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:08:02.248621: step 9510, loss = 2.14 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:08:03.485729: step 9520, loss = 2.31 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:04.728609: step 9530, loss = 2.51 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:05.959909: step 9540, loss = 2.49 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:07.196398: step 9550, loss = 2.55 (1035.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:08.432388: step 9560, loss = 2.10 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:09.650144: step 9570, loss = 2.29 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:10.887029: step 9580, loss = 2.19 (1034.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:12.117490: step 9590, loss = 2.22 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:13.343835: step 9600, loss = 2.34 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:14.579358: step 9610, loss = 2.18 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:15.799467: step 9620, loss = 2.24 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:17.025157: step 9630, loss = 2.21 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:18.239769: step 9640, loss = 2.13 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:08:19.480324: step 9650, loss = 2.23 (1031.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:20.715619: step 9660, loss = 2.13 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:21.943777: step 9670, loss = 2.26 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:23.161405: step 9680, loss = 2.32 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:24.405906: step 9690, loss = 2.29 (1028.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:25.629065: step 9700, loss = 2.26 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:26.809090: step 9710, loss = 2.18 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:08:28.026804: step 9720, loss = 2.15 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:29.243294: step 9730, loss = 2.11 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:30.457374: step 9740, loss = 2.41 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:08:31.692984: step 9750, loss = 2.24 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:32.923516: step 9760, loss = 2.60 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:34.136483: step 9770, loss = 2.27 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:08:35.334936: step 9780, loss = 2.26 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:08:36.535618: step 9790, loss = 2.36 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:08:37.752946: step 9800, loss = 2.21 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:38.983370: step 9810, loss = 2.24 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:40.237916: step 9820, loss = 2.23 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-04 22:08:41.465990: step 9830, loss = 2.41 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:42.687001: step 9840, loss = 2.26 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:43.901829: step 9850, loss = 2.36 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:08:45.118821: step 9860, loss = 2.19 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:46.345629: step 9870, loss = 2.37 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:47.569024: step 9880, loss = 2.31 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:48.806670: step 9890, loss = 2.22 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:50.115654: step 9900, loss = 2.19 (977.9 examples/sec; 0.131 sec/batch)
2017-05-04 22:08:51.222082: step 9910, loss = 2.23 (1156.9 examples/sec; 0.111 sec/batch)
2017-05-04 22:08:52.453746: step 9920, loss = 2.27 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:53.699115: step 9930, loss = 2.23 (1027.8 examples/sec; 0.125 sec/batch)
2017-05-04 22:08:54.914099: step 9940, loss = 2.37 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:08:56.139484: step 9950, loss = 2.07 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:57.376572: step 9960, loss = 2.22 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:58.586149: step 9970, loss = 2.18 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:08:59.806594: step 9980, loss = 2.15 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:01.039139: step 9990, loss = 2.27 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:02.272687: step 10000, loss = 2.06 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:03.503024: step 10010, loss = 2.20 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:04.748441: step 10020, loss = 2.15 (1027.8 examples/sec; 0.125 sec/batch)
2017-05-04 22:09:05.966257: step 10030, loss = 2.32 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:07.192248: step 10040, loss = 2.37 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:08.425138: step 10050, loss = 2.35 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:09.647217: step 10060, loss = 2.27 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:10.904051: step 10070, loss = 2.43 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-04 22:09:12.107203: step 10080, loss = 2.18 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:09:13.348501: step 10090, loss = 2.39 (1031.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:09:14.532080: step 10100, loss = 2.18 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:09:15.778871: step 10110, loss = 2.48 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-04 22:09:17.014394: step 10120, loss = 2.27 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:09:18.223286: step 10130, loss = 2.20 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:09:19.452472: step 10140, loss = 2.24 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:20.680556: step 10150, loss = 2.31 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:21.911708: step 10160, loss = 2.34 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:23.142543: step 10170, loss = 2.10 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:24.386117: step 10180, loss = 2.29 (1029.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:09:25.595478: step 10190, loss = 2.25 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:09:26.832279: step 10200, loss = 2.02 (1034.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:09:28.062276: step 10210, loss = 2.29 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:29.289507: step 10220, loss = 2.20 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:30.518130: step 10230, loss = 2.28 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:31.741586: step 10240, loss = 2.17 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:32.964841: step 10250, loss = 2.09 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:34.178620: step 10260, loss = 2.20 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:09:35.401019: step 10270, loss = 2.19 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:36.652308: step 10280, loss = 2.16 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-04 22:09:37.878119: step 10290, loss = 2.23 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:39.065878: step 10300, loss = 2.34 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:09:40.291066: step 10310, loss = 2.35 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:41.505260: step 10320, loss = 2.32 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:09:42.727981: step 10330, loss = 2.39 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:43.946686: step 10340, loss = 2.11 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:45.167583: step 10350, loss = 2.08 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:46.375631: step 10360, loss = 2.28 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:09:47.604491: step 10370, loss = 2.27 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:48.846544: step 10380, loss = 2.12 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:09:50.087711: step 10390, loss = 2.19 (1031.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:09:51.317244: step 10400, loss = 2.45 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:52.546611: step 10410, loss = 2.26 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:53.771851: step 10420, loss = 2.34 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:55.002137: step 10430, loss = 2.29 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:56.242642: step 10440, loss = 2.23 (1031.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:09:57.470329: step 10450, loss = 2.31 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:58.685673: step 10460, loss = 2.24 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:59.911514: step 10470, loss = 2.26 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:01.150498: step 10480, loss = 2.24 (1033.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:02.364552: step 10490, loss = 2.22 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:10:03.550361: step 10500, loss = 2.51 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:10:04.792850: step 10510, loss = 2.36 (1030.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:06.016188: step 10520, loss = 2.11 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:07.237210: step 10530, loss = 2.33 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:08.471763: step 10540, loss = 2.17 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:09.697490: step 10550, loss = 2.10 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:10.938509: step 10560, loss = 2.16 (1031.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:12.159512: step 10570, loss = 2.16 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:13.404588: step 10580, loss = 2.24 (1028.0 examples/sec; 0.125 sec/batch)
2017-05-04 22:10:14.639084: step 10590, loss = 2.27 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:15.862073: step 10600, loss = 2.37 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:17.091484: step 10610, loss = 2.25 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:18.313920: step 10620, loss = 2.30 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:19.552029: step 10630, loss = 2.27 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:20.781940: step 10640, loss = 2.19 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:22.007086: step 10650, loss = 2.28 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:23.235413: step 10660, loss = 2.17 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:24.457733: step 10670, loss = 2.28 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:25.674419: step 10680, loss = 2.19 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:26.903794: step 10690, loss = 2.12 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:28.105085: step 10700, loss = 2.22 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:10:29.345047: step 10710, loss = 2.22 (1032.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:30.564701: step 10720, loss = 2.23 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:31.788213: step 10730, loss = 2.31 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:33.025132: step 10740, loss = 2.53 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:34.244998: step 10750, loss = 2.24 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:35.484635: step 10760, loss = 2.15 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:36.716314: step 10770, loss = 2.28 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:37.933496: step 10780, loss = 2.27 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:39.174308: step 10790, loss = 2.09 (1031.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:40.402731: step 10800, loss = 2.26 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:41.620972: step 10810, loss = 2.30 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:42.832342: step 10820, loss = 2.27 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:10:44.070907: step 10830, loss = 2.21 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:45.307042: step 10840, loss = 2.17 (1035.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:46.522298: step 10850, loss = 2.18 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:47.750650: step 10860, loss = 2.16 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:48.982806: step 10870, loss = 2.19 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:50.205108: step 10880, loss = 2.13 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:51.537715: step 10890, loss = 2.47 (960.5 examples/sec; 0.133 sec/batch)
2017-05-04 22:10:52.605649: step 10900, loss = 2.27 (1198.6 examples/sec; 0.107 sec/batch)
2017-05-04 22:10:53.826821: step 10910, loss = 2.02 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:55.066747: step 10920, loss = 2.02 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:56.287163: step 10930, loss = 2.37 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:57.522149: step 10940, loss = 2.25 (1036.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:58.744319: step 10950, loss = 2.20 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:59.985599: step 10960, loss = 2.24 (1031.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:11:01.210303: step 10970, loss = 2.28 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:02.421532: step 10980, loss = 2.28 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:11:03.651020: step 10990, loss = 2.11 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:04.890064: step 11000, loss = 2.23 (1033.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:11:06.113613: step 11010, loss = 2.32 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:07.345501: step 11020, loss = 2.25 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:08.590350: step 11030, loss = 2.19 (1028.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:11:09.812320: step 11040, loss = 2.21 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:11.049802: step 11050, loss = 2.15 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:11:12.274319: step 11060, loss = 2.26 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:13.506210: step 11070, loss = 2.15 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:14.737105: step 11080, loss = 2.26 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:15.928117: step 11090, loss = 2.27 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:11:17.176995: step 11100, loss = 2.13 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-04 22:11:18.401044: step 11110, loss = 2.21 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:19.632092: step 11120, loss = 2.23 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:20.876480: step 11130, loss = 2.42 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:11:22.079667: step 11140, loss = 2.17 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:11:23.313096: step 11150, loss = 2.22 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:24.535318: step 11160, loss = 2.38 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:25.760428: step 11170, loss = 2.26 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:26.987631: step 11180, loss = 2.35 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:28.216968: step 11190, loss = 2.28 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:29.436076: step 11200, loss = 2.21 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:30.665793: step 11210, loss = 2.11 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:31.891987: step 11220, loss = 2.09 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:33.118675: step 11230, loss = 2.38 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:34.334815: step 11240, loss = 2.03 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:35.573310: step 11250, loss = 2.12 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:11:36.795190: step 11260, loss = 2.19 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:38.002330: step 11270, loss = 2.16 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:11:39.243785: step 11280, loss = 2.26 (1031.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:11:40.449870: step 11290, loss = 2.23 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:11:41.675419: step 11300, loss = 2.19 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:42.907360: step 11310, loss = 2.28 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:44.140029: step 11320, loss = 2.23 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:45.366693: step 11330, loss = 2.20 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:46.604063: step 11340, loss = 2.19 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:11:47.826415: step 11350, loss = 2.20 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:49.055204: step 11360, loss = 2.26 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:50.275494: step 11370, loss = 2.11 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:51.487437: step 11380, loss = 2.15 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:11:52.738966: step 11390, loss = 2.43 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-04 22:11:53.956063: step 11400, loss = 2.25 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:55.191018: step 11410, loss = 2.13 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:56.424034: step 11420, loss = 2.15 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:57.629440: step 11430, loss = 2.31 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:11:58.835737: step 11440, loss = 2.24 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:12:00.079883: step 11450, loss = 2.19 (1028.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:01.321431: step 11460, loss = 2.26 (1031.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:02.533375: step 11470, loss = 2.09 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:12:03.786214: step 11480, loss = 2.21 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-04 22:12:04.956809: step 11490, loss = 2.11 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:12:06.167037: step 11500, loss = 2.08 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:12:07.395431: step 11510, loss = 2.26 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:08.637333: step 11520, loss = 2.32 (1030.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:09.860692: step 11530, loss = 2.22 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:11.090351: step 11540, loss = 2.09 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:12.317190: step 11550, loss = 2.18 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:13.526658: step 11560, loss = 2.29 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:12:14.743439: step 11570, loss = 2.10 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:15.976379: step 11580, loss = 2.32 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:17.228192: step 11590, loss = 2.34 (1022.5 examples/sec; 0.125 sec/batch)
2017-05-04 22:12:18.458007: step 11600, loss = 2.36 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:19.696214: step 11610, loss = 2.17 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:20.913939: step 11620, loss = 2.36 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:22.131154: step 11630, loss = 2.38 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:23.365502: step 11640, loss = 2.22 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:24.598214: step 11650, loss = 2.25 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:25.796833: step 11660, loss = 2.10 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:12:27.022821: step 11670, loss = 2.35 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:28.257999: step 11680, loss = 2.07 (1036.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:29.437431: step 11690, loss = 2.17 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:12:30.668836: step 11700, loss = 2.22 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:31.900927: step 11710, loss = 2.02 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:33.120912: step 11720, loss = 2.30 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:34.362055: step 11730, loss = 2.10 (1031.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:35.597966: step 11740, loss = 2.36 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:36.815321: step 11750, loss = 2.11 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:38.030053: step 11760, loss = 2.31 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:12:39.267682: step 11770, loss = 2.13 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:40.519637: step 11780, loss = 2.21 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-04 22:12:41.744368: step 11790, loss = 2.21 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:42.967170: step 11800, loss = 2.18 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:44.191635: step 11810, loss = 2.14 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:45.425361: step 11820, loss = 2.03 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:46.625139: step 11830, loss = 2.14 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:12:47.844904: step 11840, loss = 2.33 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:49.068423: step 11850, loss = 2.18 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:50.265143: step 11860, loss = 2.22 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:12:51.490458: step 11870, loss = 2.19 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:52.814205: step 11880, loss = 2.15 (967.0 examples/sec; 0.132 sec/batch)
2017-05-04 22:12:53.922757: step 11890, loss = 2.21 (1154.6 examples/sec; 0.111 sec/batch)
2017-05-04 22:12:55.151508: step 11900, loss = 2.16 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:56.385978: step 11910, loss = 2.27 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:57.601179: step 11920, loss = 2.14 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:58.836933: step 11930, loss = 2.23 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:13:00.061544: step 11940, loss = 2.26 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:01.283702: step 11950, loss = 2.24 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:02.505672: step 11960, loss = 2.18 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:03.743629: step 11970, loss = 2.38 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:13:04.960782: step 11980, loss = 2.31 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:06.180639: step 11990, loss = 2.12 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:07.411306: step 12000, loss = 2.08 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:08.629920: step 12010, loss = 2.26 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:09.845963: step 12020, loss = 2.13 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:11.078970: step 12030, loss = 2.22 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:12.316000: step 12040, loss = 2.18 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:13:13.523786: step 12050, loss = 2.26 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:13:14.747819: step 12060, loss = 2.09 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:15.978321: step 12070, loss = 2.14 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:17.162640: step 12080, loss = 2.25 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:13:18.407843: step 12090, loss = 2.30 (1028.0 examples/sec; 0.125 sec/batch)
2017-05-04 22:13:19.641064: step 12100, loss = 2.09 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:20.887532: step 12110, loss = 2.22 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-04 22:13:22.124659: step 12120, loss = 2.18 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:13:23.366685: step 12130, loss = 2.16 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:13:24.602759: step 12140, loss = 2.06 (1035.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:13:25.829499: step 12150, loss = 2.55 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:27.060663: step 12160, loss = 2.32 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:28.286852: step 12170, loss = 2.33 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:29.497699: step 12180, loss = 2.20 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:13:30.714318: step 12190, loss = 2.33 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:31.916887: step 12200, loss = 2.14 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:13:33.141547: step 12210, loss = 2.11 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:34.353186: step 12220, loss = 2.24 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:13:35.556588: step 12230, loss = 2.23 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:13:36.780033: step 12240, loss = 2.17 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:37.985577: step 12250, loss = 2.01 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:13:39.216719: step 12260, loss = 2.19 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:40.438646: step 12270, loss = 2.05 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:41.628779: step 12280, loss = 2.08 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:13:42.861415: step 12290, loss = 2.30 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:44.094357: step 12300, loss = 2.12 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:45.321358: step 12310, loss = 2.23 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:46.552053: step 12320, loss = 2.27 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:47.791649: step 12330, loss = 2.09 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:13:49.016283: step 12340, loss = 2.08 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:50.230963: step 12350, loss = 2.33 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:13:51.462768: step 12360, loss = 2.17 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:52.686898: step 12370, loss = 2.03 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:53.893469: step 12380, loss = 2.25 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:13:55.128755: step 12390, loss = 2.06 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:13:56.355991: step 12400, loss = 2.06 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:57.568003: step 12410, loss = 2.21 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:13:58.794408: step 12420, loss = 2.36 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:14:00.025998: step 12430, loss = 2.27 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:14:01.240714: step 12440, loss = 2.18 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:14:02.443708: step 12450, loss = 2.11 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:03.670309: step 12460, loss = 2.23 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:14:04.900140: step 12470, loss = 2.15 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:14:06.085468: step 12480, loss = 2.21 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:14:07.300535: step 12490, loss = 2.36 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:14:08.509630: step 12500, loss = 2.28 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:14:09.719862: step 12510, loss = 2.29 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:14:10.920924: step 12520, loss = 2.25 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:12.111055: step 12530, loss = 2.22 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:14:13.328373: step 12540, loss = 2.21 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:14:14.518809: step 12550, loss = 2.24 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:14:15.723030: step 12560, loss = 2.08 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:16.914826: step 12570, loss = 2.20 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:14:18.106590: step 12580, loss = 2.21 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:14:19.307079: step 12590, loss = 2.24 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:20.510386: step 12600, loss = 2.15 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:21.705906: step 12610, loss = 2.05 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:22.914528: step 12620, loss = 2.25 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:14:24.144420: step 12630, loss = 2.17 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:14:25.368600: step 12640, loss = 2.10 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:14:26.569858: step 12650, loss = 2.08 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:27.787734: step 12660, loss = 2.15 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:14:29.000363: step 12670, loss = 2.17 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:14:30.197806: step 12680, loss = 2.13 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:31.428602: step 12690, loss = 2.30 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:14:32.642648: step 12700, loss = 2.51 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:14:33.845011: step 12710, loss = 2.15 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:35.060667: step 12720, loss = 2.14 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:14:36.230204: step 12730, loss = 2.26 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:14:37.393303: step 12740, loss = 2.03 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:14:38.576068: step 12750, loss = 2.24 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:14:39.741161: step 12760, loss = 2.10 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:14:40.909744: step 12770, loss = 2.14 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:14:42.068199: step 12780, loss = 2.27 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:14:43.228520: step 12790, loss = 2.17 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:14:44.390357: step 12800, loss = 2.20 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:14:45.557376: step 12810, loss = 2.09 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:14:46.725703: step 12820, loss = 2.17 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:14:47.903836: step 12830, loss = 2.13 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:14:49.107449: step 12840, loss = 2.17 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:50.296553: step 12850, loss = 2.12 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:14:51.493808: step 12860, loss = 2.13 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:52.750937: step 12870, loss = 2.39 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-04 22:14:53.840083: step 12880, loss = 2.27 (1175.2 examples/sec; 0.109 sec/batch)
2017-05-04 22:14:55.022039: step 12890, loss = 2.14 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:14:56.208712: step 12900, loss = 2.15 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:14:57.403360: step 12910, loss = 2.24 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:14:58.573620: step 12920, loss = 2.26 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:14:59.754819: step 12930, loss = 2.10 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:00.936726: step 12940, loss = 2.25 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:02.102328: step 12950, loss = 2.23 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:03.290079: step 12960, loss = 2.25 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:04.464780: step 12970, loss = 2.33 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:05.646751: step 12980, loss = 2.27 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:06.814663: step 12990, loss = 2.11 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:07.983525: step 13000, loss = 2.20 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:09.143653: step 13010, loss = 2.16 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:15:10.299803: step 13020, loss = 2.32 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:15:11.481493: step 13030, loss = 2.07 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:12.652415: step 13040, loss = 2.19 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:13.808730: step 13050, loss = 2.11 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:15:14.985893: step 13060, loss = 2.07 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:16.178048: step 13070, loss = 2.07 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:17.357351: step 13080, loss = 2.36 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:18.543983: step 13090, loss = 2.22 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:19.734664: step 13100, loss = 2.13 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:20.924867: step 13110, loss = 2.19 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:22.109099: step 13120, loss = 2.11 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:23.311150: step 13130, loss = 2.09 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:15:24.495662: step 13140, loss = 2.18 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:25.689575: step 13150, loss = 2.50 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:26.886056: step 13160, loss = 2.06 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:15:28.083413: step 13170, loss = 2.07 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:15:29.283764: step 13180, loss = 2.18 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:15:30.470889: step 13190, loss = 2.16 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:31.664950: step 13200, loss = 2.08 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:32.848328: step 13210, loss = 2.13 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:34.040890: step 13220, loss = 2.17 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:35.234579: step 13230, loss = 2.32 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:36.423456: step 13240, loss = 2.16 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:37.599671: step 13250, loss = 2.17 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:38.766904: step 13260, loss = 2.18 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:39.947224: step 13270, loss = 2.12 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:41.119290: step 13280, loss = 2.31 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:42.286735: step 13290, loss = 2.21 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:43.469150: step 13300, loss = 2.29 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:44.654625: step 13310, loss = 2.29 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:45.807585: step 13320, loss = 2.04 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-04 22:15:46.981059: step 13330, loss = 2.33 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:48.158519: step 13340, loss = 2.45 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:49.355025: step 13350, loss = 2.05 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:15:50.514593: step 13360, loss = 2.19 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:15:51.704288: step 13370, loss = 2.22 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:52.908013: step 13380, loss = 2.28 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:15:54.076073: step 13390, loss = 2.24 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:55.267034: step 13400, loss = 2.39 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:56.424768: step 13410, loss = 2.03 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:15:57.595950: step 13420, loss = 2.17 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:58.779632: step 13430, loss = 2.06 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:59.966287: step 13440, loss = 2.28 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:01.155373: step 13450, loss = 2.19 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:02.333403: step 13460, loss = 2.13 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:03.511846: step 13470, loss = 2.13 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:04.694052: step 13480, loss = 2.07 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:05.866543: step 13490, loss = 2.13 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:07.050321: step 13500, loss = 2.20 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:08.235665: step 13510, loss = 2.11 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:09.431077: step 13520, loss = 2.13 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:16:10.620924: step 13530, loss = 2.12 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:11.799207: step 13540, loss = 2.18 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:12.979496: step 13550, loss = 2.20 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:14.172189: step 13560, loss = 2.23 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:15.354726: step 13570, loss = 2.02 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:16.529264: step 13580, loss = 2.08 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:17.707865: step 13590, loss = 2.29 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:18.891731: step 13600, loss = 2.12 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:20.054112: step 13610, loss = 2.04 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:16:21.218567: step 13620, loss = 2.21 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:16:22.391375: step 13630, loss = 2.37 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:23.577155: step 13640, loss = 2.18 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:24.762893: step 13650, loss = 2.22 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:25.927495: step 13660, loss = 2.21 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:16:27.113653: step 13670, loss = 2.55 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:28.319469: step 13680, loss = 2.17 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:16:29.497371: step 13690, loss = 2.10 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:30.686289: step 13700, loss = 1.98 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:31.878612: step 13710, loss = 2.18 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:33.067858: step 13720, loss = 2.05 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:34.257101: step 13730, loss = 2.09 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:35.435392: step 13740, loss = 2.13 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:36.609039: step 13750, loss = 2.20 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:37.766606: step 13760, loss = 2.29 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:16:38.946271: step 13770, loss = 2.14 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:40.118724: step 13780, loss = 2.24 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:41.302512: step 13790, loss = 2.11 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:42.456519: step 13800, loss = 1.94 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-04 22:16:43.628108: step 13810, loss = 2.08 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:44.797015: step 13820, loss = 1.99 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:45.949479: step 13830, loss = 2.18 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-04 22:16:47.133941: step 13840, loss = 2.04 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:48.313280: step 13850, loss = 2.15 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:49.601980: step 13860, loss = 2.27 (993.2 examples/sec; 0.129 sec/batch)
2017-05-04 22:16:50.643063: step 13870, loss = 2.25 (1229.5 examples/sec; 0.104 sec/batch)
2017-05-04 22:16:51.799708: step 13880, loss = 2.09 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:16:52.962894: step 13890, loss = 2.34 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:16:54.134604: step 13900, loss = 2.03 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:55.325767: step 13910, loss = 2.14 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:56.506654: step 13920, loss = 2.09 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:57.680409: step 13930, loss = 2.21 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:58.850072: step 13940, loss = 2.19 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:00.015324: step 13950, loss = 2.34 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:01.197892: step 13960, loss = 2.13 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:02.368108: step 13970, loss = 2.28 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:03.568672: step 13980, loss = 2.33 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:17:04.738783: step 13990, loss = 2.30 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:05.924029: step 14000, loss = 2.21 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:17:07.105873: step 14010, loss = 2.15 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:08.295281: step 14020, loss = 2.09 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:17:09.458224: step 14030, loss = 2.31 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:17:10.614055: step 14040, loss = 2.20 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:17:11.786309: step 14050, loss = 2.16 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:12.969147: step 14060, loss = 2.23 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:14.146122: step 14070, loss = 2.07 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:15.332043: step 14080, loss = 2.30 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:17:16.511102: step 14090, loss = 2.26 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:17.679411: step 14100, loss = 2.04 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:18.837723: step 14110, loss = 2.13 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:17:20.022498: step 14120, loss = 2.28 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:21.194111: step 14130, loss = 2.26 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:22.356471: step 14140, loss = 2.04 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:17:23.532306: step 14150, loss = 2.20 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:24.715674: step 14160, loss = 2.10 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:25.883120: step 14170, loss = 2.36 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:27.089789: step 14180, loss = 2.14 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:17:28.288073: step 14190, loss = 2.18 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:17:29.492644: step 14200, loss = 2.22 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:17:30.703543: step 14210, loss = 2.07 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:17:31.891163: step 14220, loss = 2.16 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:17:33.114305: step 14230, loss = 2.17 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:17:34.306265: step 14240, loss = 2.23 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:17:35.502562: step 14250, loss = 2.39 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:17:36.686993: step 14260, loss = 2.11 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:37.872989: step 14270, loss = 2.36 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:17:39.090655: step 14280, loss = 2.14 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:17:40.304931: step 14290, loss = 2.18 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:17:41.494766: step 14300, loss = 2.25 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:17:42.697455: step 14310, loss = 2.13 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:17:43.890805: step 14320, loss = 2.00 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:17:45.088554: step 14330, loss = 2.22 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:17:46.285082: step 14340, loss = 2.25 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:17:47.474968: step 14350, loss = 2.10 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:17:48.698925: step 14360, loss = 2.13 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:17:49.906019: step 14370, loss = 2.16 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:17:51.117018: step 14380, loss = 2.11 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:17:52.325517: step 14390, loss = 2.14 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:17:53.544480: step 14400, loss = 2.35 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:17:54.767714: step 14410, loss = 2.17 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:17:55.993024: step 14420, loss = 2.18 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:17:57.217872: step 14430, loss = 2.20 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:17:58.440757: step 14440, loss = 2.28 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:17:59.673093: step 14450, loss = 2.03 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:18:00.873273: step 14460, loss = 2.22 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:18:02.092311: step 14470, loss = 2.20 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:18:03.318484: step 14480, loss = 2.24 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:18:04.536522: step 14490, loss = 2.16 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:18:05.756858: step 14500, loss = 2.02 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:18:06.984595: step 14510, loss = 2.12 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:18:08.223948: step 14520, loss = 2.05 (1032.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:18:09.447899: step 14530, loss = 2.20 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:18:10.662674: step 14540, loss = 2.11 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:18:11.898177: step 14550, loss = 2.06 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:18:13.125297: step 14560, loss = 2.20 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:18:14.342513: step 14570, loss = 2.15 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:18:15.557314: step 14580, loss = 2.10 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:18:16.772529: step 14590, loss = 2.12 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:18:17.961569: step 14600, loss = 2.16 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:18:19.214118: step 14610, loss = 2.23 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-04 22:18:20.421923: step 14620, loss = 2.27 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:18:21.635511: step 14630, loss = 2.13 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:18:22.863482: step 14640, loss = 2.45 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:18:24.081539: step 14650, loss = 2.02 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:18:25.277499: step 14660, loss = 2.21 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:18:26.483012: step 14670, loss = 2.19 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:18:27.701991: step 14680, loss = 2.23 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:18:28.918863: step 14690, loss = 2.23 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:18:30.122430: step 14700, loss = 2.29 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:18:31.319986: step 14710, loss = 2.07 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:18:32.548756: step 14720, loss = 2.18 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:18:33.747495: step 14730, loss = 2.21 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:18:34.969283: step 14740, loss = 2.09 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:18:36.196015: step 14750, loss = 2.25 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:18:37.402143: step 14760, loss = 2.08 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:18:38.584749: step 14770, loss = 2.50 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:18:39.778312: step 14780, loss = 2.31 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:18:40.962770: step 14790, loss = 2.12 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:18:42.133462: step 14800, loss = 2.10 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:18:43.314337: step 14810, loss = 1.97 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:18:44.497786: step 14820, loss = 2.15 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:18:45.663126: step 14830, loss = 2.08 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:18:46.827614: step 14840, loss = 2.01 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:18:48.102408: step 14850, loss = 2.36 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-04 22:18:49.179597: step 14860, loss = 2.27 (1188.3 examples/sec; 0.108 sec/batch)
2017-05-04 22:18:50.340479: step 14870, loss = 2.09 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:18:51.533177: step 14880, loss = 2.34 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:18:52.709683: step 14890, loss = 2.15 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:18:53.918905: step 14900, loss = 2.14 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:18:55.088520: step 14910, loss = 2.10 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:18:56.280316: step 14920, loss = 2.21 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:18:57.459797: step 14930, loss = 2.40 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:18:58.637159: step 14940, loss = 2.17 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:18:59.861454: step 14950, loss = 2.33 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:01.084289: step 14960, loss = 2.08 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:02.299373: step 14970, loss = 2.02 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:03.514982: step 14980, loss = 2.17 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:04.725678: step 14990, loss = 2.17 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:05.909011: step 15000, loss = 1.98 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:19:07.100624: step 15010, loss = 2.24 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:19:08.285675: step 15020, loss = 2.21 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:19:09.461210: step 15030, loss = 2.27 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:19:10.658190: step 15040, loss = 2.22 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:11.849222: step 15050, loss = 2.20 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:19:13.047882: step 15060, loss = 2.05 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:14.239181: step 15070, loss = 2.06 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:19:15.451535: step 15080, loss = 2.26 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:16.666611: step 15090, loss = 2.19 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:17.864289: step 15100, loss = 2.12 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:19.095527: step 15110, loss = 2.23 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:19:20.305169: step 15120, loss = 2.10 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:21.522730: step 15130, loss = 2.10 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:22.739864: step 15140, loss = 2.15 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:23.954342: step 15150, loss = 2.01 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:25.163452: step 15160, loss = 2.29 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:26.375032: step 15170, loss = 2.01 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:27.576334: step 15180, loss = 2.16 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:28.795997: step 15190, loss = 2.03 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:29.991204: step 15200, loss = 2.15 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:31.232025: step 15210, loss = 2.49 (1031.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:19:32.456788: step 15220, loss = 2.10 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:33.660788: step 15230, loss = 2.24 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:34.873807: step 15240, loss = 2.15 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:36.078710: step 15250, loss = 2.13 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:37.295357: step 15260, loss = 2.12 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:38.496985: step 15270, loss = 2.24 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:39.712766: step 15280, loss = 2.18 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:40.945406: step 15290, loss = 2.08 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:19:42.155100: step 15300, loss = 2.18 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:43.388999: step 15310, loss = 2.01 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:19:44.622958: step 15320, loss = 2.30 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:19:45.843426: step 15330, loss = 1.92 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:47.073169: step 15340, loss = 2.12 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:19:48.293543: step 15350, loss = 2.20 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:49.502631: step 15360, loss = 2.13 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:50.711356: step 15370, loss = 2.15 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:51.921504: step 15380, loss = 2.32 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:53.134962: step 15390, loss = 2.19 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:54.302377: step 15400, loss = 2.10 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:19:55.493863: step 15410, loss = 2.23 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:19:56.687147: step 15420, loss = 2.13 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:19:57.879002: step 15430, loss = 2.08 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:19:59.076547: step 15440, loss = 2.11 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:00.266960: step 15450, loss = 2.06 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:01.434927: step 15460, loss = 2.19 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:20:02.598579: step 15470, loss = 2.27 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:20:03.798537: step 15480, loss = 2.01 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:05.004448: step 15490, loss = 2.21 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:20:06.202657: step 15500, loss = 2.11 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:07.413792: step 15510, loss = 2.15 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:20:08.600042: step 15520, loss = 2.18 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:09.787920: step 15530, loss = 2.09 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:10.971568: step 15540, loss = 2.18 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:12.167666: step 15550, loss = 2.03 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:13.366208: step 15560, loss = 1.94 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:14.556294: step 15570, loss = 2.00 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:15.753380: step 15580, loss = 2.18 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:16.964466: step 15590, loss = 2.07 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:20:18.143196: step 15600, loss = 2.17 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:19.320786: step 15610, loss = 2.38 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:20.471630: step 15620, loss = 2.22 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-04 22:20:21.647652: step 15630, loss = 2.22 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:22.812005: step 15640, loss = 2.17 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:20:24.000184: step 15650, loss = 2.12 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:25.169399: step 15660, loss = 2.01 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:20:26.348777: step 15670, loss = 2.12 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:27.519818: step 15680, loss = 2.13 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:20:28.700178: step 15690, loss = 2.08 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:29.868747: step 15700, loss = 2.11 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:20:31.047714: step 15710, loss = 2.22 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:32.240714: step 15720, loss = 2.18 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:33.401700: step 15730, loss = 2.33 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:20:34.572626: step 15740, loss = 1.92 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:20:35.756047: step 15750, loss = 2.15 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:36.943160: step 15760, loss = 2.09 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:38.123422: step 15770, loss = 2.20 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:39.340539: step 15780, loss = 2.35 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:20:40.530298: step 15790, loss = 2.05 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:41.731869: step 15800, loss = 2.31 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:42.929275: step 15810, loss = 2.01 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:44.137788: step 15820, loss = 2.02 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:20:45.354129: step 15830, loss = 2.10 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:20:46.657322: step 15840, loss = 1.87 (982.2 examples/sec; 0.130 sec/batch)
2017-05-04 22:20:47.776188: step 15850, loss = 2.14 (1144.0 examples/sec; 0.112 sec/batch)
2017-05-04 22:20:48.983840: step 15860, loss = 2.28 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:20:50.165023: step 15870, loss = 2.20 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:51.347859: step 15880, loss = 2.09 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:52.539888: step 15890, loss = 2.07 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:53.745026: step 15900, loss = 2.14 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:20:54.927700: step 15910, loss = 2.13 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:56.123399: step 15920, loss = 2.15 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:57.302377: step 15930, loss = 2.18 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:58.470322: step 15940, loss = 2.06 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:20:59.645435: step 15950, loss = 2.02 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:00.816838: step 15960, loss = 2.13 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:21:01.990319: step 15970, loss = 2.10 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:21:03.153485: step 15980, loss = 2.15 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:21:04.325295: step 15990, loss = 2.24 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:21:05.488822: step 16000, loss = 1.96 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:21:06.650172: step 16010, loss = 2.13 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:21:07.826877: step 16020, loss = 2.08 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:08.988270: step 16030, loss = 2.03 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:21:10.201658: step 16040, loss = 2.05 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:21:11.377567: step 16050, loss = 2.14 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:12.554975: step 16060, loss = 2.07 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:13.710538: step 16070, loss = 1.95 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:21:14.896222: step 16080, loss = 2.15 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:21:16.061860: step 16090, loss = 2.14 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:21:17.245753: step 16100, loss = 2.12 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:18.425885: step 16110, loss = 2.05 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:19.608770: step 16120, loss = 2.24 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:20.815599: step 16130, loss = 2.01 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:21:21.991570: step 16140, loss = 2.01 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:23.158398: step 16150, loss = 2.06 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:21:24.334714: step 16160, loss = 2.09 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:25.498559: step 16170, loss = 2.13 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:21:26.673745: step 16180, loss = 2.16 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:27.862050: step 16190, loss = 2.21 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:21:29.068284: step 16200, loss = 2.19 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:21:30.252131: step 16210, loss = 2.11 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:31.467827: step 16220, loss = 2.18 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:21:32.686107: step 16230, loss = 2.21 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:21:33.891973: step 16240, loss = 2.07 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:21:35.136412: step 16250, loss = 2.14 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:21:36.366577: step 16260, loss = 2.15 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:21:37.586453: step 16270, loss = 2.23 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:21:38.793920: step 16280, loss = 2.11 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:21:40.006476: step 16290, loss = 2.37 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:21:41.213999: step 16300, loss = 2.07 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:21:42.413509: step 16310, loss = 2.06 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:21:43.617657: step 16320, loss = 2.17 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:21:44.789608: step 16330, loss = 2.23 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:21:45.968112: step 16340, loss = 2.03 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:47.188128: step 16350, loss = 2.19 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:21:48.390700: step 16360, loss = 2.02 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:21:49.577169: step 16370, loss = 2.10 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:21:50.783000: step 16380, loss = 2.14 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:21:51.983532: step 16390, loss = 2.21 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:21:53.163440: step 16400, loss = 2.18 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:54.346446: step 16410, loss = 2.13 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:55.524325: step 16420, loss = 2.07 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:56.729409: step 16430, loss = 2.18 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:21:57.911291: step 16440, loss = 2.07 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:59.092091: step 16450, loss = 2.07 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:00.266779: step 16460, loss = 1.95 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:01.438985: step 16470, loss = 2.32 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:02.614633: step 16480, loss = 2.11 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:03.790211: step 16490, loss = 2.17 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:04.966929: step 16500, loss = 2.00 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:06.128450: step 16510, loss = 2.15 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:22:07.309894: step 16520, loss = 2.36 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:08.475577: step 16530, loss = 2.21 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:09.637435: step 16540, loss = 2.04 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:22:10.812064: step 16550, loss = 2.15 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:11.975864: step 16560, loss = 2.04 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:22:13.143346: step 16570, loss = 2.17 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:14.302414: step 16580, loss = 2.20 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:22:15.481426: step 16590, loss = 2.10 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:16.665829: step 16600, loss = 2.20 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:17.852867: step 16610, loss = 2.06 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:22:19.042314: step 16620, loss = 2.22 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:22:20.254185: step 16630, loss = 2.14 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:22:21.446586: step 16640, loss = 2.21 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:22:22.674260: step 16650, loss = 2.11 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:22:23.908313: step 16660, loss = 2.17 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:22:25.139420: step 16670, loss = 2.05 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:22:26.345902: step 16680, loss = 2.14 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:22:27.551095: step 16690, loss = 2.09 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:22:28.775022: step 16700, loss = 2.31 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:22:29.977899: step 16710, loss = 2.07 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:22:31.187129: step 16720, loss = 2.13 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:22:32.394289: step 16730, loss = 2.21 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:22:33.586493: step 16740, loss = 2.06 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:22:34.774439: step 16750, loss = 2.09 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:22:35.960488: step 16760, loss = 2.19 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:22:37.148741: step 16770, loss = 2.13 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:22:38.310271: step 16780, loss = 2.09 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:22:39.481678: step 16790, loss = 2.16 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:40.666217: step 16800, loss = 2.08 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:41.822841: step 16810, loss = 1.98 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:22:42.989487: step 16820, loss = 2.22 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:44.259983: step 16830, loss = 2.18 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-04 22:22:45.331358: step 16840, loss = 2.20 (1194.7 examples/sec; 0.107 sec/batch)
2017-05-04 22:22:46.513307: step 16850, loss = 2.14 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:47.682874: step 16860, loss = 2.19 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:48.841690: step 16870, loss = 2.14 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:22:50.007532: step 16880, loss = 2.58 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:51.198729: step 16890, loss = 2.16 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:22:52.381940: step 16900, loss = 2.14 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:53.551836: step 16910, loss = 2.19 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:54.729053: step 16920, loss = 2.14 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:55.893034: step 16930, loss = 2.28 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:22:57.063924: step 16940, loss = 2.01 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:58.243366: step 16950, loss = 2.20 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:59.440959: step 16960, loss = 2.12 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:23:00.616262: step 16970, loss = 2.15 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:01.786888: step 16980, loss = 2.28 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:02.962701: step 16990, loss = 2.21 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:04.136679: step 17000, loss = 2.26 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:05.309904: step 17010, loss = 2.07 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:06.466883: step 17020, loss = 2.14 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:23:07.650203: step 17030, loss = 2.14 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:08.816675: step 17040, loss = 2.10 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:09.965506: step 17050, loss = 2.23 (1114.2 examples/sec; 0.115 sec/batch)
2017-05-04 22:23:11.144558: step 17060, loss = 2.13 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:12.330688: step 17070, loss = 1.99 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:23:13.510375: step 17080, loss = 2.20 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:14.662676: step 17090, loss = 2.00 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-04 22:23:15.840775: step 17100, loss = 2.27 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:17.019791: step 17110, loss = 2.07 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:18.195746: step 17120, loss = 2.14 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:19.370813: step 17130, loss = 2.17 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:20.542589: step 17140, loss = 1.99 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:21.707510: step 17150, loss = 2.28 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:23:22.885355: step 17160, loss = 2.12 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:24.057027: step 17170, loss = 2.08 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:25.236717: step 17180, loss = 2.10 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:26.428013: step 17190, loss = 1.94 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:23:27.602346: step 17200, loss = 2.02 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:28.782598: step 17210, loss = 2.14 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:29.970282: step 17220, loss = 2.00 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:23:31.144989: step 17230, loss = 2.18 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:32.333742: step 17240, loss = 2.08 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:23:33.509435: step 17250, loss = 2.16 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:34.708787: step 17260, loss = 2.27 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:23:35.900147: step 17270, loss = 2.20 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:23:37.070280: step 17280, loss = 1.91 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:38.223380: step 17290, loss = 2.17 (1110.1 examples/sec; 0.115 sec/batch)
2017-05-04 22:23:39.407328: step 17300, loss = 2.06 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:40.586164: step 17310, loss = 2.30 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:41.744795: step 17320, loss = 1.92 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:23:42.914023: step 17330, loss = 2.13 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:44.091991: step 17340, loss = 2.07 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:45.271905: step 17350, loss = 2.16 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:46.432393: step 17360, loss = 2.10 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:23:47.590837: step 17370, loss = 2.08 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:23:48.753375: step 17380, loss = 2.06 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:23:49.905293: step 17390, loss = 2.18 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-04 22:23:51.082598: step 17400, loss = 2.04 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:52.260043: step 17410, loss = 2.25 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:53.435633: step 17420, loss = 2.08 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:54.596437: step 17430, loss = 2.25 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:23:55.791047: step 17440, loss = 2.13 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:23:56.959102: step 17450, loss = 2.13 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:58.102457: step 17460, loss = 2.08 (1119.5 examples/sec; 0.114 sec/batch)
2017-05-04 22:23:59.260445: step 17470, loss = 2.33 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:00.429822: step 17480, loss = 1.97 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:01.588907: step 17490, loss = 2.02 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:02.759582: step 17500, loss = 2.09 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:03.924760: step 17510, loss = 2.24 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:05.088166: step 17520, loss = 2.09 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:06.238768: step 17530, loss = 2.15 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-04 22:24:07.444663: step 17540, loss = 2.35 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:24:08.624286: step 17550, loss = 2.12 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:09.783716: step 17560, loss = 2.13 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:10.960612: step 17570, loss = 2.14 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:12.155224: step 17580, loss = 2.06 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:24:13.337731: step 17590, loss = 2.14 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:14.505178: step 17600, loss = 2.12 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:15.678413: step 17610, loss = 2.25 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:16.852463: step 17620, loss = 2.14 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:18.016855: step 17630, loss = 2.03 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:19.201026: step 17640, loss = 2.16 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:20.378613: step 17650, loss = 2.10 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:21.545438: step 17660, loss = 2.26 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:22.720507: step 17670, loss = 2.23 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:23.875466: step 17680, loss = 2.15 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-04 22:24:25.036432: step 17690, loss = 2.27 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:26.215704: step 17700, loss = 2.11 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:27.384500: step 17710, loss = 2.18 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:28.564511: step 17720, loss = 2.20 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:29.708786: step 17730, loss = 2.25 (1118.6 examples/sec; 0.114 sec/batch)
2017-05-04 22:24:30.871559: step 17740, loss = 2.02 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:32.043303: step 17750, loss = 2.23 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:33.217044: step 17760, loss = 2.22 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:34.390702: step 17770, loss = 2.01 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:35.560315: step 17780, loss = 2.03 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:36.724430: step 17790, loss = 2.35 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:37.866946: step 17800, loss = 2.05 (1120.3 examples/sec; 0.114 sec/batch)
2017-05-04 22:24:39.050848: step 17810, loss = 2.16 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:40.329512: step 17820, loss = 2.00 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-04 22:24:41.405376: step 17830, loss = 2.10 (1189.7 examples/sec; 0.108 sec/batch)
2017-05-04 22:24:42.577769: step 17840, loss = 2.36 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:43.744841: step 17850, loss = 2.01 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:44.927515: step 17860, loss = 2.20 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:46.074469: step 17870, loss = 2.10 (1116.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:24:47.232701: step 17880, loss = 2.19 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:48.410467: step 17890, loss = 2.18 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:49.584793: step 17900, loss = 2.28 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:50.766181: step 17910, loss = 2.26 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:51.958120: step 17920, loss = 2.03 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:24:53.124422: step 17930, loss = 2.25 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:54.262561: step 17940, loss = 2.19 (1124.6 examples/sec; 0.114 sec/batch)
2017-05-04 22:24:55.426171: step 17950, loss = 2.01 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:56.582272: step 17960, loss = 2.06 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:57.746053: step 17970, loss = 2.17 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:58.923020: step 17980, loss = 2.06 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:25:00.112680: step 17990, loss = 2.21 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:25:01.319822: step 18000, loss = 2.07 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:25:02.540554: step 18010, loss = 2.12 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:03.845603: step 18020, loss = 2.08 (980.8 examples/sec; 0.131 sec/batch)
2017-05-04 22:25:04.996397: step 18030, loss = 2.08 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-04 22:25:06.190695: step 18040, loss = 2.15 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:25:07.422767: step 18050, loss = 2.01 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:08.621419: step 18060, loss = 2.17 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:25:09.851651: step 18070, loss = 2.03 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:11.071421: step 18080, loss = 2.21 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:12.302953: step 18090, loss = 2.07 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:13.510936: step 18100, loss = 2.12 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:25:14.754644: step 18110, loss = 2.07 (1029.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:25:15.963897: step 18120, loss = 1.98 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:25:17.197119: step 18130, loss = 2.10 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:18.391346: step 18140, loss = 2.21 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:25:19.633215: step 18150, loss = 2.06 (1030.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:25:20.851124: step 18160, loss = 2.11 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:22.078276: step 18170, loss = 2.20 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:23.280523: step 18180, loss = 2.05 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:25:24.495523: step 18190, loss = 2.21 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:25.700111: step 18200, loss = 1.98 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:25:26.921849: step 18210, loss = 2.09 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:28.089540: step 18220, loss = 2.14 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:25:29.325893: step 18230, loss = 2.01 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:25:30.542542: step 18240, loss = 1.95 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:31.763569: step 18250, loss = 2.14 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:32.991430: step 18260, loss = 2.37 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:34.221928: step 18270, loss = 2.08 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:35.427741: step 18280, loss = 1.98 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:25:36.673032: step 18290, loss = 2.15 (1027.9 examples/sec; 0.125 sec/batch)
2017-05-04 22:25:37.856792: step 18300, loss = 2.07 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:25:39.109100: step 18310, loss = 2.05 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-04 22:25:40.300218: step 18320, loss = 2.20 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:25:41.533994: step 18330, loss = 2.19 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:42.728589: step 18340, loss = 2.22 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:25:43.951290: step 18350, loss = 2.01 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:45.159907: step 18360, loss = 2.19 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:25:46.370584: step 18370, loss = 2.15 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:25:47.588335: step 18380, loss = 2.15 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:48.809105: step 18390, loss = 2.02 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:50.026264: step 18400, loss = 1.97 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:51.241834: step 18410, loss = 2.14 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:52.414481: step 18420, loss = 2.20 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:25:53.637360: step 18430, loss = 2.20 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:54.871868: step 18440, loss = 2.06 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:56.102973: step 18450, loss = 2.16 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:57.316782: step 18460, loss = 2.11 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:25:58.512616: step 18470, loss = 2.29 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:25:59.743934: step 18480, loss = 2.23 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:00.964878: step 18490, loss = 2.28 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:02.192526: step 18500, loss = 2.03 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:03.413558: step 18510, loss = 2.22 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:04.616644: step 18520, loss = 2.16 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:26:05.847939: step 18530, loss = 2.00 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:07.073540: step 18540, loss = 2.09 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:08.288515: step 18550, loss = 2.09 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:09.496337: step 18560, loss = 2.07 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:10.725665: step 18570, loss = 2.23 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:11.933318: step 18580, loss = 2.11 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:13.146561: step 18590, loss = 2.12 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:14.344589: step 18600, loss = 2.16 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:26:15.587143: step 18610, loss = 2.21 (1030.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:26:16.799140: step 18620, loss = 2.12 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:17.979499: step 18630, loss = 2.16 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:26:19.204840: step 18640, loss = 2.20 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:20.416406: step 18650, loss = 2.07 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:21.619572: step 18660, loss = 2.26 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:26:22.800078: step 18670, loss = 2.04 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:26:23.988940: step 18680, loss = 2.13 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:26:25.186848: step 18690, loss = 1.92 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:26:26.373303: step 18700, loss = 2.11 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:26:27.556739: step 18710, loss = 2.10 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:26:28.780989: step 18720, loss = 2.01 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:29.970605: step 18730, loss = 2.16 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:26:31.163423: step 18740, loss = 2.18 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:26:32.350179: step 18750, loss = 2.14 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:26:33.571689: step 18760, loss = 2.09 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:34.787357: step 18770, loss = 2.07 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:35.991970: step 18780, loss = 2.10 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:26:37.212719: step 18790, loss = 2.20 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:38.423372: step 18800, loss = 2.32 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:39.786011: step 18810, loss = 2.00 (939.3 examples/sec; 0.136 sec/batch)
2017-05-04 22:26:40.850722: step 18820, loss = 2.20 (1202.2 examples/sec; 0.106 sec/batch)
2017-05-04 22:26:42.083912: step 18830, loss = 2.26 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:43.301821: step 18840, loss = 2.17 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:44.529959: step 18850, loss = 2.14 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:45.733308: step 18860, loss = 2.13 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:26:46.956874: step 18870, loss = 2.17 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:48.181378: step 18880, loss = 2.06 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:49.410069: step 18890, loss = 2.19 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:50.618466: step 18900, loss = 2.19 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:51.851057: step 18910, loss = 2.13 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:53.064122: step 18920, loss = 2.17 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:54.297944: step 18930, loss = 2.15 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:55.525618: step 18940, loss = 2.22 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:56.723953: step 18950, loss = 2.08 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:26:57.941779: step 18960, loss = 2.22 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:59.196064: step 18970, loss = 2.02 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-04 22:27:00.401601: step 18980, loss = 2.17 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:01.618074: step 18990, loss = 2.21 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:02.825437: step 19000, loss = 2.02 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:04.124422: step 19010, loss = 2.11 (985.4 examples/sec; 0.130 sec/batch)
2017-05-04 22:27:05.247569: step 19020, loss = 2.09 (1139.7 examples/sec; 0.112 sec/batch)
2017-05-04 22:27:06.454196: step 19030, loss = 2.16 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:07.668555: step 19040, loss = 2.18 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:08.909218: step 19050, loss = 2.09 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:27:10.114839: step 19060, loss = 2.07 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:11.337444: step 19070, loss = 1.93 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:12.557827: step 19080, loss = 2.02 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:13.779331: step 19090, loss = 2.18 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:14.984137: step 19100, loss = 2.28 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:27:16.209761: step 19110, loss = 1.92 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:27:17.400707: step 19120, loss = 2.04 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:27:18.621316: step 19130, loss = 2.30 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:19.841947: step 19140, loss = 2.19 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:21.073992: step 19150, loss = 1.99 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:27:22.269539: step 19160, loss = 2.07 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:27:23.506378: step 19170, loss = 2.15 (1034.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:27:24.720410: step 19180, loss = 1.93 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:25.936883: step 19190, loss = 2.09 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:27.154542: step 19200, loss = 2.24 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:28.339947: step 19210, loss = 2.06 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:27:29.556146: step 19220, loss = 2.08 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:30.774356: step 19230, loss = 2.06 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:31.993078: step 19240, loss = 2.06 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:33.237446: step 19250, loss = 2.05 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:27:34.448448: step 19260, loss = 2.10 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:35.684627: step 19270, loss = 2.19 (1035.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:27:36.898625: step 19280, loss = 2.11 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:38.123335: step 19290, loss = 2.02 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:39.342898: step 19300, loss = 2.08 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:40.550683: step 19310, loss = 2.05 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:41.753433: step 19320, loss = 2.02 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:27:42.964553: step 19330, loss = 2.06 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:44.210045: step 19340, loss = 2.11 (1027.7 examples/sec; 0.125 sec/batch)
2017-05-04 22:27:45.423972: step 19350, loss = 2.11 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:46.623482: step 19360, loss = 2.26 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:27:47.847880: step 19370, loss = 2.09 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:49.086801: step 19380, loss = 2.31 (1033.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:27:50.289768: step 19390, loss = 1.94 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:27:51.513873: step 19400, loss = 2.25 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:52.707749: step 19410, loss = 2.13 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:27:53.914641: step 19420, loss = 2.14 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:55.151184: step 19430, loss = 2.15 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:27:56.350505: step 19440, loss = 2.21 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:27:57.558694: step 19450, loss = 2.04 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:58.767634: step 19460, loss = 1.95 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:59.989332: step 19470, loss = 2.35 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:01.210163: step 19480, loss = 2.12 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:02.463737: step 19490, loss = 2.20 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-04 22:28:03.682511: step 19500, loss = 2.22 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:04.901484: step 19510, loss = 2.15 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:06.101447: step 19520, loss = 1.99 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:28:07.332201: step 19530, loss = 2.12 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:08.547591: step 19540, loss = 2.14 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:09.769638: step 19550, loss = 2.10 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:10.987159: step 19560, loss = 2.12 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:12.225939: step 19570, loss = 2.13 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:28:13.433736: step 19580, loss = 2.04 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:28:14.660691: step 19590, loss = 2.33 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:15.882623: step 19600, loss = 2.32 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:17.084337: step 19610, loss = 2.04 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:28:18.300169: step 19620, loss = 2.17 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:19.518811: step 19630, loss = 2.01 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:20.740832: step 19640, loss = 2.08 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:21.948187: step 19650, loss = 1.99 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:28:23.173794: step 19660, loss = 2.10 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:24.410355: step 19670, loss = 2.10 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:28:25.616428: step 19680, loss = 2.21 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:28:26.832303: step 19690, loss = 2.17 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:28.064911: step 19700, loss = 2.20 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:29.297077: step 19710, loss = 2.23 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:30.481595: step 19720, loss = 2.07 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:28:31.714737: step 19730, loss = 2.33 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:32.913337: step 19740, loss = 2.07 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:28:34.177175: step 19750, loss = 2.17 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-04 22:28:35.406834: step 19760, loss = 2.11 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:36.610897: step 19770, loss = 2.16 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:28:37.812531: step 19780, loss = 2.01 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:28:39.055741: step 19790, loss = 2.03 (1029.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:28:40.363998: step 19800, loss = 1.90 (978.4 examples/sec; 0.131 sec/batch)
2017-05-04 22:28:41.462368: step 19810, loss = 2.15 (1165.4 examples/sec; 0.110 sec/batch)
2017-05-04 22:28:42.661493: step 19820, loss = 2.16 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:28:43.887466: step 19830, loss = 1.96 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:45.113506: step 19840, loss = 2.16 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:46.301715: step 19850, loss = 2.35 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:28:47.526114: step 19860, loss = 2.04 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:48.778406: step 19870, loss = 2.07 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-04 22:28:49.991545: step 19880, loss = 1.95 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:28:51.239786: step 19890, loss = 2.03 (1025.4 examples/sec; 0.125 sec/batch)
2017-05-04 22:28:52.462616: step 19900, loss = 2.01 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:53.656434: step 19910, loss = 2.16 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:28:54.871156: step 19920, loss = 2.04 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:28:56.091486: step 19930, loss = 2.02 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:57.319637: step 19940, loss = 2.16 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:58.524009: step 19950, loss = 2.11 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:28:59.779685: step 19960, loss = 2.08 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-04 22:29:01.003545: step 19970, loss = 2.20 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:02.223319: step 19980, loss = 2.05 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:03.429311: step 19990, loss = 2.21 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:04.735181: step 20000, loss = 2.18 (980.2 examples/sec; 0.131 sec/batch)
2017-05-04 22:29:05.855586: step 20010, loss = 2.01 (1142.5 examples/sec; 0.112 sec/batch)
2017-05-04 22:29:07.064378: step 20020, loss = 2.31 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:08.302643: step 20030, loss = 2.24 (1033.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:29:09.531773: step 20040, loss = 2.27 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:29:10.748125: step 20050, loss = 2.16 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:11.967853: step 20060, loss = 2.03 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:13.212844: step 20070, loss = 2.03 (1028.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:29:14.431342: step 20080, loss = 1.98 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:15.654810: step 20090, loss = 2.32 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:16.863152: step 20100, loss = 2.00 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:18.086935: step 20110, loss = 2.23 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:19.309032: step 20120, loss = 2.06 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:20.529167: step 20130, loss = 1.96 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:21.729056: step 20140, loss = 2.18 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:29:22.972310: step 20150, loss = 2.14 (1029.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:29:24.199861: step 20160, loss = 2.07 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:29:25.385934: step 20170, loss = 2.07 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:29:26.600771: step 20180, loss = 2.01 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:27.815513: step 20190, loss = 2.30 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:29.013362: step 20200, loss = 2.14 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:29:30.234519: step 20210, loss = 2.00 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:31.433989: step 20220, loss = 2.22 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:29:32.670920: step 20230, loss = 2.20 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:29:33.909930: step 20240, loss = 2.23 (1033.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:29:35.135775: step 20250, loss = 2.15 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:29:36.348029: step 20260, loss = 2.22 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:37.583804: step 20270, loss = 2.00 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:29:38.806134: step 20280, loss = 2.08 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:40.018093: step 20290, loss = 2.28 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:41.255593: step 20300, loss = 2.08 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:29:42.462712: step 20310, loss = 2.04 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:43.667282: step 20320, loss = 2.08 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:29:44.899472: step 20330, loss = 2.18 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:29:46.109473: step 20340, loss = 2.02 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:47.342857: step 20350, loss = 2.08 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:29:48.563931: step 20360, loss = 2.13 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:49.786128: step 20370, loss = 2.01 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:51.020641: step 20380, loss = 2.10 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:29:52.257221: step 20390, loss = 2.30 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:29:53.432294: step 20400, loss = 1.91 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:29:54.649765: step 20410, loss = 2.05 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:55.873008: step 20420, loss = 2.21 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:57.085395: step 20430, loss = 2.10 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:58.296954: step 20440, loss = 1.96 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:59.518948: step 20450, loss = 2.15 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:00.752870: step 20460, loss = 2.15 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:01.949575: step 20470, loss = 2.06 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:30:03.174348: step 20480, loss = 2.09 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:04.401469: step 20490, loss = 2.11 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:05.608474: step 20500, loss = 1.94 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:30:06.845724: step 20510, loss = 2.02 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:30:08.059991: step 20520, loss = 2.26 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:30:09.295190: step 20530, loss = 2.20 (1036.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:30:10.514393: step 20540, loss = 2.13 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:11.763575: step 20550, loss = 2.05 (1024.7 examples/sec; 0.125 sec/batch)
2017-05-04 22:30:12.973811: step 20560, loss = 2.18 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:30:14.211971: step 20570, loss = 2.25 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:30:15.407627: step 20580, loss = 2.11 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:30:16.627091: step 20590, loss = 2.19 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:17.796426: step 20600, loss = 2.09 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:30:19.033558: step 20610, loss = 1.91 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:30:20.249056: step 20620, loss = 2.07 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:21.473284: step 20630, loss = 1.98 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:22.680441: step 20640, loss = 2.19 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:30:23.925926: step 20650, loss = 2.01 (1027.7 examples/sec; 0.125 sec/batch)
2017-05-04 22:30:25.144984: step 20660, loss = 2.15 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:26.372698: step 20670, loss = 2.16 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:27.603353: step 20680, loss = 2.17 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:28.838594: step 20690, loss = 2.07 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:30:30.067883: step 20700, loss = 2.15 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:31.283054: step 20710, loss = 2.13 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:32.490103: step 20720, loss = 2.21 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:30:33.727463: step 20730, loss = 2.29 (1034.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:30:34.935921: step 20740, loss = 2.16 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:30:36.158316: step 20750, loss = 2.18 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:37.382193: step 20760, loss = 2.21 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:38.589399: step 20770, loss = 2.03 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:30:39.813586: step 20780, loss = 2.02 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:41.160766: step 20790, loss = 2.05 (950.1 examples/sec; 0.135 sec/batch)
2017-05-04 22:30:42.243378: step 20800, loss = 2.05 (1182.3 examples/sec; 0.108 sec/batch)
2017-05-04 22:30:43.465827: step 20810, loss = 2.01 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:44.697202: step 20820, loss = 2.14 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:45.918316: step 20830, loss = 2.01 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:47.136830: step 20840, loss = 2.10 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:48.388651: step 20850, loss = 2.22 (1022.5 examples/sec; 0.125 sec/batch)
2017-05-04 22:30:49.590027: step 20860, loss = 2.03 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:30:50.797568: step 20870, loss = 2.07 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:30:52.043322: step 20880, loss = 2.20 (1027.5 examples/sec; 0.125 sec/batch)
2017-05-04 22:30:53.257275: step 20890, loss = 2.15 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:30:54.453399: step 20900, loss = 2.20 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:30:55.662319: step 20910, loss = 2.11 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:30:56.876559: step 20920, loss = 2.15 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:30:58.080397: step 20930, loss = 2.08 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:30:59.305129: step 20940, loss = 2.23 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:00.524789: step 20950, loss = 2.06 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:01.734677: step 20960, loss = 2.17 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:31:02.960053: step 20970, loss = 2.22 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:31:04.171289: step 20980, loss = 2.15 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:31:05.460014: step 20990, loss = 2.01 (993.2 examples/sec; 0.129 sec/batch)
2017-05-04 22:31:06.562032: step 21000, loss = 2.24 (1161.5 examples/sec; 0.110 sec/batch)
2017-05-04 22:31:07.778635: step 21010, loss = 2.03 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:08.966692: step 21020, loss = 2.03 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:31:10.154574: step 21030, loss = 2.17 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:31:11.351169: step 21040, loss = 1.91 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:31:12.552580: step 21050, loss = 2.06 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:31:13.762876: step 21060, loss = 2.10 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:31:14.943380: step 21070, loss = 2.21 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:31:16.118418: step 21080, loss = 2.02 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:31:17.307560: step 21090, loss = 2.12 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:31:18.487524: step 21100, loss = 1.96 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:31:19.671053: step 21110, loss = 2.04 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:31:20.857713: step 21120, loss = 2.17 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:31:22.038749: step 21130, loss = 2.35 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:31:23.208986: step 21140, loss = 1.99 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:31:24.406916: step 21150, loss = 1.96 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:31:25.575716: step 21160, loss = 2.07 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:31:26.757738: step 21170, loss = 2.17 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:31:27.948666: step 21180, loss = 2.26 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:31:29.123402: step 21190, loss = 2.23 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:31:30.335312: step 21200, loss = 2.10 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:31:31.568993: step 21210, loss = 2.16 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:31:32.789443: step 21220, loss = 2.26 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:34.007420: step 21230, loss = 1.88 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:35.237751: step 21240, loss = 1.89 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:31:36.500400: step 21250, loss = 2.15 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-04 22:31:37.678534: step 21260, loss = 2.25 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:31:38.909431: step 21270, loss = 2.27 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:31:40.120848: step 21280, loss = 2.25 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:31:41.365005: step 21290, loss = 2.03 (1028.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:31:42.571331: step 21300, loss = 2.08 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:31:43.812325: step 21310, loss = 2.13 (1031.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:31:45.022596: step 21320, loss = 2.33 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:31:46.242302: step 21330, loss = 2.30 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:47.450982: step 21340, loss = 2.05 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:31:48.683438: step 21350, loss = 2.08 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:31:49.871859: step 21360, loss = 2.17 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:31:51.104412: step 21370, loss = 2.10 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:31:52.324118: step 21380, loss = 2.30 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:53.524696: step 21390, loss = 2.13 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:31:54.747417: step 21400, loss = 2.05 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:55.983195: step 21410, loss = 2.03 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:31:57.207798: step 21420, loss = 1.95 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:58.398964: step 21430, loss = 2.23 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:31:59.640749: step 21440, loss = 2.14 (1030.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:32:00.878555: step 21450, loss = 2.05 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:32:02.077593: step 21460, loss = 2.14 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:32:03.296667: step 21470, loss = 2.16 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:04.511680: step 21480, loss = 2.05 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:05.729107: step 21490, loss = 2.17 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:06.945428: step 21500, loss = 1.97 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:08.162303: step 21510, loss = 2.14 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:09.379751: step 21520, loss = 2.01 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:10.612830: step 21530, loss = 2.19 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:11.831138: step 21540, loss = 2.24 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:13.064354: step 21550, loss = 2.23 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:14.253471: step 21560, loss = 2.05 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:32:15.464322: step 21570, loss = 2.08 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:16.688830: step 21580, loss = 2.19 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:17.863043: step 21590, loss = 2.12 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:32:19.082011: step 21600, loss = 2.17 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:20.301390: step 21610, loss = 2.08 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:21.507994: step 21620, loss = 2.09 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:22.748150: step 21630, loss = 2.01 (1032.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:32:23.956367: step 21640, loss = 2.15 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:25.171658: step 21650, loss = 2.20 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:26.405453: step 21660, loss = 2.41 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:27.601931: step 21670, loss = 2.16 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:32:28.825527: step 21680, loss = 2.03 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:30.032732: step 21690, loss = 2.06 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:31.253875: step 21700, loss = 2.04 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:32.466280: step 21710, loss = 1.97 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:33.652104: step 21720, loss = 2.15 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:32:34.876075: step 21730, loss = 1.98 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:36.109851: step 21740, loss = 2.23 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:37.329493: step 21750, loss = 2.24 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:38.548140: step 21760, loss = 2.14 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:39.750905: step 21770, loss = 2.21 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:32:41.072278: step 21780, loss = 2.06 (968.7 examples/sec; 0.132 sec/batch)
2017-05-04 22:32:42.155495: step 21790, loss = 2.22 (1181.6 examples/sec; 0.108 sec/batch)
2017-05-04 22:32:43.389353: step 21800, loss = 2.08 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:44.618762: step 21810, loss = 2.09 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:45.817071: step 21820, loss = 2.18 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:32:47.054718: step 21830, loss = 2.03 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:32:48.297015: step 21840, loss = 2.35 (1030.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:32:49.518969: step 21850, loss = 2.06 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:50.714860: step 21860, loss = 2.01 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:32:51.935823: step 21870, loss = 2.22 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:53.144719: step 21880, loss = 2.08 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:54.363896: step 21890, loss = 2.13 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:55.598004: step 21900, loss = 2.12 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:56.825725: step 21910, loss = 2.12 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:58.034983: step 21920, loss = 2.08 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:59.247682: step 21930, loss = 2.08 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:00.461816: step 21940, loss = 2.11 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:01.681604: step 21950, loss = 2.20 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:02.906724: step 21960, loss = 1.97 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:33:04.156975: step 21970, loss = 2.13 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-04 22:33:05.446459: step 21980, loss = 2.04 (992.6 examples/sec; 0.129 sec/batch)
2017-05-04 22:33:06.553724: step 21990, loss = 2.25 (1156.0 examples/sec; 0.111 sec/batch)
2017-05-04 22:33:07.767115: step 22000, loss = 1.98 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:09.017514: step 22010, loss = 2.13 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-04 22:33:10.215910: step 22020, loss = 1.99 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:33:11.420166: step 22030, loss = 2.17 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:33:12.633487: step 22040, loss = 2.13 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:13.854886: step 22050, loss = 2.12 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:15.055900: step 22060, loss = 2.20 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:33:16.284285: step 22070, loss = 2.01 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:33:17.489723: step 22080, loss = 1.90 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:18.718284: step 22090, loss = 2.11 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:33:19.942449: step 22100, loss = 2.18 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:21.168207: step 22110, loss = 2.02 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:33:22.361904: step 22120, loss = 2.15 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:33:23.597138: step 22130, loss = 2.02 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:33:24.816028: step 22140, loss = 2.07 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:26.012389: step 22150, loss = 2.05 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:33:27.237361: step 22160, loss = 2.07 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:28.478836: step 22170, loss = 2.32 (1031.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:33:29.689148: step 22180, loss = 1.79 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:30.913875: step 22190, loss = 2.19 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:32.133211: step 22200, loss = 2.26 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:33.349631: step 22210, loss = 2.06 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:34.558500: step 22220, loss = 2.21 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:35.810885: step 22230, loss = 2.17 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-04 22:33:37.019878: step 22240, loss = 2.08 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:38.255329: step 22250, loss = 2.04 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:33:39.470407: step 22260, loss = 2.13 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:40.704675: step 22270, loss = 1.97 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:33:41.902539: step 22280, loss = 2.05 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:33:43.154311: step 22290, loss = 2.09 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-04 22:33:44.350666: step 22300, loss = 2.10 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:33:45.560221: step 22310, loss = 2.14 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:46.771474: step 22320, loss = 2.15 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:48.011852: step 22330, loss = 1.98 (1032.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:33:49.227605: step 22340, loss = 2.02 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:50.443165: step 22350, loss = 2.16 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:51.653818: step 22360, loss = 2.05 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:52.896067: step 22370, loss = 2.06 (1030.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:33:54.059476: step 22380, loss = 1.92 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:33:55.281205: step 22390, loss = 2.00 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:56.487440: step 22400, loss = 2.07 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:57.692135: step 22410, loss = 2.01 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:33:58.902094: step 22420, loss = 2.09 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:00.108458: step 22430, loss = 2.12 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:01.331921: step 22440, loss = 2.18 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:34:02.551915: step 22450, loss = 1.96 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:34:03.752804: step 22460, loss = 1.90 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:04.966608: step 22470, loss = 2.16 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:06.168856: step 22480, loss = 1.98 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:07.370654: step 22490, loss = 2.39 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:08.569270: step 22500, loss = 2.07 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:09.758823: step 22510, loss = 2.22 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:34:10.958634: step 22520, loss = 2.02 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:12.150519: step 22530, loss = 2.12 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:34:13.347525: step 22540, loss = 1.96 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:14.522613: step 22550, loss = 1.95 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:34:15.725194: step 22560, loss = 2.18 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:16.931405: step 22570, loss = 2.06 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:18.109937: step 22580, loss = 2.17 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:34:19.319932: step 22590, loss = 2.20 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:20.525289: step 22600, loss = 2.10 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:21.731471: step 22610, loss = 2.20 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:22.937808: step 22620, loss = 1.98 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:24.132858: step 22630, loss = 2.08 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:25.334110: step 22640, loss = 2.08 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:26.533522: step 22650, loss = 2.26 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:27.709546: step 22660, loss = 2.10 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:34:28.884308: step 22670, loss = 2.05 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:34:30.063907: step 22680, loss = 2.07 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:34:31.247576: step 22690, loss = 2.00 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:34:32.434811: step 22700, loss = 1.91 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:34:33.619889: step 22710, loss = 2.02 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:34:34.832565: step 22720, loss = 2.16 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:36.040865: step 22730, loss = 2.07 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:37.256898: step 22740, loss = 2.07 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:34:38.469997: step 22750, loss = 2.03 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:39.654453: step 22760, loss = 2.03 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:34:40.958756: step 22770, loss = 2.03 (981.4 examples/sec; 0.130 sec/batch)
2017-05-04 22:34:42.059898: step 22780, loss = 2.31 (1162.4 examples/sec; 0.110 sec/batch)
2017-05-04 22:34:43.282995: step 22790, loss = 1.92 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:34:44.503136: step 22800, loss = 2.20 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:34:45.703669: step 22810, loss = 2.22 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:46.928310: step 22820, loss = 2.07 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:34:48.137667: step 22830, loss = 2.00 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:49.343169: step 22840, loss = 2.19 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:50.554198: step 22850, loss = 1.92 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:51.763175: step 22860, loss = 2.06 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:52.979102: step 22870, loss = 2.26 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:34:54.178566: step 22880, loss = 2.19 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:55.387440: step 22890, loss = 2.13 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:56.582419: step 22900, loss = 2.25 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:34:57.780409: step 22910, loss = 2.11 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:58.958557: step 22920, loss = 2.16 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:35:00.132808: step 22930, loss = 2.23 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:35:01.331975: step 22940, loss = 2.02 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:02.513589: step 22950, loss = 2.10 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:35:03.710856: step 22960, loss = 2.04 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:04.972139: step 22970, loss = 2.23 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-04 22:35:06.104760: step 22980, loss = 2.19 (1130.1 examples/sec; 0.113 sec/batch)
2017-05-04 22:35:07.294904: step 22990, loss = 2.00 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:35:08.474724: step 23000, loss = 2.13 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:35:09.655561: step 23010, loss = 2.01 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:35:10.830610: step 23020, loss = 2.11 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:35:12.028929: step 23030, loss = 2.21 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:13.202205: step 23040, loss = 2.00 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:35:14.402853: step 23050, loss = 2.00 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:15.606094: step 23060, loss = 2.01 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:16.800111: step 23070, loss = 2.21 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:35:18.003653: step 23080, loss = 2.03 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:19.200585: step 23090, loss = 2.26 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:20.403735: step 23100, loss = 2.10 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:21.601301: step 23110, loss = 2.08 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:22.816869: step 23120, loss = 2.18 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:24.022802: step 23130, loss = 2.28 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:35:25.244851: step 23140, loss = 2.21 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:26.458584: step 23150, loss = 2.09 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:35:27.670032: step 23160, loss = 2.06 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:35:28.881482: step 23170, loss = 2.16 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:35:30.086281: step 23180, loss = 2.10 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:31.329526: step 23190, loss = 2.07 (1029.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:35:32.545417: step 23200, loss = 2.15 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:33.766272: step 23210, loss = 2.10 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:34.988784: step 23220, loss = 2.08 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:36.232860: step 23230, loss = 1.94 (1028.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:35:37.433730: step 23240, loss = 1.99 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:38.659219: step 23250, loss = 2.21 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:35:39.864334: step 23260, loss = 2.06 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:35:41.098493: step 23270, loss = 2.10 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:35:42.304122: step 23280, loss = 2.10 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:35:43.518733: step 23290, loss = 2.10 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:35:44.741093: step 23300, loss = 2.11 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:45.962404: step 23310, loss = 1.99 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:47.151084: step 23320, loss = 2.05 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:35:48.387313: step 23330, loss = 2.28 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:35:49.617061: step 23340, loss = 1.95 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:35:50.812432: step 23350, loss = 1.91 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:52.043748: step 23360, loss = 2.12 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:35:53.231826: step 23370, loss = 1.90 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:35:54.460680: step 23380, loss = 2.05 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:35:55.673116: step 23390, loss = 2.08 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:35:56.911834: step 23400, loss = 1.95 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:35:58.115890: step 23410, loss = 2.01 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:59.338832: step 23420, loss = 2.04 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:00.539443: step 23430, loss = 2.29 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:36:01.736554: step 23440, loss = 2.15 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:36:02.978794: step 23450, loss = 2.03 (1030.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:36:04.195666: step 23460, loss = 2.10 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:05.402125: step 23470, loss = 1.95 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:36:06.606591: step 23480, loss = 2.01 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:36:07.849508: step 23490, loss = 2.05 (1029.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:36:09.052591: step 23500, loss = 2.10 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:36:10.281538: step 23510, loss = 2.09 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:36:11.503714: step 23520, loss = 2.21 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:12.740867: step 23530, loss = 2.22 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:36:13.930340: step 23540, loss = 2.00 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:36:15.159588: step 23550, loss = 2.02 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:36:16.356905: step 23560, loss = 2.00 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:36:17.541449: step 23570, loss = 2.21 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:36:18.765968: step 23580, loss = 2.36 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:19.996385: step 23590, loss = 2.07 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:36:21.231138: step 23600, loss = 2.12 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:36:22.420695: step 23610, loss = 2.06 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:36:23.656367: step 23620, loss = 2.13 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:36:24.879727: step 23630, loss = 2.19 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:26.080078: step 23640, loss = 2.05 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:36:27.324038: step 23650, loss = 2.20 (1029.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:36:28.541152: step 23660, loss = 1.99 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:29.749686: step 23670, loss = 2.08 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:36:30.934941: step 23680, loss = 1.88 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:36:32.163654: step 23690, loss = 2.06 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:36:33.378544: step 23700, loss = 2.14 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:36:34.576066: step 23710, loss = 2.11 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:36:35.818039: step 23720, loss = 1.99 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:36:37.053792: step 23730, loss = 2.17 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:36:38.246762: step 23740, loss = 1.93 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:36:39.464801: step 23750, loss = 1.97 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:40.794554: step 23760, loss = 2.06 (962.6 examples/sec; 0.133 sec/batch)
2017-05-04 22:36:41.866034: step 23770, loss = 1.96 (1194.6 examples/sec; 0.107 sec/batch)
2017-05-04 22:36:43.085157: step 23780, loss = 2.30 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:44.309291: step 23790, loss = 2.13 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:45.523124: step 23800, loss = 2.12 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:36:46.714092: step 23810, loss = 2.13 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:36:47.922445: step 23820, loss = 2.01 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:36:49.114477: step 23830, loss = 2.03 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:36:50.290699: step 23840, loss = 2.15 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:36:51.473219: step 23850, loss = 2.13 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:36:52.644572: step 23860, loss = 2.07 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:36:53.794350: step 23870, loss = 1.98 (1113.3 examples/sec; 0.115 sec/batch)
2017-05-04 22:36:54.993161: step 23880, loss = 1.99 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:36:56.166784: step 23890, loss = 2.09 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:36:57.338944: step 23900, loss = 2.21 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:36:58.513163: step 23910, loss = 2.10 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:36:59.678891: step 23920, loss = 2.00 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:00.844882: step 23930, loss = 1.97 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:02.021384: step 23940, loss = 2.19 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:03.187520: step 23950, loss = 2.12 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:04.344467: step 23960, loss = 1.94 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:37:05.521498: step 23970, loss = 2.06 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:06.677978: step 23980, loss = 2.00 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:37:07.870939: step 23990, loss = 2.11 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:37:09.029011: step 24000, loss = 2.02 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:37:10.179487: step 24010, loss = 2.06 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-04 22:37:11.361724: step 24020, loss = 2.03 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:12.529662: step 24030, loss = 1.92 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:13.695519: step 24040, loss = 2.10 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:14.868784: step 24050, loss = 2.22 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:16.038325: step 24060, loss = 2.25 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:17.220660: step 24070, loss = 2.00 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:18.372500: step 24080, loss = 2.10 (1111.3 examples/sec; 0.115 sec/batch)
2017-05-04 22:37:19.551586: step 24090, loss = 2.04 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:20.720946: step 24100, loss = 2.08 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:21.888038: step 24110, loss = 2.00 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:23.070477: step 24120, loss = 1.97 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:24.241044: step 24130, loss = 2.15 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:25.391322: step 24140, loss = 1.97 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-04 22:37:26.558155: step 24150, loss = 2.11 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:27.748123: step 24160, loss = 2.04 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:37:28.902852: step 24170, loss = 2.03 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-04 22:37:30.064889: step 24180, loss = 1.97 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:37:31.234012: step 24190, loss = 2.18 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:32.402916: step 24200, loss = 2.02 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:33.592903: step 24210, loss = 1.91 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:37:34.762424: step 24220, loss = 2.04 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:35.956956: step 24230, loss = 2.01 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:37:37.138795: step 24240, loss = 2.13 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:38.289568: step 24250, loss = 1.94 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-04 22:37:39.457312: step 24260, loss = 2.08 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:40.631384: step 24270, loss = 2.00 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:41.797785: step 24280, loss = 2.20 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:43.015543: step 24290, loss = 2.04 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:37:44.199079: step 24300, loss = 2.06 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:45.405002: step 24310, loss = 2.03 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:37:46.615583: step 24320, loss = 2.03 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:37:47.819335: step 24330, loss = 2.12 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:37:49.015813: step 24340, loss = 1.93 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:37:50.210404: step 24350, loss = 2.05 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:37:51.393528: step 24360, loss = 2.02 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:52.569956: step 24370, loss = 1.97 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:53.733784: step 24380, loss = 2.05 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:37:54.898388: step 24390, loss = 2.13 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:37:56.059161: step 24400, loss = 2.07 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:37:57.232486: step 24410, loss = 2.07 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:58.395006: step 24420, loss = 2.09 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:37:59.551019: step 24430, loss = 2.07 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:38:00.710436: step 24440, loss = 2.04 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:38:01.862168: step 24450, loss = 2.02 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-04 22:38:03.041119: step 24460, loss = 2.19 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:04.198335: step 24470, loss = 2.19 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:38:05.368652: step 24480, loss = 2.11 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:06.543623: step 24490, loss = 2.10 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:07.700193: step 24500, loss = 2.10 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:38:08.888577: step 24510, loss = 2.10 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:10.056888: step 24520, loss = 2.24 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:11.245213: step 24530, loss = 1.98 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:12.408683: step 24540, loss = 2.12 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:38:13.605507: step 24550, loss = 1.93 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:38:14.784291: step 24560, loss = 2.12 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:15.969856: step 24570, loss = 2.17 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:17.159796: step 24580, loss = 2.08 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:18.325881: step 24590, loss = 2.12 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:19.513632: step 24600, loss = 2.03 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:20.710582: step 24610, loss = 2.27 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:38:21.898850: step 24620, loss = 1.98 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:23.086435: step 24630, loss = 2.11 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:24.254986: step 24640, loss = 2.16 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:25.427695: step 24650, loss = 2.07 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:26.587862: step 24660, loss = 2.12 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:38:27.770034: step 24670, loss = 2.14 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:28.943536: step 24680, loss = 2.04 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:30.115495: step 24690, loss = 2.09 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:31.281295: step 24700, loss = 2.09 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:32.465996: step 24710, loss = 2.07 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:33.641276: step 24720, loss = 1.87 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:34.811809: step 24730, loss = 2.13 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:35.996379: step 24740, loss = 2.09 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:37.280735: step 24750, loss = 2.19 (996.6 examples/sec; 0.128 sec/batch)
2017-05-04 22:38:38.372423: step 24760, loss = 2.12 (1172.5 examples/sec; 0.109 sec/batch)
2017-05-04 22:38:39.526447: step 24770, loss = 2.17 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-04 22:38:40.713077: step 24780, loss = 2.16 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:41.874417: step 24790, loss = 2.02 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:38:43.053930: step 24800, loss = 2.13 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:44.225812: step 24810, loss = 1.94 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:45.408607: step 24820, loss = 2.08 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:46.579358: step 24830, loss = 2.17 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:47.755925: step 24840, loss = 1.94 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:48.931125: step 24850, loss = 2.07 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:50.110290: step 24860, loss = 1.97 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:51.293344: step 24870, loss = 2.02 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:52.459871: step 24880, loss = 2.01 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:53.660069: step 24890, loss = 2.05 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:38:54.874374: step 24900, loss = 2.17 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:38:56.069510: step 24910, loss = 2.20 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:38:57.272820: step 24920, loss = 2.10 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:38:58.454952: step 24930, loss = 2.20 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:59.662405: step 24940, loss = 2.13 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:00.862557: step 24950, loss = 2.13 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:02.055346: step 24960, loss = 2.08 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:39:03.273292: step 24970, loss = 2.05 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:39:04.484580: step 24980, loss = 2.11 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:05.659975: step 24990, loss = 2.15 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:39:06.853001: step 25000, loss = 2.16 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:39:08.060646: step 25010, loss = 1.95 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:09.253337: step 25020, loss = 2.01 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:39:10.443490: step 25030, loss = 1.93 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:39:11.638643: step 25040, loss = 2.11 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:12.836907: step 25050, loss = 2.13 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:14.019704: step 25060, loss = 2.01 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:39:15.199338: step 25070, loss = 2.00 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:39:16.380403: step 25080, loss = 2.02 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:39:17.571944: step 25090, loss = 2.09 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:39:18.765064: step 25100, loss = 2.17 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:39:19.963226: step 25110, loss = 2.09 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:21.191050: step 25120, loss = 1.95 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:39:22.382376: step 25130, loss = 2.06 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:39:23.592077: step 25140, loss = 2.26 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:24.789664: step 25150, loss = 2.21 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:25.990268: step 25160, loss = 2.15 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:27.205435: step 25170, loss = 1.96 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:39:28.407085: step 25180, loss = 2.14 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:29.590177: step 25190, loss = 2.06 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:39:30.788103: step 25200, loss = 2.00 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:31.973703: step 25210, loss = 2.11 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:39:33.185421: step 25220, loss = 2.10 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:34.390221: step 25230, loss = 2.10 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:35.581688: step 25240, loss = 1.98 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:39:36.791040: step 25250, loss = 2.02 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:37.998907: step 25260, loss = 2.07 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:39.219545: step 25270, loss = 1.94 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:39:40.435663: step 25280, loss = 2.02 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:39:41.641537: step 25290, loss = 2.09 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:42.846885: step 25300, loss = 2.13 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:44.066354: step 25310, loss = 2.01 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:39:45.277379: step 25320, loss = 1.96 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:46.488219: step 25330, loss = 1.91 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:47.725563: step 25340, loss = 2.07 (1034.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:39:48.947327: step 25350, loss = 2.07 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:39:50.172537: step 25360, loss = 2.09 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:39:51.384807: step 25370, loss = 2.07 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:52.599644: step 25380, loss = 2.00 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:53.824872: step 25390, loss = 1.94 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:39:55.054434: step 25400, loss = 1.95 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:39:56.270364: step 25410, loss = 2.07 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:39:57.493862: step 25420, loss = 2.14 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:39:58.724199: step 25430, loss = 2.09 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:39:59.950921: step 25440, loss = 1.93 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:40:01.158868: step 25450, loss = 2.11 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:02.376362: step 25460, loss = 2.11 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:03.581844: step 25470, loss = 2.12 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:04.787315: step 25480, loss = 2.17 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:06.007924: step 25490, loss = 2.14 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:07.224100: step 25500, loss = 2.00 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:08.440073: step 25510, loss = 2.10 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:09.661812: step 25520, loss = 2.03 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:10.873194: step 25530, loss = 2.01 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:12.100401: step 25540, loss = 1.97 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:40:13.270120: step 25550, loss = 2.10 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:40:14.484633: step 25560, loss = 2.03 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:15.678033: step 25570, loss = 1.96 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:40:16.869789: step 25580, loss = 2.10 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:40:18.051378: step 25590, loss = 2.09 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:40:19.263192: step 25600, loss = 2.21 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:20.470557: step 25610, loss = 2.05 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:21.698175: step 25620, loss = 1.98 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:40:22.927749: step 25630, loss = 1.91 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:40:24.154992: step 25640, loss = 2.00 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:40:25.383442: step 25650, loss = 2.12 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:40:26.577607: step 25660, loss = 2.06 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:40:27.803109: step 25670, loss = 2.24 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:40:29.040324: step 25680, loss = 2.03 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:40:30.249682: step 25690, loss = 2.23 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:31.453011: step 25700, loss = 2.00 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:40:32.680778: step 25710, loss = 1.84 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:40:33.898731: step 25720, loss = 2.07 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:35.104368: step 25730, loss = 1.96 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:36.432163: step 25740, loss = 2.01 (964.0 examples/sec; 0.133 sec/batch)
2017-05-04 22:40:37.535858: step 25750, loss = 2.00 (1159.7 examples/sec; 0.110 sec/batch)
2017-05-04 22:40:38.739083: step 25760, loss = 2.04 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:40:39.959975: step 25770, loss = 2.06 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:41.181357: step 25780, loss = 2.22 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:42.383215: step 25790, loss = 2.18 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:40:43.611466: step 25800, loss = 2.09 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:40:44.833901: step 25810, loss = 2.06 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:46.039727: step 25820, loss = 2.12 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:47.252824: step 25830, loss = 2.14 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:48.462052: step 25840, loss = 2.07 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:49.666036: step 25850, loss = 2.26 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:40:50.873816: step 25860, loss = 2.08 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:52.104513: step 25870, loss = 2.15 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:40:53.319962: step 25880, loss = 2.17 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:54.513834: step 25890, loss = 1.98 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:40:55.730950: step 25900, loss = 2.01 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:56.953091: step 25910, loss = 1.90 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:58.165602: step 25920, loss = 2.07 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:59.387630: step 25930, loss = 2.08 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:41:00.594083: step 25940, loss = 2.27 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:41:01.787975: step 25950, loss = 2.02 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:41:03.009458: step 25960, loss = 2.14 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:41:04.203595: step 25970, loss = 2.05 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:41:05.402760: step 25980, loss = 2.20 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:41:06.574943: step 25990, loss = 2.14 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:07.756874: step 26000, loss = 1.85 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:08.957416: step 26010, loss = 1.96 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:41:10.140107: step 26020, loss = 2.05 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:11.323352: step 26030, loss = 2.12 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:12.506299: step 26040, loss = 2.03 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:13.685142: step 26050, loss = 2.19 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:14.862718: step 26060, loss = 1.99 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:16.022624: step 26070, loss = 2.05 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:17.202121: step 26080, loss = 2.02 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:18.359062: step 26090, loss = 1.76 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:19.534515: step 26100, loss = 2.20 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:20.710010: step 26110, loss = 2.16 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:21.859010: step 26120, loss = 2.12 (1114.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:41:23.029190: step 26130, loss = 2.14 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:24.181395: step 26140, loss = 2.14 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-04 22:41:25.340592: step 26150, loss = 2.04 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:26.500986: step 26160, loss = 1.94 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:27.674531: step 26170, loss = 2.04 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:28.853796: step 26180, loss = 1.96 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:30.023344: step 26190, loss = 2.04 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:31.204879: step 26200, loss = 2.08 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:32.364796: step 26210, loss = 2.04 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:33.532263: step 26220, loss = 1.97 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:34.693185: step 26230, loss = 1.96 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:35.851859: step 26240, loss = 1.99 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:37.024728: step 26250, loss = 2.06 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:38.189933: step 26260, loss = 2.12 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:39.349879: step 26270, loss = 2.04 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:40.508584: step 26280, loss = 2.31 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:41.661277: step 26290, loss = 2.04 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-04 22:41:42.833673: step 26300, loss = 2.15 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:44.013624: step 26310, loss = 2.09 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:45.186804: step 26320, loss = 2.12 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:46.334586: step 26330, loss = 2.21 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-04 22:41:47.501125: step 26340, loss = 2.05 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:48.652077: step 26350, loss = 1.83 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-04 22:41:49.810706: step 26360, loss = 2.08 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:51.003450: step 26370, loss = 2.06 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:41:52.205987: step 26380, loss = 2.10 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:41:53.390887: step 26390, loss = 1.75 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:54.575526: step 26400, loss = 2.01 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:55.755468: step 26410, loss = 2.13 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:56.930035: step 26420, loss = 2.16 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:58.085613: step 26430, loss = 2.00 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:59.253974: step 26440, loss = 2.13 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:42:00.449350: step 26450, loss = 2.12 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:42:01.611924: step 26460, loss = 2.01 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:42:02.803280: step 26470, loss = 2.20 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:42:03.964207: step 26480, loss = 2.02 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:42:05.137057: step 26490, loss = 2.05 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:42:06.315475: step 26500, loss = 2.04 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:07.487972: step 26510, loss = 1.92 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:42:08.647576: step 26520, loss = 1.94 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:42:09.812445: step 26530, loss = 2.02 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:42:10.995657: step 26540, loss = 1.98 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:12.167835: step 26550, loss = 2.01 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:42:13.318613: step 26560, loss = 2.07 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-04 22:42:14.475422: step 26570, loss = 1.91 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:42:15.635511: step 26580, loss = 1.95 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:42:16.803862: step 26590, loss = 2.07 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:42:17.968085: step 26600, loss = 2.11 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:42:19.152021: step 26610, loss = 2.14 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:20.313844: step 26620, loss = 2.08 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:42:21.477431: step 26630, loss = 2.16 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:42:22.653556: step 26640, loss = 2.02 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:23.836416: step 26650, loss = 2.04 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:25.003465: step 26660, loss = 2.10 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:42:26.184667: step 26670, loss = 2.30 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:27.376936: step 26680, loss = 2.06 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:42:28.550591: step 26690, loss = 2.03 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:42:29.729781: step 26700, loss = 2.03 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:30.909927: step 26710, loss = 2.01 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:32.101052: step 26720, loss = 2.13 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:42:33.405391: step 26730, loss = 2.12 (981.3 examples/sec; 0.130 sec/batch)
2017-05-04 22:42:34.506277: step 26740, loss = 2.01 (1162.7 examples/sec; 0.110 sec/batch)
2017-05-04 22:42:35.720941: step 26750, loss = 2.02 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:42:36.953266: step 26760, loss = 2.14 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:42:38.163601: step 26770, loss = 2.13 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:42:39.390975: step 26780, loss = 2.06 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:42:40.619041: step 26790, loss = 2.17 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:42:41.813374: step 26800, loss = 2.09 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:42:43.023525: step 26810, loss = 1.97 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:42:44.237174: step 26820, loss = 1.88 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:42:45.432325: step 26830, loss = 2.08 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:42:46.631677: step 26840, loss = 2.17 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:42:47.837607: step 26850, loss = 2.13 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:42:49.048553: step 26860, loss = 1.92 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:42:50.234196: step 26870, loss = 2.04 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:42:51.425796: step 26880, loss = 2.18 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:42:52.614625: step 26890, loss = 2.00 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:42:53.776375: step 26900, loss = 2.14 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:42:54.961124: step 26910, loss = 2.07 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:56.150202: step 26920, loss = 1.91 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:42:57.325761: step 26930, loss = 1.91 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:58.502485: step 26940, loss = 2.01 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:59.683832: step 26950, loss = 2.64 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:00.882606: step 26960, loss = 2.28 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:43:02.075409: step 26970, loss = 1.94 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:43:03.282473: step 26980, loss = 2.03 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:43:04.470688: step 26990, loss = 1.99 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:43:05.650679: step 27000, loss = 2.12 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:06.842318: step 27010, loss = 2.00 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:43:08.027398: step 27020, loss = 2.20 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:43:09.223760: step 27030, loss = 2.15 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:43:10.398192: step 27040, loss = 2.14 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:11.581330: step 27050, loss = 1.97 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:12.768820: step 27060, loss = 1.88 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:43:13.935196: step 27070, loss = 2.07 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:15.120600: step 27080, loss = 1.99 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:43:16.314026: step 27090, loss = 2.16 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:43:17.465105: step 27100, loss = 2.10 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:43:18.636158: step 27110, loss = 2.07 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:19.818917: step 27120, loss = 1.98 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:20.995427: step 27130, loss = 1.97 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:22.135734: step 27140, loss = 1.95 (1122.5 examples/sec; 0.114 sec/batch)
2017-05-04 22:43:23.312656: step 27150, loss = 2.20 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:24.478164: step 27160, loss = 1.97 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:25.618818: step 27170, loss = 1.96 (1122.2 examples/sec; 0.114 sec/batch)
2017-05-04 22:43:26.786464: step 27180, loss = 2.01 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:27.969854: step 27190, loss = 2.03 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:29.143531: step 27200, loss = 2.18 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:30.312522: step 27210, loss = 2.22 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:31.489466: step 27220, loss = 2.18 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:32.667129: step 27230, loss = 2.07 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:33.842468: step 27240, loss = 2.07 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:35.011979: step 27250, loss = 2.20 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:36.168183: step 27260, loss = 2.19 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:43:37.314680: step 27270, loss = 2.09 (1116.4 examples/sec; 0.115 sec/batch)
2017-05-04 22:43:38.471069: step 27280, loss = 2.08 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:43:39.623274: step 27290, loss = 2.10 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:43:40.793957: step 27300, loss = 2.09 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:41.962653: step 27310, loss = 2.16 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:43.138958: step 27320, loss = 2.04 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:44.333650: step 27330, loss = 2.03 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:43:45.500923: step 27340, loss = 2.10 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:46.642051: step 27350, loss = 1.99 (1121.7 examples/sec; 0.114 sec/batch)
2017-05-04 22:43:47.820159: step 27360, loss = 2.01 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:48.991508: step 27370, loss = 2.04 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:50.157184: step 27380, loss = 2.04 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:51.335307: step 27390, loss = 2.05 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:52.504522: step 27400, loss = 2.14 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:53.700404: step 27410, loss = 2.08 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:43:54.882997: step 27420, loss = 1.99 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:56.059850: step 27430, loss = 2.03 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:57.256687: step 27440, loss = 2.04 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:43:58.435431: step 27450, loss = 2.25 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:59.629801: step 27460, loss = 1.95 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:44:00.798121: step 27470, loss = 2.23 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:01.948186: step 27480, loss = 1.96 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:44:03.100357: step 27490, loss = 2.17 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:44:04.272975: step 27500, loss = 1.86 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:05.426168: step 27510, loss = 2.20 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:44:06.568889: step 27520, loss = 2.08 (1120.1 examples/sec; 0.114 sec/batch)
2017-05-04 22:44:07.742461: step 27530, loss = 2.29 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:08.917559: step 27540, loss = 2.08 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:10.071033: step 27550, loss = 2.19 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-04 22:44:11.259794: step 27560, loss = 1.82 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:44:12.436044: step 27570, loss = 2.13 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:13.589571: step 27580, loss = 2.22 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-04 22:44:14.771721: step 27590, loss = 2.06 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:15.920789: step 27600, loss = 2.10 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-04 22:44:17.089604: step 27610, loss = 2.32 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:18.263089: step 27620, loss = 2.12 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:19.419303: step 27630, loss = 2.17 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:20.586144: step 27640, loss = 2.08 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:21.749763: step 27650, loss = 2.10 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:22.931852: step 27660, loss = 2.01 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:24.094487: step 27670, loss = 2.11 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:25.258342: step 27680, loss = 2.08 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:26.439698: step 27690, loss = 2.07 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:27.636042: step 27700, loss = 2.10 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:44:28.814934: step 27710, loss = 2.01 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:30.082677: step 27720, loss = 2.08 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-04 22:44:31.152032: step 27730, loss = 2.09 (1197.0 examples/sec; 0.107 sec/batch)
2017-05-04 22:44:32.314448: step 27740, loss = 2.12 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:33.478456: step 27750, loss = 2.14 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:34.650745: step 27760, loss = 1.93 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:35.841052: step 27770, loss = 2.34 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:44:37.012528: step 27780, loss = 2.10 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:38.198049: step 27790, loss = 2.11 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:44:39.391100: step 27800, loss = 2.35 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:44:40.604630: step 27810, loss = 2.08 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:44:41.792169: step 27820, loss = 1.88 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:44:43.020089: step 27830, loss = 2.14 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:44:44.222074: step 27840, loss = 2.10 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:44:45.414081: step 27850, loss = 1.90 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:44:46.591301: step 27860, loss = 2.23 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:47.777619: step 27870, loss = 2.04 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:44:48.957025: step 27880, loss = 2.17 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:50.112466: step 27890, loss = 2.17 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:51.263953: step 27900, loss = 2.02 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-04 22:44:52.436401: step 27910, loss = 2.06 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:53.591200: step 27920, loss = 2.03 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-04 22:44:54.771574: step 27930, loss = 1.97 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:55.939869: step 27940, loss = 2.15 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:57.105842: step 27950, loss = 2.30 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:58.265925: step 27960, loss = 2.22 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:59.425910: step 27970, loss = 1.97 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:00.592920: step 27980, loss = 2.12 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:01.751065: step 27990, loss = 2.13 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:02.942205: step 28000, loss = 2.08 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:45:04.104456: step 28010, loss = 2.20 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:05.285473: step 28020, loss = 2.05 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:06.430047: step 28030, loss = 2.08 (1118.3 examples/sec; 0.114 sec/batch)
2017-05-04 22:45:07.605554: step 28040, loss = 1.95 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:08.796002: step 28050, loss = 2.13 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:45:09.949678: step 28060, loss = 1.98 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-04 22:45:11.106897: step 28070, loss = 2.11 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:12.273996: step 28080, loss = 1.95 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:13.441844: step 28090, loss = 2.09 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:14.598019: step 28100, loss = 1.96 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:15.771514: step 28110, loss = 2.11 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:16.917519: step 28120, loss = 2.07 (1116.9 examples/sec; 0.115 sec/batch)
2017-05-04 22:45:18.074768: step 28130, loss = 2.00 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:19.257167: step 28140, loss = 2.06 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:20.438647: step 28150, loss = 2.11 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:21.616291: step 28160, loss = 2.02 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:22.811817: step 28170, loss = 1.94 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:45:24.004406: step 28180, loss = 1.96 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:45:25.181565: step 28190, loss = 1.98 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:26.349970: step 28200, loss = 2.00 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:27.534916: step 28210, loss = 1.99 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:28.718202: step 28220, loss = 2.15 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:29.894016: step 28230, loss = 1.96 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:31.072405: step 28240, loss = 2.05 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:32.251335: step 28250, loss = 2.20 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:33.413442: step 28260, loss = 1.92 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:34.597460: step 28270, loss = 2.00 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:35.777120: step 28280, loss = 2.02 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:36.942974: step 28290, loss = 1.99 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:38.091729: step 28300, loss = 2.05 (1114.2 examples/sec; 0.115 sec/batch)
2017-05-04 22:45:39.262016: step 28310, loss = 2.05 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:40.410262: step 28320, loss = 2.16 (1114.8 examples/sec; 0.115 sec/batch)
2017-05-04 22:45:41.566345: step 28330, loss = 2.06 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:42.724917: step 28340, loss = 1.99 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:43.912044: step 28350, loss = 1.85 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:45:45.074989: step 28360, loss = 1.91 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:46.234720: step 28370, loss = 2.05 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:47.405704: step 28380, loss = 2.01 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:48.586015: step 28390, loss = 2.05 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:49.739926: step 28400, loss = 2.00 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-04 22:45:50.921017: step 28410, loss = 2.06 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:52.098718: step 28420, loss = 1.95 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:53.259211: step 28430, loss = 2.28 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:54.432307: step 28440, loss = 2.00 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:55.616714: step 28450, loss = 1.98 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:56.816200: step 28460, loss = 2.13 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:45:57.986194: step 28470, loss = 2.14 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:59.179630: step 28480, loss = 2.06 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:46:00.356650: step 28490, loss = 2.31 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:01.509598: step 28500, loss = 1.93 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-04 22:46:02.665436: step 28510, loss = 2.16 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:03.840895: step 28520, loss = 2.17 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:05.015648: step 28530, loss = 2.06 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:06.174185: step 28540, loss = 1.98 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:07.359502: step 28550, loss = 2.06 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:46:08.539474: step 28560, loss = 2.04 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:09.693157: step 28570, loss = 1.94 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-04 22:46:10.842320: step 28580, loss = 2.09 (1113.8 examples/sec; 0.115 sec/batch)
2017-05-04 22:46:12.007668: step 28590, loss = 2.15 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:13.188552: step 28600, loss = 2.02 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:14.346716: step 28610, loss = 2.09 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:15.531056: step 28620, loss = 2.12 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:16.708675: step 28630, loss = 2.10 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:17.865164: step 28640, loss = 2.17 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:19.032715: step 28650, loss = 2.05 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:20.200572: step 28660, loss = 2.14 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:21.357194: step 28670, loss = 2.13 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:22.532817: step 28680, loss = 2.05 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:23.702360: step 28690, loss = 1.89 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:24.864247: step 28700, loss = 2.05 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:26.124937: step 28710, loss = 2.06 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-04 22:46:27.193309: step 28720, loss = 2.08 (1198.1 examples/sec; 0.107 sec/batch)
2017-05-04 22:46:28.354379: step 28730, loss = 2.23 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:29.524829: step 28740, loss = 2.01 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:30.687662: step 28750, loss = 1.97 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:31.868709: step 28760, loss = 1.99 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:33.040604: step 28770, loss = 2.08 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:34.223150: step 28780, loss = 2.13 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:35.436684: step 28790, loss = 1.97 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:46:36.627342: step 28800, loss = 2.14 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:46:37.807811: step 28810, loss = 2.01 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:38.983177: step 28820, loss = 1.99 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:40.177027: step 28830, loss = 2.11 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:46:41.367924: step 28840, loss = 1.92 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:46:42.548659: step 28850, loss = 2.18 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:43.717850: step 28860, loss = 1.96 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:44.885091: step 28870, loss = 1.99 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:46.043563: step 28880, loss = 2.15 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:47.213509: step 28890, loss = 2.00 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:48.383812: step 28900, loss = 2.09 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:49.552022: step 28910, loss = 2.13 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:50.708990: step 28920, loss = 1.97 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:51.885212: step 28930, loss = 1.99 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:53.062903: step 28940, loss = 2.08 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:54.221524: step 28950, loss = 1.93 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:55.399852: step 28960, loss = 2.16 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:56.557965: step 28970, loss = 2.06 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:57.713558: step 28980, loss = 1.88 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:58.886292: step 28990, loss = 1.97 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:00.034331: step 29000, loss = 2.17 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-04 22:47:01.206310: step 29010, loss = 1.97 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:02.362769: step 29020, loss = 2.01 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:47:03.538493: step 29030, loss = 2.08 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:04.715154: step 29040, loss = 1.98 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:05.878393: step 29050, loss = 1.98 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:47:07.072725: step 29060, loss = 2.12 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:08.241203: step 29070, loss = 2.05 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:09.392170: step 29080, loss = 1.72 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-04 22:47:10.560999: step 29090, loss = 2.07 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:11.765559: step 29100, loss = 1.98 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:47:12.962314: step 29110, loss = 2.13 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:47:14.163680: step 29120, loss = 1.95 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:47:15.385351: step 29130, loss = 2.09 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:47:16.589165: step 29140, loss = 2.13 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:47:17.767778: step 29150, loss = 2.03 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:18.949979: step 29160, loss = 1.90 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:20.112730: step 29170, loss = 2.12 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:47:21.294881: step 29180, loss = 1.94 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:22.468629: step 29190, loss = 1.93 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:23.670485: step 29200, loss = 2.14 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:47:24.855902: step 29210, loss = 2.15 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:26.037929: step 29220, loss = 2.12 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:27.239132: step 29230, loss = 1.91 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:47:28.413129: step 29240, loss = 1.98 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:29.587630: step 29250, loss = 2.18 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:30.779741: step 29260, loss = 1.99 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:31.976265: step 29270, loss = 2.09 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:47:33.164925: step 29280, loss = 2.08 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:34.336871: step 29290, loss = 2.04 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:35.525123: step 29300, loss = 2.19 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:36.698210: step 29310, loss = 2.03 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:37.873289: step 29320, loss = 2.11 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:39.060388: step 29330, loss = 2.06 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:40.233777: step 29340, loss = 1.95 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:41.420666: step 29350, loss = 2.21 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:42.587005: step 29360, loss = 2.17 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:43.757618: step 29370, loss = 2.07 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:44.926207: step 29380, loss = 2.01 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:46.093876: step 29390, loss = 2.27 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:47.284730: step 29400, loss = 2.06 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:48.467286: step 29410, loss = 2.03 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:49.635410: step 29420, loss = 2.06 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:50.799570: step 29430, loss = 2.01 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:47:51.971696: step 29440, loss = 1.99 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:53.134292: step 29450, loss = 2.02 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:47:54.311053: step 29460, loss = 1.83 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:55.503573: step 29470, loss = 2.22 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:56.657157: step 29480, loss = 2.05 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-04 22:47:57.823715: step 29490, loss = 1.94 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:59.019358: step 29500, loss = 2.27 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:48:00.193892: step 29510, loss = 2.10 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:01.376142: step 29520, loss = 2.15 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:02.538445: step 29530, loss = 1.98 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:48:03.714943: step 29540, loss = 2.00 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:04.905592: step 29550, loss = 1.99 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:48:06.087524: step 29560, loss = 2.13 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:07.273425: step 29570, loss = 1.95 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:48:08.471908: step 29580, loss = 2.08 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:48:09.670183: step 29590, loss = 2.17 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:48:10.875554: step 29600, loss = 1.98 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:48:12.082569: step 29610, loss = 2.13 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:48:13.269328: step 29620, loss = 1.89 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:48:14.465140: step 29630, loss = 1.87 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:48:15.621000: step 29640, loss = 2.09 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:48:16.772739: step 29650, loss = 2.06 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-04 22:48:17.929457: step 29660, loss = 2.01 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:48:19.104626: step 29670, loss = 1.97 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:20.289215: step 29680, loss = 2.10 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:21.448822: step 29690, loss = 2.08 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:48:22.718024: step 29700, loss = 1.94 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-04 22:48:23.793478: step 29710, loss = 2.06 (1190.2 examples/sec; 0.108 sec/batch)
2017-05-04 22:48:24.944485: step 29720, loss = 2.14 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-04 22:48:26.102732: step 29730, loss = 1.91 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:48:27.277794: step 29740, loss = 2.29 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:28.456686: step 29750, loss = 2.08 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:29.624755: step 29760, loss = 2.05 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:30.797865: step 29770, loss = 1.92 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:31.968900: step 29780, loss = 2.12 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:33.134808: step 29790, loss = 2.10 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:34.294389: step 29800, loss = 1.90 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:48:35.473917: step 29810, loss = 2.07 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:36.648596: step 29820, loss = 1.98 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:37.816551: step 29830, loss = 2.06 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:38.969699: step 29840, loss = 2.00 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:48:40.122938: step 29850, loss = 2.26 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-04 22:48:41.300671: step 29860, loss = 2.17 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:42.480024: step 29870, loss = 2.04 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:43.652991: step 29880, loss = 2.05 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:44.846225: step 29890, loss = 1.96 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:48:45.994498: step 29900, loss = 2.04 (1114.7 examples/sec; 0.115 sec/batch)
2017-05-04 22:48:47.179587: step 29910, loss = 1.97 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:48:48.374559: step 29920, loss = 1.97 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:48:49.556950: step 29930, loss = 2.08 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:50.748543: step 29940, loss = 2.17 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:48:51.940094: step 29950, loss = 2.03 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:48:53.134172: step 29960, loss = 2.10 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:48:54.356200: step 29970, loss = 2.01 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:48:55.565898: step 29980, loss = 2.14 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:48:56.766710: step 29990, loss = 2.03 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:48:57.967901: step 30000, loss = 1.97 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:48:59.189208: step 30010, loss = 2.00 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:49:00.405279: step 30020, loss = 1.91 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:49:01.617725: step 30030, loss = 1.82 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:49:02.797514: step 30040, loss = 1.99 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:03.997850: step 30050, loss = 2.30 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:49:05.196851: step 30060, loss = 2.08 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:49:06.382110: step 30070, loss = 2.04 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:07.575603: step 30080, loss = 2.11 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:08.794352: step 30090, loss = 2.03 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:49:09.980749: step 30100, loss = 2.00 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:11.183536: step 30110, loss = 2.02 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:49:12.408932: step 30120, loss = 2.04 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:49:13.604952: step 30130, loss = 1.95 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:49:14.823731: step 30140, loss = 1.96 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:49:16.036044: step 30150, loss = 2.15 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:49:17.225439: step 30160, loss = 2.07 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:18.410935: step 30170, loss = 2.21 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:19.605773: step 30180, loss = 1.93 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:20.785199: step 30190, loss = 1.90 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:21.955053: step 30200, loss = 1.99 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:49:23.121878: step 30210, loss = 2.10 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:49:24.310779: step 30220, loss = 2.08 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:25.492656: step 30230, loss = 1.91 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:26.672109: step 30240, loss = 2.20 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:27.853651: step 30250, loss = 1.84 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:29.052922: step 30260, loss = 2.07 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:49:30.229184: step 30270, loss = 2.11 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:31.423839: step 30280, loss = 2.06 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:32.647560: step 30290, loss = 1.99 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:49:33.845988: step 30300, loss = 2.01 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:49:35.031485: step 30310, loss = 1.96 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:36.226303: step 30320, loss = 1.96 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:37.396001: step 30330, loss = 2.09 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:49:38.573285: step 30340, loss = 2.10 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:39.756913: step 30350, loss = 1.97 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:40.930087: step 30360, loss = 2.04 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:49:42.096786: step 30370, loss = 2.13 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:49:43.262148: step 30380, loss = 2.13 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:49:44.439335: step 30390, loss = 2.08 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:45.599348: step 30400, loss = 2.08 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:49:46.779107: step 30410, loss = 2.08 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:47.945420: step 30420, loss = 2.06 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:49:49.105795: step 30430, loss = 1.97 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:49:50.271393: step 30440, loss = 2.20 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:49:51.430154: step 30450, loss = 2.14 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:49:52.622046: step 30460, loss = 2.07 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:53.794250: step 30470, loss = 2.10 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:49:54.984820: step 30480, loss = 2.08 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:56.158864: step 30490, loss = 2.03 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:49:57.353880: step 30500, loss = 2.02 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:49:58.521127: step 30510, loss = 2.03 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:49:59.682111: step 30520, loss = 2.05 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:00.844805: step 30530, loss = 2.04 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:01.994493: step 30540, loss = 2.11 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-04 22:50:03.167908: step 30550, loss = 2.07 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:04.320845: step 30560, loss = 2.20 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-04 22:50:05.475206: step 30570, loss = 2.04 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-04 22:50:06.642953: step 30580, loss = 2.10 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:07.815927: step 30590, loss = 2.15 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:08.995415: step 30600, loss = 2.03 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:50:10.145728: step 30610, loss = 2.07 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-04 22:50:11.315454: step 30620, loss = 2.08 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:12.495499: step 30630, loss = 2.27 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:50:13.654027: step 30640, loss = 1.98 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:14.833839: step 30650, loss = 2.17 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:50:15.997042: step 30660, loss = 2.01 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:17.164885: step 30670, loss = 2.18 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:18.327334: step 30680, loss = 2.09 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:19.585158: step 30690, loss = 2.04 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-04 22:50:20.653354: step 30700, loss = 2.09 (1198.3 examples/sec; 0.107 sec/batch)
2017-05-04 22:50:21.809456: step 30710, loss = 2.17 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:22.975784: step 30720, loss = 2.06 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:24.146580: step 30730, loss = 2.08 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:25.314835: step 30740, loss = 2.11 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:26.468007: step 30750, loss = 2.09 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:50:27.627445: step 30760, loss = 2.07 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:28.813093: step 30770, loss = 2.18 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:50:29.972652: step 30780, loss = 1.98 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:31.133506: step 30790, loss = 2.03 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:32.304037: step 30800, loss = 1.97 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:33.457931: step 30810, loss = 2.22 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-04 22:50:34.625539: step 30820, loss = 1.99 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:35.794509: step 30830, loss = 2.37 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:36.964110: step 30840, loss = 2.10 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:38.130083: step 30850, loss = 2.06 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:39.300059: step 30860, loss = 2.01 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:40.472899: step 30870, loss = 2.17 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:41.627322: step 30880, loss = 2.16 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-04 22:50:42.796735: step 30890, loss = 1.96 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:43.969598: step 30900, loss = 2.21 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:45.147313: step 30910, loss = 2.00 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:50:46.314456: step 30920, loss = 2.06 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:47.472687: step 30930, loss = 2.42 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:48.638962: step 30940, loss = 1.95 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:49.784809: step 30950, loss = 2.03 (1117.1 examples/sec; 0.115 sec/batch)
2017-05-04 22:50:50.949462: step 30960, loss = 2.17 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:52.094671: step 30970, loss = 2.07 (1117.7 examples/sec; 0.115 sec/batch)
2017-05-04 22:50:53.267829: step 30980, loss = 2.05 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:54.447051: step 30990, loss = 2.06 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:50:55.611859: step 31000, loss = 1.92 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:56.769803: step 31010, loss = 2.10 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:57.927376: step 31020, loss = 1.98 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:59.079839: step 31030, loss = 2.10 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-04 22:51:00.246972: step 31040, loss = 2.00 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:01.414919: step 31050, loss = 1.88 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:02.587379: step 31060, loss = 2.00 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:03.771939: step 31070, loss = 2.01 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:04.965426: step 31080, loss = 2.08 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:06.133426: step 31090, loss = 2.18 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:07.346533: step 31100, loss = 2.16 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:51:08.541343: step 31110, loss = 2.16 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:09.731914: step 31120, loss = 1.89 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:10.931045: step 31130, loss = 1.98 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:51:12.148382: step 31140, loss = 2.09 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:51:13.337940: step 31150, loss = 2.06 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:14.530761: step 31160, loss = 2.30 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:15.718023: step 31170, loss = 1.95 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:16.901068: step 31180, loss = 1.99 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:18.083288: step 31190, loss = 2.08 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:19.268571: step 31200, loss = 2.22 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:20.466642: step 31210, loss = 2.16 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:51:21.632879: step 31220, loss = 2.12 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:22.816038: step 31230, loss = 1.99 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:24.007557: step 31240, loss = 2.04 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:25.185018: step 31250, loss = 2.04 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:26.342382: step 31260, loss = 2.03 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:51:27.511425: step 31270, loss = 2.00 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:28.704326: step 31280, loss = 2.01 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:29.852145: step 31290, loss = 2.18 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-04 22:51:31.038010: step 31300, loss = 2.12 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:32.212529: step 31310, loss = 2.15 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:33.371525: step 31320, loss = 1.94 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:51:34.556701: step 31330, loss = 2.03 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:35.728097: step 31340, loss = 2.09 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:36.916828: step 31350, loss = 2.15 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:38.103076: step 31360, loss = 2.18 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:39.277154: step 31370, loss = 2.10 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:40.458898: step 31380, loss = 1.92 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:41.629772: step 31390, loss = 2.07 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:42.793735: step 31400, loss = 1.99 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:51:43.961697: step 31410, loss = 2.05 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:45.121738: step 31420, loss = 2.21 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:51:46.299946: step 31430, loss = 2.02 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:47.473808: step 31440, loss = 2.21 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:48.660503: step 31450, loss = 2.07 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:49.844331: step 31460, loss = 2.03 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:51.028902: step 31470, loss = 1.83 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:52.190655: step 31480, loss = 1.94 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:51:53.367575: step 31490, loss = 2.04 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:54.539067: step 31500, loss = 2.05 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:55.718853: step 31510, loss = 2.05 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:56.890443: step 31520, loss = 2.04 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:58.082738: step 31530, loss = 2.11 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:59.305597: step 31540, loss = 2.01 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:00.511387: step 31550, loss = 2.11 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:01.713362: step 31560, loss = 2.09 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:52:02.945892: step 31570, loss = 1.98 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:04.171744: step 31580, loss = 2.26 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:05.392765: step 31590, loss = 2.11 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:06.595849: step 31600, loss = 1.96 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:52:07.811915: step 31610, loss = 1.95 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:09.035846: step 31620, loss = 2.01 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:10.254480: step 31630, loss = 2.09 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:11.474273: step 31640, loss = 2.07 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:12.688476: step 31650, loss = 1.91 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:13.902034: step 31660, loss = 2.22 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:15.127387: step 31670, loss = 2.08 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:16.425043: step 31680, loss = 2.00 (986.4 examples/sec; 0.130 sec/batch)
2017-05-04 22:52:17.538821: step 31690, loss = 2.04 (1149.2 examples/sec; 0.111 sec/batch)
2017-05-04 22:52:18.749017: step 31700, loss = 1.95 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:19.964339: step 31710, loss = 2.02 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:21.186347: step 31720, loss = 1.98 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:22.397071: step 31730, loss = 2.28 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:23.605217: step 31740, loss = 2.07 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:24.816834: step 31750, loss = 2.07 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:26.032059: step 31760, loss = 1.94 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:27.248635: step 31770, loss = 2.16 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:28.481763: step 31780, loss = 1.99 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:29.679567: step 31790, loss = 2.10 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:52:30.918448: step 31800, loss = 1.97 (1033.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:52:32.135556: step 31810, loss = 2.18 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:33.352052: step 31820, loss = 2.23 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:34.579660: step 31830, loss = 2.07 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:35.776325: step 31840, loss = 2.04 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:52:36.994770: step 31850, loss = 2.09 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:38.206385: step 31860, loss = 1.95 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:39.431788: step 31870, loss = 1.96 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:40.650811: step 31880, loss = 2.23 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:41.855610: step 31890, loss = 2.03 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:52:43.076300: step 31900, loss = 1.93 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:44.287923: step 31910, loss = 1.94 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:45.504218: step 31920, loss = 2.27 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:46.723657: step 31930, loss = 1.92 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:47.951774: step 31940, loss = 2.22 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:49.181718: step 31950, loss = 1.97 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:50.388422: step 31960, loss = 1.98 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:51.617692: step 31970, loss = 2.07 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:52.823098: step 31980, loss = 2.04 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:54.046911: step 31990, loss = 1.95 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:55.270932: step 32000, loss = 2.02 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:56.479305: step 32010, loss = 2.06 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:57.696946: step 32020, loss = 2.11 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:58.914381: step 32030, loss = 2.15 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:00.142509: step 32040, loss = 2.17 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:01.351579: step 32050, loss = 1.90 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:02.558642: step 32060, loss = 2.03 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:03.779826: step 32070, loss = 1.96 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:04.947402: step 32080, loss = 1.95 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:53:06.145197: step 32090, loss = 2.04 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:53:07.373180: step 32100, loss = 2.08 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:08.578335: step 32110, loss = 2.12 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:09.793008: step 32120, loss = 1.90 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:11.021712: step 32130, loss = 1.98 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:12.242509: step 32140, loss = 1.86 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:13.469340: step 32150, loss = 1.94 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:14.684292: step 32160, loss = 2.04 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:15.883390: step 32170, loss = 2.11 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:53:17.109892: step 32180, loss = 2.08 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:18.339778: step 32190, loss = 2.12 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:19.549186: step 32200, loss = 2.09 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:20.766591: step 32210, loss = 2.13 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:21.966972: step 32220, loss = 2.09 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:53:23.181616: step 32230, loss = 1.86 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:24.393778: step 32240, loss = 2.12 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:25.594191: step 32250, loss = 2.06 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:53:26.803452: step 32260, loss = 2.03 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:28.037443: step 32270, loss = 2.01 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:29.250781: step 32280, loss = 2.07 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:30.475493: step 32290, loss = 1.96 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:31.722848: step 32300, loss = 1.87 (1026.2 examples/sec; 0.125 sec/batch)
2017-05-04 22:53:32.935971: step 32310, loss = 2.10 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:34.123690: step 32320, loss = 2.13 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:53:35.346206: step 32330, loss = 2.01 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:36.579587: step 32340, loss = 2.20 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:37.800456: step 32350, loss = 2.04 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:39.008590: step 32360, loss = 1.98 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:40.241483: step 32370, loss = 2.09 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:41.444376: step 32380, loss = 2.00 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:53:42.643675: step 32390, loss = 1.96 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:53:43.858077: step 32400, loss = 2.03 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:45.068752: step 32410, loss = 1.91 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:46.282919: step 32420, loss = 2.21 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:47.511384: step 32430, loss = 2.02 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:48.737127: step 32440, loss = 1.98 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:49.934149: step 32450, loss = 2.24 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:53:51.156150: step 32460, loss = 2.00 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:52.382079: step 32470, loss = 2.06 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:53.580989: step 32480, loss = 2.14 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:53:54.803644: step 32490, loss = 2.10 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:56.014315: step 32500, loss = 1.98 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:57.226530: step 32510, loss = 2.11 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:58.409555: step 32520, loss = 2.03 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:53:59.626644: step 32530, loss = 2.11 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:00.858943: step 32540, loss = 1.87 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:02.071712: step 32550, loss = 2.25 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:03.300359: step 32560, loss = 2.08 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:04.536272: step 32570, loss = 2.13 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:54:05.740837: step 32580, loss = 2.04 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:54:06.952571: step 32590, loss = 1.99 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:08.193996: step 32600, loss = 2.17 (1031.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:54:09.409284: step 32610, loss = 2.10 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:10.632472: step 32620, loss = 2.09 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:11.865277: step 32630, loss = 2.05 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:13.065710: step 32640, loss = 2.07 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:54:14.288348: step 32650, loss = 2.15 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:15.516001: step 32660, loss = 2.08 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:16.811301: step 32670, loss = 2.06 (988.2 examples/sec; 0.130 sec/batch)
2017-05-04 22:54:17.905898: step 32680, loss = 2.01 (1169.4 examples/sec; 0.109 sec/batch)
2017-05-04 22:54:19.130948: step 32690, loss = 2.19 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:20.351301: step 32700, loss = 2.07 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:21.557144: step 32710, loss = 2.13 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:22.782249: step 32720, loss = 1.99 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:24.015661: step 32730, loss = 1.86 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:25.226714: step 32740, loss = 2.07 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:26.432903: step 32750, loss = 2.06 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:27.639043: step 32760, loss = 2.08 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:28.867781: step 32770, loss = 1.94 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:30.088694: step 32780, loss = 2.14 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:31.322731: step 32790, loss = 2.05 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:32.546260: step 32800, loss = 2.10 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:33.782612: step 32810, loss = 2.25 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:54:34.987276: step 32820, loss = 2.30 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:54:36.205289: step 32830, loss = 2.05 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:37.411162: step 32840, loss = 2.03 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:38.637751: step 32850, loss = 1.94 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:39.854314: step 32860, loss = 2.02 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:41.062029: step 32870, loss = 2.01 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:42.290615: step 32880, loss = 2.13 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:43.531180: step 32890, loss = 2.19 (1031.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:54:44.734943: step 32900, loss = 2.04 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:54:45.940963: step 32910, loss = 2.12 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:47.160904: step 32920, loss = 2.09 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:48.378161: step 32930, loss = 1.99 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:49.576859: step 32940, loss = 2.08 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:54:50.781334: step 32950, loss = 1.91 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:54:51.994697: step 32960, loss = 2.05 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:53.212582: step 32970, loss = 1.91 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:54.409320: step 32980, loss = 2.00 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:54:55.640147: step 32990, loss = 2.17 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:56.862367: step 33000, loss = 1.96 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:58.067834: step 33010, loss = 2.04 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:59.285265: step 33020, loss = 2.05 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:00.512113: step 33030, loss = 2.00 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:01.749203: step 33040, loss = 2.04 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:55:02.977314: step 33050, loss = 2.06 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:04.199689: step 33060, loss = 2.05 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:05.414933: step 33070, loss = 2.03 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:06.631487: step 33080, loss = 1.98 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:07.846554: step 33090, loss = 2.06 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:09.053108: step 33100, loss = 1.98 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:10.269940: step 33110, loss = 1.94 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:11.503041: step 33120, loss = 2.15 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:12.728613: step 33130, loss = 2.05 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:13.943663: step 33140, loss = 2.04 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:15.164709: step 33150, loss = 1.99 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:16.384335: step 33160, loss = 1.91 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:17.603671: step 33170, loss = 2.00 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:18.826996: step 33180, loss = 1.99 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:20.046561: step 33190, loss = 2.09 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:21.259295: step 33200, loss = 1.97 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:22.482431: step 33210, loss = 2.13 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:23.686469: step 33220, loss = 1.96 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:55:24.916136: step 33230, loss = 2.11 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:26.108751: step 33240, loss = 1.94 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:55:27.322190: step 33250, loss = 1.94 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:28.564113: step 33260, loss = 1.91 (1030.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:55:29.731143: step 33270, loss = 2.17 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:55:30.955383: step 33280, loss = 2.03 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:32.192999: step 33290, loss = 2.07 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:55:33.402207: step 33300, loss = 2.17 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:34.619949: step 33310, loss = 2.01 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:35.833911: step 33320, loss = 1.94 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:37.063159: step 33330, loss = 1.81 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:38.278449: step 33340, loss = 2.04 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:39.491437: step 33350, loss = 2.09 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:40.721437: step 33360, loss = 2.20 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:41.936445: step 33370, loss = 2.12 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:43.147780: step 33380, loss = 2.07 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:44.367555: step 33390, loss = 1.93 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:45.566118: step 33400, loss = 2.06 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:55:46.778449: step 33410, loss = 1.99 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:47.989406: step 33420, loss = 1.92 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:49.201340: step 33430, loss = 2.13 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:50.410705: step 33440, loss = 2.07 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:51.634760: step 33450, loss = 2.13 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:52.854672: step 33460, loss = 1.87 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:54.067504: step 33470, loss = 1.99 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:55.291426: step 33480, loss = 1.99 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:56.494728: step 33490, loss = 2.01 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:55:57.711838: step 33500, loss = 1.99 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:58.939268: step 33510, loss = 2.02 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:00.150815: step 33520, loss = 2.01 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:56:01.377032: step 33530, loss = 1.97 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:02.585345: step 33540, loss = 2.11 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:56:03.811066: step 33550, loss = 2.05 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:05.034140: step 33560, loss = 1.98 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:06.258310: step 33570, loss = 1.98 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:07.469935: step 33580, loss = 2.11 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:56:08.701496: step 33590, loss = 2.04 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:09.918883: step 33600, loss = 2.06 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:11.137745: step 33610, loss = 2.02 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:12.356003: step 33620, loss = 2.25 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:13.574110: step 33630, loss = 2.09 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:14.808160: step 33640, loss = 2.14 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:16.006596: step 33650, loss = 2.00 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:56:17.329674: step 33660, loss = 2.06 (967.4 examples/sec; 0.132 sec/batch)
2017-05-04 22:56:18.437305: step 33670, loss = 1.89 (1155.6 examples/sec; 0.111 sec/batch)
2017-05-04 22:56:19.653557: step 33680, loss = 2.19 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:20.885370: step 33690, loss = 2.19 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:22.098035: step 33700, loss = 2.02 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:56:23.330341: step 33710, loss = 2.02 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:24.561434: step 33720, loss = 2.25 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:25.783298: step 33730, loss = 1.90 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:27.020528: step 33740, loss = 2.01 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:56:28.237496: step 33750, loss = 2.03 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:29.454599: step 33760, loss = 1.83 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:30.679529: step 33770, loss = 2.06 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:31.907036: step 33780, loss = 2.14 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:33.132981: step 33790, loss = 1.89 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:34.326001: step 33800, loss = 2.16 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:56:35.548168: step 33810, loss = 1.94 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:36.765731: step 33820, loss = 2.06 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:37.981566: step 33830, loss = 2.14 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:39.200482: step 33840, loss = 2.15 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:40.435789: step 33850, loss = 2.14 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:56:41.623327: step 33860, loss = 2.07 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:56:42.827203: step 33870, loss = 2.00 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:56:44.052157: step 33880, loss = 2.00 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:45.266900: step 33890, loss = 1.96 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:56:46.468679: step 33900, loss = 2.02 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:56:47.674523: step 33910, loss = 2.20 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:56:48.882980: step 33920, loss = 1.87 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:56:50.097815: step 33930, loss = 1.85 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:56:51.314403: step 33940, loss = 2.05 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:52.559343: step 33950, loss = 1.93 (1028.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:56:53.756595: step 33960, loss = 2.14 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:56:54.974162: step 33970, loss = 2.15 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:56.200958: step 33980, loss = 1.91 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:57.425380: step 33990, loss = 2.04 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:58.645351: step 34000, loss = 2.14 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:59.861857: step 34010, loss = 1.98 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:01.081877: step 34020, loss = 1.98 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:02.273452: step 34030, loss = 2.02 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:57:03.501437: step 34040, loss = 1.96 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:04.703906: step 34050, loss = 2.09 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:57:05.888675: step 34060, loss = 2.07 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:57:07.104637: step 34070, loss = 1.89 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:08.339473: step 34080, loss = 2.06 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:09.542971: step 34090, loss = 1.91 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:57:10.757626: step 34100, loss = 1.99 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:11.984278: step 34110, loss = 2.08 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:13.216665: step 34120, loss = 2.05 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:14.433923: step 34130, loss = 1.97 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:15.657344: step 34140, loss = 2.09 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:16.880882: step 34150, loss = 2.14 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:18.083334: step 34160, loss = 1.97 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:57:19.277500: step 34170, loss = 1.97 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:57:20.501134: step 34180, loss = 1.86 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:21.708971: step 34190, loss = 2.02 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:22.925818: step 34200, loss = 2.19 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:24.143073: step 34210, loss = 2.09 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:25.369487: step 34220, loss = 2.04 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:26.582734: step 34230, loss = 2.05 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:27.816760: step 34240, loss = 1.90 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:29.058573: step 34250, loss = 2.22 (1030.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:57:30.228726: step 34260, loss = 1.84 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:57:31.449276: step 34270, loss = 2.05 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:32.670799: step 34280, loss = 2.24 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:33.875171: step 34290, loss = 2.11 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:57:35.112323: step 34300, loss = 2.06 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:57:36.317377: step 34310, loss = 1.87 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:37.526685: step 34320, loss = 2.15 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:38.746275: step 34330, loss = 2.00 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:39.986202: step 34340, loss = 2.16 (1032.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:57:41.198833: step 34350, loss = 2.27 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:42.408385: step 34360, loss = 2.14 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:43.607959: step 34370, loss = 2.17 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:57:44.826692: step 34380, loss = 2.08 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:46.045208: step 34390, loss = 2.03 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:47.267744: step 34400, loss = 2.02 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:48.486944: step 34410, loss = 1.92 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:49.697631: step 34420, loss = 2.07 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:50.927293: step 34430, loss = 2.06 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:52.139715: step 34440, loss = 2.08 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:53.374013: step 34450, loss = 1.98 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:54.537139: step 34460, loss = 2.11 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:57:55.738193: step 34470, loss = 2.11 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:57:56.969533: step 34480, loss = 1.96 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:58.196391: step 34490, loss = 2.00 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:59.420342: step 34500, loss = 2.11 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:00.629089: step 34510, loss = 2.01 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:01.836793: step 34520, loss = 1.93 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:03.087513: step 34530, loss = 2.02 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-04 22:58:04.308354: step 34540, loss = 1.95 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:05.531623: step 34550, loss = 2.00 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:06.755507: step 34560, loss = 2.05 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:07.973907: step 34570, loss = 2.03 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:09.219433: step 34580, loss = 2.06 (1027.7 examples/sec; 0.125 sec/batch)
2017-05-04 22:58:10.413542: step 34590, loss = 2.07 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:58:11.645356: step 34600, loss = 2.01 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:12.854437: step 34610, loss = 1.99 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:14.059994: step 34620, loss = 2.02 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:15.282570: step 34630, loss = 2.03 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:16.510814: step 34640, loss = 2.14 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:17.804072: step 34650, loss = 1.91 (989.8 examples/sec; 0.129 sec/batch)
2017-05-04 22:58:18.924635: step 34660, loss = 2.00 (1142.3 examples/sec; 0.112 sec/batch)
2017-05-04 22:58:20.145389: step 34670, loss = 2.09 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:21.363169: step 34680, loss = 1.97 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:22.581372: step 34690, loss = 2.07 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:23.808968: step 34700, loss = 2.09 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:25.031634: step 34710, loss = 2.04 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:26.249924: step 34720, loss = 2.04 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:27.468580: step 34730, loss = 2.14 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:28.682209: step 34740, loss = 1.94 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:29.889468: step 34750, loss = 1.99 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:31.109636: step 34760, loss = 1.97 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:32.332127: step 34770, loss = 2.05 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:33.542085: step 34780, loss = 1.91 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:34.771445: step 34790, loss = 2.11 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:35.966399: step 34800, loss = 2.12 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:58:37.189346: step 34810, loss = 2.01 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:38.399787: step 34820, loss = 2.03 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:39.603182: step 34830, loss = 2.08 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:58:40.827144: step 34840, loss = 1.94 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:42.011391: step 34850, loss = 2.07 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:58:43.241444: step 34860, loss = 1.94 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:44.461069: step 34870, loss = 1.93 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:45.675429: step 34880, loss = 1.96 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:46.887121: step 34890, loss = 1.96 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:48.117080: step 34900, loss = 1.91 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:49.330189: step 34910, loss = 2.21 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:50.544107: step 34920, loss = 2.10 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:51.793203: step 34930, loss = 1.90 (1024.7 examples/sec; 0.125 sec/batch)
2017-05-04 22:58:53.022217: step 34940, loss = 1.98 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:54.251045: step 34950, loss = 2.06 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:55.472866: step 34960, loss = 1.95 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:56.701622: step 34970, loss = 1.89 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:57.900653: step 34980, loss = 1.97 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:58:59.129447: step 34990, loss = 2.10 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:00.344031: step 35000, loss = 1.93 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:01.555667: step 35010, loss = 2.03 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:02.789917: step 35020, loss = 1.99 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:04.020978: step 35030, loss = 1.96 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:05.243268: step 35040, loss = 1.95 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:06.415163: step 35050, loss = 1.99 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:59:07.631165: step 35060, loss = 1.96 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:08.843525: step 35070, loss = 2.10 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:10.054067: step 35080, loss = 2.06 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:11.289654: step 35090, loss = 2.19 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:59:12.501271: step 35100, loss = 2.10 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:13.703627: step 35110, loss = 1.98 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:59:14.948320: step 35120, loss = 2.29 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:59:16.175133: step 35130, loss = 2.06 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:17.403653: step 35140, loss = 1.96 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:18.618793: step 35150, loss = 1.89 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:19.836512: step 35160, loss = 1.89 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:21.060741: step 35170, loss = 2.04 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:22.259759: step 35180, loss = 1.92 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:59:23.462800: step 35190, loss = 1.93 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:59:24.684059: step 35200, loss = 1.82 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:25.884128: step 35210, loss = 1.88 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:59:27.118752: step 35220, loss = 2.02 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:28.334903: step 35230, loss = 2.05 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:29.554304: step 35240, loss = 2.07 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:30.723681: step 35250, loss = 2.09 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:59:31.940921: step 35260, loss = 2.09 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:33.168315: step 35270, loss = 2.02 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:34.374300: step 35280, loss = 2.01 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:35.601007: step 35290, loss = 2.16 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:36.822509: step 35300, loss = 2.08 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:38.027864: step 35310, loss = 1.97 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:39.225916: step 35320, loss = 2.12 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:59:40.440092: step 35330, loss = 2.10 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:41.634114: step 35340, loss = 2.06 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:59:42.839795: step 35350, loss = 1.97 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:44.049301: step 35360, loss = 2.06 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:45.282327: step 35370, loss = 2.02 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:46.497083: step 35380, loss = 2.16 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:47.711429: step 35390, loss = 1.96 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:48.939686: step 35400, loss = 2.08 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:50.149711: step 35410, loss = 2.08 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:51.376087: step 35420, loss = 2.07 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:52.603454: step 35430, loss = 2.17 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:53.802077: step 35440, loss = 2.00 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:59:54.996637: step 35450, loss = 2.14 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:59:56.220721: step 35460, loss = 2.00 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:57.421329: step 35470, loss = 1.89 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:59:58.638436: step 35480, loss = 2.11 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:59.863931: step 35490, loss = 2.14 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:00:01.073823: step 35500, loss = 2.03 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:02.280627: step 35510, loss = 1.97 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:03.489603: step 35520, loss = 2.10 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:04.707082: step 35530, loss = 1.93 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:05.915716: step 35540, loss = 1.96 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:07.141277: step 35550, loss = 1.87 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:00:08.369327: step 35560, loss = 2.19 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:00:09.584422: step 35570, loss = 2.04 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:10.789183: step 35580, loss = 2.22 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:00:11.982668: step 35590, loss = 1.96 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:00:13.208355: step 35600, loss = 1.97 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:00:14.414891: step 35610, loss = 2.02 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:15.627775: step 35620, loss = 1.88 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:16.853629: step 35630, loss = 2.11 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:00:18.152969: step 35640, loss = 1.95 (985.1 examples/sec; 0.130 sec/batch)
2017-05-04 23:00:19.247320: step 35650, loss = 2.21 (1169.6 examples/sec; 0.109 sec/batch)
2017-05-04 23:00:20.460945: step 35660, loss = 2.10 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:21.674150: step 35670, loss = 1.94 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:22.883635: step 35680, loss = 1.86 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:24.103581: step 35690, loss = 2.03 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:25.311685: step 35700, loss = 2.17 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:26.516447: step 35710, loss = 2.06 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:00:27.735040: step 35720, loss = 2.00 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:28.962796: step 35730, loss = 2.02 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:00:30.172607: step 35740, loss = 1.95 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:31.395319: step 35750, loss = 2.08 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:32.598666: step 35760, loss = 2.07 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:00:33.810576: step 35770, loss = 1.97 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:35.023213: step 35780, loss = 2.07 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:36.248339: step 35790, loss = 2.06 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:00:37.459419: step 35800, loss = 2.01 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:38.686840: step 35810, loss = 2.12 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:00:39.893720: step 35820, loss = 1.92 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:41.109481: step 35830, loss = 2.14 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:42.288477: step 35840, loss = 2.14 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:00:43.510527: step 35850, loss = 2.03 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:44.723893: step 35860, loss = 2.17 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:45.939419: step 35870, loss = 2.05 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:47.154293: step 35880, loss = 1.99 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:48.366190: step 35890, loss = 2.01 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:49.568999: step 35900, loss = 1.95 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:00:50.783335: step 35910, loss = 2.01 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:52.002420: step 35920, loss = 2.12 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:53.220809: step 35930, loss = 2.07 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:54.437215: step 35940, loss = 1.97 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:55.659566: step 35950, loss = 2.00 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:56.883483: step 35960, loss = 2.01 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:58.076022: step 35970, loss = 2.00 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:00:59.300774: step 35980, loss = 2.05 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:00.520475: step 35990, loss = 2.11 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:01.732720: step 36000, loss = 1.93 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:02.945340: step 36010, loss = 1.99 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:04.155025: step 36020, loss = 2.03 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:05.370398: step 36030, loss = 1.91 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:06.560940: step 36040, loss = 1.91 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:01:07.782913: step 36050, loss = 2.10 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:09.011953: step 36060, loss = 1.92 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:01:10.222299: step 36070, loss = 1.99 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:11.456351: step 36080, loss = 2.04 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:01:12.668466: step 36090, loss = 2.15 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:13.867177: step 36100, loss = 2.09 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:01:15.086086: step 36110, loss = 1.93 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:16.307303: step 36120, loss = 1.99 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:17.515613: step 36130, loss = 2.07 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:18.742868: step 36140, loss = 2.11 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:01:19.952333: step 36150, loss = 1.94 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:21.175779: step 36160, loss = 1.95 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:22.385373: step 36170, loss = 1.96 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:23.607152: step 36180, loss = 2.04 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:24.816404: step 36190, loss = 2.20 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:26.006561: step 36200, loss = 1.98 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:01:27.225790: step 36210, loss = 1.93 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:28.446830: step 36220, loss = 2.01 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:29.658984: step 36230, loss = 1.83 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:30.882877: step 36240, loss = 2.03 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:32.102123: step 36250, loss = 2.09 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:33.312016: step 36260, loss = 2.16 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:34.523548: step 36270, loss = 2.16 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:35.746369: step 36280, loss = 2.08 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:36.967977: step 36290, loss = 2.08 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:38.173811: step 36300, loss = 1.93 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:39.392942: step 36310, loss = 2.02 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:40.621555: step 36320, loss = 1.86 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:01:41.830634: step 36330, loss = 1.97 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:43.055549: step 36340, loss = 1.94 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:44.272506: step 36350, loss = 1.95 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:45.493166: step 36360, loss = 1.96 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:46.703750: step 36370, loss = 2.03 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:47.914582: step 36380, loss = 2.10 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:49.138536: step 36390, loss = 1.96 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:50.348833: step 36400, loss = 2.01 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:51.565038: step 36410, loss = 1.88 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:52.787681: step 36420, loss = 1.84 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:54.017331: step 36430, loss = 1.91 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:01:55.197436: step 36440, loss = 2.02 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:01:56.415255: step 36450, loss = 2.15 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:57.623442: step 36460, loss = 2.01 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:58.852610: step 36470, loss = 2.10 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:02:00.075662: step 36480, loss = 2.08 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:01.290118: step 36490, loss = 1.99 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:02.502603: step 36500, loss = 2.17 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:03.724815: step 36510, loss = 1.88 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:04.936012: step 36520, loss = 2.13 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:06.133027: step 36530, loss = 2.12 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:02:07.362361: step 36540, loss = 1.94 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:02:08.601377: step 36550, loss = 2.02 (1033.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:02:09.798374: step 36560, loss = 2.13 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:02:11.015646: step 36570, loss = 2.06 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:12.239279: step 36580, loss = 2.15 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:13.450763: step 36590, loss = 2.13 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:14.662780: step 36600, loss = 1.90 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:15.861098: step 36610, loss = 2.10 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:02:17.088938: step 36620, loss = 2.13 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:02:18.385943: step 36630, loss = 2.05 (986.9 examples/sec; 0.130 sec/batch)
2017-05-04 23:02:19.482364: step 36640, loss = 1.86 (1167.4 examples/sec; 0.110 sec/batch)
2017-05-04 23:02:20.702437: step 36650, loss = 2.08 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:21.914531: step 36660, loss = 2.19 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:23.128982: step 36670, loss = 1.96 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:24.353986: step 36680, loss = 2.22 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:02:25.570896: step 36690, loss = 2.00 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:26.775283: step 36700, loss = 2.16 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:02:27.980798: step 36710, loss = 1.87 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:29.228907: step 36720, loss = 1.98 (1025.5 examples/sec; 0.125 sec/batch)
2017-05-04 23:02:30.450203: step 36730, loss = 1.89 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:31.660669: step 36740, loss = 2.13 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:32.878747: step 36750, loss = 2.16 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:34.095089: step 36760, loss = 2.05 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:35.299669: step 36770, loss = 2.14 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:02:36.512878: step 36780, loss = 1.94 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:37.726215: step 36790, loss = 2.02 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:38.923064: step 36800, loss = 2.02 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:02:40.139682: step 36810, loss = 2.09 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:41.341301: step 36820, loss = 2.04 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:02:42.531958: step 36830, loss = 2.17 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:02:43.725439: step 36840, loss = 2.05 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:02:44.911572: step 36850, loss = 2.03 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:02:46.094278: step 36860, loss = 2.02 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:02:47.279323: step 36870, loss = 1.98 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:02:48.448818: step 36880, loss = 1.98 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:02:49.592648: step 36890, loss = 1.96 (1119.0 examples/sec; 0.114 sec/batch)
2017-05-04 23:02:50.767430: step 36900, loss = 1.95 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:02:51.938763: step 36910, loss = 2.14 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:02:53.113657: step 36920, loss = 2.11 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:02:54.266923: step 36930, loss = 2.11 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:02:55.454015: step 36940, loss = 2.05 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:02:56.618433: step 36950, loss = 2.11 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:02:57.780059: step 36960, loss = 1.94 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:02:58.951836: step 36970, loss = 2.10 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:03:00.108889: step 36980, loss = 2.04 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:03:01.266000: step 36990, loss = 2.01 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:03:02.446525: step 37000, loss = 2.05 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:03.623930: step 37010, loss = 2.00 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:04.795100: step 37020, loss = 1.93 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:03:05.957821: step 37030, loss = 1.97 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:03:07.130936: step 37040, loss = 2.22 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:03:08.333031: step 37050, loss = 2.16 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:03:09.499038: step 37060, loss = 1.96 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:03:10.681063: step 37070, loss = 2.04 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:11.876760: step 37080, loss = 1.95 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:03:13.074956: step 37090, loss = 1.99 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:03:14.259708: step 37100, loss = 1.91 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:15.447275: step 37110, loss = 1.91 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:03:16.638631: step 37120, loss = 2.07 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:03:17.803677: step 37130, loss = 1.94 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:03:18.985926: step 37140, loss = 2.10 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:20.170088: step 37150, loss = 2.02 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:21.358149: step 37160, loss = 1.85 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:03:22.548515: step 37170, loss = 2.00 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:03:23.748721: step 37180, loss = 1.97 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:03:24.963494: step 37190, loss = 1.88 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:03:26.146812: step 37200, loss = 2.14 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:27.348709: step 37210, loss = 1.97 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:03:28.539923: step 37220, loss = 2.03 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:03:29.709362: step 37230, loss = 1.81 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:03:30.885053: step 37240, loss = 1.86 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:32.071145: step 37250, loss = 1.91 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:03:33.248308: step 37260, loss = 2.03 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:34.420018: step 37270, loss = 1.99 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:03:35.595669: step 37280, loss = 1.97 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:36.787813: step 37290, loss = 2.04 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:03:37.960929: step 37300, loss = 2.04 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:03:39.172018: step 37310, loss = 2.12 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:03:40.380791: step 37320, loss = 1.87 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:03:41.586158: step 37330, loss = 2.26 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:03:42.814633: step 37340, loss = 1.87 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:03:44.055490: step 37350, loss = 2.18 (1031.5 examples/sec; 0.124 sec/batch)
2017-05-04 23:03:45.267631: step 37360, loss = 1.85 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:03:46.505322: step 37370, loss = 2.00 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-04 23:03:47.718372: step 37380, loss = 2.19 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:03:48.957069: step 37390, loss = 1.97 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-04 23:03:50.170487: step 37400, loss = 2.06 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:03:51.394967: step 37410, loss = 2.04 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:03:52.595578: step 37420, loss = 2.21 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:03:53.779906: step 37430, loss = 2.01 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:55.015562: step 37440, loss = 2.10 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-04 23:03:56.267875: step 37450, loss = 1.95 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-04 23:03:57.475993: step 37460, loss = 2.12 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:03:58.709821: step 37470, loss = 1.93 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:03:59.931425: step 37480, loss = 1.96 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:01.170683: step 37490, loss = 1.93 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-04 23:04:02.403099: step 37500, loss = 2.05 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:03.644474: step 37510, loss = 2.16 (1031.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:04:04.875208: step 37520, loss = 1.98 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:06.102510: step 37530, loss = 1.97 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:07.328280: step 37540, loss = 1.95 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:08.562647: step 37550, loss = 1.92 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:09.759847: step 37560, loss = 2.05 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:04:11.002254: step 37570, loss = 2.06 (1030.3 examples/sec; 0.124 sec/batch)
2017-05-04 23:04:12.223952: step 37580, loss = 1.92 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:13.446972: step 37590, loss = 1.90 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:14.665926: step 37600, loss = 2.04 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:15.882310: step 37610, loss = 2.07 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:17.205506: step 37620, loss = 2.12 (967.4 examples/sec; 0.132 sec/batch)
2017-05-04 23:04:18.280015: step 37630, loss = 2.02 (1191.2 examples/sec; 0.107 sec/batch)
2017-05-04 23:04:19.533086: step 37640, loss = 2.11 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-04 23:04:20.761809: step 37650, loss = 2.00 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:21.962556: step 37660, loss = 2.06 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:04:23.203405: step 37670, loss = 2.13 (1031.6 examples/sec; 0.124 sec/batch)
2017-05-04 23:04:24.413984: step 37680, loss = 2.00 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:04:25.658632: step 37690, loss = 1.94 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-04 23:04:26.850704: step 37700, loss = 2.08 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:04:28.085454: step 37710, loss = 1.85 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:29.307648: step 37720, loss = 2.07 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:30.521784: step 37730, loss = 2.06 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:04:31.738468: step 37740, loss = 2.37 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:32.978748: step 37750, loss = 2.00 (1032.0 examples/sec; 0.124 sec/batch)
2017-05-04 23:04:34.164965: step 37760, loss = 1.85 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:04:35.405433: step 37770, loss = 2.14 (1031.9 examples/sec; 0.124 sec/batch)
2017-05-04 23:04:36.630330: step 37780, loss = 1.87 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:37.868091: step 37790, loss = 2.02 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:04:39.084989: step 37800, loss = 1.96 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:40.324015: step 37810, loss = 1.98 (1033.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:04:41.617447: step 37820, loss = 2.29 (989.6 examples/sec; 0.129 sec/batch)
2017-05-04 23:04:42.715433: step 37830, loss = 2.04 (1165.8 examples/sec; 0.110 sec/batch)
2017-05-04 23:04:43.927281: step 37840, loss = 2.09 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:04:45.174923: step 37850, loss = 2.06 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-04 23:04:46.389156: step 37860, loss = 2.23 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:04:47.615547: step 37870, loss = 1.99 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:48.831542: step 37880, loss = 1.93 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:50.045034: step 37890, loss = 2.07 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:04:51.274088: step 37900, loss = 2.09 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:52.518642: step 37910, loss = 2.00 (1028.5 examples/sec; 0.124 sec/batch)
2017-05-04 23:04:53.716537: step 37920, loss = 2.16 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:04:54.963491: step 37930, loss = 2.10 (1026.5 examples/sec; 0.125 sec/batch)
2017-05-04 23:04:56.196921: step 37940, loss = 2.15 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:57.410419: step 37950, loss = 1.99 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:04:58.619169: step 37960, loss = 2.07 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:04:59.863794: step 37970, loss = 2.20 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-04 23:05:01.108195: step 37980, loss = 1.96 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-04 23:05:02.323614: step 37990, loss = 1.95 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:03.554200: step 38000, loss = 2.07 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:04.801783: step 38010, loss = 2.01 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-04 23:05:05.962222: step 38020, loss = 2.00 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:05:07.187766: step 38030, loss = 2.00 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:08.406268: step 38040, loss = 2.18 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:09.629741: step 38050, loss = 2.04 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:10.837615: step 38060, loss = 2.01 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:12.080256: step 38070, loss = 1.99 (1030.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:05:13.277530: step 38080, loss = 1.94 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:05:14.494716: step 38090, loss = 2.06 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:15.700416: step 38100, loss = 2.29 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:16.942826: step 38110, loss = 2.01 (1030.3 examples/sec; 0.124 sec/batch)
2017-05-04 23:05:18.137534: step 38120, loss = 1.94 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:05:19.367905: step 38130, loss = 2.03 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:20.601888: step 38140, loss = 2.17 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:21.805956: step 38150, loss = 2.04 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:05:23.013549: step 38160, loss = 1.97 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:24.251853: step 38170, loss = 2.17 (1033.7 examples/sec; 0.124 sec/batch)
2017-05-04 23:05:25.477089: step 38180, loss = 1.99 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:26.706684: step 38190, loss = 2.06 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:27.916652: step 38200, loss = 1.99 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:29.164044: step 38210, loss = 2.18 (1026.1 examples/sec; 0.125 sec/batch)
2017-05-04 23:05:30.342151: step 38220, loss = 1.96 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:05:31.538269: step 38230, loss = 1.96 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:05:32.761166: step 38240, loss = 2.02 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:33.985939: step 38250, loss = 2.09 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:35.229890: step 38260, loss = 1.95 (1029.0 examples/sec; 0.124 sec/batch)
2017-05-04 23:05:36.439942: step 38270, loss = 2.03 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:37.662106: step 38280, loss = 1.99 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:38.867268: step 38290, loss = 2.15 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:40.088724: step 38300, loss = 1.97 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:41.318804: step 38310, loss = 2.07 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:42.519918: step 38320, loss = 2.12 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:05:43.760147: step 38330, loss = 2.12 (1032.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:05:44.961092: step 38340, loss = 2.03 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:05:46.163752: step 38350, loss = 2.15 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:05:47.371738: step 38360, loss = 2.02 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:48.596831: step 38370, loss = 1.82 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:49.808733: step 38380, loss = 1.99 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:51.028826: step 38390, loss = 1.87 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:52.244701: step 38400, loss = 1.94 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:53.477661: step 38410, loss = 1.94 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:54.673872: step 38420, loss = 2.09 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:05:55.888213: step 38430, loss = 2.11 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:57.114719: step 38440, loss = 2.00 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:58.331047: step 38450, loss = 1.87 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:59.542076: step 38460, loss = 2.02 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:06:00.784613: step 38470, loss = 2.08 (1030.2 examples/sec; 0.124 sec/batch)
2017-05-04 23:06:01.977410: step 38480, loss = 1.98 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:06:03.205113: step 38490, loss = 2.07 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:06:04.412632: step 38500, loss = 1.82 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:06:05.642360: step 38510, loss = 1.90 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:06:06.851067: step 38520, loss = 1.98 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:06:08.068428: step 38530, loss = 1.80 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:06:09.282890: step 38540, loss = 2.00 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:06:10.507960: step 38550, loss = 2.21 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:06:11.721028: step 38560, loss = 2.16 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:06:12.917280: step 38570, loss = 1.95 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:06:14.124090: step 38580, loss = 1.93 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:06:15.337814: step 38590, loss = 1.97 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:06:16.505741: step 38600, loss = 2.01 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:17.776559: step 38610, loss = 1.95 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-04 23:06:18.854960: step 38620, loss = 2.09 (1186.9 examples/sec; 0.108 sec/batch)
2017-05-04 23:06:20.034775: step 38630, loss = 2.08 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:21.210153: step 38640, loss = 1.92 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:22.375935: step 38650, loss = 2.23 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:23.549492: step 38660, loss = 2.16 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:24.730495: step 38670, loss = 2.06 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:25.891452: step 38680, loss = 1.94 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:06:27.085554: step 38690, loss = 2.02 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:06:28.256938: step 38700, loss = 1.97 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:29.423154: step 38710, loss = 1.99 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:30.602804: step 38720, loss = 2.02 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:31.803617: step 38730, loss = 2.32 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:06:32.966497: step 38740, loss = 2.03 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:06:34.149154: step 38750, loss = 2.03 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:35.319473: step 38760, loss = 2.13 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:36.483789: step 38770, loss = 2.15 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:06:37.649756: step 38780, loss = 1.91 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:38.828215: step 38790, loss = 2.16 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:40.002614: step 38800, loss = 2.07 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:41.202847: step 38810, loss = 2.03 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:06:42.345772: step 38820, loss = 2.10 (1119.9 examples/sec; 0.114 sec/batch)
2017-05-04 23:06:43.514925: step 38830, loss = 1.92 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:44.688446: step 38840, loss = 2.19 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:45.859289: step 38850, loss = 2.01 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:47.016331: step 38860, loss = 2.26 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:06:48.185987: step 38870, loss = 2.21 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:49.345099: step 38880, loss = 1.94 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:06:50.508415: step 38890, loss = 2.01 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:06:51.689121: step 38900, loss = 1.94 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:52.855058: step 38910, loss = 1.91 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:54.019305: step 38920, loss = 2.07 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:06:55.205330: step 38930, loss = 2.04 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:06:56.375682: step 38940, loss = 1.97 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:57.553680: step 38950, loss = 2.08 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:58.720749: step 38960, loss = 2.10 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:59.882518: step 38970, loss = 2.03 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:07:01.070208: step 38980, loss = 2.07 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:07:02.250972: step 38990, loss = 1.92 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:07:03.452702: step 39000, loss = 2.14 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:04.657405: step 39010, loss = 1.93 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:05.831133: step 39020, loss = 1.96 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:07:07.028334: step 39030, loss = 2.17 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:08.248981: step 39040, loss = 1.92 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:07:09.421203: step 39050, loss = 2.04 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:07:10.605620: step 39060, loss = 2.07 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:07:11.796034: step 39070, loss = 1.95 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:07:13.002826: step 39080, loss = 2.13 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:07:14.207038: step 39090, loss = 2.03 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:15.420842: step 39100, loss = 2.04 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:07:16.624976: step 39110, loss = 1.96 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:17.794785: step 39120, loss = 1.89 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:07:18.982630: step 39130, loss = 1.98 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:07:20.165531: step 39140, loss = 1.99 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:07:21.326173: step 39150, loss = 2.22 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:07:22.484770: step 39160, loss = 1.93 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:07:23.659498: step 39170, loss = 2.00 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:07:24.831804: step 39180, loss = 2.06 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:07:25.982154: step 39190, loss = 2.12 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:07:27.149895: step 39200, loss = 2.20 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:07:28.325129: step 39210, loss = 1.99 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:07:29.490109: step 39220, loss = 1.93 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:07:30.688328: step 39230, loss = 1.93 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:31.875693: step 39240, loss = 1.93 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:07:33.081038: step 39250, loss = 1.99 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:07:34.286214: step 39260, loss = 1.98 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:07:35.508602: step 39270, loss = 2.10 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:07:36.726641: step 39280, loss = 2.00 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:07:37.942598: step 39290, loss = 1.97 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:07:39.183399: step 39300, loss = 2.05 (1031.6 examples/sec; 0.124 sec/batch)
2017-05-04 23:07:40.370531: step 39310, loss = 2.07 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:07:41.579511: step 39320, loss = 2.03 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:07:42.799689: step 39330, loss = 2.18 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:07:44.010704: step 39340, loss = 1.99 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:07:45.220571: step 39350, loss = 2.13 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:07:46.418421: step 39360, loss = 1.97 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:47.615428: step 39370, loss = 1.95 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:48.807855: step 39380, loss = 2.28 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:07:50.003640: step 39390, loss = 2.11 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:51.230116: step 39400, loss = 2.04 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:07:52.433063: step 39410, loss = 1.88 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:53.649829: step 39420, loss = 2.02 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:07:54.861665: step 39430, loss = 1.86 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:07:56.061036: step 39440, loss = 1.98 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:57.262161: step 39450, loss = 2.05 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:58.434120: step 39460, loss = 2.03 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:07:59.607566: step 39470, loss = 2.12 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:00.782376: step 39480, loss = 2.04 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:01.948873: step 39490, loss = 2.18 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:03.127318: step 39500, loss = 2.02 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:04.306482: step 39510, loss = 2.07 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:05.491808: step 39520, loss = 2.01 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:08:06.671474: step 39530, loss = 2.08 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:07.851972: step 39540, loss = 2.02 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:09.016026: step 39550, loss = 2.04 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:08:10.166777: step 39560, loss = 2.12 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:08:11.340939: step 39570, loss = 1.89 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:12.519381: step 39580, loss = 2.22 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:13.680043: step 39590, loss = 1.92 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:08:14.964949: step 39600, loss = 2.06 (996.2 examples/sec; 0.128 sec/batch)
2017-05-04 23:08:16.060326: step 39610, loss = 1.94 (1168.5 examples/sec; 0.110 sec/batch)
2017-05-04 23:08:17.237977: step 39620, loss = 2.02 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:18.407222: step 39630, loss = 1.92 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:19.571630: step 39640, loss = 2.11 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:08:20.743295: step 39650, loss = 2.09 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:21.899641: step 39660, loss = 2.02 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:08:23.064426: step 39670, loss = 1.95 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:08:24.222721: step 39680, loss = 2.08 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:08:25.404607: step 39690, loss = 2.00 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:26.574873: step 39700, loss = 1.96 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:27.752009: step 39710, loss = 2.26 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:28.914569: step 39720, loss = 1.83 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:08:30.092054: step 39730, loss = 1.94 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:31.276814: step 39740, loss = 2.02 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:32.446845: step 39750, loss = 1.95 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:33.614002: step 39760, loss = 1.91 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:34.767358: step 39770, loss = 2.08 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:08:35.968284: step 39780, loss = 1.94 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:08:37.197118: step 39790, loss = 1.88 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:08:38.425660: step 39800, loss = 2.10 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:08:39.601296: step 39810, loss = 1.92 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:40.799539: step 39820, loss = 2.07 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:08:41.974449: step 39830, loss = 1.99 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:43.152583: step 39840, loss = 1.97 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:44.345471: step 39850, loss = 2.20 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:08:45.522490: step 39860, loss = 2.11 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:46.689193: step 39870, loss = 2.08 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:47.881074: step 39880, loss = 2.00 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:08:49.094786: step 39890, loss = 1.93 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:08:50.274781: step 39900, loss = 2.03 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:51.482367: step 39910, loss = 2.08 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:08:52.696266: step 39920, loss = 1.98 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:08:53.930663: step 39930, loss = 1.88 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:08:55.129724: step 39940, loss = 1.98 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:08:56.306114: step 39950, loss = 1.98 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:57.475728: step 39960, loss = 1.89 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:58.637612: step 39970, loss = 2.09 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:08:59.822449: step 39980, loss = 1.92 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:00.991520: step 39990, loss = 1.98 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:02.148844: step 40000, loss = 2.04 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:09:03.310078: step 40010, loss = 1.93 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:09:04.502493: step 40020, loss = 2.08 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:05.639422: step 40030, loss = 2.15 (1125.8 examples/sec; 0.114 sec/batch)
2017-05-04 23:09:06.821275: step 40040, loss = 2.16 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:07.979176: step 40050, loss = 2.09 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:09:09.141345: step 40060, loss = 2.04 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:09:10.290994: step 40070, loss = 2.02 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:09:11.471902: step 40080, loss = 2.07 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:12.660595: step 40090, loss = 2.09 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:13.837938: step 40100, loss = 1.96 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:15.028280: step 40110, loss = 1.99 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:16.218889: step 40120, loss = 1.91 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:17.436801: step 40130, loss = 2.10 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:09:18.630914: step 40140, loss = 2.09 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:19.822159: step 40150, loss = 2.01 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:21.005348: step 40160, loss = 2.02 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:22.184891: step 40170, loss = 1.94 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:23.368379: step 40180, loss = 2.09 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:24.537484: step 40190, loss = 2.23 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:25.713588: step 40200, loss = 2.04 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:26.886929: step 40210, loss = 2.15 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:28.060325: step 40220, loss = 2.09 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:29.236842: step 40230, loss = 2.11 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:30.400840: step 40240, loss = 2.05 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:09:31.554784: step 40250, loss = 2.06 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-04 23:09:32.722311: step 40260, loss = 2.04 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:33.890500: step 40270, loss = 1.99 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:35.060549: step 40280, loss = 1.93 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:36.230990: step 40290, loss = 2.17 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:37.407669: step 40300, loss = 1.97 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:38.585134: step 40310, loss = 1.92 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:39.775050: step 40320, loss = 2.13 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:40.954603: step 40330, loss = 1.92 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:42.131328: step 40340, loss = 1.85 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:43.338007: step 40350, loss = 1.96 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:09:44.539316: step 40360, loss = 2.01 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:09:45.712817: step 40370, loss = 2.06 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:46.882749: step 40380, loss = 1.98 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:48.072295: step 40390, loss = 2.09 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:49.252369: step 40400, loss = 1.93 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:50.433276: step 40410, loss = 1.92 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:51.610289: step 40420, loss = 2.02 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:52.795890: step 40430, loss = 1.96 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:53.985081: step 40440, loss = 1.91 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:55.189255: step 40450, loss = 2.04 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:09:56.385027: step 40460, loss = 1.93 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:09:57.564264: step 40470, loss = 2.09 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:58.753294: step 40480, loss = 2.11 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:59.958901: step 40490, loss = 2.13 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:10:01.171940: step 40500, loss = 2.11 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:10:02.379931: step 40510, loss = 2.07 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:10:03.592814: step 40520, loss = 2.09 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:10:04.766559: step 40530, loss = 1.99 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:05.937668: step 40540, loss = 2.04 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:07.136049: step 40550, loss = 1.88 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:10:08.304587: step 40560, loss = 2.20 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:09.470514: step 40570, loss = 2.11 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:10.627370: step 40580, loss = 2.10 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:11.913215: step 40590, loss = 2.01 (995.5 examples/sec; 0.129 sec/batch)
2017-05-04 23:10:12.968725: step 40600, loss = 1.98 (1212.7 examples/sec; 0.106 sec/batch)
2017-05-04 23:10:14.148025: step 40610, loss = 1.94 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:15.336945: step 40620, loss = 2.04 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:10:16.504202: step 40630, loss = 2.08 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:17.668487: step 40640, loss = 2.04 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:18.832556: step 40650, loss = 2.08 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:20.023429: step 40660, loss = 2.05 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:10:21.202027: step 40670, loss = 2.06 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:22.354006: step 40680, loss = 1.89 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:10:23.513657: step 40690, loss = 1.89 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:24.692892: step 40700, loss = 2.00 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:25.855331: step 40710, loss = 2.04 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:27.035659: step 40720, loss = 1.98 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:28.188141: step 40730, loss = 2.05 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:10:29.346695: step 40740, loss = 1.97 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:30.514660: step 40750, loss = 2.08 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:31.659834: step 40760, loss = 2.12 (1117.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:10:32.841862: step 40770, loss = 2.02 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:33.974887: step 40780, loss = 1.98 (1129.7 examples/sec; 0.113 sec/batch)
2017-05-04 23:10:35.145113: step 40790, loss = 2.00 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:36.317488: step 40800, loss = 1.92 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:37.477618: step 40810, loss = 2.28 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:38.647446: step 40820, loss = 2.08 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:39.830352: step 40830, loss = 2.05 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:40.990610: step 40840, loss = 2.18 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:42.172873: step 40850, loss = 2.05 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:43.346084: step 40860, loss = 1.95 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:44.522192: step 40870, loss = 1.96 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:45.704508: step 40880, loss = 1.93 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:46.876535: step 40890, loss = 2.08 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:48.038867: step 40900, loss = 1.82 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:49.254082: step 40910, loss = 2.09 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:10:50.387152: step 40920, loss = 1.90 (1129.7 examples/sec; 0.113 sec/batch)
2017-05-04 23:10:51.576861: step 40930, loss = 1.85 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:10:52.750275: step 40940, loss = 2.09 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:53.894849: step 40950, loss = 2.00 (1118.3 examples/sec; 0.114 sec/batch)
2017-05-04 23:10:55.068480: step 40960, loss = 1.83 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:56.239654: step 40970, loss = 2.28 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:57.428938: step 40980, loss = 2.15 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:10:58.587750: step 40990, loss = 2.09 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:59.762923: step 41000, loss = 2.06 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:11:00.959282: step 41010, loss = 1.95 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:11:02.105330: step 41020, loss = 1.94 (1116.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:11:03.264365: step 41030, loss = 1.87 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:04.448754: step 41040, loss = 2.03 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:11:05.607900: step 41050, loss = 1.87 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:06.793865: step 41060, loss = 1.96 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:11:07.957182: step 41070, loss = 2.08 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:09.134755: step 41080, loss = 2.05 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:11:10.291875: step 41090, loss = 1.99 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:11.484129: step 41100, loss = 1.80 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:11:12.653087: step 41110, loss = 2.01 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:13.822435: step 41120, loss = 2.06 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:14.977612: step 41130, loss = 2.01 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:16.139607: step 41140, loss = 2.09 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:17.311015: step 41150, loss = 2.18 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:18.467197: step 41160, loss = 2.01 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:19.622832: step 41170, loss = 2.05 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:20.801100: step 41180, loss = 2.13 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:11:21.969107: step 41190, loss = 1.91 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:23.121696: step 41200, loss = 1.86 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:11:24.311195: step 41210, loss = 1.89 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:11:25.483137: step 41220, loss = 2.11 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:26.672601: step 41230, loss = 2.10 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:11:27.829650: step 41240, loss = 2.09 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:28.998270: step 41250, loss = 1.85 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:30.166306: step 41260, loss = 2.18 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:31.331573: step 41270, loss = 2.01 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:32.491186: step 41280, loss = 1.86 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:33.641271: step 41290, loss = 2.07 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-04 23:11:34.792677: step 41300, loss = 2.06 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:11:35.965894: step 41310, loss = 1.99 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:37.146071: step 41320, loss = 2.04 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:11:38.321293: step 41330, loss = 2.00 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:11:39.487425: step 41340, loss = 1.98 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:40.659396: step 41350, loss = 2.04 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:41.809063: step 41360, loss = 2.00 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:11:42.980464: step 41370, loss = 1.82 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:44.171585: step 41380, loss = 2.17 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:11:45.341365: step 41390, loss = 2.05 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:46.485048: step 41400, loss = 1.88 (1119.2 examples/sec; 0.114 sec/batch)
2017-05-04 23:11:47.650646: step 41410, loss = 2.07 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:48.811586: step 41420, loss = 1.96 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:49.973606: step 41430, loss = 1.95 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:51.147445: step 41440, loss = 2.09 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:52.312119: step 41450, loss = 2.10 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:53.486954: step 41460, loss = 2.06 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:54.646042: step 41470, loss = 1.98 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:55.836350: step 41480, loss = 1.92 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:11:57.008332: step 41490, loss = 1.93 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:58.149542: step 41500, loss = 2.07 (1121.6 examples/sec; 0.114 sec/batch)
2017-05-04 23:11:59.344771: step 41510, loss = 1.93 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:12:00.532506: step 41520, loss = 2.18 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:12:01.685008: step 41530, loss = 2.00 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:12:02.853600: step 41540, loss = 2.05 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:12:04.040900: step 41550, loss = 2.25 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:12:05.220823: step 41560, loss = 2.09 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:12:06.380374: step 41570, loss = 2.07 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:12:07.670601: step 41580, loss = 2.01 (992.1 examples/sec; 0.129 sec/batch)
2017-05-04 23:12:08.758417: step 41590, loss = 2.20 (1176.7 examples/sec; 0.109 sec/batch)
2017-05-04 23:12:09.920779: step 41600, loss = 2.07 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:12:11.108381: step 41610, loss = 1.86 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:12:12.293071: step 41620, loss = 2.04 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:12:13.507173: step 41630, loss = 1.86 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:14.745366: step 41640, loss = 2.01 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-04 23:12:15.970285: step 41650, loss = 2.07 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:17.197942: step 41660, loss = 1.88 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:12:18.409991: step 41670, loss = 1.90 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:19.621793: step 41680, loss = 1.90 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:20.840878: step 41690, loss = 2.06 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:22.061856: step 41700, loss = 1.94 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:23.272051: step 41710, loss = 2.02 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:24.486283: step 41720, loss = 2.18 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:25.691466: step 41730, loss = 1.95 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:26.905054: step 41740, loss = 2.00 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:28.130474: step 41750, loss = 1.95 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:12:29.345584: step 41760, loss = 2.11 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:30.567041: step 41770, loss = 1.99 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:31.784817: step 41780, loss = 1.96 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:32.995714: step 41790, loss = 2.02 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:34.187432: step 41800, loss = 1.96 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:12:35.412577: step 41810, loss = 1.99 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:12:36.639905: step 41820, loss = 2.04 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:12:37.847779: step 41830, loss = 1.93 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:39.050473: step 41840, loss = 2.03 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:12:40.258992: step 41850, loss = 1.94 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:41.476587: step 41860, loss = 2.02 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:42.699430: step 41870, loss = 2.15 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:43.917013: step 41880, loss = 2.01 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:45.129786: step 41890, loss = 1.98 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:46.333517: step 41900, loss = 2.14 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:12:47.552175: step 41910, loss = 1.99 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:48.785337: step 41920, loss = 2.14 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:12:49.985137: step 41930, loss = 1.90 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:12:51.205929: step 41940, loss = 2.01 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:52.416224: step 41950, loss = 1.95 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:53.633911: step 41960, loss = 2.04 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:54.862042: step 41970, loss = 2.19 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:12:56.047136: step 41980, loss = 2.13 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:12:57.265723: step 41990, loss = 1.92 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:58.466786: step 42000, loss = 2.21 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:12:59.695374: step 42010, loss = 1.86 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:13:00.916895: step 42020, loss = 2.09 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:13:02.119898: step 42030, loss = 1.87 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:13:03.319667: step 42040, loss = 2.06 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:13:04.536788: step 42050, loss = 2.07 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:13:05.721762: step 42060, loss = 1.89 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:13:06.931955: step 42070, loss = 2.05 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:13:08.139287: step 42080, loss = 2.08 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:13:09.340779: step 42090, loss = 1.93 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:13:10.533575: step 42100, loss = 1.92 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:13:11.737927: step 42110, loss = 2.32 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:13:12.956674: step 42120, loss = 2.07 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:13:14.146281: step 42130, loss = 2.00 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:13:15.330303: step 42140, loss = 2.15 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:13:16.524126: step 42150, loss = 2.11 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:13:17.683982: step 42160, loss = 1.95 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:13:18.851420: step 42170, loss = 2.06 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:20.025174: step 42180, loss = 2.06 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:21.191863: step 42190, loss = 2.10 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:22.346885: step 42200, loss = 2.07 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:13:23.515770: step 42210, loss = 2.08 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:24.686704: step 42220, loss = 2.04 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:25.844845: step 42230, loss = 2.04 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:13:27.012058: step 42240, loss = 2.07 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:28.187653: step 42250, loss = 1.87 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:13:29.330187: step 42260, loss = 2.03 (1120.3 examples/sec; 0.114 sec/batch)
2017-05-04 23:13:30.487410: step 42270, loss = 1.98 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:13:31.646565: step 42280, loss = 2.10 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:13:32.814561: step 42290, loss = 2.15 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:33.976142: step 42300, loss = 2.01 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:13:35.160522: step 42310, loss = 2.13 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:13:36.332187: step 42320, loss = 1.93 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:37.505807: step 42330, loss = 1.99 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:38.672934: step 42340, loss = 2.11 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:39.861214: step 42350, loss = 2.25 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:13:41.057281: step 42360, loss = 1.99 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:13:42.237648: step 42370, loss = 1.84 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:13:43.426356: step 42380, loss = 2.03 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:13:44.614871: step 42390, loss = 1.95 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:13:45.789200: step 42400, loss = 2.01 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:46.993792: step 42410, loss = 2.20 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:13:48.173993: step 42420, loss = 2.13 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:13:49.349059: step 42430, loss = 1.86 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:13:50.497118: step 42440, loss = 2.07 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:13:51.670396: step 42450, loss = 2.13 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:52.834713: step 42460, loss = 2.13 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:13:53.987122: step 42470, loss = 2.02 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:13:55.133559: step 42480, loss = 2.14 (1116.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:13:56.294319: step 42490, loss = 1.90 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:13:57.447931: step 42500, loss = 1.96 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:13:58.622787: step 42510, loss = 2.17 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:59.780119: step 42520, loss = 2.13 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:00.978910: step 42530, loss = 2.04 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:14:02.150672: step 42540, loss = 1.99 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:03.325323: step 42550, loss = 2.09 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:04.496483: step 42560, loss = 1.92 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:05.766074: step 42570, loss = 2.18 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-04 23:14:06.839071: step 42580, loss = 1.84 (1192.9 examples/sec; 0.107 sec/batch)
2017-05-04 23:14:08.022414: step 42590, loss = 2.05 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:09.184930: step 42600, loss = 1.85 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:10.338920: step 42610, loss = 1.94 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-04 23:14:11.513481: step 42620, loss = 1.98 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:12.678447: step 42630, loss = 1.94 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:13.835298: step 42640, loss = 2.05 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:14.996969: step 42650, loss = 1.98 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:16.166662: step 42660, loss = 1.86 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:17.322624: step 42670, loss = 2.13 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:18.499079: step 42680, loss = 1.93 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:19.656793: step 42690, loss = 2.05 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:20.840002: step 42700, loss = 2.05 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:22.002920: step 42710, loss = 2.17 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:23.162097: step 42720, loss = 2.06 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:24.319318: step 42730, loss = 2.05 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:25.486258: step 42740, loss = 2.14 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:26.648364: step 42750, loss = 2.01 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:27.834731: step 42760, loss = 2.06 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:14:29.013395: step 42770, loss = 1.94 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:30.195927: step 42780, loss = 2.11 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:31.362527: step 42790, loss = 2.06 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:32.540291: step 42800, loss = 2.03 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:33.698099: step 42810, loss = 2.05 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:34.881737: step 42820, loss = 2.12 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:36.054639: step 42830, loss = 2.06 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:37.229392: step 42840, loss = 1.98 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:38.409907: step 42850, loss = 2.04 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:39.588403: step 42860, loss = 1.96 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:40.789554: step 42870, loss = 1.94 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:14:41.968377: step 42880, loss = 2.06 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:43.171655: step 42890, loss = 2.01 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:14:44.348370: step 42900, loss = 1.90 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:45.515992: step 42910, loss = 1.95 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:46.698981: step 42920, loss = 2.09 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:47.891171: step 42930, loss = 2.01 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:14:49.074404: step 42940, loss = 2.15 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:50.258813: step 42950, loss = 1.90 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:51.439711: step 42960, loss = 1.99 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:52.628345: step 42970, loss = 2.11 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:14:53.838815: step 42980, loss = 2.05 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:14:55.030754: step 42990, loss = 2.02 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:14:56.239790: step 43000, loss = 1.92 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:14:57.447829: step 43010, loss = 2.00 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:14:58.653815: step 43020, loss = 2.00 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:14:59.864272: step 43030, loss = 1.97 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:15:01.073540: step 43040, loss = 2.07 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:15:02.275552: step 43050, loss = 2.07 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:15:03.486286: step 43060, loss = 1.89 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:15:04.689643: step 43070, loss = 1.99 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:15:05.891344: step 43080, loss = 2.07 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:15:07.108308: step 43090, loss = 2.15 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:15:08.324898: step 43100, loss = 1.89 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:15:09.537510: step 43110, loss = 2.04 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:15:10.768763: step 43120, loss = 2.08 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:15:11.980120: step 43130, loss = 2.09 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:15:13.191648: step 43140, loss = 1.94 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:15:14.383301: step 43150, loss = 1.93 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:15:15.582020: step 43160, loss = 2.09 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:15:16.770006: step 43170, loss = 2.05 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:15:17.945217: step 43180, loss = 2.08 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:19.123813: step 43190, loss = 1.91 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:20.322058: step 43200, loss = 1.94 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:15:21.506317: step 43210, loss = 2.00 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:22.676932: step 43220, loss = 1.91 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:23.859409: step 43230, loss = 2.06 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:25.030181: step 43240, loss = 1.85 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:26.185937: step 43250, loss = 1.94 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:15:27.370422: step 43260, loss = 2.17 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:28.549369: step 43270, loss = 1.95 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:29.714142: step 43280, loss = 1.98 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:15:30.879553: step 43290, loss = 2.06 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:32.033796: step 43300, loss = 2.08 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-04 23:15:33.208816: step 43310, loss = 2.09 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:34.354321: step 43320, loss = 2.04 (1117.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:15:35.536670: step 43330, loss = 2.09 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:36.713059: step 43340, loss = 2.20 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:37.856756: step 43350, loss = 2.18 (1119.2 examples/sec; 0.114 sec/batch)
2017-05-04 23:15:39.028599: step 43360, loss = 1.87 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:40.182794: step 43370, loss = 1.88 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-04 23:15:41.371835: step 43380, loss = 2.05 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:15:42.534776: step 43390, loss = 2.10 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:15:43.718893: step 43400, loss = 1.89 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:44.878067: step 43410, loss = 2.03 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:15:46.045283: step 43420, loss = 1.94 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:47.198869: step 43430, loss = 1.90 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:15:48.365218: step 43440, loss = 1.95 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:49.541314: step 43450, loss = 1.92 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:50.702824: step 43460, loss = 1.94 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:15:51.873553: step 43470, loss = 2.05 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:53.037082: step 43480, loss = 1.96 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:15:54.171467: step 43490, loss = 2.03 (1128.4 examples/sec; 0.113 sec/batch)
2017-05-04 23:15:55.341986: step 43500, loss = 1.94 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:56.504549: step 43510, loss = 1.97 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:15:57.663450: step 43520, loss = 2.17 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:15:58.818719: step 43530, loss = 1.98 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:00.010381: step 43540, loss = 2.01 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:16:01.183798: step 43550, loss = 1.98 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:02.438310: step 43560, loss = 2.02 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-04 23:16:03.505290: step 43570, loss = 2.09 (1199.6 examples/sec; 0.107 sec/batch)
2017-05-04 23:16:04.697995: step 43580, loss = 2.22 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:16:05.836028: step 43590, loss = 2.16 (1124.7 examples/sec; 0.114 sec/batch)
2017-05-04 23:16:06.995514: step 43600, loss = 2.15 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:08.168927: step 43610, loss = 1.93 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:09.327937: step 43620, loss = 1.93 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:10.486800: step 43630, loss = 2.07 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:11.664976: step 43640, loss = 1.87 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:16:12.836849: step 43650, loss = 2.01 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:13.996147: step 43660, loss = 1.91 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:15.192749: step 43670, loss = 2.16 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:16:16.359924: step 43680, loss = 2.03 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:17.527027: step 43690, loss = 1.84 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:18.720931: step 43700, loss = 2.02 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:16:19.885831: step 43710, loss = 1.75 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:21.055361: step 43720, loss = 1.86 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:22.212715: step 43730, loss = 1.88 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:23.366366: step 43740, loss = 1.93 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:16:24.552052: step 43750, loss = 1.94 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:16:25.732764: step 43760, loss = 2.00 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:16:26.911212: step 43770, loss = 2.00 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:16:28.079427: step 43780, loss = 2.16 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:29.257344: step 43790, loss = 2.04 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:16:30.434092: step 43800, loss = 2.17 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:16:31.601769: step 43810, loss = 2.01 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:32.778071: step 43820, loss = 2.06 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:16:33.941664: step 43830, loss = 2.00 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:35.110784: step 43840, loss = 2.08 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:36.265353: step 43850, loss = 2.17 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:16:37.442381: step 43860, loss = 2.15 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:16:38.597013: step 43870, loss = 1.95 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:16:39.755336: step 43880, loss = 2.06 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:40.943449: step 43890, loss = 2.05 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:16:42.096275: step 43900, loss = 1.92 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:16:43.279033: step 43910, loss = 2.00 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:16:44.445345: step 43920, loss = 1.96 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:45.597609: step 43930, loss = 2.07 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:16:46.779516: step 43940, loss = 1.91 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:16:47.953189: step 43950, loss = 2.21 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:49.126055: step 43960, loss = 2.11 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:50.286727: step 43970, loss = 2.01 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:51.457562: step 43980, loss = 2.15 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:52.632189: step 43990, loss = 2.20 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:53.795106: step 44000, loss = 1.93 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:54.945668: step 44010, loss = 2.07 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:16:56.097919: step 44020, loss = 1.83 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:16:57.274309: step 44030, loss = 2.04 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:16:58.418278: step 44040, loss = 1.96 (1118.9 examples/sec; 0.114 sec/batch)
2017-05-04 23:16:59.584221: step 44050, loss = 1.91 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:00.737350: step 44060, loss = 2.03 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-04 23:17:01.913871: step 44070, loss = 1.89 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:03.072993: step 44080, loss = 2.05 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:17:04.242050: step 44090, loss = 1.95 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:05.406720: step 44100, loss = 2.17 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:17:06.547596: step 44110, loss = 1.92 (1121.9 examples/sec; 0.114 sec/batch)
2017-05-04 23:17:07.720540: step 44120, loss = 1.89 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:08.900825: step 44130, loss = 1.99 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:10.046140: step 44140, loss = 2.04 (1117.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:17:11.222189: step 44150, loss = 2.04 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:12.390698: step 44160, loss = 1.95 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:13.555980: step 44170, loss = 2.10 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:14.718258: step 44180, loss = 1.87 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:17:15.875298: step 44190, loss = 2.14 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:17:17.044702: step 44200, loss = 1.89 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:18.201597: step 44210, loss = 1.95 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:17:19.381647: step 44220, loss = 1.84 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:20.556381: step 44230, loss = 2.04 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:21.718515: step 44240, loss = 1.96 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:17:22.897290: step 44250, loss = 1.93 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:24.090718: step 44260, loss = 2.02 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:17:25.274182: step 44270, loss = 1.97 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:26.465640: step 44280, loss = 1.95 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:17:27.656670: step 44290, loss = 2.00 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:17:28.848372: step 44300, loss = 1.93 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:17:30.026761: step 44310, loss = 2.09 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:31.211285: step 44320, loss = 1.93 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:32.394070: step 44330, loss = 2.13 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:33.555020: step 44340, loss = 2.28 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:17:34.737900: step 44350, loss = 2.02 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:35.909669: step 44360, loss = 1.96 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:37.098134: step 44370, loss = 1.91 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:17:38.265547: step 44380, loss = 2.26 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:39.434828: step 44390, loss = 2.21 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:40.608313: step 44400, loss = 1.81 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:41.779057: step 44410, loss = 1.83 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:42.970777: step 44420, loss = 2.11 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:17:44.175911: step 44430, loss = 2.02 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:17:45.386619: step 44440, loss = 1.86 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:17:46.579092: step 44450, loss = 1.87 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:17:47.793515: step 44460, loss = 1.97 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:17:49.027525: step 44470, loss = 2.17 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:17:50.233517: step 44480, loss = 2.01 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:17:51.443304: step 44490, loss = 2.01 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:17:52.666383: step 44500, loss = 1.91 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:17:53.873689: step 44510, loss = 2.10 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:17:55.082465: step 44520, loss = 1.92 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:17:56.318082: step 44530, loss = 2.18 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-04 23:17:57.528507: step 44540, loss = 2.01 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:17:58.899136: step 44550, loss = 2.03 (933.9 examples/sec; 0.137 sec/batch)
2017-05-04 23:17:59.964902: step 44560, loss = 2.12 (1201.0 examples/sec; 0.107 sec/batch)
2017-05-04 23:18:01.176694: step 44570, loss = 2.05 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:02.391977: step 44580, loss = 1.96 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:03.628294: step 44590, loss = 1.91 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-04 23:18:04.825154: step 44600, loss = 1.96 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:18:06.050192: step 44610, loss = 2.01 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:18:07.264572: step 44620, loss = 1.93 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:08.487436: step 44630, loss = 1.96 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:09.701475: step 44640, loss = 2.10 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:10.940943: step 44650, loss = 2.07 (1032.7 examples/sec; 0.124 sec/batch)
2017-05-04 23:18:12.173774: step 44660, loss = 2.04 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:18:13.405043: step 44670, loss = 1.95 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:18:14.607584: step 44680, loss = 1.99 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:18:15.823059: step 44690, loss = 2.05 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:17.049456: step 44700, loss = 1.96 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:18:18.252780: step 44710, loss = 2.01 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:18:19.464354: step 44720, loss = 2.06 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:20.691743: step 44730, loss = 1.90 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:18:21.869869: step 44740, loss = 2.25 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:18:23.183131: step 44750, loss = 2.02 (974.7 examples/sec; 0.131 sec/batch)
2017-05-04 23:18:24.286817: step 44760, loss = 2.03 (1159.7 examples/sec; 0.110 sec/batch)
2017-05-04 23:18:25.502896: step 44770, loss = 2.05 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:26.697692: step 44780, loss = 2.09 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:18:27.898379: step 44790, loss = 2.00 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:18:29.091747: step 44800, loss = 1.98 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:18:30.290500: step 44810, loss = 1.93 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:18:31.497222: step 44820, loss = 1.96 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:32.702026: step 44830, loss = 2.07 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:18:33.893946: step 44840, loss = 2.16 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:18:35.164554: step 44850, loss = 1.96 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-04 23:18:36.313954: step 44860, loss = 2.02 (1113.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:18:37.527028: step 44870, loss = 2.02 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:38.746649: step 44880, loss = 2.07 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:39.967736: step 44890, loss = 2.10 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:41.201995: step 44900, loss = 1.84 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:18:42.429182: step 44910, loss = 2.05 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:18:43.656298: step 44920, loss = 2.06 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:18:44.873375: step 44930, loss = 2.00 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:46.061445: step 44940, loss = 1.83 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:18:47.249364: step 44950, loss = 2.00 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:18:48.463096: step 44960, loss = 1.97 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:49.678212: step 44970, loss = 1.87 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:50.908841: step 44980, loss = 2.01 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:18:52.100828: step 44990, loss = 2.06 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:18:53.317688: step 45000, loss = 2.01 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:54.528580: step 45010, loss = 2.14 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:55.724101: step 45020, loss = 1.94 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:18:56.904106: step 45030, loss = 2.02 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:18:58.069088: step 45040, loss = 2.10 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:18:59.257094: step 45050, loss = 2.15 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:00.437969: step 45060, loss = 2.15 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:01.610389: step 45070, loss = 1.91 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:02.801216: step 45080, loss = 1.92 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:03.997531: step 45090, loss = 2.01 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:19:05.178849: step 45100, loss = 2.15 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:06.361035: step 45110, loss = 2.03 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:07.537954: step 45120, loss = 2.00 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:08.719710: step 45130, loss = 2.00 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:09.886196: step 45140, loss = 2.08 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:11.074166: step 45150, loss = 1.98 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:12.271965: step 45160, loss = 1.99 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:19:13.453165: step 45170, loss = 2.35 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:14.618164: step 45180, loss = 2.03 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:15.782604: step 45190, loss = 2.03 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:19:16.955393: step 45200, loss = 1.85 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:18.103974: step 45210, loss = 1.84 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:19:19.266917: step 45220, loss = 1.97 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:19:20.459800: step 45230, loss = 2.15 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:21.612209: step 45240, loss = 1.82 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:19:22.770911: step 45250, loss = 1.96 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:19:23.947609: step 45260, loss = 2.05 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:25.121843: step 45270, loss = 2.01 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:26.275698: step 45280, loss = 2.08 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:19:27.444799: step 45290, loss = 2.00 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:28.615772: step 45300, loss = 2.04 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:29.772218: step 45310, loss = 2.19 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:19:30.931529: step 45320, loss = 1.89 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:19:32.104612: step 45330, loss = 2.20 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:33.277582: step 45340, loss = 2.03 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:34.416963: step 45350, loss = 2.00 (1123.4 examples/sec; 0.114 sec/batch)
2017-05-04 23:19:35.572771: step 45360, loss = 2.05 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:19:36.740634: step 45370, loss = 2.14 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:37.923675: step 45380, loss = 1.94 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:39.112816: step 45390, loss = 1.94 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:40.301625: step 45400, loss = 1.97 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:41.492337: step 45410, loss = 2.10 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:42.677704: step 45420, loss = 2.02 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:43.840278: step 45430, loss = 1.94 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:19:45.015659: step 45440, loss = 1.98 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:46.160084: step 45450, loss = 1.98 (1118.5 examples/sec; 0.114 sec/batch)
2017-05-04 23:19:47.345333: step 45460, loss = 2.05 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:48.534760: step 45470, loss = 2.11 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:49.706279: step 45480, loss = 2.00 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:50.891788: step 45490, loss = 1.95 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:52.055529: step 45500, loss = 2.04 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:19:53.227092: step 45510, loss = 1.95 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:54.406056: step 45520, loss = 2.06 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:55.605506: step 45530, loss = 2.16 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:19:56.880077: step 45540, loss = 2.04 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-04 23:19:57.948294: step 45550, loss = 2.01 (1198.3 examples/sec; 0.107 sec/batch)
2017-05-04 23:19:59.124400: step 45560, loss = 2.04 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:00.288045: step 45570, loss = 2.08 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:01.445516: step 45580, loss = 2.12 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:02.639117: step 45590, loss = 1.92 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:20:03.786319: step 45600, loss = 1.97 (1115.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:20:04.965093: step 45610, loss = 1.98 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:06.097377: step 45620, loss = 2.01 (1130.5 examples/sec; 0.113 sec/batch)
2017-05-04 23:20:07.290625: step 45630, loss = 1.95 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:20:08.435200: step 45640, loss = 1.87 (1118.3 examples/sec; 0.114 sec/batch)
2017-05-04 23:20:09.605107: step 45650, loss = 1.83 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:10.763604: step 45660, loss = 1.86 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:11.929761: step 45670, loss = 1.89 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:13.105054: step 45680, loss = 2.06 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:14.266751: step 45690, loss = 2.01 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:15.451986: step 45700, loss = 2.09 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:20:16.627319: step 45710, loss = 2.10 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:17.787836: step 45720, loss = 2.11 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:18.985363: step 45730, loss = 1.92 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:20:20.152688: step 45740, loss = 1.84 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:21.318577: step 45750, loss = 2.04 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:22.473827: step 45760, loss = 2.11 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:23.648893: step 45770, loss = 1.93 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:24.800338: step 45780, loss = 2.03 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:20:25.960565: step 45790, loss = 1.95 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:27.122383: step 45800, loss = 2.02 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:28.308138: step 45810, loss = 1.95 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:20:29.466532: step 45820, loss = 2.24 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:30.642559: step 45830, loss = 2.07 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:31.822287: step 45840, loss = 1.95 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:33.008607: step 45850, loss = 1.94 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:20:34.169032: step 45860, loss = 2.04 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:35.340584: step 45870, loss = 1.96 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:36.511097: step 45880, loss = 1.93 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:37.657043: step 45890, loss = 1.84 (1117.0 examples/sec; 0.115 sec/batch)
2017-05-04 23:20:38.828152: step 45900, loss = 2.09 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:39.988424: step 45910, loss = 1.93 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:41.163484: step 45920, loss = 2.14 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:42.337337: step 45930, loss = 1.89 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:43.516621: step 45940, loss = 2.12 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:44.682309: step 45950, loss = 2.09 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:45.844952: step 45960, loss = 2.00 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:47.032320: step 45970, loss = 2.23 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:20:48.206506: step 45980, loss = 2.06 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:49.408676: step 45990, loss = 2.03 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:20:50.616458: step 46000, loss = 2.02 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:20:51.817364: step 46010, loss = 1.91 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:20:53.034577: step 46020, loss = 1.86 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:20:54.238015: step 46030, loss = 1.94 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:20:55.450497: step 46040, loss = 2.02 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:20:56.666804: step 46050, loss = 2.07 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:20:57.861815: step 46060, loss = 2.17 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:20:59.070152: step 46070, loss = 2.01 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:21:00.281671: step 46080, loss = 1.90 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:21:01.480627: step 46090, loss = 1.92 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:21:02.649888: step 46100, loss = 1.99 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:21:03.825419: step 46110, loss = 2.03 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:05.029508: step 46120, loss = 2.10 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:21:06.191994: step 46130, loss = 2.06 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:07.395411: step 46140, loss = 1.77 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:21:08.566612: step 46150, loss = 1.93 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:21:09.738304: step 46160, loss = 2.02 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:21:10.898724: step 46170, loss = 2.01 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:12.063316: step 46180, loss = 2.07 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:13.240379: step 46190, loss = 1.98 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:14.387523: step 46200, loss = 1.90 (1115.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:21:15.548880: step 46210, loss = 1.97 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:16.715502: step 46220, loss = 2.05 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:21:17.867340: step 46230, loss = 1.96 (1111.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:21:19.027832: step 46240, loss = 1.88 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:20.211155: step 46250, loss = 1.97 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:21.367104: step 46260, loss = 1.97 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:22.522561: step 46270, loss = 2.26 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:23.677920: step 46280, loss = 2.01 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:24.866490: step 46290, loss = 1.79 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:21:26.028341: step 46300, loss = 2.06 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:27.197566: step 46310, loss = 1.99 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:21:28.351265: step 46320, loss = 1.94 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:21:29.520724: step 46330, loss = 1.97 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:21:30.684009: step 46340, loss = 2.12 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:31.852929: step 46350, loss = 2.12 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:21:33.008431: step 46360, loss = 2.00 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:34.164475: step 46370, loss = 2.17 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:35.358491: step 46380, loss = 1.98 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:21:36.544914: step 46390, loss = 2.05 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:21:37.705844: step 46400, loss = 2.25 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:38.893582: step 46410, loss = 2.02 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:21:40.071930: step 46420, loss = 1.99 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:41.251178: step 46430, loss = 2.11 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:42.419655: step 46440, loss = 1.96 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:21:43.601430: step 46450, loss = 2.02 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:44.802900: step 46460, loss = 1.98 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:21:45.987103: step 46470, loss = 1.87 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:47.184128: step 46480, loss = 2.07 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:21:48.383648: step 46490, loss = 2.05 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:21:49.555634: step 46500, loss = 1.90 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:21:50.741335: step 46510, loss = 2.16 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:21:51.951824: step 46520, loss = 1.89 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:21:53.263211: step 46530, loss = 1.98 (976.1 examples/sec; 0.131 sec/batch)
2017-05-04 23:21:54.349951: step 46540, loss = 1.97 (1177.8 examples/sec; 0.109 sec/batch)
2017-05-04 23:21:55.558100: step 46550, loss = 2.19 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:21:56.764191: step 46560, loss = 2.08 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:21:57.960582: step 46570, loss = 1.96 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:21:59.162759: step 46580, loss = 1.96 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:22:00.354883: step 46590, loss = 2.06 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:22:01.549129: step 46600, loss = 2.02 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:22:02.720812: step 46610, loss = 2.13 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:03.893088: step 46620, loss = 1.90 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:05.062093: step 46630, loss = 1.95 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:06.249001: step 46640, loss = 1.97 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:22:07.414115: step 46650, loss = 1.97 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:08.582103: step 46660, loss = 2.09 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:09.773820: step 46670, loss = 2.08 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:22:10.960205: step 46680, loss = 1.93 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:22:12.150847: step 46690, loss = 1.96 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:22:13.346747: step 46700, loss = 1.97 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:22:14.513396: step 46710, loss = 1.84 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:15.669679: step 46720, loss = 2.13 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:16.837654: step 46730, loss = 2.01 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:17.987520: step 46740, loss = 1.92 (1113.2 examples/sec; 0.115 sec/batch)
2017-05-04 23:22:19.155110: step 46750, loss = 2.04 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:20.299851: step 46760, loss = 1.98 (1118.2 examples/sec; 0.114 sec/batch)
2017-05-04 23:22:21.452030: step 46770, loss = 2.10 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:22:22.599159: step 46780, loss = 2.20 (1115.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:22:23.782289: step 46790, loss = 1.96 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:24.963772: step 46800, loss = 1.93 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:26.117747: step 46810, loss = 2.08 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-04 23:22:27.297656: step 46820, loss = 1.93 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:28.476437: step 46830, loss = 2.20 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:29.636795: step 46840, loss = 1.81 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:30.815081: step 46850, loss = 2.05 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:32.005416: step 46860, loss = 1.91 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:22:33.167422: step 46870, loss = 2.09 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:34.324348: step 46880, loss = 1.94 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:35.487053: step 46890, loss = 2.16 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:36.658940: step 46900, loss = 1.97 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:37.802762: step 46910, loss = 1.94 (1119.1 examples/sec; 0.114 sec/batch)
2017-05-04 23:22:38.984038: step 46920, loss = 1.97 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:40.141795: step 46930, loss = 2.02 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:41.314133: step 46940, loss = 1.96 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:42.480461: step 46950, loss = 2.01 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:43.637782: step 46960, loss = 2.02 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:44.798219: step 46970, loss = 1.90 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:45.956704: step 46980, loss = 1.99 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:47.141950: step 46990, loss = 2.16 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:22:48.307867: step 47000, loss = 1.97 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:49.479492: step 47010, loss = 2.01 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:50.653433: step 47020, loss = 2.22 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:51.838409: step 47030, loss = 1.85 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:53.005670: step 47040, loss = 1.89 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:54.181470: step 47050, loss = 1.84 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:55.362464: step 47060, loss = 1.94 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:56.545342: step 47070, loss = 2.01 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:57.703491: step 47080, loss = 2.03 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:58.862418: step 47090, loss = 1.79 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:00.017561: step 47100, loss = 2.01 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:01.176065: step 47110, loss = 2.09 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:02.327685: step 47120, loss = 2.19 (1111.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:23:03.488209: step 47130, loss = 1.96 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:04.672343: step 47140, loss = 1.94 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:23:05.832682: step 47150, loss = 2.01 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:06.994275: step 47160, loss = 2.05 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:08.151184: step 47170, loss = 1.96 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:09.323498: step 47180, loss = 2.00 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:10.489649: step 47190, loss = 1.98 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:11.652974: step 47200, loss = 1.96 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:12.819617: step 47210, loss = 1.97 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:13.963486: step 47220, loss = 2.11 (1119.0 examples/sec; 0.114 sec/batch)
2017-05-04 23:23:15.126759: step 47230, loss = 1.99 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:16.291780: step 47240, loss = 2.19 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:17.465665: step 47250, loss = 1.93 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:18.625842: step 47260, loss = 1.80 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:19.820854: step 47270, loss = 2.00 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:23:20.994414: step 47280, loss = 1.89 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:22.139264: step 47290, loss = 1.93 (1118.1 examples/sec; 0.114 sec/batch)
2017-05-04 23:23:23.298986: step 47300, loss = 2.05 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:24.472390: step 47310, loss = 1.96 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:25.626692: step 47320, loss = 2.07 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:23:26.805431: step 47330, loss = 1.90 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:23:27.984376: step 47340, loss = 2.08 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:23:29.160773: step 47350, loss = 1.94 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:23:30.317773: step 47360, loss = 1.89 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:31.503024: step 47370, loss = 1.98 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:23:32.670756: step 47380, loss = 1.92 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:33.834261: step 47390, loss = 2.01 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:35.002419: step 47400, loss = 2.08 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:36.180676: step 47410, loss = 1.96 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:23:37.347318: step 47420, loss = 2.10 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:38.514598: step 47430, loss = 1.96 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:39.688661: step 47440, loss = 1.95 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:40.863663: step 47450, loss = 1.97 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:42.021661: step 47460, loss = 2.00 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:43.220157: step 47470, loss = 2.01 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:23:44.403997: step 47480, loss = 2.01 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:23:45.574171: step 47490, loss = 2.04 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:46.751157: step 47500, loss = 2.07 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:23:47.916883: step 47510, loss = 2.02 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:49.193845: step 47520, loss = 1.93 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-04 23:23:50.274517: step 47530, loss = 1.92 (1184.4 examples/sec; 0.108 sec/batch)
2017-05-04 23:23:51.445910: step 47540, loss = 2.08 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:52.631753: step 47550, loss = 2.03 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:23:53.760861: step 47560, loss = 2.01 (1133.6 examples/sec; 0.113 sec/batch)
2017-05-04 23:23:54.934606: step 47570, loss = 1.92 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:56.108292: step 47580, loss = 2.07 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:57.302507: step 47590, loss = 1.98 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:23:58.468264: step 47600, loss = 2.06 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:59.639691: step 47610, loss = 1.89 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:00.801552: step 47620, loss = 1.98 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:24:01.971248: step 47630, loss = 2.07 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:03.135450: step 47640, loss = 1.98 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:24:04.321218: step 47650, loss = 1.94 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:05.503426: step 47660, loss = 2.03 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:06.666590: step 47670, loss = 2.12 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:24:07.845866: step 47680, loss = 1.84 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:09.045754: step 47690, loss = 1.90 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:24:10.209905: step 47700, loss = 1.95 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:24:11.403124: step 47710, loss = 1.82 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:12.576262: step 47720, loss = 1.83 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:13.747284: step 47730, loss = 1.86 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:14.919411: step 47740, loss = 2.03 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:16.108519: step 47750, loss = 1.97 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:17.291348: step 47760, loss = 1.80 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:18.468101: step 47770, loss = 2.14 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:19.660035: step 47780, loss = 2.05 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:20.856539: step 47790, loss = 2.07 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:24:22.051011: step 47800, loss = 1.99 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:23.232692: step 47810, loss = 2.01 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:24.397848: step 47820, loss = 2.07 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:25.569721: step 47830, loss = 1.95 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:26.744535: step 47840, loss = 1.95 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:27.935984: step 47850, loss = 1.90 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:29.135030: step 47860, loss = 1.96 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:24:30.310332: step 47870, loss = 1.99 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:31.507807: step 47880, loss = 1.98 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:24:32.696346: step 47890, loss = 1.91 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:33.857500: step 47900, loss = 1.89 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:24:35.056142: step 47910, loss = 1.81 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:24:36.258885: step 47920, loss = 2.02 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:24:37.438999: step 47930, loss = 2.13 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:38.635915: step 47940, loss = 2.09 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:24:39.789340: step 47950, loss = 1.90 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:24:40.952764: step 47960, loss = 1.82 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:24:42.112817: step 47970, loss = 1.94 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:24:43.294617: step 47980, loss = 1.94 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:44.473045: step 47990, loss = 2.02 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:45.644339: step 48000, loss = 2.05 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:46.824665: step 48010, loss = 2.04 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:48.007100: step 48020, loss = 2.01 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:49.173598: step 48030, loss = 1.93 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:50.373434: step 48040, loss = 1.89 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:24:51.560409: step 48050, loss = 2.22 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:52.746650: step 48060, loss = 2.19 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:53.923622: step 48070, loss = 1.90 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:55.108894: step 48080, loss = 2.02 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:56.310154: step 48090, loss = 2.11 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:24:57.494739: step 48100, loss = 2.15 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:58.669170: step 48110, loss = 1.92 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:59.849293: step 48120, loss = 1.78 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:25:01.047863: step 48130, loss = 1.94 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:25:02.238489: step 48140, loss = 2.12 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:03.414513: step 48150, loss = 1.88 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:25:04.606076: step 48160, loss = 1.99 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:05.775214: step 48170, loss = 1.91 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:06.966148: step 48180, loss = 1.98 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:08.150684: step 48190, loss = 1.90 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:25:09.327889: step 48200, loss = 2.05 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:25:10.485622: step 48210, loss = 2.09 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:11.643035: step 48220, loss = 2.05 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:12.822474: step 48230, loss = 1.86 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:25:13.960975: step 48240, loss = 1.92 (1124.3 examples/sec; 0.114 sec/batch)
2017-05-04 23:25:15.121999: step 48250, loss = 1.99 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:16.294869: step 48260, loss = 2.09 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:17.455774: step 48270, loss = 2.05 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:18.627199: step 48280, loss = 2.00 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:19.794169: step 48290, loss = 1.89 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:20.979915: step 48300, loss = 2.03 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:22.156348: step 48310, loss = 1.96 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:25:23.328402: step 48320, loss = 1.98 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:24.499097: step 48330, loss = 1.97 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:25.652332: step 48340, loss = 1.97 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:25:26.828571: step 48350, loss = 2.05 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:25:27.990059: step 48360, loss = 2.12 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:29.150911: step 48370, loss = 1.95 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:30.303885: step 48380, loss = 1.96 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-04 23:25:31.476726: step 48390, loss = 1.84 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:32.643551: step 48400, loss = 1.97 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:33.810497: step 48410, loss = 2.07 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:34.968189: step 48420, loss = 1.97 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:36.134629: step 48430, loss = 2.01 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:37.299693: step 48440, loss = 1.95 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:38.466749: step 48450, loss = 2.10 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:39.648453: step 48460, loss = 2.14 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:25:40.808416: step 48470, loss = 2.16 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:41.971379: step 48480, loss = 1.98 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:43.172161: step 48490, loss = 1.94 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:25:44.366392: step 48500, loss = 1.86 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:45.654883: step 48510, loss = 2.13 (993.4 examples/sec; 0.129 sec/batch)
2017-05-04 23:25:46.753592: step 48520, loss = 2.09 (1165.0 examples/sec; 0.110 sec/batch)
2017-05-04 23:25:47.963394: step 48530, loss = 2.01 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:25:49.171536: step 48540, loss = 2.00 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:25:50.358433: step 48550, loss = 2.05 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:51.553332: step 48560, loss = 2.07 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:52.754688: step 48570, loss = 2.03 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:25:53.943051: step 48580, loss = 1.88 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:55.132765: step 48590, loss = 1.90 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:56.296292: step 48600, loss = 2.18 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:57.462610: step 48610, loss = 2.11 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:58.634064: step 48620, loss = 1.95 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:59.797058: step 48630, loss = 2.05 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:00.952625: step 48640, loss = 1.90 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:02.095433: step 48650, loss = 1.97 (1120.0 examples/sec; 0.114 sec/batch)
2017-05-04 23:26:03.256855: step 48660, loss = 1.88 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:04.429477: step 48670, loss = 1.83 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:05.593078: step 48680, loss = 1.94 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:06.750847: step 48690, loss = 2.05 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:07.927491: step 48700, loss = 1.90 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:26:09.082586: step 48710, loss = 1.96 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:10.233001: step 48720, loss = 2.05 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:26:11.379181: step 48730, loss = 1.81 (1116.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:26:12.573520: step 48740, loss = 2.07 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:26:13.725091: step 48750, loss = 1.93 (1111.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:26:14.884567: step 48760, loss = 1.95 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:16.031460: step 48770, loss = 2.14 (1116.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:26:17.191685: step 48780, loss = 1.87 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:18.363299: step 48790, loss = 1.85 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:19.518795: step 48800, loss = 2.18 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:20.675407: step 48810, loss = 1.95 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:21.857870: step 48820, loss = 1.99 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:26:23.017972: step 48830, loss = 1.90 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:24.199961: step 48840, loss = 2.25 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:26:25.364712: step 48850, loss = 1.91 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:26.531842: step 48860, loss = 1.81 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:27.720825: step 48870, loss = 1.89 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:26:28.885649: step 48880, loss = 2.18 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:30.044328: step 48890, loss = 1.81 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:31.218290: step 48900, loss = 2.00 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:32.385388: step 48910, loss = 2.05 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:33.550954: step 48920, loss = 2.02 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:34.713344: step 48930, loss = 2.05 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:35.881811: step 48940, loss = 2.05 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:37.032085: step 48950, loss = 2.06 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:26:38.198152: step 48960, loss = 2.05 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:39.376954: step 48970, loss = 1.90 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:26:40.559966: step 48980, loss = 1.95 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:26:41.719937: step 48990, loss = 1.96 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:42.891474: step 49000, loss = 1.80 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:44.032736: step 49010, loss = 1.97 (1121.6 examples/sec; 0.114 sec/batch)
2017-05-04 23:26:45.197592: step 49020, loss = 2.02 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:46.356337: step 49030, loss = 2.07 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:47.513893: step 49040, loss = 1.91 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:48.707409: step 49050, loss = 1.94 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:26:49.872758: step 49060, loss = 2.10 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:51.060304: step 49070, loss = 1.95 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:26:52.234166: step 49080, loss = 1.86 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:53.404148: step 49090, loss = 1.92 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:54.568398: step 49100, loss = 2.01 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:55.742838: step 49110, loss = 1.98 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:56.927368: step 49120, loss = 2.04 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:26:58.087070: step 49130, loss = 2.06 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:59.255140: step 49140, loss = 1.82 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:00.426581: step 49150, loss = 1.79 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:01.596002: step 49160, loss = 1.98 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:02.762373: step 49170, loss = 1.84 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:03.940361: step 49180, loss = 2.03 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:05.110816: step 49190, loss = 2.10 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:06.268333: step 49200, loss = 1.88 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:27:07.437876: step 49210, loss = 2.00 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:08.625158: step 49220, loss = 1.94 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:27:09.785746: step 49230, loss = 2.13 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:27:10.963368: step 49240, loss = 2.06 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:12.118821: step 49250, loss = 1.93 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:27:13.282500: step 49260, loss = 1.94 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:27:14.463367: step 49270, loss = 1.96 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:15.631950: step 49280, loss = 2.06 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:16.816218: step 49290, loss = 2.00 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:17.983025: step 49300, loss = 1.83 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:19.162493: step 49310, loss = 2.05 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:20.326223: step 49320, loss = 1.97 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:27:21.503690: step 49330, loss = 1.89 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:22.697733: step 49340, loss = 1.98 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:27:23.880083: step 49350, loss = 1.92 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:25.065958: step 49360, loss = 1.94 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:27:26.229426: step 49370, loss = 2.10 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:27:27.405998: step 49380, loss = 1.91 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:28.598569: step 49390, loss = 2.14 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:27:29.797148: step 49400, loss = 2.01 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:30.985764: step 49410, loss = 2.04 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:27:32.168438: step 49420, loss = 2.14 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:33.352348: step 49430, loss = 1.93 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:34.542745: step 49440, loss = 2.04 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:27:35.740363: step 49450, loss = 1.89 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:36.943441: step 49460, loss = 1.92 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:38.136853: step 49470, loss = 2.16 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:27:39.340512: step 49480, loss = 1.96 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:40.552452: step 49490, loss = 2.08 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:27:41.871309: step 49500, loss = 1.98 (970.5 examples/sec; 0.132 sec/batch)
2017-05-04 23:27:42.987259: step 49510, loss = 1.92 (1147.0 examples/sec; 0.112 sec/batch)
2017-05-04 23:27:44.205555: step 49520, loss = 2.10 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:27:45.422470: step 49530, loss = 1.95 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:27:46.649619: step 49540, loss = 1.91 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:27:47.850621: step 49550, loss = 2.07 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:49.060648: step 49560, loss = 2.08 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:27:50.253716: step 49570, loss = 1.93 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:27:51.490197: step 49580, loss = 2.18 (1035.2 examples/sec; 0.124 sec/batch)
2017-05-04 23:27:52.699788: step 49590, loss = 2.05 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:27:53.901891: step 49600, loss = 2.15 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:55.090821: step 49610, loss = 2.01 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:27:56.293423: step 49620, loss = 2.03 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:57.490011: step 49630, loss = 1.80 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:58.690080: step 49640, loss = 2.08 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:59.890333: step 49650, loss = 1.96 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:28:01.094199: step 49660, loss = 1.78 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:28:02.305077: step 49670, loss = 1.92 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:28:03.501734: step 49680, loss = 1.96 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:28:04.695052: step 49690, loss = 1.81 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:28:05.898309: step 49700, loss = 2.06 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:28:07.051147: step 49710, loss = 1.92 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:28:08.235465: step 49720, loss = 2.01 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:09.421025: step 49730, loss = 2.14 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:28:10.565998: step 49740, loss = 2.13 (1117.9 examples/sec; 0.114 sec/batch)
2017-05-04 23:28:11.749344: step 49750, loss = 1.94 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:12.929122: step 49760, loss = 2.10 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:14.095946: step 49770, loss = 2.06 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:15.255387: step 49780, loss = 1.93 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:16.423932: step 49790, loss = 1.93 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:17.575965: step 49800, loss = 1.96 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:28:18.744954: step 49810, loss = 1.96 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:19.911815: step 49820, loss = 1.89 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:21.053812: step 49830, loss = 1.91 (1120.8 examples/sec; 0.114 sec/batch)
2017-05-04 23:28:22.228781: step 49840, loss = 2.13 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:23.403808: step 49850, loss = 2.02 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:24.585150: step 49860, loss = 1.89 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:25.754561: step 49870, loss = 2.05 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:26.920504: step 49880, loss = 1.86 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:28.086269: step 49890, loss = 1.96 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:29.266132: step 49900, loss = 1.82 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:30.423556: step 49910, loss = 2.09 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:31.579785: step 49920, loss = 1.96 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:32.741334: step 49930, loss = 1.95 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:33.903307: step 49940, loss = 1.89 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:35.059292: step 49950, loss = 2.12 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:36.247306: step 49960, loss = 1.93 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:28:37.426869: step 49970, loss = 1.99 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:38.592770: step 49980, loss = 2.09 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:39.771480: step 49990, loss = 1.83 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:40.943140: step 50000, loss = 2.12 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:42.104504: step 50010, loss = 1.93 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:43.264823: step 50020, loss = 2.12 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:44.435170: step 50030, loss = 1.88 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:45.603057: step 50040, loss = 2.10 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:46.771530: step 50050, loss = 2.18 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:47.938576: step 50060, loss = 2.08 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:49.098551: step 50070, loss = 2.09 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:50.261571: step 50080, loss = 2.13 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:51.453746: step 50090, loss = 1.82 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:28:52.651867: step 50100, loss = 2.07 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:28:53.860368: step 50110, loss = 1.89 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:28:55.030119: step 50120, loss = 1.95 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:56.201951: step 50130, loss = 1.96 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:57.359996: step 50140, loss = 2.01 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:58.518544: step 50150, loss = 1.99 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:59.660448: step 50160, loss = 1.96 (1120.9 examples/sec; 0.114 sec/batch)
2017-05-04 23:29:00.826534: step 50170, loss = 2.08 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:01.974748: step 50180, loss = 2.02 (1114.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:29:03.170595: step 50190, loss = 2.06 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:29:04.320554: step 50200, loss = 2.08 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:29:05.501748: step 50210, loss = 1.85 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:06.673645: step 50220, loss = 1.98 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:07.834035: step 50230, loss = 2.02 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:09.018397: step 50240, loss = 1.86 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:10.176916: step 50250, loss = 2.03 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:11.360884: step 50260, loss = 2.10 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:12.531222: step 50270, loss = 1.95 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:13.693128: step 50280, loss = 2.01 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:14.865186: step 50290, loss = 2.07 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:16.053621: step 50300, loss = 2.17 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:29:17.219072: step 50310, loss = 1.92 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:18.385104: step 50320, loss = 2.19 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:19.541550: step 50330, loss = 2.05 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:20.727740: step 50340, loss = 1.96 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:29:21.922111: step 50350, loss = 1.98 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:29:23.117154: step 50360, loss = 2.00 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:29:24.301210: step 50370, loss = 2.02 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:25.473014: step 50380, loss = 1.87 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:26.622521: step 50390, loss = 1.92 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:29:27.804138: step 50400, loss = 2.01 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:28.975550: step 50410, loss = 2.12 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:30.129134: step 50420, loss = 1.96 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:29:31.293593: step 50430, loss = 2.02 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:32.458855: step 50440, loss = 2.06 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:33.613158: step 50450, loss = 2.01 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:29:34.771675: step 50460, loss = 2.13 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:35.935599: step 50470, loss = 1.98 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:37.099510: step 50480, loss = 2.05 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:38.347926: step 50490, loss = 1.97 (1025.3 examples/sec; 0.125 sec/batch)
2017-05-04 23:29:39.416093: step 50500, loss = 1.89 (1198.3 examples/sec; 0.107 sec/batch)
2017-05-04 23:29:40.585322: step 50510, loss = 2.05 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:41.764327: step 50520, loss = 1.89 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:42.932740: step 50530, loss = 1.94 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:44.110997: step 50540, loss = 2.06 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:45.267960: step 50550, loss = 2.10 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:46.430875: step 50560, loss = 2.09 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:47.602522: step 50570, loss = 2.06 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:48.776861: step 50580, loss = 1.94 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:49.922899: step 50590, loss = 2.05 (1116.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:29:51.083489: step 50600, loss = 2.10 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:52.275666: step 50610, loss = 1.94 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:29:53.452015: step 50620, loss = 1.98 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:54.621082: step 50630, loss = 1.80 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:55.796042: step 50640, loss = 1.74 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:56.963270: step 50650, loss = 2.00 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:58.124261: step 50660, loss = 2.03 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:59.301640: step 50670, loss = 2.07 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:00.485457: step 50680, loss = 1.87 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:01.664978: step 50690, loss = 2.05 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:02.841598: step 50700, loss = 2.16 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:04.013121: step 50710, loss = 1.81 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:05.195814: step 50720, loss = 2.01 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:06.348486: step 50730, loss = 1.92 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:30:07.522376: step 50740, loss = 2.00 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:08.697262: step 50750, loss = 1.89 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:09.861611: step 50760, loss = 1.99 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:11.035184: step 50770, loss = 2.05 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:12.191812: step 50780, loss = 2.01 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:13.340126: step 50790, loss = 1.91 (1114.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:30:14.504834: step 50800, loss = 2.07 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:15.673810: step 50810, loss = 2.15 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:16.843320: step 50820, loss = 1.95 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:17.990222: step 50830, loss = 2.01 (1116.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:30:19.154613: step 50840, loss = 2.06 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:20.332404: step 50850, loss = 1.91 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:21.474002: step 50860, loss = 1.96 (1121.2 examples/sec; 0.114 sec/batch)
2017-05-04 23:30:22.642797: step 50870, loss = 2.06 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:23.812422: step 50880, loss = 2.06 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:24.972115: step 50890, loss = 1.94 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:26.163447: step 50900, loss = 1.92 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:30:27.322698: step 50910, loss = 2.06 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:28.491814: step 50920, loss = 2.10 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:29.641380: step 50930, loss = 2.12 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:30:30.806487: step 50940, loss = 2.04 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:31.967933: step 50950, loss = 1.98 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:33.130752: step 50960, loss = 1.97 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:34.283048: step 50970, loss = 2.11 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:30:35.446539: step 50980, loss = 2.05 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:36.634067: step 50990, loss = 2.02 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:30:37.807724: step 51000, loss = 1.91 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:38.976036: step 51010, loss = 1.84 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:40.157800: step 51020, loss = 2.00 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:41.327760: step 51030, loss = 1.96 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:42.515559: step 51040, loss = 2.09 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:30:43.723360: step 51050, loss = 1.95 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:30:44.942182: step 51060, loss = 2.04 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:30:46.142498: step 51070, loss = 1.96 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:30:47.377188: step 51080, loss = 1.99 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:30:48.582167: step 51090, loss = 1.97 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:30:49.808363: step 51100, loss = 2.02 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:30:51.040362: step 51110, loss = 1.99 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:30:52.270855: step 51120, loss = 1.91 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:30:53.503184: step 51130, loss = 1.96 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:30:54.718729: step 51140, loss = 1.91 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:30:55.977664: step 51150, loss = 2.07 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-04 23:30:57.179850: step 51160, loss = 2.05 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:30:58.394749: step 51170, loss = 2.09 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:30:59.593265: step 51180, loss = 1.99 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:00.848562: step 51190, loss = 2.06 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-04 23:31:02.074903: step 51200, loss = 1.92 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:31:03.301505: step 51210, loss = 1.97 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:31:04.504536: step 51220, loss = 2.08 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:05.731476: step 51230, loss = 1.83 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:31:06.933668: step 51240, loss = 2.14 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:08.186180: step 51250, loss = 1.97 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-04 23:31:09.377387: step 51260, loss = 1.93 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:31:10.613069: step 51270, loss = 2.04 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-04 23:31:11.832361: step 51280, loss = 2.09 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:13.034321: step 51290, loss = 1.83 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:14.236611: step 51300, loss = 2.05 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:15.464437: step 51310, loss = 1.86 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:31:16.714459: step 51320, loss = 2.07 (1024.0 examples/sec; 0.125 sec/batch)
2017-05-04 23:31:17.942262: step 51330, loss = 1.88 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:31:19.158372: step 51340, loss = 1.77 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:20.385585: step 51350, loss = 2.02 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:31:21.588954: step 51360, loss = 2.13 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:22.819630: step 51370, loss = 1.95 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:31:24.041314: step 51380, loss = 2.00 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:25.261862: step 51390, loss = 2.01 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:26.477076: step 51400, loss = 1.90 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:27.673917: step 51410, loss = 2.10 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:28.886651: step 51420, loss = 2.09 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:31:30.103726: step 51430, loss = 1.96 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:31.312448: step 51440, loss = 1.99 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:31:32.543263: step 51450, loss = 2.27 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:31:33.753268: step 51460, loss = 2.06 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:31:34.970356: step 51470, loss = 1.94 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:36.312437: step 51480, loss = 1.99 (953.7 examples/sec; 0.134 sec/batch)
2017-05-04 23:31:37.410952: step 51490, loss = 1.90 (1165.2 examples/sec; 0.110 sec/batch)
2017-05-04 23:31:38.598682: step 51500, loss = 1.91 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:31:39.804438: step 51510, loss = 2.19 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:31:41.025085: step 51520, loss = 1.96 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:42.244376: step 51530, loss = 2.06 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:43.456272: step 51540, loss = 2.12 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:31:44.655292: step 51550, loss = 2.14 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:45.861859: step 51560, loss = 1.95 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:31:47.075963: step 51570, loss = 1.94 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:31:48.269630: step 51580, loss = 2.11 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:31:49.468679: step 51590, loss = 2.01 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:50.645340: step 51600, loss = 1.80 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:31:51.853368: step 51610, loss = 2.16 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:31:53.054859: step 51620, loss = 2.08 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:54.230358: step 51630, loss = 2.08 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:31:55.430695: step 51640, loss = 2.02 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:56.625148: step 51650, loss = 2.00 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:31:57.813595: step 51660, loss = 1.96 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:31:59.036420: step 51670, loss = 2.04 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:32:00.312706: step 51680, loss = 2.00 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-04 23:32:01.450877: step 51690, loss = 1.96 (1124.6 examples/sec; 0.114 sec/batch)
2017-05-04 23:32:02.641143: step 51700, loss = 2.02 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:03.842950: step 51710, loss = 2.16 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:32:05.038140: step 51720, loss = 1.91 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:32:06.227454: step 51730, loss = 1.88 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:07.398377: step 51740, loss = 1.90 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:08.587107: step 51750, loss = 2.08 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:09.768149: step 51760, loss = 2.05 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:32:10.952483: step 51770, loss = 2.08 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:32:12.129113: step 51780, loss = 1.89 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:32:13.302859: step 51790, loss = 2.03 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:14.489710: step 51800, loss = 2.05 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:15.692122: step 51810, loss = 1.99 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:32:16.900441: step 51820, loss = 2.05 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:32:18.109127: step 51830, loss = 2.05 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:32:19.322781: step 51840, loss = 2.08 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:32:20.528662: step 51850, loss = 2.22 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:32:21.702134: step 51860, loss = 2.01 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:22.890084: step 51870, loss = 1.94 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:24.080019: step 51880, loss = 1.91 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:25.266892: step 51890, loss = 1.98 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:26.448729: step 51900, loss = 1.92 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:32:27.622485: step 51910, loss = 2.02 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:28.801355: step 51920, loss = 2.07 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:32:29.972808: step 51930, loss = 2.04 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:31.114787: step 51940, loss = 2.01 (1120.9 examples/sec; 0.114 sec/batch)
2017-05-04 23:32:32.285790: step 51950, loss = 2.00 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:33.449038: step 51960, loss = 2.21 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:32:34.614915: step 51970, loss = 2.04 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:35.802931: step 51980, loss = 1.91 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:36.985937: step 51990, loss = 2.12 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:32:38.148536: step 52000, loss = 2.02 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:32:39.318412: step 52010, loss = 1.95 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:40.489634: step 52020, loss = 1.91 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:41.659679: step 52030, loss = 1.93 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:42.842096: step 52040, loss = 2.19 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:32:44.007160: step 52050, loss = 2.12 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:45.196331: step 52060, loss = 2.03 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:46.345657: step 52070, loss = 1.89 (1113.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:32:47.516670: step 52080, loss = 1.97 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:48.691409: step 52090, loss = 1.93 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:49.850758: step 52100, loss = 1.98 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:32:50.996762: step 52110, loss = 1.89 (1116.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:32:52.170692: step 52120, loss = 2.03 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:53.341855: step 52130, loss = 1.98 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:54.510868: step 52140, loss = 1.93 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:55.668576: step 52150, loss = 2.08 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:32:56.833739: step 52160, loss = 1.91 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:57.976047: step 52170, loss = 2.09 (1120.6 examples/sec; 0.114 sec/batch)
2017-05-04 23:32:59.154327: step 52180, loss = 1.92 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:33:00.332632: step 52190, loss = 2.02 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:33:01.523478: step 52200, loss = 2.09 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:33:02.674637: step 52210, loss = 2.03 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:33:03.846369: step 52220, loss = 2.05 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:33:05.021813: step 52230, loss = 2.01 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:33:06.179634: step 52240, loss = 2.08 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:33:07.364203: step 52250, loss = 1.91 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:33:08.553008: step 52260, loss = 1.83 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:33:09.722374: step 52270, loss = 1.86 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:33:10.907400: step 52280, loss = 2.04 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:33:12.093890: step 52290, loss = 1.98 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:33:13.280175: step 52300, loss = 1.92 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:33:14.465715: step 52310, loss = 1.89 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:33:15.656620: step 52320, loss = 2.02 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:33:16.841561: step 52330, loss = 1.83 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:33:18.005719: step 52340, loss = 2.04 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:33:19.207353: step 52350, loss = 1.97 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:33:20.392466: step 52360, loss = 1.91 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:33:21.583111: step 52370, loss = 1.99 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:33:22.785820: step 52380, loss = 2.07 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:33:23.992972: step 52390, loss = 2.01 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:33:25.211905: step 52400, loss = 2.02 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:26.421165: step 52410, loss = 1.92 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:33:27.640593: step 52420, loss = 2.15 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:28.839501: step 52430, loss = 2.04 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:33:30.057808: step 52440, loss = 1.96 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:31.277699: step 52450, loss = 1.89 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:32.496044: step 52460, loss = 2.14 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:33.816391: step 52470, loss = 1.97 (969.4 examples/sec; 0.132 sec/batch)
2017-05-04 23:33:34.911115: step 52480, loss = 2.02 (1169.2 examples/sec; 0.109 sec/batch)
2017-05-04 23:33:36.153126: step 52490, loss = 1.99 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-04 23:33:37.363093: step 52500, loss = 1.98 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:33:38.598748: step 52510, loss = 1.93 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-04 23:33:39.824550: step 52520, loss = 1.82 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:33:41.051877: step 52530, loss = 1.98 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:33:42.243827: step 52540, loss = 2.00 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:33:43.460442: step 52550, loss = 1.91 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:44.666220: step 52560, loss = 1.99 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:33:45.868995: step 52570, loss = 2.00 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:33:47.084548: step 52580, loss = 2.09 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:48.303597: step 52590, loss = 2.03 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:49.498957: step 52600, loss = 1.90 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:33:50.714417: step 52610, loss = 2.03 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:51.943415: step 52620, loss = 2.03 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:33:53.159004: step 52630, loss = 1.92 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:54.356551: step 52640, loss = 2.00 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:33:55.567617: step 52650, loss = 1.80 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:33:56.768085: step 52660, loss = 1.90 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:33:58.057337: step 52670, loss = 1.94 (992.8 examples/sec; 0.129 sec/batch)
2017-05-04 23:33:59.184836: step 52680, loss = 2.21 (1135.3 examples/sec; 0.113 sec/batch)
2017-05-04 23:34:00.394701: step 52690, loss = 1.95 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:01.630907: step 52700, loss = 2.24 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-04 23:34:02.853347: step 52710, loss = 1.93 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:04.061608: step 52720, loss = 2.08 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:05.305375: step 52730, loss = 1.87 (1029.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:34:06.525477: step 52740, loss = 2.00 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:07.745210: step 52750, loss = 2.01 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:08.965648: step 52760, loss = 1.87 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:10.197855: step 52770, loss = 1.98 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:11.409527: step 52780, loss = 2.07 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:12.632859: step 52790, loss = 1.95 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:13.838936: step 52800, loss = 2.01 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:15.078975: step 52810, loss = 1.92 (1032.2 examples/sec; 0.124 sec/batch)
2017-05-04 23:34:16.269921: step 52820, loss = 2.18 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:34:17.498002: step 52830, loss = 1.91 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:18.697647: step 52840, loss = 1.92 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:34:19.916329: step 52850, loss = 2.03 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:21.115840: step 52860, loss = 1.93 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:34:22.319156: step 52870, loss = 2.00 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:34:23.531035: step 52880, loss = 2.09 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:24.758462: step 52890, loss = 1.98 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:25.951550: step 52900, loss = 1.94 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:34:27.209029: step 52910, loss = 1.97 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-04 23:34:28.417585: step 52920, loss = 1.86 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:29.627705: step 52930, loss = 1.85 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:30.851548: step 52940, loss = 1.98 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:32.081376: step 52950, loss = 1.94 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:33.300187: step 52960, loss = 2.01 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:34.513558: step 52970, loss = 2.08 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:35.715557: step 52980, loss = 2.03 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:34:36.956683: step 52990, loss = 2.05 (1031.3 examples/sec; 0.124 sec/batch)
2017-05-04 23:34:38.166212: step 53000, loss = 2.05 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:39.377169: step 53010, loss = 2.02 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:40.575196: step 53020, loss = 2.11 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:34:41.785668: step 53030, loss = 1.99 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:43.017831: step 53040, loss = 1.90 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:44.221500: step 53050, loss = 2.06 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:34:45.428988: step 53060, loss = 1.91 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:46.620541: step 53070, loss = 2.07 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:34:47.829034: step 53080, loss = 1.89 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:49.067587: step 53090, loss = 1.91 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-04 23:34:50.272452: step 53100, loss = 2.07 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:34:51.497273: step 53110, loss = 1.82 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:52.724759: step 53120, loss = 1.96 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:53.947568: step 53130, loss = 2.02 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:55.156739: step 53140, loss = 1.97 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:56.389853: step 53150, loss = 1.94 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:57.572595: step 53160, loss = 2.01 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:34:58.795329: step 53170, loss = 2.11 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:00.024311: step 53180, loss = 2.05 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:01.238913: step 53190, loss = 2.02 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:35:02.458913: step 53200, loss = 2.21 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:03.664117: step 53210, loss = 2.00 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:35:04.903734: step 53220, loss = 2.07 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-04 23:35:06.107833: step 53230, loss = 1.97 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:07.329990: step 53240, loss = 1.98 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:08.575037: step 53250, loss = 2.05 (1028.1 examples/sec; 0.125 sec/batch)
2017-05-04 23:35:09.760438: step 53260, loss = 1.96 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:35:10.971496: step 53270, loss = 1.96 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:35:12.180657: step 53280, loss = 1.94 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:35:13.409099: step 53290, loss = 2.15 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:14.610886: step 53300, loss = 1.88 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:15.822637: step 53310, loss = 2.08 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:35:17.053580: step 53320, loss = 2.03 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:18.257504: step 53330, loss = 2.06 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:19.474291: step 53340, loss = 2.14 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:20.700442: step 53350, loss = 1.94 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:21.895267: step 53360, loss = 1.98 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:35:23.111722: step 53370, loss = 2.10 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:24.345856: step 53380, loss = 1.97 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:25.593037: step 53390, loss = 2.06 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-04 23:35:26.794590: step 53400, loss = 2.05 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:28.017679: step 53410, loss = 2.05 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:29.237910: step 53420, loss = 1.99 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:30.433060: step 53430, loss = 1.96 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:31.636187: step 53440, loss = 2.12 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:32.858651: step 53450, loss = 1.94 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:34.175356: step 53460, loss = 2.24 (972.1 examples/sec; 0.132 sec/batch)
2017-05-04 23:35:35.272393: step 53470, loss = 1.94 (1166.8 examples/sec; 0.110 sec/batch)
2017-05-04 23:35:36.505043: step 53480, loss = 1.96 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:37.700656: step 53490, loss = 1.99 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:38.922452: step 53500, loss = 1.95 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:40.152865: step 53510, loss = 2.19 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:41.349813: step 53520, loss = 1.91 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:42.582151: step 53530, loss = 2.13 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:43.778173: step 53540, loss = 2.02 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:45.003101: step 53550, loss = 1.94 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:46.206310: step 53560, loss = 1.96 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:47.441783: step 53570, loss = 1.91 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:35:48.642609: step 53580, loss = 2.00 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:49.882602: step 53590, loss = 1.90 (1032.3 examples/sec; 0.124 sec/batch)
2017-05-04 23:35:51.072828: step 53600, loss = 2.01 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:35:52.315882: step 53610, loss = 1.91 (1029.7 examples/sec; 0.124 sec/batch)
2017-05-04 23:35:53.535209: step 53620, loss = 1.97 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:54.756937: step 53630, loss = 1.93 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:55.956497: step 53640, loss = 1.90 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:57.182402: step 53650, loss = 1.98 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:58.418953: step 53660, loss = 2.04 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:35:59.551050: step 53670, loss = 1.84 (1130.6 examples/sec; 0.113 sec/batch)
2017-05-04 23:36:00.739729: step 53680, loss = 1.93 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:36:01.900207: step 53690, loss = 1.95 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:36:03.072311: step 53700, loss = 2.00 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:04.242547: step 53710, loss = 1.94 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:05.417534: step 53720, loss = 1.96 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:06.613366: step 53730, loss = 2.05 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:36:07.770355: step 53740, loss = 1.95 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:36:08.943545: step 53750, loss = 1.90 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:10.107498: step 53760, loss = 1.96 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:36:11.260206: step 53770, loss = 1.94 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:36:12.410504: step 53780, loss = 1.94 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:36:13.572071: step 53790, loss = 1.93 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:36:14.740343: step 53800, loss = 1.83 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:15.920828: step 53810, loss = 2.03 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:17.100359: step 53820, loss = 2.03 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:18.269598: step 53830, loss = 1.95 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:19.447438: step 53840, loss = 2.05 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:20.604711: step 53850, loss = 2.12 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:36:21.771151: step 53860, loss = 2.04 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:22.943695: step 53870, loss = 1.98 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:24.112744: step 53880, loss = 2.02 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:25.268192: step 53890, loss = 1.95 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:36:26.417041: step 53900, loss = 2.14 (1114.2 examples/sec; 0.115 sec/batch)
2017-05-04 23:36:27.585031: step 53910, loss = 1.82 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:28.767677: step 53920, loss = 2.01 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:29.918222: step 53930, loss = 2.21 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:36:31.094185: step 53940, loss = 2.02 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:32.238744: step 53950, loss = 1.91 (1118.3 examples/sec; 0.114 sec/batch)
2017-05-04 23:36:33.404201: step 53960, loss = 1.94 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:34.576968: step 53970, loss = 2.13 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:35.752508: step 53980, loss = 1.92 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:36.935907: step 53990, loss = 2.14 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:38.107463: step 54000, loss = 1.89 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:39.292519: step 54010, loss = 1.95 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:36:40.488370: step 54020, loss = 2.10 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:36:41.674457: step 54030, loss = 2.13 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:36:42.857583: step 54040, loss = 1.94 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:44.045992: step 54050, loss = 1.98 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:36:45.227256: step 54060, loss = 1.95 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:46.403474: step 54070, loss = 2.06 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:47.589086: step 54080, loss = 2.03 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:36:48.740349: step 54090, loss = 2.16 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:36:49.910497: step 54100, loss = 1.96 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:51.064155: step 54110, loss = 1.97 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:36:52.225058: step 54120, loss = 2.02 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:36:53.394437: step 54130, loss = 1.91 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:54.562111: step 54140, loss = 1.92 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:55.760945: step 54150, loss = 1.93 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:36:56.954148: step 54160, loss = 2.00 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:36:58.136930: step 54170, loss = 2.00 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:59.314887: step 54180, loss = 1.78 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:00.484270: step 54190, loss = 2.01 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:01.661121: step 54200, loss = 2.03 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:02.859627: step 54210, loss = 2.17 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:37:04.023928: step 54220, loss = 1.92 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:37:05.207451: step 54230, loss = 1.94 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:06.370041: step 54240, loss = 2.01 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:37:07.557592: step 54250, loss = 2.03 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:37:08.738893: step 54260, loss = 2.03 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:09.907809: step 54270, loss = 1.83 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:11.074582: step 54280, loss = 1.98 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:12.244459: step 54290, loss = 2.04 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:13.408912: step 54300, loss = 1.90 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:37:14.591684: step 54310, loss = 2.24 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:15.794507: step 54320, loss = 2.07 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:37:16.979897: step 54330, loss = 1.99 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:37:18.156089: step 54340, loss = 1.96 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:19.349092: step 54350, loss = 2.05 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:37:20.522645: step 54360, loss = 1.99 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:21.702582: step 54370, loss = 1.86 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:22.894463: step 54380, loss = 1.93 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:37:24.111592: step 54390, loss = 1.97 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:37:25.311568: step 54400, loss = 1.95 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:37:26.520582: step 54410, loss = 2.11 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:37:27.732990: step 54420, loss = 2.15 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:37:28.944042: step 54430, loss = 1.84 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:37:30.147914: step 54440, loss = 2.06 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:37:31.428346: step 54450, loss = 2.07 (999.7 examples/sec; 0.128 sec/batch)
2017-05-04 23:37:32.529094: step 54460, loss = 2.10 (1162.9 examples/sec; 0.110 sec/batch)
2017-05-04 23:37:33.708514: step 54470, loss = 1.91 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:34.910585: step 54480, loss = 2.05 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:37:36.090775: step 54490, loss = 2.00 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:37.275959: step 54500, loss = 2.06 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:37:38.448217: step 54510, loss = 2.00 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:39.611141: step 54520, loss = 1.96 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:37:40.802260: step 54530, loss = 1.82 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:37:41.966435: step 54540, loss = 2.07 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:37:43.138137: step 54550, loss = 1.86 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:44.334108: step 54560, loss = 2.01 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:37:45.505038: step 54570, loss = 2.02 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:46.693793: step 54580, loss = 2.19 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:37:47.891391: step 54590, loss = 1.99 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:37:49.094365: step 54600, loss = 1.96 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:37:50.277690: step 54610, loss = 2.00 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:51.452933: step 54620, loss = 1.90 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:52.611382: step 54630, loss = 2.07 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:37:53.777538: step 54640, loss = 1.85 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:54.948568: step 54650, loss = 2.13 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:56.158201: step 54660, loss = 1.93 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:37:57.354576: step 54670, loss = 2.13 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:37:58.549936: step 54680, loss = 2.43 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:37:59.763292: step 54690, loss = 1.92 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:38:00.974769: step 54700, loss = 1.97 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:38:02.148315: step 54710, loss = 2.03 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:03.327451: step 54720, loss = 1.97 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:04.484519: step 54730, loss = 1.92 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:05.646412: step 54740, loss = 2.01 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:06.807142: step 54750, loss = 2.01 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:07.991732: step 54760, loss = 1.98 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:09.168224: step 54770, loss = 2.01 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:10.331583: step 54780, loss = 1.99 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:11.486748: step 54790, loss = 1.96 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:12.661051: step 54800, loss = 2.06 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:13.813330: step 54810, loss = 1.92 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:38:14.987017: step 54820, loss = 2.16 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:16.150603: step 54830, loss = 1.99 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:17.308766: step 54840, loss = 1.97 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:18.475180: step 54850, loss = 1.98 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:19.616276: step 54860, loss = 1.89 (1121.7 examples/sec; 0.114 sec/batch)
2017-05-04 23:38:20.795457: step 54870, loss = 2.05 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:21.960258: step 54880, loss = 1.97 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:23.129596: step 54890, loss = 1.84 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:24.311401: step 54900, loss = 1.87 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:25.482269: step 54910, loss = 1.92 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:26.654278: step 54920, loss = 1.90 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:27.817611: step 54930, loss = 1.99 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:28.988908: step 54940, loss = 1.93 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:30.154822: step 54950, loss = 2.12 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:31.333440: step 54960, loss = 1.98 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:32.497382: step 54970, loss = 1.88 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:33.659878: step 54980, loss = 1.99 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:34.829567: step 54990, loss = 1.90 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:35.990351: step 55000, loss = 1.83 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:37.182135: step 55010, loss = 1.97 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:38:38.352964: step 55020, loss = 1.91 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:39.527612: step 55030, loss = 2.04 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:40.675737: step 55040, loss = 2.00 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:38:41.840387: step 55050, loss = 2.04 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:43.013571: step 55060, loss = 1.87 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:44.194568: step 55070, loss = 2.12 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:45.355775: step 55080, loss = 2.21 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:46.519953: step 55090, loss = 1.95 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:47.686776: step 55100, loss = 2.05 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:48.864295: step 55110, loss = 1.82 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:50.023907: step 55120, loss = 2.01 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:51.194552: step 55130, loss = 1.99 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:52.362577: step 55140, loss = 2.01 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:53.530743: step 55150, loss = 2.05 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:54.735819: step 55160, loss = 1.85 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:38:55.897982: step 55170, loss = 2.00 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:57.071337: step 55180, loss = 2.04 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:58.248156: step 55190, loss = 2.01 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:59.417053: step 55200, loss = 2.05 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:39:00.594427: step 55210, loss = 1.92 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:39:01.744545: step 55220, loss = 2.05 (1112.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:39:02.914582: step 55230, loss = 1.94 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:39:04.103632: step 55240, loss = 1.93 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:39:05.281019: step 55250, loss = 2.16 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:39:06.462832: step 55260, loss = 1.98 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:39:07.655605: step 55270, loss = 1.88 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:39:08.852833: step 55280, loss = 1.93 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:10.041827: step 55290, loss = 1.97 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:39:11.221450: step 55300, loss = 2.01 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:39:12.408395: step 55310, loss = 2.04 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:39:13.593155: step 55320, loss = 2.09 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:39:14.782945: step 55330, loss = 1.99 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:39:15.968920: step 55340, loss = 1.93 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:39:17.171571: step 55350, loss = 1.89 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:18.351853: step 55360, loss = 2.04 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:39:19.550371: step 55370, loss = 2.05 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:20.762100: step 55380, loss = 2.07 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:39:21.950756: step 55390, loss = 1.99 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:39:23.157354: step 55400, loss = 2.10 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:39:24.356509: step 55410, loss = 2.04 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:25.534677: step 55420, loss = 1.94 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:39:26.755730: step 55430, loss = 1.99 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:39:28.062814: step 55440, loss = 1.94 (979.3 examples/sec; 0.131 sec/batch)
2017-05-04 23:39:29.193499: step 55450, loss = 1.94 (1132.1 examples/sec; 0.113 sec/batch)
2017-05-04 23:39:30.419174: step 55460, loss = 2.09 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:39:31.633620: step 55470, loss = 2.08 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:39:32.854161: step 55480, loss = 2.01 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:39:34.054866: step 55490, loss = 1.82 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:35.254271: step 55500, loss = 1.97 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:36.464793: step 55510, loss = 2.00 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:39:37.646899: step 55520, loss = 1.97 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:39:38.850911: step 55530, loss = 1.91 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:40.071657: step 55540, loss = 2.01 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:39:41.305716: step 55550, loss = 1.95 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:39:42.536956: step 55560, loss = 1.95 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:39:43.735556: step 55570, loss = 1.93 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:44.942928: step 55580, loss = 2.17 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:39:46.129722: step 55590, loss = 1.82 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:39:47.352101: step 55600, loss = 2.05 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:39:48.541277: step 55610, loss = 1.97 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:39:49.748084: step 55620, loss = 1.94 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:39:50.959601: step 55630, loss = 2.03 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:39:52.143724: step 55640, loss = 1.89 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:39:53.351404: step 55650, loss = 1.96 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:39:54.538608: step 55660, loss = 2.08 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:39:55.752788: step 55670, loss = 2.01 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:39:56.974757: step 55680, loss = 2.06 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:39:58.188959: step 55690, loss = 2.02 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:39:59.397098: step 55700, loss = 1.93 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:00.612694: step 55710, loss = 1.97 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:01.832572: step 55720, loss = 1.98 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:03.079384: step 55730, loss = 1.95 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-04 23:40:04.296700: step 55740, loss = 1.99 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:05.533456: step 55750, loss = 1.99 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-04 23:40:06.745043: step 55760, loss = 1.92 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:07.974779: step 55770, loss = 1.85 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:40:09.197877: step 55780, loss = 1.99 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:10.414842: step 55790, loss = 2.07 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:11.636705: step 55800, loss = 2.00 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:12.848182: step 55810, loss = 1.91 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:14.060073: step 55820, loss = 1.95 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:15.279936: step 55830, loss = 1.99 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:16.482992: step 55840, loss = 1.99 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:17.696608: step 55850, loss = 2.08 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:18.914167: step 55860, loss = 1.99 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:20.130833: step 55870, loss = 2.03 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:21.353637: step 55880, loss = 1.92 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:22.559668: step 55890, loss = 1.96 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:23.777038: step 55900, loss = 2.01 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:24.992582: step 55910, loss = 1.99 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:26.212864: step 55920, loss = 1.94 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:27.432661: step 55930, loss = 1.99 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:28.649458: step 55940, loss = 2.18 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:29.841977: step 55950, loss = 2.12 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:40:31.052791: step 55960, loss = 2.12 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:32.274874: step 55970, loss = 2.03 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:33.483281: step 55980, loss = 2.06 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:34.704044: step 55990, loss = 1.92 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:35.923603: step 56000, loss = 1.90 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:37.123380: step 56010, loss = 2.03 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:38.340902: step 56020, loss = 2.04 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:39.554179: step 56030, loss = 1.88 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:40.756479: step 56040, loss = 2.04 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:41.960053: step 56050, loss = 2.01 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:43.187440: step 56060, loss = 2.10 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:40:44.412948: step 56070, loss = 1.89 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:40:45.622548: step 56080, loss = 2.01 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:46.844682: step 56090, loss = 2.00 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:48.046583: step 56100, loss = 2.03 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:49.278197: step 56110, loss = 1.95 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:40:50.480812: step 56120, loss = 2.03 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:51.698654: step 56130, loss = 2.00 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:52.934993: step 56140, loss = 2.13 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-04 23:40:54.131802: step 56150, loss = 2.09 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:55.338706: step 56160, loss = 1.85 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:56.544994: step 56170, loss = 1.95 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:57.759665: step 56180, loss = 1.99 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:58.976072: step 56190, loss = 1.97 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:00.185749: step 56200, loss = 1.93 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:01.402448: step 56210, loss = 2.08 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:02.634124: step 56220, loss = 1.93 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:41:03.853341: step 56230, loss = 1.91 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:05.046570: step 56240, loss = 1.95 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:41:06.261770: step 56250, loss = 2.10 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:07.478477: step 56260, loss = 1.81 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:08.707597: step 56270, loss = 2.02 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:41:09.913608: step 56280, loss = 1.92 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:11.161011: step 56290, loss = 1.86 (1026.1 examples/sec; 0.125 sec/batch)
2017-05-04 23:41:12.387135: step 56300, loss = 2.08 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:41:13.590922: step 56310, loss = 2.08 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:41:14.819184: step 56320, loss = 1.93 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:41:16.045342: step 56330, loss = 2.09 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:41:17.244176: step 56340, loss = 1.92 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:41:18.439955: step 56350, loss = 2.20 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:41:19.652348: step 56360, loss = 1.90 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:20.858167: step 56370, loss = 1.97 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:22.068087: step 56380, loss = 1.95 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:23.294764: step 56390, loss = 1.99 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:41:24.510057: step 56400, loss = 2.00 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:25.716343: step 56410, loss = 2.12 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:26.928329: step 56420, loss = 2.13 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:28.245569: step 56430, loss = 2.04 (971.7 examples/sec; 0.132 sec/batch)
2017-05-04 23:41:29.360002: step 56440, loss = 2.17 (1148.6 examples/sec; 0.111 sec/batch)
2017-05-04 23:41:30.551404: step 56450, loss = 1.88 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:41:31.767498: step 56460, loss = 1.95 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:32.997932: step 56470, loss = 1.98 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:41:34.223164: step 56480, loss = 1.87 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:41:35.452743: step 56490, loss = 1.92 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:41:36.668850: step 56500, loss = 2.25 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:37.877964: step 56510, loss = 2.06 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:39.095668: step 56520, loss = 2.06 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:40.323630: step 56530, loss = 1.91 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:41:41.541413: step 56540, loss = 2.07 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:42.740177: step 56550, loss = 1.95 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:41:43.949631: step 56560, loss = 2.09 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:45.167218: step 56570, loss = 1.83 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:46.352327: step 56580, loss = 1.95 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:41:47.566260: step 56590, loss = 1.91 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:48.792277: step 56600, loss = 2.06 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:41:50.007813: step 56610, loss = 1.86 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:51.224158: step 56620, loss = 2.05 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:52.434910: step 56630, loss = 1.94 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:53.646275: step 56640, loss = 1.96 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:54.865372: step 56650, loss = 2.03 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:56.072045: step 56660, loss = 2.00 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:57.308037: step 56670, loss = 1.96 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-04 23:41:58.521638: step 56680, loss = 2.13 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:59.739607: step 56690, loss = 2.15 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:00.971837: step 56700, loss = 1.90 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:02.196762: step 56710, loss = 1.93 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:03.408620: step 56720, loss = 1.97 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:04.635393: step 56730, loss = 1.91 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:05.846041: step 56740, loss = 2.16 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:07.069816: step 56750, loss = 2.02 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:08.277132: step 56760, loss = 1.85 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:09.503583: step 56770, loss = 1.93 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:10.710997: step 56780, loss = 1.97 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:11.942538: step 56790, loss = 2.05 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:13.158091: step 56800, loss = 1.83 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:14.373097: step 56810, loss = 1.99 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:15.584767: step 56820, loss = 1.94 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:16.794959: step 56830, loss = 1.91 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:18.011644: step 56840, loss = 2.07 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:19.238335: step 56850, loss = 1.88 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:20.457575: step 56860, loss = 1.95 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:21.661972: step 56870, loss = 1.91 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:42:22.875019: step 56880, loss = 2.01 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:24.084010: step 56890, loss = 1.93 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:25.314016: step 56900, loss = 2.02 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:26.525850: step 56910, loss = 1.98 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:27.749762: step 56920, loss = 2.04 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:28.966732: step 56930, loss = 1.94 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:30.174927: step 56940, loss = 1.85 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:31.405699: step 56950, loss = 1.96 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:32.617761: step 56960, loss = 2.07 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:33.821945: step 56970, loss = 1.97 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:42:35.036484: step 56980, loss = 2.04 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:36.259420: step 56990, loss = 1.82 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:37.484820: step 57000, loss = 2.00 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:38.695836: step 57010, loss = 1.85 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:39.922050: step 57020, loss = 1.95 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:41.111799: step 57030, loss = 2.17 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:42:42.330146: step 57040, loss = 2.04 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:43.545482: step 57050, loss = 1.98 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:44.745409: step 57060, loss = 1.93 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:42:45.964873: step 57070, loss = 2.00 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:47.193549: step 57080, loss = 1.86 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:48.415044: step 57090, loss = 1.88 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:49.636899: step 57100, loss = 1.93 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:50.845274: step 57110, loss = 2.05 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:52.049956: step 57120, loss = 1.99 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:42:53.205512: step 57130, loss = 1.99 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:42:54.381735: step 57140, loss = 1.88 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:42:55.552318: step 57150, loss = 1.93 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:42:56.728977: step 57160, loss = 1.84 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:42:57.911676: step 57170, loss = 2.12 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:42:59.094453: step 57180, loss = 2.05 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:43:00.267285: step 57190, loss = 1.88 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:01.419330: step 57200, loss = 1.99 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:43:02.583319: step 57210, loss = 1.85 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:43:03.754378: step 57220, loss = 1.92 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:04.957804: step 57230, loss = 1.83 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:43:06.112588: step 57240, loss = 2.08 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:43:07.294642: step 57250, loss = 1.92 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:43:08.439118: step 57260, loss = 1.94 (1118.4 examples/sec; 0.114 sec/batch)
2017-05-04 23:43:09.606544: step 57270, loss = 2.06 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:10.769527: step 57280, loss = 2.08 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:43:11.939847: step 57290, loss = 2.04 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:13.113767: step 57300, loss = 2.15 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:14.294826: step 57310, loss = 1.93 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:43:15.458573: step 57320, loss = 2.09 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:43:16.632844: step 57330, loss = 1.98 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:17.801773: step 57340, loss = 1.96 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:18.968734: step 57350, loss = 1.95 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:20.113572: step 57360, loss = 2.00 (1118.1 examples/sec; 0.114 sec/batch)
2017-05-04 23:43:21.287196: step 57370, loss = 1.99 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:22.451499: step 57380, loss = 2.02 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:43:23.622340: step 57390, loss = 2.13 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:24.789916: step 57400, loss = 1.83 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:25.961431: step 57410, loss = 2.01 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:27.253635: step 57420, loss = 1.96 (990.6 examples/sec; 0.129 sec/batch)
2017-05-04 23:43:28.350124: step 57430, loss = 1.97 (1167.4 examples/sec; 0.110 sec/batch)
2017-05-04 23:43:29.525621: step 57440, loss = 1.98 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:43:30.721178: step 57450, loss = 1.96 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:43:31.936656: step 57460, loss = 2.12 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:43:33.130995: step 57470, loss = 2.06 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:43:34.326982: step 57480, loss = 2.10 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:43:35.537452: step 57490, loss = 2.08 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:43:36.757216: step 57500, loss = 2.09 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:43:37.959414: step 57510, loss = 1.80 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:43:39.195008: step 57520, loss = 1.98 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-04 23:43:40.392628: step 57530, loss = 1.91 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:43:41.597497: step 57540, loss = 1.96 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:43:42.816414: step 57550, loss = 2.01 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:43:44.045889: step 57560, loss = 1.88 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:43:45.272743: step 57570, loss = 1.93 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:43:46.473748: step 57580, loss = 2.04 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:43:47.694611: step 57590, loss = 1.85 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:43:48.915093: step 57600, loss = 2.08 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:43:50.121335: step 57610, loss = 1.84 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:43:51.435333: step 57620, loss = 2.00 (974.1 examples/sec; 0.131 sec/batch)
2017-05-04 23:43:52.549426: step 57630, loss = 1.91 (1148.9 examples/sec; 0.111 sec/batch)
2017-05-04 23:43:53.749835: step 57640, loss = 1.92 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:43:54.963619: step 57650, loss = 1.92 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:43:56.189770: step 57660, loss = 1.90 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:43:57.408963: step 57670, loss = 1.95 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:43:58.631321: step 57680, loss = 1.79 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:43:59.846316: step 57690, loss = 1.91 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:01.072630: step 57700, loss = 1.91 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:44:02.277385: step 57710, loss = 1.83 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:03.474205: step 57720, loss = 2.06 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:04.714419: step 57730, loss = 1.87 (1032.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:44:05.890396: step 57740, loss = 1.90 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:44:07.136527: step 57750, loss = 1.97 (1027.2 examples/sec; 0.125 sec/batch)
2017-05-04 23:44:08.315440: step 57760, loss = 2.02 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:44:09.546387: step 57770, loss = 2.09 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:44:10.756081: step 57780, loss = 1.97 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:11.954194: step 57790, loss = 2.03 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:13.163605: step 57800, loss = 2.02 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:14.368575: step 57810, loss = 2.05 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:15.569725: step 57820, loss = 1.99 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:16.789716: step 57830, loss = 2.12 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:18.002207: step 57840, loss = 2.12 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:19.230491: step 57850, loss = 2.09 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:44:20.445035: step 57860, loss = 1.95 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:21.641586: step 57870, loss = 1.94 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:22.862552: step 57880, loss = 2.03 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:24.083369: step 57890, loss = 1.86 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:25.299165: step 57900, loss = 2.05 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:26.510500: step 57910, loss = 1.87 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:27.713384: step 57920, loss = 2.09 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:28.951917: step 57930, loss = 1.91 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-04 23:44:30.156977: step 57940, loss = 2.01 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:31.361878: step 57950, loss = 1.96 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:32.578010: step 57960, loss = 1.83 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:33.791447: step 57970, loss = 2.17 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:35.012502: step 57980, loss = 2.11 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:36.242946: step 57990, loss = 1.97 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:44:37.450597: step 58000, loss = 1.93 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:38.681921: step 58010, loss = 2.01 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:44:39.868807: step 58020, loss = 1.90 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:44:41.102792: step 58030, loss = 1.90 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:44:42.312973: step 58040, loss = 2.15 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:43.520052: step 58050, loss = 1.81 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:44.735105: step 58060, loss = 1.94 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:45.937308: step 58070, loss = 1.92 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:47.167575: step 58080, loss = 1.92 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:44:48.375729: step 58090, loss = 1.80 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:49.583280: step 58100, loss = 1.97 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:50.791293: step 58110, loss = 1.87 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:52.015960: step 58120, loss = 1.87 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:53.234405: step 58130, loss = 2.10 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:54.417986: step 58140, loss = 1.94 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:44:55.645898: step 58150, loss = 1.94 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:44:56.873438: step 58160, loss = 1.90 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:44:58.070880: step 58170, loss = 2.07 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:59.260355: step 58180, loss = 1.99 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:45:00.470109: step 58190, loss = 1.99 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:45:01.658084: step 58200, loss = 2.06 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:45:02.853665: step 58210, loss = 2.06 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:45:04.017712: step 58220, loss = 2.06 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:05.200023: step 58230, loss = 1.99 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:06.372365: step 58240, loss = 1.94 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:07.545323: step 58250, loss = 1.88 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:08.730112: step 58260, loss = 1.92 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:09.880455: step 58270, loss = 1.99 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:45:11.053962: step 58280, loss = 1.83 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:12.234493: step 58290, loss = 2.05 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:13.407903: step 58300, loss = 1.91 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:14.577016: step 58310, loss = 1.85 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:15.741070: step 58320, loss = 1.92 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:16.904685: step 58330, loss = 1.89 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:18.071381: step 58340, loss = 2.02 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:19.241060: step 58350, loss = 2.04 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:20.404190: step 58360, loss = 2.07 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:21.572712: step 58370, loss = 1.91 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:22.771483: step 58380, loss = 1.96 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:45:23.916594: step 58390, loss = 2.03 (1117.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:45:25.082167: step 58400, loss = 2.02 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:26.324588: step 58410, loss = 2.06 (1030.3 examples/sec; 0.124 sec/batch)
2017-05-04 23:45:27.397148: step 58420, loss = 1.89 (1193.4 examples/sec; 0.107 sec/batch)
2017-05-04 23:45:28.578626: step 58430, loss = 2.12 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:29.766672: step 58440, loss = 2.02 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:45:30.950402: step 58450, loss = 2.02 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:32.111082: step 58460, loss = 1.96 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:33.283318: step 58470, loss = 1.92 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:34.463638: step 58480, loss = 1.99 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:35.624300: step 58490, loss = 1.97 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:36.776988: step 58500, loss = 2.05 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:45:37.941425: step 58510, loss = 2.05 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:39.108299: step 58520, loss = 1.89 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:40.299102: step 58530, loss = 2.18 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:45:41.492597: step 58540, loss = 1.96 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:45:42.670942: step 58550, loss = 1.96 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:43.856315: step 58560, loss = 2.07 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:45:45.023378: step 58570, loss = 2.05 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:46.192742: step 58580, loss = 1.94 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:47.361862: step 58590, loss = 1.99 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:48.557622: step 58600, loss = 1.86 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:45:49.730918: step 58610, loss = 1.94 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:50.900337: step 58620, loss = 1.78 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:52.067169: step 58630, loss = 1.90 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:53.240427: step 58640, loss = 1.84 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:54.407937: step 58650, loss = 1.99 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:55.591630: step 58660, loss = 1.84 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:56.780205: step 58670, loss = 1.98 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:45:57.970194: step 58680, loss = 1.95 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:45:59.155479: step 58690, loss = 2.01 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:46:00.324746: step 58700, loss = 1.88 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:01.494469: step 58710, loss = 2.07 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:02.654115: step 58720, loss = 2.05 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:03.830435: step 58730, loss = 1.93 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:05.015772: step 58740, loss = 1.92 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:46:06.187036: step 58750, loss = 1.82 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:07.370838: step 58760, loss = 1.85 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:08.537721: step 58770, loss = 1.91 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:09.716527: step 58780, loss = 2.00 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:10.880003: step 58790, loss = 2.09 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:12.048959: step 58800, loss = 2.09 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:13.220026: step 58810, loss = 2.08 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:14.355781: step 58820, loss = 2.15 (1127.0 examples/sec; 0.114 sec/batch)
2017-05-04 23:46:15.542491: step 58830, loss = 2.11 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:46:16.718015: step 58840, loss = 2.08 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:17.849260: step 58850, loss = 2.03 (1131.5 examples/sec; 0.113 sec/batch)
2017-05-04 23:46:19.032614: step 58860, loss = 1.95 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:20.195908: step 58870, loss = 1.88 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:21.360776: step 58880, loss = 1.85 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:22.550254: step 58890, loss = 1.99 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:46:23.740555: step 58900, loss = 1.80 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:46:24.902523: step 58910, loss = 1.91 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:26.054775: step 58920, loss = 1.83 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:46:27.220539: step 58930, loss = 2.03 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:28.394199: step 58940, loss = 2.10 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:29.575393: step 58950, loss = 1.86 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:30.723753: step 58960, loss = 1.91 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:46:31.888515: step 58970, loss = 1.92 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:33.064122: step 58980, loss = 2.00 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:34.215815: step 58990, loss = 1.93 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:46:35.385872: step 59000, loss = 2.11 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:36.552420: step 59010, loss = 1.98 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:37.731265: step 59020, loss = 2.04 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:38.888748: step 59030, loss = 2.04 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:40.071480: step 59040, loss = 1.88 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:41.260527: step 59050, loss = 2.04 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:46:42.415111: step 59060, loss = 1.98 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:46:43.590110: step 59070, loss = 1.97 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:44.747176: step 59080, loss = 1.88 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:45.920820: step 59090, loss = 1.76 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:47.098797: step 59100, loss = 1.93 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:48.267064: step 59110, loss = 1.92 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:49.429640: step 59120, loss = 2.11 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:50.615511: step 59130, loss = 2.14 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:46:51.776400: step 59140, loss = 2.11 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:52.947384: step 59150, loss = 1.98 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:54.116106: step 59160, loss = 1.97 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:55.303662: step 59170, loss = 1.96 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:46:56.487976: step 59180, loss = 1.96 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:57.669367: step 59190, loss = 1.91 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:58.848113: step 59200, loss = 2.05 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:47:00.085678: step 59210, loss = 1.97 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-04 23:47:01.288324: step 59220, loss = 1.93 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:47:02.482088: step 59230, loss = 1.95 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:47:03.674740: step 59240, loss = 2.14 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:47:04.863605: step 59250, loss = 1.94 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:47:06.044843: step 59260, loss = 1.84 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:47:07.233044: step 59270, loss = 2.13 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:47:08.425012: step 59280, loss = 2.03 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:47:09.617964: step 59290, loss = 1.84 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:47:10.813152: step 59300, loss = 2.03 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:47:12.008347: step 59310, loss = 1.88 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:47:13.218151: step 59320, loss = 1.88 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:14.428022: step 59330, loss = 2.04 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:15.677598: step 59340, loss = 1.98 (1024.3 examples/sec; 0.125 sec/batch)
2017-05-04 23:47:16.915722: step 59350, loss = 2.13 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-04 23:47:18.144976: step 59360, loss = 1.83 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:47:19.365129: step 59370, loss = 1.91 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:20.581164: step 59380, loss = 2.22 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:21.794398: step 59390, loss = 1.95 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:23.106901: step 59400, loss = 1.88 (975.2 examples/sec; 0.131 sec/batch)
2017-05-04 23:47:24.225618: step 59410, loss = 2.01 (1144.2 examples/sec; 0.112 sec/batch)
2017-05-04 23:47:25.446522: step 59420, loss = 1.94 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:26.667865: step 59430, loss = 1.92 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:27.880424: step 59440, loss = 1.99 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:29.101901: step 59450, loss = 2.02 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:30.321571: step 59460, loss = 1.93 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:31.531764: step 59470, loss = 1.91 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:32.746937: step 59480, loss = 2.04 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:33.951794: step 59490, loss = 1.85 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:47:35.183496: step 59500, loss = 1.97 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:47:36.393084: step 59510, loss = 1.93 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:37.612937: step 59520, loss = 1.87 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:38.833796: step 59530, loss = 1.97 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:40.053189: step 59540, loss = 1.97 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:41.285146: step 59550, loss = 1.91 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:47:42.503632: step 59560, loss = 1.80 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:43.726305: step 59570, loss = 2.20 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:44.953593: step 59580, loss = 1.85 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:47:46.172478: step 59590, loss = 2.08 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:47.368360: step 59600, loss = 2.11 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:47:48.583033: step 59610, loss = 2.01 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:49.801262: step 59620, loss = 2.15 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:51.021751: step 59630, loss = 2.05 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:52.231306: step 59640, loss = 2.03 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:53.462345: step 59650, loss = 1.96 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:47:54.668835: step 59660, loss = 1.88 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:55.878467: step 59670, loss = 2.04 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:57.074971: step 59680, loss = 1.97 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:47:58.261740: step 59690, loss = 2.07 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:47:59.479718: step 59700, loss = 2.11 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:00.685406: step 59710, loss = 1.85 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:01.872039: step 59720, loss = 1.86 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:48:03.072690: step 59730, loss = 1.91 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:48:04.279112: step 59740, loss = 1.88 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:05.473411: step 59750, loss = 1.93 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:48:06.661632: step 59760, loss = 1.82 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:48:07.851001: step 59770, loss = 1.89 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:48:09.045154: step 59780, loss = 1.98 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:48:10.242110: step 59790, loss = 1.83 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:48:11.452348: step 59800, loss = 1.92 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:12.670315: step 59810, loss = 2.04 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:13.878324: step 59820, loss = 1.95 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:15.104671: step 59830, loss = 2.19 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:48:16.334151: step 59840, loss = 2.10 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:48:17.561116: step 59850, loss = 1.99 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:48:18.785976: step 59860, loss = 1.93 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:20.003894: step 59870, loss = 1.87 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:21.232302: step 59880, loss = 1.95 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:48:22.445700: step 59890, loss = 1.91 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:23.659408: step 59900, loss = 1.84 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:24.874052: step 59910, loss = 1.83 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:26.081045: step 59920, loss = 1.96 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:27.301079: step 59930, loss = 2.08 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:28.528349: step 59940, loss = 1.97 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:48:29.741613: step 59950, loss = 1.91 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:30.955181: step 59960, loss = 2.06 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:32.172740: step 59970, loss = 2.16 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:33.392966: step 59980, loss = 1.96 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:34.605110: step 59990, loss = 1.91 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:35.806820: step 60000, loss = 1.94 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:48:37.028779: step 60010, loss = 1.94 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:38.243637: step 60020, loss = 1.89 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:39.460108: step 60030, loss = 2.05 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:40.688110: step 60040, loss = 2.06 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:48:41.891327: step 60050, loss = 2.05 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:48:43.105758: step 60060, loss = 2.09 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:44.319773: step 60070, loss = 1.92 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:45.526571: step 60080, loss = 1.96 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:46.744691: step 60090, loss = 2.00 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:47.967849: step 60100, loss = 2.02 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:49.196284: step 60110, loss = 1.93 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:48:50.427679: step 60120, loss = 2.06 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:48:51.661923: step 60130, loss = 1.94 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:48:52.878601: step 60140, loss = 2.02 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:54.118391: step 60150, loss = 1.86 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-04 23:48:55.340923: step 60160, loss = 1.92 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:56.562039: step 60170, loss = 2.09 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:57.783543: step 60180, loss = 2.06 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:58.991996: step 60190, loss = 2.13 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:00.172138: step 60200, loss = 1.96 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:49:01.387414: step 60210, loss = 1.89 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:02.603723: step 60220, loss = 1.99 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:03.818995: step 60230, loss = 1.85 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:05.039043: step 60240, loss = 2.18 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:06.239539: step 60250, loss = 1.89 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:49:07.450700: step 60260, loss = 1.94 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:08.667219: step 60270, loss = 1.93 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:09.871279: step 60280, loss = 2.08 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:49:11.097450: step 60290, loss = 2.07 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:49:12.330606: step 60300, loss = 1.98 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:49:13.543197: step 60310, loss = 2.10 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:14.765552: step 60320, loss = 1.96 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:15.971410: step 60330, loss = 1.94 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:17.196561: step 60340, loss = 2.00 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:49:18.414931: step 60350, loss = 1.97 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:19.625086: step 60360, loss = 1.87 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:20.836875: step 60370, loss = 2.00 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:22.041131: step 60380, loss = 1.79 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:49:23.353463: step 60390, loss = 1.86 (975.4 examples/sec; 0.131 sec/batch)
2017-05-04 23:49:24.448413: step 60400, loss = 1.99 (1169.0 examples/sec; 0.109 sec/batch)
2017-05-04 23:49:25.650156: step 60410, loss = 2.10 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:49:26.871586: step 60420, loss = 2.06 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:28.092617: step 60430, loss = 1.90 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:29.314841: step 60440, loss = 1.94 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:30.521025: step 60450, loss = 1.88 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:31.737192: step 60460, loss = 1.96 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:32.944793: step 60470, loss = 2.11 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:34.147408: step 60480, loss = 1.86 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:49:35.372403: step 60490, loss = 2.05 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:36.602632: step 60500, loss = 1.91 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:49:37.822588: step 60510, loss = 2.07 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:39.055696: step 60520, loss = 1.86 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:49:40.278649: step 60530, loss = 1.95 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:41.483540: step 60540, loss = 1.93 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:49:42.708383: step 60550, loss = 1.92 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:43.912333: step 60560, loss = 2.08 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:49:45.125379: step 60570, loss = 2.01 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:46.335593: step 60580, loss = 1.99 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:47.542459: step 60590, loss = 2.07 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:48.758343: step 60600, loss = 1.87 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:49.953564: step 60610, loss = 1.89 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:49:51.155023: step 60620, loss = 2.17 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:49:52.346405: step 60630, loss = 2.11 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:49:53.517501: step 60640, loss = 2.09 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:49:54.699017: step 60650, loss = 1.85 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:49:55.888171: step 60660, loss = 1.88 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:49:57.051776: step 60670, loss = 1.76 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:49:58.233015: step 60680, loss = 1.91 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:49:59.409799: step 60690, loss = 1.88 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:50:00.595000: step 60700, loss = 2.07 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:50:01.789691: step 60710, loss = 1.85 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:50:02.985810: step 60720, loss = 2.00 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:04.186855: step 60730, loss = 2.14 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:05.365776: step 60740, loss = 1.92 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:50:06.561799: step 60750, loss = 2.05 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:07.747616: step 60760, loss = 1.94 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:50:08.932756: step 60770, loss = 1.95 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:50:10.099355: step 60780, loss = 1.96 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:50:11.282219: step 60790, loss = 1.92 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:50:12.453640: step 60800, loss = 2.07 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:50:13.621885: step 60810, loss = 2.03 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:50:14.820892: step 60820, loss = 1.91 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:16.020998: step 60830, loss = 1.93 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:17.248841: step 60840, loss = 2.07 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:50:18.445948: step 60850, loss = 1.97 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:19.634048: step 60860, loss = 1.96 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:50:20.834039: step 60870, loss = 1.97 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:22.027645: step 60880, loss = 1.90 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:50:23.245820: step 60890, loss = 2.00 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:50:24.459793: step 60900, loss = 2.00 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:25.654635: step 60910, loss = 2.06 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:50:26.860376: step 60920, loss = 2.00 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:28.042403: step 60930, loss = 1.98 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:50:29.241561: step 60940, loss = 1.91 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:30.437984: step 60950, loss = 1.99 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:31.651098: step 60960, loss = 2.04 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:32.869679: step 60970, loss = 1.88 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:50:34.077680: step 60980, loss = 2.10 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:35.281989: step 60990, loss = 2.06 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:36.527031: step 61000, loss = 1.93 (1028.1 examples/sec; 0.125 sec/batch)
2017-05-04 23:50:37.725902: step 61010, loss = 1.96 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:38.977158: step 61020, loss = 1.95 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-04 23:50:40.192277: step 61030, loss = 1.91 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:50:41.401568: step 61040, loss = 2.03 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:42.612684: step 61050, loss = 2.09 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:43.829202: step 61060, loss = 1.92 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:50:45.022611: step 61070, loss = 1.96 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:50:46.224867: step 61080, loss = 1.98 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:47.460846: step 61090, loss = 1.83 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-04 23:50:48.683407: step 61100, loss = 2.03 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:50:49.890829: step 61110, loss = 1.93 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:51.109740: step 61120, loss = 1.94 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:50:52.319929: step 61130, loss = 1.94 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:53.528289: step 61140, loss = 2.08 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:54.737995: step 61150, loss = 1.90 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:55.938168: step 61160, loss = 1.96 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:57.155271: step 61170, loss = 1.81 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:50:58.362519: step 61180, loss = 2.04 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:59.573143: step 61190, loss = 1.90 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:00.794212: step 61200, loss = 1.91 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:02.007317: step 61210, loss = 2.17 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:03.225451: step 61220, loss = 2.11 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:04.442792: step 61230, loss = 2.01 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:05.645791: step 61240, loss = 2.00 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:51:06.870622: step 61250, loss = 1.97 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:08.087090: step 61260, loss = 1.99 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:09.297288: step 61270, loss = 1.91 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:10.519943: step 61280, loss = 1.98 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:11.752877: step 61290, loss = 2.08 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:51:12.970690: step 61300, loss = 1.97 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:14.185285: step 61310, loss = 1.97 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:15.392079: step 61320, loss = 1.93 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:16.602630: step 61330, loss = 1.89 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:17.812381: step 61340, loss = 2.04 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:19.032825: step 61350, loss = 1.97 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:20.240022: step 61360, loss = 2.07 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:21.466411: step 61370, loss = 1.98 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:51:22.757489: step 61380, loss = 1.80 (991.4 examples/sec; 0.129 sec/batch)
2017-05-04 23:51:23.845762: step 61390, loss = 1.90 (1176.2 examples/sec; 0.109 sec/batch)
2017-05-04 23:51:25.068784: step 61400, loss = 2.54 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:26.274162: step 61410, loss = 1.93 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:27.489839: step 61420, loss = 1.99 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:28.716866: step 61430, loss = 1.96 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:51:29.919458: step 61440, loss = 1.90 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:51:31.162601: step 61450, loss = 1.94 (1029.7 examples/sec; 0.124 sec/batch)
2017-05-04 23:51:32.369018: step 61460, loss = 2.05 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:33.583159: step 61470, loss = 1.94 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:34.799838: step 61480, loss = 2.12 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:36.009615: step 61490, loss = 1.90 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:37.228435: step 61500, loss = 1.91 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:38.443325: step 61510, loss = 2.14 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:39.652570: step 61520, loss = 2.01 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:40.880975: step 61530, loss = 2.08 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:51:42.084422: step 61540, loss = 1.87 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:51:43.295053: step 61550, loss = 2.00 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:44.502843: step 61560, loss = 1.94 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:45.707518: step 61570, loss = 2.06 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:51:46.922316: step 61580, loss = 2.07 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:48.125027: step 61590, loss = 1.80 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:51:49.331042: step 61600, loss = 1.99 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:50.549048: step 61610, loss = 2.00 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:51.788102: step 61620, loss = 1.83 (1033.0 examples/sec; 0.124 sec/batch)
2017-05-04 23:51:53.012625: step 61630, loss = 2.03 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:54.239669: step 61640, loss = 2.14 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:51:55.449293: step 61650, loss = 2.04 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:56.669550: step 61660, loss = 2.06 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:57.880145: step 61670, loss = 1.88 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:59.099782: step 61680, loss = 1.80 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:00.311714: step 61690, loss = 1.98 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:01.528623: step 61700, loss = 1.86 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:02.734894: step 61710, loss = 1.96 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:03.951616: step 61720, loss = 1.92 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:05.162140: step 61730, loss = 2.01 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:06.365561: step 61740, loss = 1.81 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:52:07.595566: step 61750, loss = 2.05 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:08.796460: step 61760, loss = 1.97 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:52:09.998564: step 61770, loss = 1.95 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:52:11.199028: step 61780, loss = 2.16 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:52:12.416957: step 61790, loss = 1.83 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:13.633353: step 61800, loss = 2.13 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:14.848480: step 61810, loss = 2.00 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:16.077079: step 61820, loss = 1.90 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:17.304939: step 61830, loss = 1.93 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:18.520904: step 61840, loss = 1.88 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:19.760066: step 61850, loss = 1.90 (1033.0 examples/sec; 0.124 sec/batch)
2017-05-04 23:52:20.974918: step 61860, loss = 1.95 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:22.200829: step 61870, loss = 1.94 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:23.418127: step 61880, loss = 1.93 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:24.643763: step 61890, loss = 1.95 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:25.846319: step 61900, loss = 2.11 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:52:27.051875: step 61910, loss = 1.98 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:28.275977: step 61920, loss = 1.90 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:29.482048: step 61930, loss = 1.90 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:30.703335: step 61940, loss = 1.88 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:31.925507: step 61950, loss = 1.90 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:33.127047: step 61960, loss = 1.99 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:52:34.335493: step 61970, loss = 2.03 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:35.538616: step 61980, loss = 1.94 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:52:36.750152: step 61990, loss = 2.13 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:37.986568: step 62000, loss = 1.85 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-04 23:52:39.220039: step 62010, loss = 2.14 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:40.436645: step 62020, loss = 1.96 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:41.657830: step 62030, loss = 1.99 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:42.878616: step 62040, loss = 1.91 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:44.110174: step 62050, loss = 1.90 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:45.336137: step 62060, loss = 1.86 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:46.521407: step 62070, loss = 1.96 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:52:47.748696: step 62080, loss = 1.87 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:48.964522: step 62090, loss = 2.01 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:50.179202: step 62100, loss = 2.14 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:51.376896: step 62110, loss = 1.83 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:52:52.586544: step 62120, loss = 1.81 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:53.788800: step 62130, loss = 1.84 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:52:55.014909: step 62140, loss = 1.98 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:56.241662: step 62150, loss = 1.88 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:57.459074: step 62160, loss = 1.91 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:58.662447: step 62170, loss = 1.93 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:52:59.848503: step 62180, loss = 2.07 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:53:01.054368: step 62190, loss = 2.00 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:02.291350: step 62200, loss = 1.98 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-04 23:53:03.529103: step 62210, loss = 1.98 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:53:04.748823: step 62220, loss = 2.04 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:53:05.954017: step 62230, loss = 2.05 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:07.167603: step 62240, loss = 2.04 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:08.380656: step 62250, loss = 2.02 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:09.583160: step 62260, loss = 1.93 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:53:10.815286: step 62270, loss = 1.85 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:53:12.030298: step 62280, loss = 2.11 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:53:13.238946: step 62290, loss = 2.02 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:14.452547: step 62300, loss = 1.90 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:15.656491: step 62310, loss = 1.88 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:53:16.862037: step 62320, loss = 1.97 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:18.075183: step 62330, loss = 1.94 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:19.299137: step 62340, loss = 1.98 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:53:20.509425: step 62350, loss = 1.94 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:21.722166: step 62360, loss = 1.90 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:23.026312: step 62370, loss = 2.39 (981.5 examples/sec; 0.130 sec/batch)
2017-05-04 23:53:24.142796: step 62380, loss = 2.03 (1146.4 examples/sec; 0.112 sec/batch)
2017-05-04 23:53:25.351171: step 62390, loss = 1.92 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:26.526920: step 62400, loss = 1.92 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:53:27.721334: step 62410, loss = 1.98 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:53:28.903212: step 62420, loss = 2.03 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:53:30.137845: step 62430, loss = 2.07 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:53:31.336821: step 62440, loss = 1.86 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:53:32.546028: step 62450, loss = 1.86 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:33.751055: step 62460, loss = 2.05 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:34.955288: step 62470, loss = 2.02 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:53:36.165342: step 62480, loss = 1.99 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:37.369562: step 62490, loss = 2.03 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:53:38.564425: step 62500, loss = 1.95 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:53:39.757936: step 62510, loss = 1.93 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:53:40.950324: step 62520, loss = 1.99 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:53:42.139271: step 62530, loss = 2.11 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:53:43.316272: step 62540, loss = 1.96 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:53:44.482633: step 62550, loss = 2.02 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:53:45.637086: step 62560, loss = 2.02 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:53:46.808799: step 62570, loss = 1.88 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:53:47.980472: step 62580, loss = 2.15 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:53:49.133339: step 62590, loss = 2.04 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:53:50.289324: step 62600, loss = 2.09 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:53:51.450452: step 62610, loss = 2.00 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:53:52.629626: step 62620, loss = 1.90 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:53:53.777928: step 62630, loss = 1.98 (1114.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:53:54.951420: step 62640, loss = 2.13 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:53:56.117305: step 62650, loss = 1.92 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:53:57.297779: step 62660, loss = 1.89 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:53:58.455759: step 62670, loss = 1.97 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:53:59.616288: step 62680, loss = 2.15 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:54:00.783844: step 62690, loss = 2.11 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:01.923476: step 62700, loss = 2.00 (1123.2 examples/sec; 0.114 sec/batch)
2017-05-04 23:54:03.090009: step 62710, loss = 2.06 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:04.286406: step 62720, loss = 1.98 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:54:05.430873: step 62730, loss = 2.02 (1118.4 examples/sec; 0.114 sec/batch)
2017-05-04 23:54:06.608012: step 62740, loss = 1.88 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:07.770945: step 62750, loss = 1.99 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:54:08.945439: step 62760, loss = 1.96 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:10.118489: step 62770, loss = 2.01 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:11.291893: step 62780, loss = 1.90 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:12.460198: step 62790, loss = 1.98 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:13.628883: step 62800, loss = 2.05 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:14.805623: step 62810, loss = 1.81 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:15.982880: step 62820, loss = 1.90 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:17.176429: step 62830, loss = 2.06 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:54:18.372975: step 62840, loss = 1.98 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:54:19.554936: step 62850, loss = 1.89 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:20.758257: step 62860, loss = 1.97 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:54:21.956527: step 62870, loss = 1.99 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:54:23.158146: step 62880, loss = 1.94 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:54:24.372426: step 62890, loss = 2.15 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:54:25.582062: step 62900, loss = 1.86 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:54:26.798497: step 62910, loss = 2.00 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:54:28.014099: step 62920, loss = 1.95 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:54:29.228227: step 62930, loss = 2.12 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:54:30.441593: step 62940, loss = 1.77 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:54:31.650429: step 62950, loss = 1.91 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:54:32.898858: step 62960, loss = 1.96 (1025.3 examples/sec; 0.125 sec/batch)
2017-05-04 23:54:34.077350: step 62970, loss = 1.87 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:35.281394: step 62980, loss = 1.91 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:54:36.487170: step 62990, loss = 1.93 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:54:37.672700: step 63000, loss = 1.94 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:54:38.865401: step 63010, loss = 2.03 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:54:40.058781: step 63020, loss = 1.97 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:54:41.233305: step 63030, loss = 1.93 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:42.404679: step 63040, loss = 1.83 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:43.574288: step 63050, loss = 1.88 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:44.773953: step 63060, loss = 2.02 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:54:45.944690: step 63070, loss = 1.99 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:47.109148: step 63080, loss = 1.94 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:54:48.297008: step 63090, loss = 1.91 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:54:49.476050: step 63100, loss = 1.96 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:50.655771: step 63110, loss = 2.00 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:51.833305: step 63120, loss = 2.12 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:53.012323: step 63130, loss = 1.97 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:54.170579: step 63140, loss = 1.91 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:54:55.351737: step 63150, loss = 2.25 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:56.531159: step 63160, loss = 2.01 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:57.703192: step 63170, loss = 1.93 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:58.872576: step 63180, loss = 2.04 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:00.038377: step 63190, loss = 2.05 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:01.209926: step 63200, loss = 2.23 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:02.383756: step 63210, loss = 1.87 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:03.548043: step 63220, loss = 1.91 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:04.714569: step 63230, loss = 1.95 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:05.863430: step 63240, loss = 1.99 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:55:07.044681: step 63250, loss = 1.84 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:55:08.199412: step 63260, loss = 1.82 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:55:09.359080: step 63270, loss = 1.94 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:10.522547: step 63280, loss = 1.89 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:11.684007: step 63290, loss = 2.01 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:12.855062: step 63300, loss = 1.84 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:14.020647: step 63310, loss = 1.95 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:15.194762: step 63320, loss = 1.95 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:16.359416: step 63330, loss = 1.88 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:17.522976: step 63340, loss = 1.82 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:18.692326: step 63350, loss = 1.92 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:19.942735: step 63360, loss = 1.98 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-04 23:55:21.021934: step 63370, loss = 1.94 (1186.1 examples/sec; 0.108 sec/batch)
2017-05-04 23:55:22.158882: step 63380, loss = 2.02 (1125.8 examples/sec; 0.114 sec/batch)
2017-05-04 23:55:23.328390: step 63390, loss = 1.80 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:24.499391: step 63400, loss = 1.94 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:25.670784: step 63410, loss = 2.02 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:26.847951: step 63420, loss = 2.09 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:55:28.015276: step 63430, loss = 1.92 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:29.182199: step 63440, loss = 1.98 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:30.345100: step 63450, loss = 1.95 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:31.503516: step 63460, loss = 2.00 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:32.683514: step 63470, loss = 2.08 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:55:33.829502: step 63480, loss = 1.88 (1116.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:55:34.999404: step 63490, loss = 2.00 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:36.199613: step 63500, loss = 2.10 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:55:37.365046: step 63510, loss = 1.89 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:38.519117: step 63520, loss = 2.00 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:55:39.685522: step 63530, loss = 2.09 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:40.859460: step 63540, loss = 2.03 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:42.009809: step 63550, loss = 1.94 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:55:43.186990: step 63560, loss = 2.19 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:55:44.337936: step 63570, loss = 1.94 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:55:45.496986: step 63580, loss = 2.05 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:46.655937: step 63590, loss = 1.89 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:47.810668: step 63600, loss = 2.03 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:55:48.973348: step 63610, loss = 1.98 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:50.136717: step 63620, loss = 2.23 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:51.290024: step 63630, loss = 2.00 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:55:52.475429: step 63640, loss = 1.95 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:55:53.626375: step 63650, loss = 1.85 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:55:54.788749: step 63660, loss = 1.93 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:55.981307: step 63670, loss = 2.03 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:55:57.161485: step 63680, loss = 1.90 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:55:58.324369: step 63690, loss = 2.03 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:59.506091: step 63700, loss = 2.03 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:00.694356: step 63710, loss = 1.85 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:56:01.835875: step 63720, loss = 2.02 (1121.3 examples/sec; 0.114 sec/batch)
2017-05-04 23:56:02.993707: step 63730, loss = 1.94 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:04.185831: step 63740, loss = 1.92 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:56:05.330982: step 63750, loss = 2.01 (1117.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:56:06.516450: step 63760, loss = 2.04 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:56:07.674537: step 63770, loss = 2.02 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:08.856838: step 63780, loss = 2.02 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:10.015350: step 63790, loss = 1.94 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:11.195462: step 63800, loss = 2.00 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:12.351728: step 63810, loss = 1.98 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:13.517987: step 63820, loss = 2.04 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:14.650873: step 63830, loss = 2.02 (1129.9 examples/sec; 0.113 sec/batch)
2017-05-04 23:56:15.824684: step 63840, loss = 2.06 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:16.987471: step 63850, loss = 2.04 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:18.126219: step 63860, loss = 1.83 (1124.0 examples/sec; 0.114 sec/batch)
2017-05-04 23:56:19.307359: step 63870, loss = 2.04 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:20.471104: step 63880, loss = 1.91 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:21.628454: step 63890, loss = 2.03 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:22.785636: step 63900, loss = 1.88 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:23.966568: step 63910, loss = 2.01 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:25.148940: step 63920, loss = 2.01 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:26.297763: step 63930, loss = 1.83 (1114.2 examples/sec; 0.115 sec/batch)
2017-05-04 23:56:27.462299: step 63940, loss = 1.96 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:28.635328: step 63950, loss = 2.11 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:29.792017: step 63960, loss = 1.98 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:30.963110: step 63970, loss = 1.80 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:32.150647: step 63980, loss = 1.96 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:56:33.322757: step 63990, loss = 1.92 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:34.502902: step 64000, loss = 1.88 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:35.684318: step 64010, loss = 1.99 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:36.847740: step 64020, loss = 2.05 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:37.994309: step 64030, loss = 2.00 (1116.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:56:39.165975: step 64040, loss = 1.91 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:40.342481: step 64050, loss = 2.07 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:41.536280: step 64060, loss = 1.89 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:56:42.707357: step 64070, loss = 2.03 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:43.873360: step 64080, loss = 2.00 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:45.049764: step 64090, loss = 1.98 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:46.206563: step 64100, loss = 1.92 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:47.370757: step 64110, loss = 1.98 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:48.558686: step 64120, loss = 1.91 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:56:49.696472: step 64130, loss = 2.14 (1125.0 examples/sec; 0.114 sec/batch)
2017-05-04 23:56:50.880673: step 64140, loss = 1.85 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:52.071909: step 64150, loss = 1.96 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:56:53.269493: step 64160, loss = 2.00 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:56:54.428620: step 64170, loss = 2.08 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:55.617616: step 64180, loss = 1.88 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:56:56.799872: step 64190, loss = 1.98 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:57.953716: step 64200, loss = 1.92 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:56:59.128107: step 64210, loss = 1.95 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:00.300430: step 64220, loss = 2.08 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:01.472164: step 64230, loss = 1.96 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:02.655504: step 64240, loss = 1.86 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:03.831877: step 64250, loss = 2.05 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:05.004145: step 64260, loss = 2.02 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:06.158729: step 64270, loss = 1.95 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:57:07.335984: step 64280, loss = 1.90 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:08.510718: step 64290, loss = 2.09 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:09.681292: step 64300, loss = 1.89 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:10.855493: step 64310, loss = 2.10 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:12.036279: step 64320, loss = 1.94 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:13.247575: step 64330, loss = 1.94 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:57:14.449392: step 64340, loss = 1.89 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:57:15.738227: step 64350, loss = 1.99 (993.1 examples/sec; 0.129 sec/batch)
2017-05-04 23:57:16.820380: step 64360, loss = 1.89 (1182.8 examples/sec; 0.108 sec/batch)
2017-05-04 23:57:17.999912: step 64370, loss = 1.96 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:19.186688: step 64380, loss = 2.00 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:57:20.375828: step 64390, loss = 2.03 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:57:21.570738: step 64400, loss = 1.98 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:57:22.766460: step 64410, loss = 1.94 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:57:23.980974: step 64420, loss = 2.00 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:57:25.180628: step 64430, loss = 1.99 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:57:26.359448: step 64440, loss = 1.75 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:27.534591: step 64450, loss = 1.93 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:28.713206: step 64460, loss = 1.93 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:29.914938: step 64470, loss = 1.89 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:57:31.120464: step 64480, loss = 2.02 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:57:32.308823: step 64490, loss = 1.97 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:57:33.493247: step 64500, loss = 2.22 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:34.663164: step 64510, loss = 1.99 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:35.838703: step 64520, loss = 2.08 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:37.023204: step 64530, loss = 1.98 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:38.192068: step 64540, loss = 1.93 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:39.366984: step 64550, loss = 1.92 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:40.525922: step 64560, loss = 1.92 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:57:41.688548: step 64570, loss = 1.87 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:57:42.847138: step 64580, loss = 2.08 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:57:44.028412: step 64590, loss = 1.77 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:45.218866: step 64600, loss = 2.04 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:57:46.369329: step 64610, loss = 2.15 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:57:47.534167: step 64620, loss = 1.93 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:57:48.707933: step 64630, loss = 1.97 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:49.863595: step 64640, loss = 1.81 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:57:51.024513: step 64650, loss = 2.00 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:57:52.192846: step 64660, loss = 1.93 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:53.365915: step 64670, loss = 2.04 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:54.520822: step 64680, loss = 2.02 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:57:55.679353: step 64690, loss = 1.96 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:57:56.832751: step 64700, loss = 2.10 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:57:58.006388: step 64710, loss = 2.12 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:59.174926: step 64720, loss = 2.02 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:00.350821: step 64730, loss = 2.14 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:01.525214: step 64740, loss = 2.00 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:02.724992: step 64750, loss = 1.86 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:58:03.879852: step 64760, loss = 1.94 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:58:05.072937: step 64770, loss = 2.16 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:58:06.217310: step 64780, loss = 1.95 (1118.5 examples/sec; 0.114 sec/batch)
2017-05-04 23:58:07.383296: step 64790, loss = 1.93 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:08.551337: step 64800, loss = 2.10 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:09.719191: step 64810, loss = 1.91 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:10.868118: step 64820, loss = 1.89 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:58:12.026055: step 64830, loss = 2.00 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:13.203608: step 64840, loss = 1.98 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:14.350994: step 64850, loss = 2.01 (1115.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:58:15.521086: step 64860, loss = 2.02 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:16.687311: step 64870, loss = 1.78 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:17.826434: step 64880, loss = 1.98 (1123.7 examples/sec; 0.114 sec/batch)
2017-05-04 23:58:19.002283: step 64890, loss = 1.85 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:20.176834: step 64900, loss = 1.88 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:21.374538: step 64910, loss = 2.01 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:58:22.532270: step 64920, loss = 1.91 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:23.707933: step 64930, loss = 1.97 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:24.879870: step 64940, loss = 1.90 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:26.030667: step 64950, loss = 1.83 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:58:27.189779: step 64960, loss = 1.97 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:28.363054: step 64970, loss = 2.01 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:29.508477: step 64980, loss = 1.98 (1117.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:58:30.671845: step 64990, loss = 1.93 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:31.835505: step 65000, loss = 1.96 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:33.002377: step 65010, loss = 1.75 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:34.176183: step 65020, loss = 1.95 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:35.350735: step 65030, loss = 2.08 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:36.506008: step 65040, loss = 2.11 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:37.667859: step 65050, loss = 1.98 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:38.854469: step 65060, loss = 2.02 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:58:40.020791: step 65070, loss = 1.94 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:41.176337: step 65080, loss = 2.10 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:42.343962: step 65090, loss = 2.20 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:43.515620: step 65100, loss = 1.91 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:44.677606: step 65110, loss = 1.80 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:45.826727: step 65120, loss = 1.85 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:58:47.004570: step 65130, loss = 1.92 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:48.180970: step 65140, loss = 1.96 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:49.340799: step 65150, loss = 1.88 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:50.505201: step 65160, loss = 1.92 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:51.674551: step 65170, loss = 2.08 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:52.842650: step 65180, loss = 1.97 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:54.054983: step 65190, loss = 1.96 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:58:55.228509: step 65200, loss = 2.02 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:56.398981: step 65210, loss = 1.86 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:57.560122: step 65220, loss = 2.03 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:58.740482: step 65230, loss = 2.04 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:59.920561: step 65240, loss = 2.00 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:01.099543: step 65250, loss = 1.85 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:02.257121: step 65260, loss = 2.02 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:03.426455: step 65270, loss = 1.98 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:04.591332: step 65280, loss = 1.90 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:05.754713: step 65290, loss = 2.02 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:06.923330: step 65300, loss = 1.84 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:08.089943: step 65310, loss = 1.80 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:09.244784: step 65320, loss = 2.04 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:59:10.404146: step 65330, loss = 1.94 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:11.662590: step 65340, loss = 1.95 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-04 23:59:12.734612: step 65350, loss = 1.86 (1194.0 examples/sec; 0.107 sec/batch)
2017-05-04 23:59:13.891491: step 65360, loss = 1.90 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:15.062467: step 65370, loss = 2.02 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:16.240517: step 65380, loss = 1.93 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:17.402863: step 65390, loss = 2.05 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:18.560914: step 65400, loss = 1.86 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:19.730697: step 65410, loss = 1.94 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:20.906170: step 65420, loss = 1.97 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:22.072503: step 65430, loss = 1.99 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:23.236893: step 65440, loss = 1.96 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:24.408394: step 65450, loss = 1.86 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:25.562653: step 65460, loss = 1.99 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:59:26.721678: step 65470, loss = 2.04 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:27.885968: step 65480, loss = 1.89 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:29.056324: step 65490, loss = 2.12 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:30.224116: step 65500, loss = 1.95 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:31.391307: step 65510, loss = 1.96 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:32.567668: step 65520, loss = 2.19 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:33.732344: step 65530, loss = 1.89 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:34.920566: step 65540, loss = 1.90 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:59:36.081358: step 65550, loss = 1.97 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:37.252077: step 65560, loss = 1.91 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:38.434756: step 65570, loss = 1.97 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:39.621330: step 65580, loss = 1.93 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:59:40.809208: step 65590, loss = 2.17 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:59:42.006746: step 65600, loss = 1.94 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:59:43.180756: step 65610, loss = 1.97 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:44.354923: step 65620, loss = 1.88 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:45.521527: step 65630, loss = 2.08 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:46.703282: step 65640, loss = 1.93 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:47.877072: step 65650, loss = 2.19 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:49.065139: step 65660, loss = 1.95 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:59:50.232273: step 65670, loss = 1.95 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:51.415627: step 65680, loss = 1.87 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:52.585887: step 65690, loss = 2.08 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:53.734921: step 65700, loss = 1.93 (1114.0 examples/sec; 0.115 sec/batch)
2017-05-04 23:59:54.902417: step 65710, loss = 2.13 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:56.087014: step 65720, loss = 1.96 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:57.251619: step 65730, loss = 1.97 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:58.414502: step 65740, loss = 2.06 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:59.585571: step 65750, loss = 2.03 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:00.761043: step 65760, loss = 2.13 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:01.923191: step 65770, loss = 2.12 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:03.112284: step 65780, loss = 1.82 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:00:04.321544: step 65790, loss = 1.92 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:00:05.505035: step 65800, loss = 1.91 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:06.691869: step 65810, loss = 1.96 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:00:07.862185: step 65820, loss = 1.82 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:09.059330: step 65830, loss = 1.87 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:00:10.242224: step 65840, loss = 2.25 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:11.431888: step 65850, loss = 2.05 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:00:12.620554: step 65860, loss = 1.94 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:00:13.793810: step 65870, loss = 1.86 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:14.993978: step 65880, loss = 1.84 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:00:16.172971: step 65890, loss = 1.97 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:17.346823: step 65900, loss = 1.87 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:18.503886: step 65910, loss = 2.07 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:19.683094: step 65920, loss = 1.96 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:20.825103: step 65930, loss = 1.98 (1120.8 examples/sec; 0.114 sec/batch)
2017-05-05 00:00:21.979998: step 65940, loss = 2.10 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:00:23.162193: step 65950, loss = 1.95 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:24.349529: step 65960, loss = 1.83 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:00:25.511099: step 65970, loss = 2.06 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:26.665797: step 65980, loss = 1.98 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:00:27.838984: step 65990, loss = 2.04 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:29.009774: step 66000, loss = 1.96 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:30.173605: step 66010, loss = 1.92 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:31.338547: step 66020, loss = 1.80 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:32.490010: step 66030, loss = 2.10 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:00:33.635702: step 66040, loss = 1.95 (1117.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:00:34.815562: step 66050, loss = 2.04 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:35.976079: step 66060, loss = 1.81 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:37.151830: step 66070, loss = 1.93 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:38.316129: step 66080, loss = 1.98 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:39.472861: step 66090, loss = 1.97 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:40.635515: step 66100, loss = 1.85 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:41.783709: step 66110, loss = 1.89 (1114.8 examples/sec; 0.115 sec/batch)
2017-05-05 00:00:42.964639: step 66120, loss = 1.88 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:44.146198: step 66130, loss = 1.98 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:45.304114: step 66140, loss = 1.89 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:46.466057: step 66150, loss = 1.90 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:47.661386: step 66160, loss = 1.96 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:00:48.821535: step 66170, loss = 1.98 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:49.977710: step 66180, loss = 1.99 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:51.145997: step 66190, loss = 1.96 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:52.337691: step 66200, loss = 2.00 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:00:53.477639: step 66210, loss = 2.04 (1122.9 examples/sec; 0.114 sec/batch)
2017-05-05 00:00:54.637289: step 66220, loss = 1.82 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:55.813810: step 66230, loss = 1.97 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:56.976409: step 66240, loss = 1.92 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:58.142006: step 66250, loss = 2.04 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:59.308947: step 66260, loss = 2.01 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:00.478610: step 66270, loss = 2.02 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:01.659019: step 66280, loss = 2.06 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:02.807458: step 66290, loss = 1.97 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:01:03.961146: step 66300, loss = 2.12 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:01:05.128524: step 66310, loss = 1.92 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:06.278537: step 66320, loss = 2.04 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:01:07.550908: step 66330, loss = 1.82 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-05 00:01:08.622031: step 66340, loss = 1.90 (1195.0 examples/sec; 0.107 sec/batch)
2017-05-05 00:01:09.794168: step 66350, loss = 2.03 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:10.973719: step 66360, loss = 2.02 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:12.158093: step 66370, loss = 2.01 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:13.347714: step 66380, loss = 2.01 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:01:14.506259: step 66390, loss = 2.04 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:15.682665: step 66400, loss = 1.97 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:16.856172: step 66410, loss = 2.01 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:18.003450: step 66420, loss = 1.96 (1115.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:01:19.187407: step 66430, loss = 1.99 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:20.378119: step 66440, loss = 1.84 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:01:21.527155: step 66450, loss = 1.93 (1114.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:01:22.708219: step 66460, loss = 2.01 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:23.859686: step 66470, loss = 2.13 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:01:25.032089: step 66480, loss = 1.87 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:26.186177: step 66490, loss = 2.14 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:01:27.354255: step 66500, loss = 1.94 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:28.538513: step 66510, loss = 2.04 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:29.693891: step 66520, loss = 1.98 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:30.896117: step 66530, loss = 2.03 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:01:32.055646: step 66540, loss = 2.10 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:33.235682: step 66550, loss = 1.86 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:34.417610: step 66560, loss = 2.07 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:35.589717: step 66570, loss = 1.97 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:36.753868: step 66580, loss = 1.91 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:37.930519: step 66590, loss = 1.90 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:39.112424: step 66600, loss = 1.93 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:40.294809: step 66610, loss = 1.98 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:41.459802: step 66620, loss = 1.84 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:42.642714: step 66630, loss = 1.94 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:43.810884: step 66640, loss = 2.03 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:44.992565: step 66650, loss = 2.10 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:46.142793: step 66660, loss = 2.08 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-05 00:01:47.324593: step 66670, loss = 1.97 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:48.483541: step 66680, loss = 1.88 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:49.657184: step 66690, loss = 2.08 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:50.838424: step 66700, loss = 1.90 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:52.002303: step 66710, loss = 2.04 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:53.159951: step 66720, loss = 2.02 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:54.331470: step 66730, loss = 2.08 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:55.518119: step 66740, loss = 1.87 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:01:56.710128: step 66750, loss = 1.96 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:01:57.908428: step 66760, loss = 1.98 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:01:59.100837: step 66770, loss = 1.80 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:02:00.272407: step 66780, loss = 1.95 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:02:01.453470: step 66790, loss = 2.02 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:02:02.634819: step 66800, loss = 2.05 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:02:03.799801: step 66810, loss = 1.88 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:02:04.988564: step 66820, loss = 1.85 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:02:06.191775: step 66830, loss = 1.90 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:07.390132: step 66840, loss = 2.08 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:08.603091: step 66850, loss = 1.96 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:02:09.796058: step 66860, loss = 2.10 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:02:10.999510: step 66870, loss = 2.09 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:12.185365: step 66880, loss = 1.89 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:02:13.360927: step 66890, loss = 2.10 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:02:14.538566: step 66900, loss = 2.07 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:02:15.710495: step 66910, loss = 1.80 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:02:16.903379: step 66920, loss = 1.98 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:02:18.100338: step 66930, loss = 2.14 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:19.309858: step 66940, loss = 2.05 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:02:20.525911: step 66950, loss = 1.83 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:21.742433: step 66960, loss = 1.99 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:22.970291: step 66970, loss = 1.85 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:02:24.206846: step 66980, loss = 1.90 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-05 00:02:25.420497: step 66990, loss = 2.02 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:02:26.620618: step 67000, loss = 1.89 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:27.874007: step 67010, loss = 1.90 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-05 00:02:29.078010: step 67020, loss = 1.97 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:30.280843: step 67030, loss = 2.10 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:31.481507: step 67040, loss = 1.95 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:32.692995: step 67050, loss = 1.90 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:02:33.905354: step 67060, loss = 1.91 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:02:35.146648: step 67070, loss = 1.92 (1031.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:02:36.345100: step 67080, loss = 1.86 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:37.587477: step 67090, loss = 1.98 (1030.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:02:38.781712: step 67100, loss = 2.00 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:02:40.008477: step 67110, loss = 2.00 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:02:41.238114: step 67120, loss = 1.85 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:02:42.433677: step 67130, loss = 2.21 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:43.660370: step 67140, loss = 1.94 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:02:44.894955: step 67150, loss = 1.86 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:02:46.099590: step 67160, loss = 2.19 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:47.317398: step 67170, loss = 1.89 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:48.546556: step 67180, loss = 1.95 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:02:49.761773: step 67190, loss = 1.87 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:50.990526: step 67200, loss = 2.01 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:02:52.207418: step 67210, loss = 1.92 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:53.423825: step 67220, loss = 1.90 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:54.647337: step 67230, loss = 1.92 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:55.854516: step 67240, loss = 1.99 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:02:57.075596: step 67250, loss = 1.91 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:58.283315: step 67260, loss = 1.95 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:02:59.487309: step 67270, loss = 1.90 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:03:00.681477: step 67280, loss = 1.99 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:03:01.887800: step 67290, loss = 1.88 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:03.103473: step 67300, loss = 1.88 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:04.345291: step 67310, loss = 2.07 (1030.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:03:05.663702: step 67320, loss = 1.96 (970.9 examples/sec; 0.132 sec/batch)
2017-05-05 00:03:06.737841: step 67330, loss = 2.05 (1191.7 examples/sec; 0.107 sec/batch)
2017-05-05 00:03:07.956103: step 67340, loss = 2.00 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:09.192498: step 67350, loss = 2.03 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:03:10.382200: step 67360, loss = 1.90 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:03:11.610005: step 67370, loss = 1.87 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:03:12.827051: step 67380, loss = 1.96 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:14.042967: step 67390, loss = 1.98 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:15.281807: step 67400, loss = 1.97 (1033.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:03:16.494663: step 67410, loss = 1.88 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:17.675319: step 67420, loss = 1.91 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:03:18.888727: step 67430, loss = 2.04 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:20.100435: step 67440, loss = 1.96 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:21.314399: step 67450, loss = 2.02 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:22.524455: step 67460, loss = 2.02 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:23.762971: step 67470, loss = 1.88 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:03:24.961157: step 67480, loss = 1.94 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:03:26.167418: step 67490, loss = 1.97 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:27.375302: step 67500, loss = 2.14 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:28.598208: step 67510, loss = 2.06 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:29.869177: step 67520, loss = 1.94 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-05 00:03:31.016161: step 67530, loss = 1.95 (1116.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:03:32.222182: step 67540, loss = 2.00 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:33.421682: step 67550, loss = 1.90 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:03:34.595960: step 67560, loss = 1.84 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:03:35.812607: step 67570, loss = 1.91 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:37.006640: step 67580, loss = 1.84 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:03:38.202591: step 67590, loss = 1.82 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:03:39.399666: step 67600, loss = 1.89 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:03:40.590445: step 67610, loss = 2.13 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:03:41.768382: step 67620, loss = 1.98 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:03:42.953488: step 67630, loss = 1.97 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:03:44.163818: step 67640, loss = 1.98 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:45.359728: step 67650, loss = 1.93 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:03:46.547658: step 67660, loss = 1.96 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:03:47.763687: step 67670, loss = 2.00 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:48.973648: step 67680, loss = 2.41 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:50.194654: step 67690, loss = 1.84 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:51.415871: step 67700, loss = 2.04 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:52.628930: step 67710, loss = 1.90 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:53.842449: step 67720, loss = 1.96 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:55.035560: step 67730, loss = 1.91 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:03:56.240036: step 67740, loss = 2.02 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:03:57.420576: step 67750, loss = 1.90 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:03:58.583423: step 67760, loss = 1.96 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:03:59.754954: step 67770, loss = 2.05 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:00.908962: step 67780, loss = 2.01 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:04:02.072135: step 67790, loss = 2.10 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:03.250993: step 67800, loss = 1.96 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:04:04.410701: step 67810, loss = 1.94 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:05.569799: step 67820, loss = 2.04 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:06.745194: step 67830, loss = 2.01 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:04:07.916579: step 67840, loss = 1.92 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:09.086035: step 67850, loss = 2.15 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:10.257404: step 67860, loss = 2.01 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:11.422424: step 67870, loss = 1.93 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:12.587212: step 67880, loss = 1.96 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:13.761826: step 67890, loss = 1.80 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:14.945267: step 67900, loss = 2.08 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:04:16.105182: step 67910, loss = 2.05 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:17.266287: step 67920, loss = 2.01 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:18.412356: step 67930, loss = 1.95 (1116.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:04:19.580442: step 67940, loss = 2.01 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:20.750872: step 67950, loss = 2.02 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:21.889686: step 67960, loss = 1.89 (1124.0 examples/sec; 0.114 sec/batch)
2017-05-05 00:04:23.063574: step 67970, loss = 1.96 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:24.241809: step 67980, loss = 2.00 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:04:25.406357: step 67990, loss = 2.05 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:26.567503: step 68000, loss = 1.98 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:27.715219: step 68010, loss = 1.89 (1115.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:04:28.876109: step 68020, loss = 2.05 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:30.066459: step 68030, loss = 1.92 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:04:31.229457: step 68040, loss = 2.10 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:32.401427: step 68050, loss = 1.96 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:33.570378: step 68060, loss = 1.91 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:34.734003: step 68070, loss = 1.87 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:35.901520: step 68080, loss = 1.93 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:37.061187: step 68090, loss = 1.98 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:38.226452: step 68100, loss = 1.92 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:39.398515: step 68110, loss = 1.90 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:40.597952: step 68120, loss = 1.93 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:04:41.742941: step 68130, loss = 2.02 (1117.9 examples/sec; 0.114 sec/batch)
2017-05-05 00:04:42.900783: step 68140, loss = 2.08 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:44.080555: step 68150, loss = 1.98 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:04:45.265553: step 68160, loss = 1.98 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:04:46.450354: step 68170, loss = 1.96 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:04:47.646046: step 68180, loss = 2.02 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:04:48.851867: step 68190, loss = 1.90 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:04:50.046391: step 68200, loss = 2.06 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:04:51.245854: step 68210, loss = 2.02 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:04:52.450064: step 68220, loss = 1.88 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:04:53.649564: step 68230, loss = 2.11 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:04:54.846320: step 68240, loss = 1.88 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:04:56.060943: step 68250, loss = 2.03 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:04:57.263659: step 68260, loss = 1.90 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:04:58.462100: step 68270, loss = 1.95 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:04:59.682474: step 68280, loss = 1.91 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:00.911305: step 68290, loss = 1.83 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:05:02.122677: step 68300, loss = 2.25 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:03.469328: step 68310, loss = 2.06 (950.5 examples/sec; 0.135 sec/batch)
2017-05-05 00:05:04.557127: step 68320, loss = 1.99 (1176.7 examples/sec; 0.109 sec/batch)
2017-05-05 00:05:05.768592: step 68330, loss = 2.00 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:06.995176: step 68340, loss = 2.04 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:05:08.224882: step 68350, loss = 1.86 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:05:09.441206: step 68360, loss = 2.02 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:10.665421: step 68370, loss = 2.07 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:11.883024: step 68380, loss = 2.05 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:13.121165: step 68390, loss = 1.90 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:05:14.336593: step 68400, loss = 2.00 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:15.555217: step 68410, loss = 2.00 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:16.774474: step 68420, loss = 1.81 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:17.993326: step 68430, loss = 2.08 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:19.209447: step 68440, loss = 1.99 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:20.430875: step 68450, loss = 1.97 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:21.644039: step 68460, loss = 1.87 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:22.875733: step 68470, loss = 1.87 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:05:24.091438: step 68480, loss = 2.03 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:25.309341: step 68490, loss = 1.95 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:26.520568: step 68500, loss = 1.71 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:27.808084: step 68510, loss = 2.01 (994.2 examples/sec; 0.129 sec/batch)
2017-05-05 00:05:28.928201: step 68520, loss = 1.91 (1142.8 examples/sec; 0.112 sec/batch)
2017-05-05 00:05:30.161481: step 68530, loss = 1.89 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:05:31.370738: step 68540, loss = 1.95 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:32.606769: step 68550, loss = 1.87 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:05:33.799397: step 68560, loss = 1.98 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:05:35.020324: step 68570, loss = 1.96 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:36.259874: step 68580, loss = 1.97 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:05:37.484191: step 68590, loss = 2.07 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:38.702069: step 68600, loss = 1.87 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:39.930542: step 68610, loss = 1.86 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:05:41.133471: step 68620, loss = 1.93 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:05:42.361849: step 68630, loss = 2.03 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:05:43.579271: step 68640, loss = 1.80 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:44.800790: step 68650, loss = 1.91 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:46.002579: step 68660, loss = 2.00 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:05:47.217159: step 68670, loss = 2.01 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:48.432739: step 68680, loss = 1.89 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:49.642689: step 68690, loss = 2.01 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:50.844982: step 68700, loss = 1.96 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:05:52.083464: step 68710, loss = 1.96 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:05:53.302109: step 68720, loss = 2.15 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:54.514478: step 68730, loss = 1.99 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:55.733513: step 68740, loss = 1.94 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:56.936377: step 68750, loss = 1.96 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:05:58.121638: step 68760, loss = 1.96 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:05:59.352629: step 68770, loss = 2.01 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:06:00.580651: step 68780, loss = 2.10 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:06:01.815518: step 68790, loss = 1.99 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:06:03.037399: step 68800, loss = 1.95 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:06:04.266964: step 68810, loss = 1.85 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:06:05.482462: step 68820, loss = 2.00 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:06:06.718569: step 68830, loss = 1.95 (1035.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:06:07.936804: step 68840, loss = 1.98 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:06:09.170032: step 68850, loss = 2.01 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:06:10.379318: step 68860, loss = 2.03 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:06:11.601378: step 68870, loss = 1.99 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:06:12.810112: step 68880, loss = 2.08 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:06:14.035616: step 68890, loss = 1.82 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:06:15.253163: step 68900, loss = 1.82 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:06:16.448542: step 68910, loss = 2.05 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:06:17.671494: step 68920, loss = 1.97 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:06:18.862834: step 68930, loss = 1.96 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:06:20.083857: step 68940, loss = 2.02 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:06:21.300314: step 68950, loss = 2.00 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:06:22.486500: step 68960, loss = 1.89 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:06:23.709505: step 68970, loss = 2.02 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:06:24.929901: step 68980, loss = 1.93 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:06:26.121285: step 68990, loss = 1.80 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:06:27.311781: step 69000, loss = 1.76 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:06:28.515082: step 69010, loss = 1.96 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:06:29.712686: step 69020, loss = 1.92 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:06:30.917463: step 69030, loss = 2.15 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:06:32.109139: step 69040, loss = 1.92 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:06:33.283986: step 69050, loss = 1.96 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:06:34.443633: step 69060, loss = 2.04 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:06:35.629886: step 69070, loss = 2.08 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:06:36.813103: step 69080, loss = 2.04 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:06:37.966331: step 69090, loss = 1.93 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:06:39.137306: step 69100, loss = 1.88 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:06:40.308946: step 69110, loss = 1.95 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:06:41.503826: step 69120, loss = 1.87 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:06:42.666351: step 69130, loss = 1.94 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:06:43.863166: step 69140, loss = 1.89 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:06:45.052272: step 69150, loss = 2.08 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:06:46.219421: step 69160, loss = 1.91 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:06:47.401914: step 69170, loss = 1.91 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:06:48.583320: step 69180, loss = 1.97 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:06:49.746959: step 69190, loss = 1.91 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:06:50.924723: step 69200, loss = 1.97 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:06:52.119613: step 69210, loss = 1.95 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:06:53.274927: step 69220, loss = 2.07 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:06:54.419950: step 69230, loss = 1.93 (1117.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:06:55.597327: step 69240, loss = 1.89 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:06:56.753976: step 69250, loss = 1.91 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:06:57.909863: step 69260, loss = 1.83 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:06:59.076216: step 69270, loss = 2.14 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:07:00.238027: step 69280, loss = 2.04 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:07:01.406437: step 69290, loss = 1.95 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:07:02.693617: step 69300, loss = 2.09 (994.4 examples/sec; 0.129 sec/batch)
2017-05-05 00:07:03.774459: step 69310, loss = 1.77 (1184.3 examples/sec; 0.108 sec/batch)
2017-05-05 00:07:04.975359: step 69320, loss = 2.09 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:07:06.169336: step 69330, loss = 1.99 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:07:07.374584: step 69340, loss = 1.84 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:07:08.561933: step 69350, loss = 1.97 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:07:09.734842: step 69360, loss = 1.99 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:07:10.925126: step 69370, loss = 2.03 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:07:12.098537: step 69380, loss = 1.99 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:07:13.291004: step 69390, loss = 1.99 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:07:14.473408: step 69400, loss = 1.75 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:07:15.671118: step 69410, loss = 1.97 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:07:16.852887: step 69420, loss = 1.93 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:07:18.033362: step 69430, loss = 1.94 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:07:19.213649: step 69440, loss = 1.85 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:07:20.396782: step 69450, loss = 2.06 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:07:21.560392: step 69460, loss = 1.99 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:07:22.744745: step 69470, loss = 2.17 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:07:23.954612: step 69480, loss = 1.97 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:07:25.160598: step 69490, loss = 1.82 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:07:26.491592: step 69500, loss = 1.97 (961.7 examples/sec; 0.133 sec/batch)
2017-05-05 00:07:27.599678: step 69510, loss = 1.88 (1155.2 examples/sec; 0.111 sec/batch)
2017-05-05 00:07:28.805729: step 69520, loss = 1.95 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:07:30.018444: step 69530, loss = 1.91 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:07:31.224249: step 69540, loss = 2.04 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:07:32.461421: step 69550, loss = 1.80 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:07:33.654461: step 69560, loss = 2.16 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:07:34.882477: step 69570, loss = 1.93 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:07:36.104380: step 69580, loss = 1.86 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:37.322786: step 69590, loss = 1.92 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:38.516451: step 69600, loss = 1.87 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:07:39.740519: step 69610, loss = 2.00 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:40.974689: step 69620, loss = 1.87 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:07:42.214264: step 69630, loss = 1.98 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:07:43.414302: step 69640, loss = 2.00 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:07:44.654159: step 69650, loss = 2.07 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:07:45.876452: step 69660, loss = 1.97 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:47.096940: step 69670, loss = 1.94 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:48.321764: step 69680, loss = 2.03 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:49.531045: step 69690, loss = 1.98 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:07:50.729394: step 69700, loss = 1.88 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:07:51.938654: step 69710, loss = 1.91 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:07:53.181943: step 69720, loss = 2.01 (1029.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:07:54.384098: step 69730, loss = 1.91 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:07:55.611040: step 69740, loss = 2.15 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:07:56.829640: step 69750, loss = 2.08 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:58.041567: step 69760, loss = 2.02 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:07:59.281777: step 69770, loss = 1.73 (1032.1 examples/sec; 0.124 sec/batch)
2017-05-05 00:08:00.490793: step 69780, loss = 1.98 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:08:01.700223: step 69790, loss = 2.02 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:08:02.963468: step 69800, loss = 1.96 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-05 00:08:04.202354: step 69810, loss = 1.85 (1033.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:08:05.391435: step 69820, loss = 1.92 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:08:06.598884: step 69830, loss = 2.00 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:08:07.831021: step 69840, loss = 1.96 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:09.056384: step 69850, loss = 1.87 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:10.298842: step 69860, loss = 1.95 (1030.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:08:11.527311: step 69870, loss = 1.90 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:12.745304: step 69880, loss = 2.10 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:13.962872: step 69890, loss = 2.00 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:15.170255: step 69900, loss = 2.13 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:08:16.404261: step 69910, loss = 1.87 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:17.586415: step 69920, loss = 1.85 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:08:18.839397: step 69930, loss = 1.88 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-05 00:08:20.028514: step 69940, loss = 1.97 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:08:21.266047: step 69950, loss = 1.92 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:08:22.471995: step 69960, loss = 1.93 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:08:23.693422: step 69970, loss = 2.00 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:24.913936: step 69980, loss = 2.07 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:26.125123: step 69990, loss = 1.85 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:08:27.360598: step 70000, loss = 2.17 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:08:28.594338: step 70010, loss = 1.89 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:29.807626: step 70020, loss = 2.08 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:08:31.016597: step 70030, loss = 2.17 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:08:32.245054: step 70040, loss = 1.85 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:33.448441: step 70050, loss = 1.89 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:08:34.669694: step 70060, loss = 1.82 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:35.926509: step 70070, loss = 1.82 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-05 00:08:37.141596: step 70080, loss = 1.90 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:38.358802: step 70090, loss = 1.94 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:39.552790: step 70100, loss = 1.95 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:08:40.795086: step 70110, loss = 1.86 (1030.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:08:41.992397: step 70120, loss = 2.06 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:08:43.230510: step 70130, loss = 2.02 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:08:44.451469: step 70140, loss = 2.07 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:45.666758: step 70150, loss = 2.21 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:46.886880: step 70160, loss = 1.92 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:48.109307: step 70170, loss = 1.79 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:49.326683: step 70180, loss = 1.74 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:50.559412: step 70190, loss = 1.87 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:51.783139: step 70200, loss = 1.99 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:53.008087: step 70210, loss = 2.09 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:54.213708: step 70220, loss = 2.15 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:08:55.446785: step 70230, loss = 1.87 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:56.658414: step 70240, loss = 1.95 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:08:57.888756: step 70250, loss = 1.95 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:59.081922: step 70260, loss = 2.17 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:09:00.316259: step 70270, loss = 2.07 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:01.536624: step 70280, loss = 1.81 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:02.846602: step 70290, loss = 1.93 (977.1 examples/sec; 0.131 sec/batch)
2017-05-05 00:09:03.939129: step 70300, loss = 2.11 (1171.6 examples/sec; 0.109 sec/batch)
2017-05-05 00:09:05.166084: step 70310, loss = 2.07 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:06.357905: step 70320, loss = 2.04 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:09:07.618286: step 70330, loss = 2.09 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-05 00:09:08.830109: step 70340, loss = 1.97 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:10.047691: step 70350, loss = 1.98 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:11.273874: step 70360, loss = 2.27 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:12.494968: step 70370, loss = 1.96 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:13.710030: step 70380, loss = 1.88 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:14.918017: step 70390, loss = 2.00 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:16.113677: step 70400, loss = 2.09 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:09:17.355404: step 70410, loss = 1.90 (1030.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:09:18.567657: step 70420, loss = 2.00 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:19.808926: step 70430, loss = 1.83 (1031.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:09:21.039437: step 70440, loss = 2.06 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:22.267165: step 70450, loss = 2.10 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:23.468290: step 70460, loss = 1.97 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:09:24.711448: step 70470, loss = 2.14 (1029.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:09:25.927570: step 70480, loss = 1.90 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:27.242612: step 70490, loss = 2.01 (973.4 examples/sec; 0.132 sec/batch)
2017-05-05 00:09:28.331853: step 70500, loss = 1.99 (1175.1 examples/sec; 0.109 sec/batch)
2017-05-05 00:09:29.557768: step 70510, loss = 2.02 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:30.770538: step 70520, loss = 1.93 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:32.010345: step 70530, loss = 1.89 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:09:33.235464: step 70540, loss = 1.94 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:34.447605: step 70550, loss = 1.92 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:35.662624: step 70560, loss = 1.95 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:36.892574: step 70570, loss = 1.97 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:38.122966: step 70580, loss = 1.93 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:39.359639: step 70590, loss = 1.89 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:09:40.559250: step 70600, loss = 1.80 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:09:41.788275: step 70610, loss = 2.05 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:43.002803: step 70620, loss = 1.91 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:44.217781: step 70630, loss = 1.90 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:45.416990: step 70640, loss = 1.93 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:09:46.628196: step 70650, loss = 1.86 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:47.847481: step 70660, loss = 2.03 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:49.094408: step 70670, loss = 2.05 (1026.5 examples/sec; 0.125 sec/batch)
2017-05-05 00:09:50.289329: step 70680, loss = 1.84 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:09:51.503712: step 70690, loss = 1.96 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:52.721800: step 70700, loss = 1.94 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:53.935939: step 70710, loss = 1.91 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:55.158818: step 70720, loss = 2.04 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:56.384341: step 70730, loss = 2.00 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:57.598140: step 70740, loss = 2.05 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:58.831489: step 70750, loss = 1.98 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:00.047559: step 70760, loss = 1.96 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:01.265035: step 70770, loss = 2.02 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:02.481655: step 70780, loss = 2.01 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:03.713576: step 70790, loss = 1.91 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:04.927010: step 70800, loss = 1.91 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:06.166615: step 70810, loss = 1.89 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:10:07.386099: step 70820, loss = 1.85 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:08.609027: step 70830, loss = 1.90 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:09.826278: step 70840, loss = 2.19 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:11.069495: step 70850, loss = 1.94 (1029.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:10:12.278197: step 70860, loss = 1.80 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:13.506200: step 70870, loss = 1.98 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:14.724650: step 70880, loss = 1.97 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:15.922609: step 70890, loss = 1.93 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:10:17.147212: step 70900, loss = 1.67 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:18.349961: step 70910, loss = 1.96 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:10:19.566359: step 70920, loss = 1.96 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:20.787745: step 70930, loss = 2.13 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:21.996849: step 70940, loss = 1.77 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:23.215730: step 70950, loss = 1.93 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:24.467342: step 70960, loss = 2.05 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-05 00:10:25.645096: step 70970, loss = 1.97 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:10:26.869272: step 70980, loss = 1.99 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:28.079729: step 70990, loss = 1.86 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:29.316624: step 71000, loss = 1.95 (1034.9 examples/sec; 0.124 sec/batch)
2017-05-05 00:10:30.512950: step 71010, loss = 2.07 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:10:31.741022: step 71020, loss = 1.99 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:32.957478: step 71030, loss = 1.96 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:34.192896: step 71040, loss = 1.99 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-05 00:10:35.424581: step 71050, loss = 1.92 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:36.632868: step 71060, loss = 2.09 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:37.843069: step 71070, loss = 2.05 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:39.071633: step 71080, loss = 1.98 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:40.285282: step 71090, loss = 2.01 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:41.486718: step 71100, loss = 2.04 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:10:42.703655: step 71110, loss = 2.09 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:43.938445: step 71120, loss = 1.90 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:45.162593: step 71130, loss = 1.83 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:46.353127: step 71140, loss = 1.94 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:10:47.579939: step 71150, loss = 1.98 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:48.803622: step 71160, loss = 1.92 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:50.042172: step 71170, loss = 1.75 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:10:51.264374: step 71180, loss = 1.97 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:52.487728: step 71190, loss = 1.76 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:53.701201: step 71200, loss = 1.91 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:54.923612: step 71210, loss = 1.99 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:56.143765: step 71220, loss = 2.06 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:57.361891: step 71230, loss = 2.07 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:58.582571: step 71240, loss = 2.01 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:59.821846: step 71250, loss = 2.00 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-05 00:11:01.024567: step 71260, loss = 1.84 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:11:02.260488: step 71270, loss = 1.93 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:11:03.593297: step 71280, loss = 2.07 (960.4 examples/sec; 0.133 sec/batch)
2017-05-05 00:11:04.665471: step 71290, loss = 1.93 (1193.8 examples/sec; 0.107 sec/batch)
2017-05-05 00:11:05.888636: step 71300, loss = 1.99 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:07.126801: step 71310, loss = 2.16 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:11:08.321610: step 71320, loss = 1.92 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:11:09.536925: step 71330, loss = 2.14 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:10.733023: step 71340, loss = 1.96 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:11:11.959375: step 71350, loss = 1.91 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:11:13.193579: step 71360, loss = 1.97 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:11:14.410080: step 71370, loss = 1.98 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:15.608050: step 71380, loss = 1.90 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:11:16.823382: step 71390, loss = 1.96 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:18.051031: step 71400, loss = 1.90 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:11:19.252456: step 71410, loss = 1.91 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:11:20.480534: step 71420, loss = 2.10 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:11:21.719299: step 71430, loss = 1.89 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:11:22.930420: step 71440, loss = 2.15 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:24.140318: step 71450, loss = 1.95 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:25.356497: step 71460, loss = 2.17 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:26.555775: step 71470, loss = 1.89 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:11:27.874272: step 71480, loss = 1.90 (970.8 examples/sec; 0.132 sec/batch)
2017-05-05 00:11:29.004695: step 71490, loss = 1.88 (1132.3 examples/sec; 0.113 sec/batch)
2017-05-05 00:11:30.206441: step 71500, loss = 1.90 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:11:31.446692: step 71510, loss = 2.08 (1032.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:11:32.663940: step 71520, loss = 2.06 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:33.882794: step 71530, loss = 2.03 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:35.123728: step 71540, loss = 1.94 (1031.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:11:36.341735: step 71550, loss = 1.92 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:37.566645: step 71560, loss = 2.09 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:38.807038: step 71570, loss = 1.92 (1031.9 examples/sec; 0.124 sec/batch)
2017-05-05 00:11:40.007706: step 71580, loss = 2.00 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:11:41.252389: step 71590, loss = 1.82 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:11:42.470472: step 71600, loss = 1.96 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:43.713573: step 71610, loss = 1.75 (1029.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:11:44.934178: step 71620, loss = 2.10 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:46.149694: step 71630, loss = 1.87 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:47.365310: step 71640, loss = 1.83 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:48.599433: step 71650, loss = 2.03 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:11:49.815535: step 71660, loss = 2.07 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:51.036173: step 71670, loss = 2.05 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:52.205269: step 71680, loss = 1.97 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:11:53.440715: step 71690, loss = 1.95 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-05 00:11:54.649280: step 71700, loss = 1.87 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:55.879613: step 71710, loss = 1.96 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:11:57.074529: step 71720, loss = 2.04 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:11:58.273943: step 71730, loss = 2.07 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:11:59.489498: step 71740, loss = 2.00 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:00.720649: step 71750, loss = 2.06 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:01.928850: step 71760, loss = 1.81 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:03.147381: step 71770, loss = 1.90 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:04.370790: step 71780, loss = 1.92 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:05.598806: step 71790, loss = 1.84 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:06.803799: step 71800, loss = 2.07 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:12:08.044446: step 71810, loss = 1.97 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:12:09.270127: step 71820, loss = 1.89 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:10.460018: step 71830, loss = 1.89 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:12:11.672689: step 71840, loss = 1.99 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:12.917397: step 71850, loss = 2.04 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:12:14.108747: step 71860, loss = 1.83 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:12:15.344747: step 71870, loss = 1.97 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:12:16.559608: step 71880, loss = 1.89 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:17.781981: step 71890, loss = 1.95 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:19.009191: step 71900, loss = 1.85 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:20.246542: step 71910, loss = 1.98 (1034.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:12:21.470631: step 71920, loss = 2.00 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:22.698271: step 71930, loss = 1.94 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:23.916733: step 71940, loss = 1.96 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:25.131622: step 71950, loss = 2.20 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:26.354528: step 71960, loss = 2.02 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:27.586430: step 71970, loss = 2.02 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:28.795340: step 71980, loss = 2.00 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:30.006726: step 71990, loss = 2.08 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:31.234283: step 72000, loss = 1.91 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:32.466843: step 72010, loss = 1.98 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:33.694783: step 72020, loss = 1.96 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:34.914561: step 72030, loss = 1.92 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:36.127448: step 72040, loss = 1.94 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:37.347866: step 72050, loss = 1.84 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:38.574011: step 72060, loss = 1.95 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:39.787123: step 72070, loss = 1.89 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:40.982334: step 72080, loss = 1.74 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:12:42.192920: step 72090, loss = 1.91 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:43.418203: step 72100, loss = 2.03 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:44.658065: step 72110, loss = 1.98 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:12:45.883595: step 72120, loss = 1.86 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:47.107989: step 72130, loss = 1.85 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:48.322738: step 72140, loss = 2.03 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:49.541832: step 72150, loss = 2.02 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:50.772088: step 72160, loss = 1.80 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:51.998773: step 72170, loss = 1.89 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:53.214929: step 72180, loss = 2.19 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:54.423423: step 72190, loss = 1.89 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:55.653713: step 72200, loss = 2.11 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:56.869929: step 72210, loss = 1.95 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:58.092976: step 72220, loss = 2.06 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:59.303422: step 72230, loss = 1.81 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:13:00.518959: step 72240, loss = 2.04 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:01.732851: step 72250, loss = 1.80 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:13:02.930178: step 72260, loss = 1.94 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:13:04.264402: step 72270, loss = 1.89 (959.4 examples/sec; 0.133 sec/batch)
2017-05-05 00:13:05.346385: step 72280, loss = 1.86 (1183.0 examples/sec; 0.108 sec/batch)
2017-05-05 00:13:06.567589: step 72290, loss = 2.07 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:07.779107: step 72300, loss = 2.00 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:13:09.016012: step 72310, loss = 1.89 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:13:10.216076: step 72320, loss = 1.91 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:13:11.469479: step 72330, loss = 1.97 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-05 00:13:12.672676: step 72340, loss = 2.01 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:13:13.876371: step 72350, loss = 1.84 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:13:15.096325: step 72360, loss = 1.93 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:16.320352: step 72370, loss = 2.07 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:17.537361: step 72380, loss = 2.01 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:18.770301: step 72390, loss = 1.86 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:13:20.016053: step 72400, loss = 2.05 (1027.5 examples/sec; 0.125 sec/batch)
2017-05-05 00:13:21.213059: step 72410, loss = 1.91 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:13:22.437417: step 72420, loss = 1.81 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:23.674515: step 72430, loss = 1.94 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:13:24.914956: step 72440, loss = 1.80 (1031.9 examples/sec; 0.124 sec/batch)
2017-05-05 00:13:26.105134: step 72450, loss = 1.84 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:13:27.319798: step 72460, loss = 1.93 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:13:28.623265: step 72470, loss = 1.89 (982.0 examples/sec; 0.130 sec/batch)
2017-05-05 00:13:29.738045: step 72480, loss = 1.94 (1148.2 examples/sec; 0.111 sec/batch)
2017-05-05 00:13:30.966715: step 72490, loss = 1.84 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:13:32.172540: step 72500, loss = 1.96 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:13:33.399019: step 72510, loss = 1.90 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:13:34.631887: step 72520, loss = 1.96 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:13:35.851090: step 72530, loss = 2.14 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:37.065441: step 72540, loss = 2.04 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:13:38.297043: step 72550, loss = 1.91 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:13:39.529520: step 72560, loss = 1.88 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:13:40.748854: step 72570, loss = 2.12 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:41.957431: step 72580, loss = 1.91 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:13:43.219894: step 72590, loss = 1.96 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-05 00:13:44.435263: step 72600, loss = 2.05 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:45.642409: step 72610, loss = 1.90 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:13:46.865256: step 72620, loss = 1.94 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:48.090085: step 72630, loss = 1.83 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:49.312676: step 72640, loss = 1.97 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:50.533777: step 72650, loss = 1.84 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:51.761926: step 72660, loss = 2.03 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:13:52.981443: step 72670, loss = 1.97 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:54.178843: step 72680, loss = 1.98 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:13:55.396171: step 72690, loss = 1.90 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:56.588580: step 72700, loss = 1.76 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:13:57.787454: step 72710, loss = 1.96 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:13:58.987816: step 72720, loss = 1.82 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:14:00.186219: step 72730, loss = 1.98 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:14:01.388861: step 72740, loss = 1.91 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:14:02.558028: step 72750, loss = 1.78 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:14:03.768048: step 72760, loss = 2.03 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:04.980969: step 72770, loss = 2.06 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:06.195419: step 72780, loss = 2.02 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:07.410753: step 72790, loss = 1.95 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:08.640047: step 72800, loss = 2.08 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:09.843448: step 72810, loss = 2.05 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:14:11.068369: step 72820, loss = 1.94 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:12.287495: step 72830, loss = 1.89 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:13.507826: step 72840, loss = 1.88 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:14.754856: step 72850, loss = 1.86 (1026.4 examples/sec; 0.125 sec/batch)
2017-05-05 00:14:15.957318: step 72860, loss = 1.89 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:14:17.168698: step 72870, loss = 1.90 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:18.375448: step 72880, loss = 2.05 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:19.608018: step 72890, loss = 1.90 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:20.824546: step 72900, loss = 2.13 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:22.044696: step 72910, loss = 1.96 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:23.286451: step 72920, loss = 1.95 (1030.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:14:24.500845: step 72930, loss = 1.91 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:25.703964: step 72940, loss = 1.97 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:14:26.919695: step 72950, loss = 1.92 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:28.136321: step 72960, loss = 1.94 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:29.347762: step 72970, loss = 2.05 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:30.550916: step 72980, loss = 2.06 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:14:31.790746: step 72990, loss = 1.89 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:14:32.991204: step 73000, loss = 1.90 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:14:34.230645: step 73010, loss = 1.94 (1032.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:14:35.464919: step 73020, loss = 1.92 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:36.681661: step 73030, loss = 1.90 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:37.891760: step 73040, loss = 1.96 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:39.109678: step 73050, loss = 2.01 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:40.325845: step 73060, loss = 1.94 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:41.499316: step 73070, loss = 1.96 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:14:42.735904: step 73080, loss = 1.98 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-05 00:14:43.962078: step 73090, loss = 2.19 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:45.194056: step 73100, loss = 1.97 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:46.414434: step 73110, loss = 1.97 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:47.612283: step 73120, loss = 1.85 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:14:48.845556: step 73130, loss = 1.99 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:50.050479: step 73140, loss = 1.97 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:14:51.278170: step 73150, loss = 1.77 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:52.500349: step 73160, loss = 1.93 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:53.706629: step 73170, loss = 1.93 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:54.946776: step 73180, loss = 1.96 (1032.1 examples/sec; 0.124 sec/batch)
2017-05-05 00:14:56.177430: step 73190, loss = 1.93 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:57.395542: step 73200, loss = 2.03 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:58.612536: step 73210, loss = 1.82 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:59.836161: step 73220, loss = 1.91 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:01.049634: step 73230, loss = 1.87 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:02.274491: step 73240, loss = 2.02 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:03.501716: step 73250, loss = 1.89 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:15:04.821193: step 73260, loss = 2.05 (970.1 examples/sec; 0.132 sec/batch)
2017-05-05 00:15:05.900901: step 73270, loss = 1.89 (1185.5 examples/sec; 0.108 sec/batch)
2017-05-05 00:15:07.127491: step 73280, loss = 1.97 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:15:08.359293: step 73290, loss = 2.10 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:15:09.571131: step 73300, loss = 2.01 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:10.793437: step 73310, loss = 1.92 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:12.009344: step 73320, loss = 1.88 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:13.244577: step 73330, loss = 1.80 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:15:14.473266: step 73340, loss = 1.90 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:15:15.707141: step 73350, loss = 1.90 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:15:16.940908: step 73360, loss = 1.89 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:15:18.167416: step 73370, loss = 2.04 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:15:19.389315: step 73380, loss = 1.85 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:20.606287: step 73390, loss = 2.08 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:21.822625: step 73400, loss = 1.96 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:23.031594: step 73410, loss = 1.87 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:24.258880: step 73420, loss = 1.86 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:15:25.488127: step 73430, loss = 1.82 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:15:26.714307: step 73440, loss = 1.89 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:15:27.935003: step 73450, loss = 1.95 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:29.237448: step 73460, loss = 2.29 (982.8 examples/sec; 0.130 sec/batch)
2017-05-05 00:15:30.322077: step 73470, loss = 1.98 (1180.1 examples/sec; 0.108 sec/batch)
2017-05-05 00:15:31.541056: step 73480, loss = 1.90 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:32.749935: step 73490, loss = 1.93 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:33.954501: step 73500, loss = 2.00 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:15:35.178944: step 73510, loss = 2.03 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:36.371446: step 73520, loss = 1.93 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:15:37.587460: step 73530, loss = 2.04 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:38.799852: step 73540, loss = 1.94 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:40.014628: step 73550, loss = 1.96 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:41.212694: step 73560, loss = 2.01 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:15:42.410050: step 73570, loss = 2.02 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:15:43.590992: step 73580, loss = 2.08 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:15:44.805689: step 73590, loss = 1.84 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:45.983061: step 73600, loss = 1.81 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:15:47.178650: step 73610, loss = 1.99 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:15:48.360816: step 73620, loss = 1.83 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:15:49.537092: step 73630, loss = 1.89 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:15:50.732700: step 73640, loss = 1.92 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:15:51.927204: step 73650, loss = 2.02 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:15:53.109626: step 73660, loss = 1.88 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:15:54.310367: step 73670, loss = 1.97 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:15:55.525194: step 73680, loss = 1.99 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:56.749038: step 73690, loss = 1.93 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:57.967592: step 73700, loss = 1.99 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:59.207771: step 73710, loss = 2.13 (1032.1 examples/sec; 0.124 sec/batch)
2017-05-05 00:16:00.413071: step 73720, loss = 2.03 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:01.647144: step 73730, loss = 2.01 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:16:02.845196: step 73740, loss = 1.87 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:16:04.074826: step 73750, loss = 1.94 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:16:05.310193: step 73760, loss = 1.97 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-05 00:16:06.525060: step 73770, loss = 1.99 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:07.737645: step 73780, loss = 2.03 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:08.954460: step 73790, loss = 1.94 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:10.168984: step 73800, loss = 2.16 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:11.413159: step 73810, loss = 1.95 (1028.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:16:12.620280: step 73820, loss = 1.77 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:13.832287: step 73830, loss = 1.96 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:15.067375: step 73840, loss = 1.84 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:16:16.302326: step 73850, loss = 1.91 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:16:17.455305: step 73860, loss = 1.97 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:16:18.677372: step 73870, loss = 1.98 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:19.891208: step 73880, loss = 2.03 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:21.126675: step 73890, loss = 1.92 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:16:22.348326: step 73900, loss = 1.88 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:23.544528: step 73910, loss = 1.86 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:16:24.749607: step 73920, loss = 2.06 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:25.977124: step 73930, loss = 1.88 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:16:27.191426: step 73940, loss = 1.92 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:28.405383: step 73950, loss = 2.07 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:29.616199: step 73960, loss = 1.84 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:30.837645: step 73970, loss = 2.12 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:32.045980: step 73980, loss = 2.08 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:33.270759: step 73990, loss = 1.91 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:34.461117: step 74000, loss = 1.98 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:16:35.698569: step 74010, loss = 2.02 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:16:36.923560: step 74020, loss = 2.01 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:38.142323: step 74030, loss = 2.07 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:39.356943: step 74040, loss = 2.07 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:40.587404: step 74050, loss = 2.08 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:16:41.761290: step 74060, loss = 2.14 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:16:42.978478: step 74070, loss = 1.99 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:44.209911: step 74080, loss = 1.79 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:16:45.415747: step 74090, loss = 1.95 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:46.627427: step 74100, loss = 2.09 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:47.849976: step 74110, loss = 2.00 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:49.065633: step 74120, loss = 1.97 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:50.260557: step 74130, loss = 1.94 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:16:51.483199: step 74140, loss = 1.84 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:52.711614: step 74150, loss = 1.92 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:16:53.934782: step 74160, loss = 1.82 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:55.158964: step 74170, loss = 1.92 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:56.367134: step 74180, loss = 1.97 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:57.541974: step 74190, loss = 2.07 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:16:58.737925: step 74200, loss = 1.91 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:16:59.946880: step 74210, loss = 1.98 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:01.168072: step 74220, loss = 1.92 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:02.389053: step 74230, loss = 2.06 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:03.619442: step 74240, loss = 1.88 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:04.945896: step 74250, loss = 1.91 (965.0 examples/sec; 0.133 sec/batch)
2017-05-05 00:17:06.049845: step 74260, loss = 1.98 (1159.5 examples/sec; 0.110 sec/batch)
2017-05-05 00:17:07.284790: step 74270, loss = 2.00 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:08.499370: step 74280, loss = 1.95 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:09.719258: step 74290, loss = 1.95 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:10.937714: step 74300, loss = 1.88 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:12.158778: step 74310, loss = 1.92 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:13.362901: step 74320, loss = 1.91 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:17:14.581625: step 74330, loss = 1.95 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:15.783846: step 74340, loss = 1.91 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:17:17.014744: step 74350, loss = 1.89 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:18.218081: step 74360, loss = 1.84 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:17:19.445828: step 74370, loss = 2.03 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:20.642731: step 74380, loss = 1.87 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:17:21.860892: step 74390, loss = 2.00 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:23.070628: step 74400, loss = 1.82 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:24.299536: step 74410, loss = 1.93 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:25.498976: step 74420, loss = 1.78 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:17:26.735517: step 74430, loss = 2.04 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-05 00:17:27.967115: step 74440, loss = 2.02 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:29.269800: step 74450, loss = 1.98 (982.6 examples/sec; 0.130 sec/batch)
2017-05-05 00:17:30.366855: step 74460, loss = 2.10 (1166.8 examples/sec; 0.110 sec/batch)
2017-05-05 00:17:31.589032: step 74470, loss = 2.06 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:32.809015: step 74480, loss = 2.08 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:34.017699: step 74490, loss = 1.85 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:35.224111: step 74500, loss = 2.06 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:36.454543: step 74510, loss = 2.00 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:37.642390: step 74520, loss = 1.80 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:17:38.868557: step 74530, loss = 1.94 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:40.083353: step 74540, loss = 1.86 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:41.319287: step 74550, loss = 1.77 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:17:42.519042: step 74560, loss = 1.91 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:17:43.734569: step 74570, loss = 2.06 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:44.969498: step 74580, loss = 1.97 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:46.173907: step 74590, loss = 1.90 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:17:47.400594: step 74600, loss = 2.10 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:48.629895: step 74610, loss = 2.00 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:49.849330: step 74620, loss = 2.08 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:51.090591: step 74630, loss = 1.81 (1031.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:17:52.329299: step 74640, loss = 1.95 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:17:53.503354: step 74650, loss = 1.93 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:17:54.730496: step 74660, loss = 1.92 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:55.952128: step 74670, loss = 2.01 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:57.177122: step 74680, loss = 2.01 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:58.405503: step 74690, loss = 2.05 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:59.616909: step 74700, loss = 1.87 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:00.851697: step 74710, loss = 1.98 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:18:02.053946: step 74720, loss = 2.05 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:18:03.275903: step 74730, loss = 1.94 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:04.512836: step 74740, loss = 1.98 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:18:05.736859: step 74750, loss = 2.06 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:06.947343: step 74760, loss = 1.95 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:08.180438: step 74770, loss = 1.93 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:18:09.391493: step 74780, loss = 2.20 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:10.608042: step 74790, loss = 1.91 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:11.828792: step 74800, loss = 1.95 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:13.060323: step 74810, loss = 2.02 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:18:14.273672: step 74820, loss = 2.07 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:15.510354: step 74830, loss = 1.91 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:18:16.730222: step 74840, loss = 2.03 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:17.942283: step 74850, loss = 1.97 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:19.144703: step 74860, loss = 1.98 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:18:20.392046: step 74870, loss = 1.87 (1026.2 examples/sec; 0.125 sec/batch)
2017-05-05 00:18:21.599848: step 74880, loss = 1.99 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:22.841840: step 74890, loss = 1.95 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:18:24.064277: step 74900, loss = 1.94 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:25.278985: step 74910, loss = 1.94 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:26.489957: step 74920, loss = 1.81 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:27.726034: step 74930, loss = 2.02 (1035.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:18:28.930334: step 74940, loss = 1.88 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:18:30.169909: step 74950, loss = 1.96 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:18:31.380507: step 74960, loss = 1.86 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:32.607605: step 74970, loss = 1.94 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:18:33.831533: step 74980, loss = 1.99 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:35.034451: step 74990, loss = 1.89 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:18:36.276560: step 75000, loss = 1.89 (1030.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:18:37.490407: step 75010, loss = 1.80 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:38.708870: step 75020, loss = 1.92 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:39.936604: step 75030, loss = 1.95 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:18:41.147426: step 75040, loss = 1.90 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:42.336016: step 75050, loss = 2.00 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:18:43.559681: step 75060, loss = 1.99 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:44.790071: step 75070, loss = 1.96 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:18:45.993828: step 75080, loss = 1.91 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:18:47.216383: step 75090, loss = 1.92 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:48.440279: step 75100, loss = 1.93 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:49.658888: step 75110, loss = 2.01 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:50.852087: step 75120, loss = 1.87 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:18:52.098896: step 75130, loss = 2.06 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-05 00:18:53.298581: step 75140, loss = 1.99 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:18:54.510221: step 75150, loss = 1.89 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:55.726883: step 75160, loss = 1.97 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:56.959079: step 75170, loss = 1.99 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:18:58.162916: step 75180, loss = 2.01 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:18:59.381519: step 75190, loss = 1.91 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:00.608435: step 75200, loss = 1.93 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:01.832305: step 75210, loss = 1.99 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:03.042465: step 75220, loss = 1.81 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:04.292550: step 75230, loss = 2.04 (1023.9 examples/sec; 0.125 sec/batch)
2017-05-05 00:19:05.608443: step 75240, loss = 1.83 (972.7 examples/sec; 0.132 sec/batch)
2017-05-05 00:19:06.704993: step 75250, loss = 1.94 (1167.3 examples/sec; 0.110 sec/batch)
2017-05-05 00:19:07.919731: step 75260, loss = 1.85 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:09.143992: step 75270, loss = 1.94 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:10.360627: step 75280, loss = 2.00 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:11.566959: step 75290, loss = 2.00 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:12.794203: step 75300, loss = 1.92 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:14.019356: step 75310, loss = 1.87 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:15.211868: step 75320, loss = 2.08 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:19:16.463415: step 75330, loss = 1.89 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-05 00:19:17.683939: step 75340, loss = 1.90 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:18.897708: step 75350, loss = 1.87 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:20.132115: step 75360, loss = 1.88 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:21.371913: step 75370, loss = 2.09 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:19:22.568890: step 75380, loss = 1.98 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:19:23.796749: step 75390, loss = 1.84 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:25.029710: step 75400, loss = 1.95 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:26.230011: step 75410, loss = 1.99 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:19:27.438397: step 75420, loss = 1.96 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:28.686257: step 75430, loss = 2.04 (1025.8 examples/sec; 0.125 sec/batch)
2017-05-05 00:19:29.980208: step 75440, loss = 1.92 (989.2 examples/sec; 0.129 sec/batch)
2017-05-05 00:19:31.100158: step 75450, loss = 1.95 (1142.9 examples/sec; 0.112 sec/batch)
2017-05-05 00:19:32.309400: step 75460, loss = 1.90 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:33.536610: step 75470, loss = 2.01 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:34.747952: step 75480, loss = 1.93 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:35.959797: step 75490, loss = 1.86 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:37.189198: step 75500, loss = 2.12 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:38.397256: step 75510, loss = 1.90 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:39.612139: step 75520, loss = 1.89 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:40.831616: step 75530, loss = 1.89 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:42.047052: step 75540, loss = 1.87 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:43.267030: step 75550, loss = 1.91 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:44.475979: step 75560, loss = 1.85 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:45.716556: step 75570, loss = 2.17 (1031.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:19:46.918292: step 75580, loss = 2.01 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:19:48.156849: step 75590, loss = 1.97 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:19:49.381980: step 75600, loss = 2.03 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:50.615221: step 75610, loss = 1.95 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:51.830115: step 75620, loss = 1.94 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:53.047428: step 75630, loss = 1.96 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:54.230940: step 75640, loss = 2.07 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:19:55.451413: step 75650, loss = 1.97 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:56.678427: step 75660, loss = 1.84 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:57.879577: step 75670, loss = 1.85 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:19:59.105320: step 75680, loss = 1.81 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:00.338174: step 75690, loss = 1.81 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:01.542308: step 75700, loss = 1.96 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:20:02.783426: step 75710, loss = 2.00 (1031.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:20:04.005687: step 75720, loss = 1.94 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:05.215870: step 75730, loss = 1.85 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:06.425574: step 75740, loss = 1.84 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:07.677220: step 75750, loss = 1.80 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-05 00:20:08.894774: step 75760, loss = 1.90 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:10.093811: step 75770, loss = 1.90 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:20:11.321615: step 75780, loss = 2.06 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:12.544452: step 75790, loss = 2.11 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:13.759857: step 75800, loss = 2.06 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:14.998137: step 75810, loss = 1.95 (1033.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:20:16.216985: step 75820, loss = 1.92 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:17.422460: step 75830, loss = 2.00 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:18.624746: step 75840, loss = 1.85 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:20:19.876547: step 75850, loss = 1.83 (1022.5 examples/sec; 0.125 sec/batch)
2017-05-05 00:20:21.100979: step 75860, loss = 1.95 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:22.305716: step 75870, loss = 1.89 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:20:23.530205: step 75880, loss = 2.03 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:24.744887: step 75890, loss = 1.90 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:25.951566: step 75900, loss = 1.90 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:27.182512: step 75910, loss = 1.94 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:28.401109: step 75920, loss = 1.88 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:29.606491: step 75930, loss = 1.85 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:30.832608: step 75940, loss = 1.87 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:32.055026: step 75950, loss = 1.82 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:33.261903: step 75960, loss = 1.99 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:34.478559: step 75970, loss = 1.93 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:35.702180: step 75980, loss = 1.98 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:36.919465: step 75990, loss = 1.99 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:38.139184: step 76000, loss = 1.89 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:39.375563: step 76010, loss = 2.00 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:20:40.559833: step 76020, loss = 1.98 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:20:41.759722: step 76030, loss = 1.87 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:20:42.958801: step 76040, loss = 1.82 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:20:44.181585: step 76050, loss = 1.96 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:45.395110: step 76060, loss = 2.07 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:46.606949: step 76070, loss = 1.84 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:47.816571: step 76080, loss = 1.92 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:49.048600: step 76090, loss = 1.97 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:50.254748: step 76100, loss = 2.06 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:51.487392: step 76110, loss = 1.98 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:52.704105: step 76120, loss = 2.02 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:53.930438: step 76130, loss = 1.79 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:55.146276: step 76140, loss = 1.97 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:56.389515: step 76150, loss = 1.82 (1029.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:20:57.586699: step 76160, loss = 2.04 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:20:58.804812: step 76170, loss = 1.93 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:00.027578: step 76180, loss = 2.13 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:01.265379: step 76190, loss = 1.81 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-05 00:21:02.474618: step 76200, loss = 1.95 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:03.708407: step 76210, loss = 1.90 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:04.932964: step 76220, loss = 1.78 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:06.261331: step 76230, loss = 2.02 (963.6 examples/sec; 0.133 sec/batch)
2017-05-05 00:21:07.353921: step 76240, loss = 2.00 (1171.5 examples/sec; 0.109 sec/batch)
2017-05-05 00:21:08.574778: step 76250, loss = 1.91 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:09.786994: step 76260, loss = 1.93 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:11.013814: step 76270, loss = 2.01 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:12.231610: step 76280, loss = 1.97 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:13.456317: step 76290, loss = 1.97 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:14.648807: step 76300, loss = 1.99 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:21:15.880435: step 76310, loss = 1.98 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:17.097393: step 76320, loss = 1.78 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:18.311880: step 76330, loss = 1.83 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:19.527208: step 76340, loss = 2.07 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:20.735895: step 76350, loss = 2.04 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:21.950750: step 76360, loss = 1.92 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:23.184320: step 76370, loss = 1.89 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:24.400543: step 76380, loss = 1.83 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:25.639264: step 76390, loss = 2.01 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:21:26.821366: step 76400, loss = 2.18 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:21:28.035445: step 76410, loss = 2.07 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:29.239319: step 76420, loss = 1.99 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:21:30.527940: step 76430, loss = 1.86 (993.3 examples/sec; 0.129 sec/batch)
2017-05-05 00:21:31.664724: step 76440, loss = 1.94 (1126.0 examples/sec; 0.114 sec/batch)
2017-05-05 00:21:32.875430: step 76450, loss = 1.84 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:34.085528: step 76460, loss = 1.88 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:35.317164: step 76470, loss = 1.95 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:36.524626: step 76480, loss = 1.85 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:37.734471: step 76490, loss = 2.07 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:38.946420: step 76500, loss = 2.07 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:40.172278: step 76510, loss = 2.10 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:41.383086: step 76520, loss = 1.99 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:42.599841: step 76530, loss = 1.88 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:43.813967: step 76540, loss = 2.12 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:45.073040: step 76550, loss = 1.97 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-05 00:21:46.273076: step 76560, loss = 1.95 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:21:47.503757: step 76570, loss = 1.97 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:48.708833: step 76580, loss = 2.07 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:49.935197: step 76590, loss = 1.94 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:51.174467: step 76600, loss = 2.01 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-05 00:21:52.374992: step 76610, loss = 1.93 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:21:53.576752: step 76620, loss = 2.06 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:21:54.797994: step 76630, loss = 1.92 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:56.016804: step 76640, loss = 1.95 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:57.271567: step 76650, loss = 2.04 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-05 00:21:58.468963: step 76660, loss = 1.94 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:21:59.712511: step 76670, loss = 2.08 (1029.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:22:00.917704: step 76680, loss = 2.11 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:02.166671: step 76690, loss = 2.02 (1024.8 examples/sec; 0.125 sec/batch)
2017-05-05 00:22:03.375788: step 76700, loss = 1.93 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:04.598582: step 76710, loss = 1.88 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:05.793705: step 76720, loss = 1.91 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:22:07.031748: step 76730, loss = 1.89 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-05 00:22:08.245468: step 76740, loss = 1.93 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:09.460533: step 76750, loss = 1.86 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:10.658611: step 76760, loss = 1.92 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:22:11.888400: step 76770, loss = 1.87 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:13.101855: step 76780, loss = 2.01 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:14.314136: step 76790, loss = 1.89 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:15.555445: step 76800, loss = 2.02 (1031.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:22:16.784791: step 76810, loss = 2.04 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:17.992134: step 76820, loss = 2.00 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:19.195584: step 76830, loss = 2.09 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:22:20.412690: step 76840, loss = 1.80 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:21.629713: step 76850, loss = 1.95 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:22.844392: step 76860, loss = 1.97 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:24.089994: step 76870, loss = 1.83 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-05 00:22:25.298457: step 76880, loss = 2.01 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:26.530998: step 76890, loss = 1.85 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:27.777467: step 76900, loss = 2.01 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-05 00:22:29.004662: step 76910, loss = 1.91 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:30.220861: step 76920, loss = 1.90 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:31.433491: step 76930, loss = 1.99 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:32.634327: step 76940, loss = 1.96 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:22:33.870082: step 76950, loss = 2.06 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:22:35.097550: step 76960, loss = 1.88 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:36.308009: step 76970, loss = 1.99 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:37.534450: step 76980, loss = 2.04 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:38.757839: step 76990, loss = 1.92 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:39.962397: step 77000, loss = 1.84 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:22:41.180123: step 77010, loss = 2.01 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:42.386343: step 77020, loss = 2.13 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:43.571537: step 77030, loss = 1.91 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:22:44.783973: step 77040, loss = 2.05 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:46.020455: step 77050, loss = 1.94 (1035.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:22:47.223032: step 77060, loss = 2.03 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:22:48.454518: step 77070, loss = 1.96 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:49.676358: step 77080, loss = 1.85 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:50.898399: step 77090, loss = 1.86 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:52.120488: step 77100, loss = 1.93 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:53.344909: step 77110, loss = 1.98 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:54.571920: step 77120, loss = 1.89 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:55.795336: step 77130, loss = 1.93 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:57.021952: step 77140, loss = 1.96 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:58.240059: step 77150, loss = 1.87 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:59.472436: step 77160, loss = 1.94 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:23:00.687181: step 77170, loss = 1.99 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:01.891114: step 77180, loss = 1.99 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:23:03.117731: step 77190, loss = 2.10 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:23:04.334147: step 77200, loss = 1.86 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:05.553994: step 77210, loss = 1.96 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:06.872464: step 77220, loss = 1.91 (970.8 examples/sec; 0.132 sec/batch)
2017-05-05 00:23:07.952701: step 77230, loss = 2.04 (1184.9 examples/sec; 0.108 sec/batch)
2017-05-05 00:23:09.168713: step 77240, loss = 1.93 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:10.395747: step 77250, loss = 1.86 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:23:11.611672: step 77260, loss = 1.83 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:12.826306: step 77270, loss = 1.92 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:14.030664: step 77280, loss = 1.88 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:23:15.238654: step 77290, loss = 1.79 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:16.444853: step 77300, loss = 1.93 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:17.660527: step 77310, loss = 1.90 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:18.879947: step 77320, loss = 2.04 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:20.094505: step 77330, loss = 1.94 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:21.323578: step 77340, loss = 2.02 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:23:22.545183: step 77350, loss = 1.79 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:23.758315: step 77360, loss = 1.96 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:24.987564: step 77370, loss = 2.00 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:23:26.182378: step 77380, loss = 1.93 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:23:27.417899: step 77390, loss = 1.91 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:23:28.621996: step 77400, loss = 1.93 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:23:29.858562: step 77410, loss = 1.88 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-05 00:23:31.137127: step 77420, loss = 1.77 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-05 00:23:32.269556: step 77430, loss = 1.89 (1130.3 examples/sec; 0.113 sec/batch)
2017-05-05 00:23:33.464835: step 77440, loss = 2.07 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:23:34.702532: step 77450, loss = 1.92 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:23:35.913190: step 77460, loss = 1.89 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:37.151607: step 77470, loss = 1.88 (1033.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:23:38.369060: step 77480, loss = 1.90 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:39.588119: step 77490, loss = 2.04 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:40.830948: step 77500, loss = 2.02 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-05 00:23:42.061733: step 77510, loss = 1.94 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:23:43.263524: step 77520, loss = 2.02 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:23:44.492462: step 77530, loss = 1.97 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:23:45.690544: step 77540, loss = 1.94 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:23:46.933557: step 77550, loss = 1.83 (1029.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:23:48.156915: step 77560, loss = 2.03 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:49.381918: step 77570, loss = 2.00 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:23:50.606358: step 77580, loss = 1.97 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:51.842352: step 77590, loss = 1.91 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:23:53.067035: step 77600, loss = 1.90 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:54.280266: step 77610, loss = 1.93 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:55.476340: step 77620, loss = 2.11 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:23:56.715437: step 77630, loss = 2.08 (1033.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:23:57.922285: step 77640, loss = 2.00 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:59.169914: step 77650, loss = 1.89 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-05 00:24:00.374438: step 77660, loss = 1.89 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:24:01.581197: step 77670, loss = 1.92 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:02.798622: step 77680, loss = 2.09 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:04.029923: step 77690, loss = 2.02 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:05.243746: step 77700, loss = 1.93 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:06.463913: step 77710, loss = 1.80 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:07.689050: step 77720, loss = 2.11 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:08.907366: step 77730, loss = 1.97 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:10.117567: step 77740, loss = 1.81 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:11.377453: step 77750, loss = 1.99 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-05 00:24:12.591468: step 77760, loss = 1.88 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:13.808462: step 77770, loss = 1.98 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:15.028506: step 77780, loss = 1.96 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:16.254878: step 77790, loss = 2.19 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:17.485504: step 77800, loss = 1.88 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:18.710908: step 77810, loss = 1.97 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:19.876948: step 77820, loss = 1.83 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:24:21.105396: step 77830, loss = 1.98 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:22.314954: step 77840, loss = 2.00 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:23.529375: step 77850, loss = 1.92 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:24.733650: step 77860, loss = 2.05 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:24:25.956044: step 77870, loss = 1.95 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:27.170281: step 77880, loss = 2.00 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:28.386481: step 77890, loss = 1.90 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:29.589510: step 77900, loss = 1.94 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:24:30.839699: step 77910, loss = 1.85 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-05 00:24:32.061613: step 77920, loss = 1.81 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:33.289100: step 77930, loss = 1.94 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:34.492152: step 77940, loss = 1.97 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:24:35.730111: step 77950, loss = 1.87 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:24:36.950771: step 77960, loss = 1.86 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:38.172532: step 77970, loss = 1.94 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:39.380558: step 77980, loss = 2.07 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:40.605560: step 77990, loss = 2.14 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:41.815598: step 78000, loss = 1.97 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:43.048618: step 78010, loss = 1.96 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:44.249432: step 78020, loss = 1.80 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:24:45.496549: step 78030, loss = 1.97 (1026.4 examples/sec; 0.125 sec/batch)
2017-05-05 00:24:46.702419: step 78040, loss = 2.11 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:47.929558: step 78050, loss = 1.92 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:49.160220: step 78060, loss = 2.11 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:50.381900: step 78070, loss = 1.88 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:51.583492: step 78080, loss = 1.75 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:24:52.804854: step 78090, loss = 1.94 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:54.023506: step 78100, loss = 1.83 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:55.241521: step 78110, loss = 1.87 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:56.452905: step 78120, loss = 1.93 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:57.682766: step 78130, loss = 2.03 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:58.886655: step 78140, loss = 1.92 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:25:00.093733: step 78150, loss = 1.87 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:01.304429: step 78160, loss = 1.94 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:02.529767: step 78170, loss = 1.90 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:25:03.759833: step 78180, loss = 1.95 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:25:04.974125: step 78190, loss = 1.90 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:06.184718: step 78200, loss = 1.99 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:07.516354: step 78210, loss = 1.85 (961.2 examples/sec; 0.133 sec/batch)
2017-05-05 00:25:08.605424: step 78220, loss = 2.04 (1175.3 examples/sec; 0.109 sec/batch)
2017-05-05 00:25:09.809448: step 78230, loss = 1.90 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:25:11.026893: step 78240, loss = 1.95 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:25:12.239705: step 78250, loss = 2.03 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:13.454431: step 78260, loss = 1.89 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:14.688330: step 78270, loss = 1.93 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:25:15.906115: step 78280, loss = 1.81 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:25:17.115897: step 78290, loss = 2.00 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:18.327854: step 78300, loss = 1.83 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:19.552579: step 78310, loss = 2.05 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:25:20.783593: step 78320, loss = 1.96 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:25:21.997721: step 78330, loss = 1.92 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:23.201718: step 78340, loss = 1.84 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:25:24.426358: step 78350, loss = 1.78 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:25:25.637479: step 78360, loss = 2.00 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:26.868973: step 78370, loss = 1.94 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:25:28.069912: step 78380, loss = 2.08 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:25:29.283872: step 78390, loss = 1.97 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:30.477370: step 78400, loss = 1.87 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:25:31.823060: step 78410, loss = 1.90 (951.2 examples/sec; 0.135 sec/batch)
2017-05-05 00:25:32.913302: step 78420, loss = 2.03 (1174.0 examples/sec; 0.109 sec/batch)
2017-05-05 00:25:34.113744: step 78430, loss = 1.91 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:25:35.303046: step 78440, loss = 1.95 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:25:36.492560: step 78450, loss = 1.94 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:25:37.684814: step 78460, loss = 1.94 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:25:38.864403: step 78470, loss = 2.07 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:25:40.031833: step 78480, loss = 1.94 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:25:41.219951: step 78490, loss = 1.93 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:25:42.403565: step 78500, loss = 1.85 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:25:43.596195: step 78510, loss = 2.06 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:25:44.737688: step 78520, loss = 1.98 (1121.3 examples/sec; 0.114 sec/batch)
2017-05-05 00:25:45.888441: step 78530, loss = 1.84 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:25:47.064332: step 78540, loss = 1.97 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:25:48.230757: step 78550, loss = 1.95 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:25:49.396542: step 78560, loss = 2.01 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:25:50.561566: step 78570, loss = 1.80 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:25:51.728601: step 78580, loss = 1.94 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:25:52.915604: step 78590, loss = 1.98 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:25:54.079163: step 78600, loss = 1.95 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:25:55.257841: step 78610, loss = 1.74 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:25:56.427551: step 78620, loss = 1.85 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:25:57.618451: step 78630, loss = 1.89 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:25:58.803210: step 78640, loss = 1.81 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:25:59.997352: step 78650, loss = 1.95 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:26:01.164359: step 78660, loss = 2.18 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:02.318917: step 78670, loss = 1.68 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:26:03.487605: step 78680, loss = 1.74 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:04.662591: step 78690, loss = 2.01 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:05.830690: step 78700, loss = 2.04 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:07.009895: step 78710, loss = 1.97 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:26:08.170441: step 78720, loss = 1.99 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:09.346473: step 78730, loss = 1.98 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:26:10.489940: step 78740, loss = 1.94 (1119.4 examples/sec; 0.114 sec/batch)
2017-05-05 00:26:11.664258: step 78750, loss = 1.94 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:12.837390: step 78760, loss = 2.04 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:13.985099: step 78770, loss = 1.92 (1115.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:26:15.158489: step 78780, loss = 1.86 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:16.326889: step 78790, loss = 1.80 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:17.472814: step 78800, loss = 1.78 (1117.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:26:18.665710: step 78810, loss = 2.11 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:26:19.849432: step 78820, loss = 1.82 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:26:21.028406: step 78830, loss = 2.25 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:26:22.187838: step 78840, loss = 1.99 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:23.357752: step 78850, loss = 2.01 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:24.527203: step 78860, loss = 1.91 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:25.689216: step 78870, loss = 1.89 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:26.855036: step 78880, loss = 1.78 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:28.040018: step 78890, loss = 1.94 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:26:29.202438: step 78900, loss = 1.89 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:30.367217: step 78910, loss = 1.94 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:31.535910: step 78920, loss = 2.02 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:32.716868: step 78930, loss = 1.93 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:26:33.874292: step 78940, loss = 1.96 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:35.046565: step 78950, loss = 1.93 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:36.195275: step 78960, loss = 1.84 (1114.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:26:37.351770: step 78970, loss = 2.02 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:38.497084: step 78980, loss = 1.80 (1117.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:26:39.680935: step 78990, loss = 2.02 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:26:40.851407: step 79000, loss = 1.88 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:41.987032: step 79010, loss = 1.85 (1127.1 examples/sec; 0.114 sec/batch)
2017-05-05 00:26:43.150795: step 79020, loss = 1.88 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:44.348670: step 79030, loss = 2.02 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:26:45.526394: step 79040, loss = 1.87 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:26:46.674130: step 79050, loss = 2.00 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:26:47.862654: step 79060, loss = 2.00 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:26:49.022284: step 79070, loss = 2.05 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:50.206564: step 79080, loss = 1.95 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:26:51.374123: step 79090, loss = 1.97 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:52.535685: step 79100, loss = 1.94 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:53.686563: step 79110, loss = 1.79 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:26:54.862673: step 79120, loss = 1.79 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:26:56.031689: step 79130, loss = 1.88 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:57.198012: step 79140, loss = 2.23 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:58.342712: step 79150, loss = 1.94 (1118.2 examples/sec; 0.114 sec/batch)
2017-05-05 00:26:59.521713: step 79160, loss = 1.82 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:27:00.712183: step 79170, loss = 1.92 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:27:01.877734: step 79180, loss = 2.00 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:03.050203: step 79190, loss = 2.02 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:04.288062: step 79200, loss = 1.97 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:27:05.360793: step 79210, loss = 1.99 (1193.2 examples/sec; 0.107 sec/batch)
2017-05-05 00:27:06.517070: step 79220, loss = 1.95 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:07.692413: step 79230, loss = 1.82 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:27:08.893335: step 79240, loss = 1.95 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:27:10.041576: step 79250, loss = 1.86 (1114.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:27:11.210626: step 79260, loss = 1.86 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:12.384121: step 79270, loss = 2.05 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:13.543912: step 79280, loss = 1.95 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:14.720482: step 79290, loss = 2.00 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:27:15.910424: step 79300, loss = 1.92 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:27:17.110975: step 79310, loss = 2.07 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:27:18.288216: step 79320, loss = 1.93 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:27:19.448530: step 79330, loss = 1.87 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:20.614947: step 79340, loss = 1.87 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:21.763430: step 79350, loss = 2.01 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:27:22.927289: step 79360, loss = 1.76 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:24.092002: step 79370, loss = 1.97 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:25.263463: step 79380, loss = 1.98 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:26.419644: step 79390, loss = 1.84 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:27.591193: step 79400, loss = 2.02 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:28.766343: step 79410, loss = 1.91 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:27:29.953530: step 79420, loss = 2.23 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:27:31.128112: step 79430, loss = 1.90 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:32.305163: step 79440, loss = 2.16 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:27:33.471399: step 79450, loss = 1.86 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:34.637005: step 79460, loss = 2.15 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:35.804581: step 79470, loss = 1.89 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:36.964114: step 79480, loss = 1.91 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:38.115184: step 79490, loss = 2.07 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:27:39.280283: step 79500, loss = 1.90 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:40.459085: step 79510, loss = 1.93 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:27:41.616652: step 79520, loss = 1.92 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:42.788695: step 79530, loss = 1.88 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:43.949416: step 79540, loss = 1.84 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:45.116666: step 79550, loss = 1.79 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:46.280856: step 79560, loss = 2.05 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:47.440181: step 79570, loss = 2.12 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:48.592849: step 79580, loss = 2.16 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:27:49.741647: step 79590, loss = 2.00 (1114.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:27:50.913819: step 79600, loss = 1.97 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:52.110803: step 79610, loss = 1.89 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:27:53.274110: step 79620, loss = 1.74 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:54.448427: step 79630, loss = 1.99 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:55.622756: step 79640, loss = 1.95 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:56.796272: step 79650, loss = 1.94 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:57.936427: step 79660, loss = 1.86 (1122.7 examples/sec; 0.114 sec/batch)
2017-05-05 00:27:59.108587: step 79670, loss = 1.88 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:00.282888: step 79680, loss = 2.01 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:01.447005: step 79690, loss = 1.86 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:02.633921: step 79700, loss = 1.88 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:28:03.814086: step 79710, loss = 1.94 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:04.968401: step 79720, loss = 1.89 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:28:06.130721: step 79730, loss = 1.74 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:07.304518: step 79740, loss = 2.03 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:08.462948: step 79750, loss = 1.83 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:09.622407: step 79760, loss = 2.05 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:10.788632: step 79770, loss = 1.99 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:11.960073: step 79780, loss = 1.99 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:13.114213: step 79790, loss = 2.03 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:28:14.278790: step 79800, loss = 2.03 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:15.438861: step 79810, loss = 1.95 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:16.602468: step 79820, loss = 1.95 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:17.779319: step 79830, loss = 1.82 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:18.942696: step 79840, loss = 1.96 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:20.090212: step 79850, loss = 1.89 (1115.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:28:21.264661: step 79860, loss = 2.00 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:22.428980: step 79870, loss = 2.01 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:23.606545: step 79880, loss = 2.00 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:24.750667: step 79890, loss = 2.08 (1118.8 examples/sec; 0.114 sec/batch)
2017-05-05 00:28:25.930027: step 79900, loss = 2.00 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:27.098460: step 79910, loss = 1.98 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:28.271806: step 79920, loss = 1.81 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:29.447029: step 79930, loss = 1.88 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:30.586398: step 79940, loss = 1.82 (1123.4 examples/sec; 0.114 sec/batch)
2017-05-05 00:28:31.758475: step 79950, loss = 1.99 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:32.938778: step 79960, loss = 1.89 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:34.098669: step 79970, loss = 1.99 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:35.257178: step 79980, loss = 1.93 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:36.444755: step 79990, loss = 1.85 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:28:37.628777: step 80000, loss = 2.06 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:38.795095: step 80010, loss = 2.04 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:39.954379: step 80020, loss = 2.02 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:41.134467: step 80030, loss = 1.94 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:42.308357: step 80040, loss = 1.95 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:43.489330: step 80050, loss = 1.97 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:44.654749: step 80060, loss = 2.06 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:45.815559: step 80070, loss = 1.93 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:46.994378: step 80080, loss = 2.02 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:48.160548: step 80090, loss = 2.00 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:49.331584: step 80100, loss = 2.01 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:50.487700: step 80110, loss = 1.90 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:51.656163: step 80120, loss = 1.92 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:52.818351: step 80130, loss = 1.80 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:54.097976: step 80140, loss = 2.03 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-05 00:28:55.206461: step 80150, loss = 1.90 (1154.7 examples/sec; 0.111 sec/batch)
2017-05-05 00:28:56.372531: step 80160, loss = 2.06 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:57.534379: step 80170, loss = 2.01 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:58.689741: step 80180, loss = 1.94 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:59.940772: step 80190, loss = 1.76 (1023.2 examples/sec; 0.125 sec/batch)
2017-05-05 00:29:01.017280: step 80200, loss = 1.90 (1189.0 examples/sec; 0.108 sec/batch)
2017-05-05 00:29:02.180116: step 80210, loss = 1.98 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:29:03.341946: step 80220, loss = 1.76 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:29:04.511411: step 80230, loss = 1.99 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:29:05.653465: step 80240, loss = 1.87 (1120.8 examples/sec; 0.114 sec/batch)
2017-05-05 00:29:06.830356: step 80250, loss = 2.08 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:07.999964: step 80260, loss = 2.01 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:29:09.172831: step 80270, loss = 2.05 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:29:10.353091: step 80280, loss = 1.86 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:11.532544: step 80290, loss = 2.05 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:12.706333: step 80300, loss = 1.94 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:29:13.855148: step 80310, loss = 1.96 (1114.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:29:15.019022: step 80320, loss = 1.88 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:29:16.207332: step 80330, loss = 2.17 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:17.377983: step 80340, loss = 1.90 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:29:18.542989: step 80350, loss = 1.77 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:29:19.729056: step 80360, loss = 1.78 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:20.912349: step 80370, loss = 1.98 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:22.097204: step 80380, loss = 1.98 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:23.288383: step 80390, loss = 2.02 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:24.492718: step 80400, loss = 2.07 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:29:25.669667: step 80410, loss = 1.92 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:26.878184: step 80420, loss = 1.94 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:29:28.081706: step 80430, loss = 1.89 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:29:29.286133: step 80440, loss = 1.93 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:29:30.470482: step 80450, loss = 2.00 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:31.669263: step 80460, loss = 1.85 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:29:32.866696: step 80470, loss = 1.95 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:29:34.043285: step 80480, loss = 1.91 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:35.225174: step 80490, loss = 2.04 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:36.404292: step 80500, loss = 1.95 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:37.581418: step 80510, loss = 1.89 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:38.764836: step 80520, loss = 2.06 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:39.957434: step 80530, loss = 2.09 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:41.143976: step 80540, loss = 1.83 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:42.322483: step 80550, loss = 1.86 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:43.528603: step 80560, loss = 2.01 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:29:44.736422: step 80570, loss = 1.79 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:29:45.909910: step 80580, loss = 1.85 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:29:47.097571: step 80590, loss = 2.04 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:48.299449: step 80600, loss = 1.90 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:29:49.476073: step 80610, loss = 1.85 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:50.678364: step 80620, loss = 1.90 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:29:51.851944: step 80630, loss = 2.04 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:29:53.035967: step 80640, loss = 1.97 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:54.225103: step 80650, loss = 1.82 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:55.417829: step 80660, loss = 1.87 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:56.614058: step 80670, loss = 2.05 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:29:57.822937: step 80680, loss = 1.95 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:29:59.035323: step 80690, loss = 1.99 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:30:00.240317: step 80700, loss = 1.99 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:30:01.424735: step 80710, loss = 1.88 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:02.610487: step 80720, loss = 1.89 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:30:03.788350: step 80730, loss = 1.85 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:04.966466: step 80740, loss = 1.97 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:06.119043: step 80750, loss = 1.89 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:30:07.276162: step 80760, loss = 1.82 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:30:08.445508: step 80770, loss = 1.88 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:09.589869: step 80780, loss = 1.91 (1118.5 examples/sec; 0.114 sec/batch)
2017-05-05 00:30:10.755675: step 80790, loss = 1.83 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:11.912916: step 80800, loss = 1.82 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:30:13.067943: step 80810, loss = 1.96 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:30:14.222405: step 80820, loss = 1.98 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:30:15.390896: step 80830, loss = 1.96 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:16.570321: step 80840, loss = 1.91 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:17.751622: step 80850, loss = 2.01 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:18.944199: step 80860, loss = 1.85 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:30:20.109789: step 80870, loss = 1.91 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:21.286080: step 80880, loss = 2.04 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:22.441433: step 80890, loss = 1.92 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:30:23.622750: step 80900, loss = 1.98 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:24.791526: step 80910, loss = 1.91 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:25.974574: step 80920, loss = 2.06 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:27.148427: step 80930, loss = 2.03 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:28.331391: step 80940, loss = 1.93 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:29.509929: step 80950, loss = 2.03 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:30.675755: step 80960, loss = 1.95 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:31.872378: step 80970, loss = 1.93 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:30:33.070900: step 80980, loss = 1.74 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:30:34.244378: step 80990, loss = 1.90 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:35.463058: step 81000, loss = 2.08 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:30:36.688074: step 81010, loss = 1.98 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:30:37.885608: step 81020, loss = 2.00 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:30:39.082331: step 81030, loss = 2.02 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:30:40.280090: step 81040, loss = 1.84 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:30:41.463467: step 81050, loss = 1.95 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:42.630282: step 81060, loss = 2.03 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:43.826668: step 81070, loss = 2.00 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:30:45.009860: step 81080, loss = 1.97 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:46.175415: step 81090, loss = 1.84 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:47.363415: step 81100, loss = 1.85 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:30:48.560549: step 81110, loss = 1.91 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:30:49.718777: step 81120, loss = 1.85 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:30:50.906479: step 81130, loss = 1.86 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:30:52.077146: step 81140, loss = 1.95 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:53.254042: step 81150, loss = 2.08 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:54.412393: step 81160, loss = 1.99 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:30:55.579278: step 81170, loss = 2.12 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:56.858103: step 81180, loss = 1.90 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-05 00:30:57.916577: step 81190, loss = 1.88 (1209.3 examples/sec; 0.106 sec/batch)
2017-05-05 00:30:59.099345: step 81200, loss = 1.99 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:00.281055: step 81210, loss = 1.85 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:01.456842: step 81220, loss = 1.88 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:02.613048: step 81230, loss = 1.83 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:03.797649: step 81240, loss = 2.00 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:04.970128: step 81250, loss = 1.92 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:06.130833: step 81260, loss = 1.88 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:07.327617: step 81270, loss = 1.93 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:31:08.494349: step 81280, loss = 1.92 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:09.671008: step 81290, loss = 1.98 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:10.822391: step 81300, loss = 1.89 (1111.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:31:11.998871: step 81310, loss = 1.90 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:13.176399: step 81320, loss = 1.98 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:14.317000: step 81330, loss = 2.08 (1122.2 examples/sec; 0.114 sec/batch)
2017-05-05 00:31:15.507590: step 81340, loss = 1.87 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:31:16.697683: step 81350, loss = 2.00 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:31:17.853122: step 81360, loss = 1.91 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:19.025443: step 81370, loss = 1.99 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:20.187280: step 81380, loss = 1.84 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:21.353436: step 81390, loss = 1.95 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:22.517860: step 81400, loss = 1.84 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:23.691296: step 81410, loss = 1.93 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:24.857038: step 81420, loss = 2.08 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:26.013789: step 81430, loss = 1.87 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:27.174694: step 81440, loss = 1.94 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:28.338512: step 81450, loss = 2.02 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:29.506407: step 81460, loss = 1.96 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:30.659938: step 81470, loss = 1.88 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:31:31.827509: step 81480, loss = 1.94 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:32.988077: step 81490, loss = 1.99 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:34.139202: step 81500, loss = 1.87 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:31:35.307086: step 81510, loss = 1.98 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:36.485702: step 81520, loss = 2.03 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:37.639396: step 81530, loss = 1.86 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:31:38.815066: step 81540, loss = 2.04 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:39.982987: step 81550, loss = 1.87 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:41.147922: step 81560, loss = 2.03 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:42.297055: step 81570, loss = 1.96 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:31:43.464725: step 81580, loss = 1.93 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:44.626373: step 81590, loss = 2.01 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:45.775316: step 81600, loss = 1.94 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:31:46.941070: step 81610, loss = 1.89 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:48.097934: step 81620, loss = 1.80 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:49.261399: step 81630, loss = 2.13 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:50.425549: step 81640, loss = 1.86 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:51.601359: step 81650, loss = 1.99 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:52.766029: step 81660, loss = 1.97 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:53.939421: step 81670, loss = 1.97 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:55.103475: step 81680, loss = 2.01 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:56.283705: step 81690, loss = 1.88 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:57.465782: step 81700, loss = 1.82 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:58.648704: step 81710, loss = 1.82 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:59.836498: step 81720, loss = 1.96 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:01.031729: step 81730, loss = 2.13 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:32:02.202192: step 81740, loss = 1.87 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:03.378571: step 81750, loss = 1.93 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:04.567745: step 81760, loss = 1.95 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:05.731338: step 81770, loss = 1.91 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:06.916008: step 81780, loss = 1.94 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:08.110595: step 81790, loss = 1.97 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:09.318966: step 81800, loss = 1.91 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:32:10.533307: step 81810, loss = 1.98 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:32:11.729033: step 81820, loss = 1.92 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:32:12.895110: step 81830, loss = 2.01 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:14.066816: step 81840, loss = 2.09 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:15.251218: step 81850, loss = 2.11 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:16.460065: step 81860, loss = 1.84 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:32:17.647419: step 81870, loss = 1.99 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:18.842394: step 81880, loss = 2.02 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:20.029831: step 81890, loss = 1.87 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:21.223792: step 81900, loss = 2.04 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:22.411603: step 81910, loss = 2.04 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:23.576422: step 81920, loss = 1.92 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:24.745508: step 81930, loss = 1.95 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:25.905903: step 81940, loss = 2.02 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:27.077422: step 81950, loss = 1.82 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:28.247209: step 81960, loss = 1.88 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:29.409738: step 81970, loss = 1.98 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:30.628307: step 81980, loss = 2.03 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:32:31.794091: step 81990, loss = 2.00 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:32.974595: step 82000, loss = 2.06 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:34.148811: step 82010, loss = 1.92 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:35.347717: step 82020, loss = 1.99 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:32:36.534510: step 82030, loss = 1.88 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:37.706836: step 82040, loss = 2.02 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:38.897397: step 82050, loss = 1.90 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:40.082809: step 82060, loss = 1.88 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:41.250620: step 82070, loss = 1.97 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:42.406376: step 82080, loss = 1.94 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:43.591799: step 82090, loss = 1.96 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:44.767071: step 82100, loss = 1.90 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:45.926488: step 82110, loss = 2.22 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:47.082801: step 82120, loss = 1.99 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:48.275928: step 82130, loss = 2.07 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:49.438400: step 82140, loss = 1.85 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:50.616094: step 82150, loss = 2.01 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:51.791688: step 82160, loss = 2.09 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:53.073416: step 82170, loss = 1.85 (998.7 examples/sec; 0.128 sec/batch)
2017-05-05 00:32:54.114686: step 82180, loss = 1.92 (1229.3 examples/sec; 0.104 sec/batch)
2017-05-05 00:32:55.278354: step 82190, loss = 1.91 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:56.442920: step 82200, loss = 1.96 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:57.612546: step 82210, loss = 1.89 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:58.776807: step 82220, loss = 1.82 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:59.948157: step 82230, loss = 1.88 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:01.126679: step 82240, loss = 2.05 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:33:02.295536: step 82250, loss = 1.97 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:03.459508: step 82260, loss = 1.93 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:04.617429: step 82270, loss = 1.97 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:05.770940: step 82280, loss = 2.14 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:33:06.950652: step 82290, loss = 1.80 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:33:08.138002: step 82300, loss = 1.84 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:33:09.302276: step 82310, loss = 1.82 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:10.466894: step 82320, loss = 1.99 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:11.629091: step 82330, loss = 1.86 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:12.813675: step 82340, loss = 1.86 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:33:13.984525: step 82350, loss = 1.87 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:15.164188: step 82360, loss = 2.00 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:33:16.339306: step 82370, loss = 1.91 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:33:17.495723: step 82380, loss = 1.94 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:18.659217: step 82390, loss = 1.86 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:19.821349: step 82400, loss = 1.98 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:20.989949: step 82410, loss = 1.95 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:22.140686: step 82420, loss = 1.95 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:33:23.309384: step 82430, loss = 1.94 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:24.467354: step 82440, loss = 2.05 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:25.611709: step 82450, loss = 1.85 (1118.5 examples/sec; 0.114 sec/batch)
2017-05-05 00:33:26.785706: step 82460, loss = 1.87 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:27.977885: step 82470, loss = 2.05 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:33:29.156408: step 82480, loss = 1.96 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:33:30.317037: step 82490, loss = 1.87 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:31.487979: step 82500, loss = 1.90 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:32.655113: step 82510, loss = 1.97 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:33.812411: step 82520, loss = 1.90 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:34.973377: step 82530, loss = 1.89 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:36.151782: step 82540, loss = 1.90 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:33:37.318460: step 82550, loss = 1.85 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:38.487882: step 82560, loss = 2.04 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:39.662177: step 82570, loss = 2.01 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:40.839377: step 82580, loss = 1.98 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:33:42.037661: step 82590, loss = 1.84 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:33:43.234374: step 82600, loss = 1.91 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:33:44.440365: step 82610, loss = 2.08 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:33:45.621094: step 82620, loss = 2.00 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:33:46.837427: step 82630, loss = 2.08 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:33:48.038416: step 82640, loss = 2.01 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:33:49.262415: step 82650, loss = 1.87 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:33:50.491264: step 82660, loss = 1.94 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:33:51.705058: step 82670, loss = 1.95 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:33:52.935097: step 82680, loss = 1.77 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:33:54.142015: step 82690, loss = 1.96 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:33:55.360575: step 82700, loss = 2.00 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:33:56.584907: step 82710, loss = 2.03 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:33:57.781179: step 82720, loss = 1.89 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:33:59.012086: step 82730, loss = 1.98 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:00.230334: step 82740, loss = 1.87 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:01.446197: step 82750, loss = 2.08 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:02.641636: step 82760, loss = 1.87 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:34:03.839744: step 82770, loss = 1.96 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:34:05.073142: step 82780, loss = 1.93 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:06.286449: step 82790, loss = 1.93 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:07.500973: step 82800, loss = 2.12 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:08.717312: step 82810, loss = 1.90 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:09.922608: step 82820, loss = 2.02 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:11.156661: step 82830, loss = 1.87 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:12.386689: step 82840, loss = 1.93 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:13.595479: step 82850, loss = 1.83 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:14.818502: step 82860, loss = 1.83 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:16.032183: step 82870, loss = 1.97 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:17.255895: step 82880, loss = 1.87 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:18.468580: step 82890, loss = 2.08 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:19.684271: step 82900, loss = 1.92 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:20.898576: step 82910, loss = 1.88 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:22.103118: step 82920, loss = 2.05 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:34:23.328095: step 82930, loss = 1.81 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:24.536733: step 82940, loss = 1.93 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:25.757538: step 82950, loss = 1.92 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:26.974459: step 82960, loss = 2.04 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:28.184807: step 82970, loss = 1.90 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:29.402641: step 82980, loss = 1.90 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:30.612901: step 82990, loss = 2.06 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:31.842857: step 83000, loss = 2.14 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:33.055492: step 83010, loss = 2.07 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:34.270943: step 83020, loss = 1.81 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:35.488700: step 83030, loss = 2.22 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:36.706033: step 83040, loss = 2.05 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:37.914347: step 83050, loss = 1.89 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:39.136060: step 83060, loss = 1.98 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:40.351653: step 83070, loss = 1.99 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:41.559995: step 83080, loss = 2.01 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:42.791323: step 83090, loss = 1.92 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:44.007706: step 83100, loss = 1.82 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:45.237125: step 83110, loss = 2.05 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:46.450406: step 83120, loss = 2.02 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:47.656286: step 83130, loss = 2.00 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:48.876443: step 83140, loss = 1.84 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:50.064575: step 83150, loss = 1.90 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:34:51.357152: step 83160, loss = 1.81 (990.3 examples/sec; 0.129 sec/batch)
2017-05-05 00:34:52.464796: step 83170, loss = 1.90 (1155.6 examples/sec; 0.111 sec/batch)
2017-05-05 00:34:53.683887: step 83180, loss = 1.89 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:54.918954: step 83190, loss = 1.87 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:34:56.132306: step 83200, loss = 1.93 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:57.342788: step 83210, loss = 2.04 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:58.568948: step 83220, loss = 2.09 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:59.791168: step 83230, loss = 2.03 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:00.996621: step 83240, loss = 1.89 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:02.205931: step 83250, loss = 1.82 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:03.424429: step 83260, loss = 2.02 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:04.627646: step 83270, loss = 1.79 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:35:05.849032: step 83280, loss = 1.99 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:07.071027: step 83290, loss = 1.83 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:08.290770: step 83300, loss = 2.11 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:09.498174: step 83310, loss = 1.94 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:10.700116: step 83320, loss = 2.12 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:35:11.936309: step 83330, loss = 2.04 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:35:13.160746: step 83340, loss = 2.13 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:14.374283: step 83350, loss = 1.99 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:15.567273: step 83360, loss = 1.98 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:35:16.791495: step 83370, loss = 1.89 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:18.016917: step 83380, loss = 1.87 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:19.226396: step 83390, loss = 2.05 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:20.445013: step 83400, loss = 1.91 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:21.649407: step 83410, loss = 1.93 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:35:22.875816: step 83420, loss = 2.02 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:24.098844: step 83430, loss = 2.02 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:25.319174: step 83440, loss = 1.89 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:26.533206: step 83450, loss = 2.05 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:27.772905: step 83460, loss = 1.89 (1032.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:35:28.992410: step 83470, loss = 1.88 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:30.194877: step 83480, loss = 1.77 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:35:31.419469: step 83490, loss = 2.31 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:32.623543: step 83500, loss = 2.03 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:35:33.832536: step 83510, loss = 2.00 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:35.066326: step 83520, loss = 1.99 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:36.292061: step 83530, loss = 1.93 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:37.526978: step 83540, loss = 2.07 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:38.746554: step 83550, loss = 1.87 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:39.932504: step 83560, loss = 1.95 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:35:41.154395: step 83570, loss = 1.94 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:42.361817: step 83580, loss = 1.91 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:43.597700: step 83590, loss = 1.89 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:35:44.805473: step 83600, loss = 1.79 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:46.010372: step 83610, loss = 1.75 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:35:47.227101: step 83620, loss = 2.03 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:48.453610: step 83630, loss = 2.09 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:49.656288: step 83640, loss = 1.95 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:35:50.884994: step 83650, loss = 2.04 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:52.120215: step 83660, loss = 1.93 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:35:53.327574: step 83670, loss = 1.92 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:54.548906: step 83680, loss = 1.94 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:55.774068: step 83690, loss = 1.90 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:56.996678: step 83700, loss = 2.06 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:58.209358: step 83710, loss = 2.03 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:59.430482: step 83720, loss = 1.90 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:00.656361: step 83730, loss = 2.03 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:36:01.850711: step 83740, loss = 1.83 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:36:03.088120: step 83750, loss = 1.85 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:36:04.277526: step 83760, loss = 1.84 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:36:05.498200: step 83770, loss = 1.99 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:06.705711: step 83780, loss = 2.05 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:07.924459: step 83790, loss = 1.90 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:09.149331: step 83800, loss = 1.93 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:10.341438: step 83810, loss = 2.00 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:36:11.568729: step 83820, loss = 1.90 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:36:12.799644: step 83830, loss = 1.82 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:36:13.996609: step 83840, loss = 2.02 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:36:15.208263: step 83850, loss = 1.93 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:16.422096: step 83860, loss = 1.91 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:17.635653: step 83870, loss = 2.07 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:18.846845: step 83880, loss = 1.82 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:20.063974: step 83890, loss = 1.84 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:21.281570: step 83900, loss = 1.95 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:22.488695: step 83910, loss = 1.87 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:23.716994: step 83920, loss = 2.02 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:36:24.944716: step 83930, loss = 1.93 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:36:26.152605: step 83940, loss = 1.96 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:27.377165: step 83950, loss = 2.06 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:28.550891: step 83960, loss = 1.95 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:36:29.756611: step 83970, loss = 1.94 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:30.974500: step 83980, loss = 1.98 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:32.167700: step 83990, loss = 1.82 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:36:33.360935: step 84000, loss = 1.72 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:36:34.553292: step 84010, loss = 2.08 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:36:35.767542: step 84020, loss = 1.97 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:36.967072: step 84030, loss = 1.92 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:36:38.156300: step 84040, loss = 1.74 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:36:39.364219: step 84050, loss = 1.92 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:40.575636: step 84060, loss = 1.85 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:41.767712: step 84070, loss = 2.04 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:36:43.008721: step 84080, loss = 2.10 (1031.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:36:44.238004: step 84090, loss = 1.88 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:36:45.460848: step 84100, loss = 1.93 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:46.676454: step 84110, loss = 1.99 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:47.888444: step 84120, loss = 1.83 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:49.124374: step 84130, loss = 1.93 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:36:50.340768: step 84140, loss = 1.87 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:51.648992: step 84150, loss = 1.89 (978.4 examples/sec; 0.131 sec/batch)
2017-05-05 00:36:52.755754: step 84160, loss = 1.92 (1156.5 examples/sec; 0.111 sec/batch)
2017-05-05 00:36:53.972832: step 84170, loss = 1.98 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:55.197774: step 84180, loss = 1.92 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:56.427682: step 84190, loss = 1.89 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:36:57.633843: step 84200, loss = 1.96 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:58.857028: step 84210, loss = 2.04 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:37:00.074803: step 84220, loss = 2.06 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:37:01.282783: step 84230, loss = 2.14 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:02.495504: step 84240, loss = 1.96 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:03.718211: step 84250, loss = 1.90 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:37:04.919680: step 84260, loss = 2.08 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:37:06.140647: step 84270, loss = 1.90 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:37:07.355006: step 84280, loss = 1.98 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:08.569461: step 84290, loss = 2.01 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:09.782962: step 84300, loss = 1.94 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:11.018250: step 84310, loss = 1.88 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:37:12.241693: step 84320, loss = 1.87 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:37:13.461414: step 84330, loss = 1.70 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:37:14.678975: step 84340, loss = 2.05 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:37:15.859505: step 84350, loss = 1.93 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:37:17.057522: step 84360, loss = 1.82 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:37:18.272355: step 84370, loss = 2.09 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:19.490700: step 84380, loss = 1.88 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:37:20.700571: step 84390, loss = 1.97 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:21.907266: step 84400, loss = 2.08 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:23.133307: step 84410, loss = 1.94 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:37:24.346022: step 84420, loss = 1.90 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:25.556584: step 84430, loss = 2.22 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:26.756329: step 84440, loss = 1.89 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:37:27.982040: step 84450, loss = 1.85 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:37:29.220792: step 84460, loss = 1.99 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:37:30.425649: step 84470, loss = 1.89 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:37:31.660428: step 84480, loss = 1.99 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:37:32.878013: step 84490, loss = 1.92 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:37:34.078841: step 84500, loss = 1.85 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:37:35.307806: step 84510, loss = 2.13 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:37:36.536766: step 84520, loss = 2.12 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:37:37.755699: step 84530, loss = 1.83 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:37:38.973633: step 84540, loss = 1.97 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:37:40.146354: step 84550, loss = 1.94 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:37:41.344218: step 84560, loss = 1.98 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:37:42.546402: step 84570, loss = 1.86 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:37:43.731755: step 84580, loss = 1.93 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:37:44.938789: step 84590, loss = 2.03 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:46.129028: step 84600, loss = 2.08 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:37:47.320563: step 84610, loss = 1.90 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:37:48.482811: step 84620, loss = 2.03 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:37:49.646393: step 84630, loss = 1.96 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:37:50.809456: step 84640, loss = 1.83 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:37:51.962376: step 84650, loss = 1.89 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:37:53.131338: step 84660, loss = 1.85 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:37:54.282562: step 84670, loss = 1.86 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:37:55.453011: step 84680, loss = 1.99 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:37:56.617562: step 84690, loss = 1.86 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:37:57.791495: step 84700, loss = 2.02 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:37:58.971378: step 84710, loss = 2.17 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:00.157496: step 84720, loss = 1.91 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:38:01.324084: step 84730, loss = 2.00 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:02.487700: step 84740, loss = 2.04 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:38:03.666988: step 84750, loss = 1.83 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:04.819306: step 84760, loss = 1.92 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-05 00:38:05.988340: step 84770, loss = 2.04 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:07.157085: step 84780, loss = 1.89 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:08.310931: step 84790, loss = 1.86 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:38:09.488362: step 84800, loss = 2.07 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:10.669443: step 84810, loss = 1.87 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:11.834740: step 84820, loss = 2.17 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:13.020986: step 84830, loss = 1.96 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:38:14.176115: step 84840, loss = 2.12 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:38:15.368012: step 84850, loss = 1.90 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:38:16.547820: step 84860, loss = 1.92 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:17.716214: step 84870, loss = 1.77 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:18.900008: step 84880, loss = 1.92 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:20.076425: step 84890, loss = 1.87 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:21.253772: step 84900, loss = 1.94 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:22.419599: step 84910, loss = 1.86 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:23.603171: step 84920, loss = 1.83 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:24.794801: step 84930, loss = 1.85 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:38:25.962583: step 84940, loss = 1.90 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:27.173724: step 84950, loss = 1.88 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:38:28.327841: step 84960, loss = 1.81 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:38:29.479360: step 84970, loss = 1.92 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:38:30.645158: step 84980, loss = 1.80 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:31.812995: step 84990, loss = 2.09 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:32.964378: step 85000, loss = 1.87 (1111.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:38:34.151348: step 85010, loss = 1.91 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:38:35.332600: step 85020, loss = 1.97 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:36.492492: step 85030, loss = 1.90 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:38:37.673758: step 85040, loss = 1.83 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:38.866194: step 85050, loss = 1.89 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:38:40.029124: step 85060, loss = 1.87 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:38:41.225604: step 85070, loss = 1.89 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:38:42.397115: step 85080, loss = 2.15 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:43.555906: step 85090, loss = 1.85 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:38:44.742664: step 85100, loss = 2.01 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:38:45.917776: step 85110, loss = 1.75 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:47.088430: step 85120, loss = 1.86 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:48.285520: step 85130, loss = 1.81 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:38:49.548960: step 85140, loss = 1.92 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-05 00:38:50.630050: step 85150, loss = 1.89 (1184.0 examples/sec; 0.108 sec/batch)
2017-05-05 00:38:51.828123: step 85160, loss = 1.82 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:38:53.020460: step 85170, loss = 2.16 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:38:54.230593: step 85180, loss = 2.03 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:38:55.413163: step 85190, loss = 1.97 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:56.616487: step 85200, loss = 1.98 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:38:57.802880: step 85210, loss = 1.89 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:38:58.986220: step 85220, loss = 1.96 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:00.205919: step 85230, loss = 1.93 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:39:01.404770: step 85240, loss = 1.83 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:39:02.587522: step 85250, loss = 1.94 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:03.814729: step 85260, loss = 1.85 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:39:05.001292: step 85270, loss = 1.98 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:06.186075: step 85280, loss = 2.01 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:07.376883: step 85290, loss = 1.93 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:08.570445: step 85300, loss = 1.95 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:09.762479: step 85310, loss = 1.88 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:10.961147: step 85320, loss = 1.98 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:39:12.166283: step 85330, loss = 2.13 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:39:13.340906: step 85340, loss = 2.00 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:14.523561: step 85350, loss = 2.15 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:15.690401: step 85360, loss = 1.93 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:16.866368: step 85370, loss = 2.06 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:18.023011: step 85380, loss = 1.92 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:39:19.206951: step 85390, loss = 1.89 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:20.364094: step 85400, loss = 1.91 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:39:21.540004: step 85410, loss = 1.91 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:22.698048: step 85420, loss = 1.95 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:39:23.868968: step 85430, loss = 1.99 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:25.038708: step 85440, loss = 1.85 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:26.178061: step 85450, loss = 2.00 (1123.4 examples/sec; 0.114 sec/batch)
2017-05-05 00:39:27.344552: step 85460, loss = 1.74 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:28.516795: step 85470, loss = 1.82 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:29.673189: step 85480, loss = 1.84 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:39:30.828366: step 85490, loss = 1.96 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:39:32.008566: step 85500, loss = 1.98 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:33.157624: step 85510, loss = 1.93 (1114.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:39:34.318774: step 85520, loss = 1.90 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:39:35.497793: step 85530, loss = 1.98 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:36.665709: step 85540, loss = 2.01 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:37.832331: step 85550, loss = 1.98 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:38.998321: step 85560, loss = 1.88 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:40.169647: step 85570, loss = 2.18 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:41.314147: step 85580, loss = 1.87 (1118.4 examples/sec; 0.114 sec/batch)
2017-05-05 00:39:42.474305: step 85590, loss = 2.20 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:39:43.656452: step 85600, loss = 1.86 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:44.816881: step 85610, loss = 1.93 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:39:45.992150: step 85620, loss = 1.86 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:47.179746: step 85630, loss = 1.88 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:48.358261: step 85640, loss = 1.84 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:49.552360: step 85650, loss = 1.92 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:50.738387: step 85660, loss = 1.96 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:51.920130: step 85670, loss = 2.01 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:53.100487: step 85680, loss = 1.95 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:54.275571: step 85690, loss = 1.91 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:55.463535: step 85700, loss = 1.94 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:56.644695: step 85710, loss = 1.89 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:57.837468: step 85720, loss = 1.91 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:59.014857: step 85730, loss = 2.01 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:00.183091: step 85740, loss = 1.94 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:01.347419: step 85750, loss = 1.98 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:02.518444: step 85760, loss = 1.97 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:03.695308: step 85770, loss = 2.15 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:04.843927: step 85780, loss = 2.06 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:40:05.983179: step 85790, loss = 1.88 (1123.5 examples/sec; 0.114 sec/batch)
2017-05-05 00:40:07.152499: step 85800, loss = 1.98 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:08.328319: step 85810, loss = 1.91 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:09.490636: step 85820, loss = 1.81 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:10.669990: step 85830, loss = 2.06 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:11.822519: step 85840, loss = 1.97 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:40:13.009122: step 85850, loss = 1.81 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:40:14.160793: step 85860, loss = 1.98 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:40:15.324763: step 85870, loss = 1.84 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:16.510456: step 85880, loss = 1.86 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:40:17.655619: step 85890, loss = 1.92 (1117.8 examples/sec; 0.115 sec/batch)
2017-05-05 00:40:18.809112: step 85900, loss = 1.88 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:40:19.987203: step 85910, loss = 1.89 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:21.159204: step 85920, loss = 1.78 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:22.320976: step 85930, loss = 1.84 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:23.471912: step 85940, loss = 2.06 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:40:24.636273: step 85950, loss = 1.99 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:25.799986: step 85960, loss = 1.93 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:26.969563: step 85970, loss = 1.89 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:28.142572: step 85980, loss = 1.92 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:29.311400: step 85990, loss = 1.90 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:30.470003: step 86000, loss = 2.05 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:31.646647: step 86010, loss = 1.84 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:32.806606: step 86020, loss = 2.01 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:33.963437: step 86030, loss = 1.82 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:35.133764: step 86040, loss = 1.93 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:36.309604: step 86050, loss = 1.96 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:37.478888: step 86060, loss = 1.94 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:38.644327: step 86070, loss = 1.83 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:39.820843: step 86080, loss = 1.98 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:40.985057: step 86090, loss = 1.88 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:42.139278: step 86100, loss = 1.87 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:40:43.313516: step 86110, loss = 1.88 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:44.487433: step 86120, loss = 2.00 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:45.763952: step 86130, loss = 1.86 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-05 00:40:46.828911: step 86140, loss = 1.93 (1201.9 examples/sec; 0.106 sec/batch)
2017-05-05 00:40:48.001312: step 86150, loss = 1.98 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:49.178922: step 86160, loss = 1.96 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:50.361043: step 86170, loss = 1.79 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:51.553414: step 86180, loss = 1.97 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:40:52.746799: step 86190, loss = 2.18 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:40:53.924496: step 86200, loss = 1.81 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:55.107514: step 86210, loss = 1.94 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:56.319004: step 86220, loss = 2.07 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:40:57.512292: step 86230, loss = 1.84 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:40:58.690785: step 86240, loss = 1.98 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:59.872165: step 86250, loss = 1.91 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:01.049709: step 86260, loss = 1.93 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:02.218431: step 86270, loss = 1.97 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:03.399585: step 86280, loss = 2.07 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:04.558803: step 86290, loss = 2.03 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:05.704271: step 86300, loss = 1.86 (1117.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:41:06.854818: step 86310, loss = 1.93 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:41:08.040978: step 86320, loss = 1.84 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:41:09.205418: step 86330, loss = 1.95 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:10.335987: step 86340, loss = 1.81 (1132.2 examples/sec; 0.113 sec/batch)
2017-05-05 00:41:11.507067: step 86350, loss = 1.80 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:12.681641: step 86360, loss = 1.82 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:13.835301: step 86370, loss = 2.06 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:41:15.026624: step 86380, loss = 1.79 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:41:16.190830: step 86390, loss = 1.94 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:17.364646: step 86400, loss = 1.87 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:18.530702: step 86410, loss = 1.91 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:19.708831: step 86420, loss = 2.10 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:20.879677: step 86430, loss = 2.12 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:22.043400: step 86440, loss = 2.08 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:23.226606: step 86450, loss = 1.96 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:24.403502: step 86460, loss = 2.00 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:25.581387: step 86470, loss = 1.93 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:26.756340: step 86480, loss = 1.89 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:27.900412: step 86490, loss = 2.01 (1118.8 examples/sec; 0.114 sec/batch)
2017-05-05 00:41:29.073387: step 86500, loss = 2.00 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:30.229891: step 86510, loss = 1.85 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:31.398481: step 86520, loss = 1.84 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:32.570809: step 86530, loss = 1.91 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:33.725616: step 86540, loss = 1.95 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:41:34.905498: step 86550, loss = 1.97 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:36.080526: step 86560, loss = 1.81 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:37.255903: step 86570, loss = 1.96 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:38.423499: step 86580, loss = 1.93 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:39.608374: step 86590, loss = 1.91 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:40.793904: step 86600, loss = 1.84 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:41:41.955963: step 86610, loss = 1.89 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:43.128001: step 86620, loss = 1.88 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:44.274139: step 86630, loss = 1.93 (1116.8 examples/sec; 0.115 sec/batch)
2017-05-05 00:41:45.434988: step 86640, loss = 2.04 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:46.611130: step 86650, loss = 2.02 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:47.768600: step 86660, loss = 1.88 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:48.938317: step 86670, loss = 1.99 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:50.093115: step 86680, loss = 1.92 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:41:51.267918: step 86690, loss = 1.92 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:52.432747: step 86700, loss = 1.96 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:53.609586: step 86710, loss = 1.95 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:54.768666: step 86720, loss = 1.84 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:55.935876: step 86730, loss = 2.02 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:57.083825: step 86740, loss = 1.93 (1115.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:41:58.244806: step 86750, loss = 1.95 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:59.400796: step 86760, loss = 1.95 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:00.563753: step 86770, loss = 1.95 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:01.735901: step 86780, loss = 1.94 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:02.903037: step 86790, loss = 1.88 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:04.077745: step 86800, loss = 1.88 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:05.255625: step 86810, loss = 2.04 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:06.417175: step 86820, loss = 2.00 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:07.579957: step 86830, loss = 1.89 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:08.737480: step 86840, loss = 1.84 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:09.895065: step 86850, loss = 1.94 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:11.063331: step 86860, loss = 1.95 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:12.228051: step 86870, loss = 1.78 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:13.390840: step 86880, loss = 1.88 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:14.550124: step 86890, loss = 2.02 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:15.720473: step 86900, loss = 1.97 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:16.878016: step 86910, loss = 2.00 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:18.037029: step 86920, loss = 1.90 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:19.225518: step 86930, loss = 1.94 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:42:20.390617: step 86940, loss = 1.84 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:21.578878: step 86950, loss = 2.10 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:42:22.737859: step 86960, loss = 1.87 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:23.906525: step 86970, loss = 2.13 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:25.078817: step 86980, loss = 1.95 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:26.243408: step 86990, loss = 1.90 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:27.400224: step 87000, loss = 2.03 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:28.568735: step 87010, loss = 1.82 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:29.753531: step 87020, loss = 1.73 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:30.908004: step 87030, loss = 1.88 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:42:32.074089: step 87040, loss = 2.06 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:33.246937: step 87050, loss = 1.99 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:34.422180: step 87060, loss = 2.10 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:35.587798: step 87070, loss = 2.04 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:36.761100: step 87080, loss = 1.88 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:37.915514: step 87090, loss = 1.93 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 00:42:39.076190: step 87100, loss = 1.89 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:40.240940: step 87110, loss = 1.91 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:41.490751: step 87120, loss = 2.00 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-05 00:42:42.580035: step 87130, loss = 2.04 (1175.1 examples/sec; 0.109 sec/batch)
2017-05-05 00:42:43.757351: step 87140, loss = 1.85 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:44.928167: step 87150, loss = 1.93 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:46.096325: step 87160, loss = 1.98 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:47.266059: step 87170, loss = 1.99 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:48.436363: step 87180, loss = 2.00 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:49.608779: step 87190, loss = 2.02 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:50.792724: step 87200, loss = 2.05 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:51.969985: step 87210, loss = 1.93 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:53.119438: step 87220, loss = 2.06 (1113.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:42:54.287074: step 87230, loss = 1.85 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:55.460869: step 87240, loss = 2.00 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:56.633627: step 87250, loss = 1.78 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:57.779629: step 87260, loss = 2.05 (1116.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:42:58.952830: step 87270, loss = 1.94 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:00.130537: step 87280, loss = 2.03 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:01.297558: step 87290, loss = 1.86 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:02.479497: step 87300, loss = 1.99 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:03.655509: step 87310, loss = 1.95 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:04.831340: step 87320, loss = 1.96 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:06.003117: step 87330, loss = 2.00 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:07.176673: step 87340, loss = 1.88 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:08.352714: step 87350, loss = 2.00 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:09.544925: step 87360, loss = 1.84 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:43:10.703944: step 87370, loss = 1.98 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:11.870534: step 87380, loss = 2.10 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:13.044022: step 87390, loss = 2.08 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:14.227916: step 87400, loss = 2.01 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:15.397053: step 87410, loss = 1.84 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:16.585448: step 87420, loss = 1.86 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:43:17.765502: step 87430, loss = 1.94 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:18.947339: step 87440, loss = 1.95 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:20.107855: step 87450, loss = 1.83 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:21.287656: step 87460, loss = 1.84 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:22.447814: step 87470, loss = 1.92 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:23.622196: step 87480, loss = 2.02 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:24.775881: step 87490, loss = 1.85 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:43:25.950725: step 87500, loss = 1.88 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:27.122433: step 87510, loss = 2.12 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:28.302033: step 87520, loss = 1.86 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:29.471870: step 87530, loss = 1.98 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:30.633582: step 87540, loss = 1.97 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:31.818758: step 87550, loss = 2.00 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:43:32.964042: step 87560, loss = 1.86 (1117.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:43:34.113440: step 87570, loss = 1.84 (1113.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:43:35.276624: step 87580, loss = 1.92 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:36.453929: step 87590, loss = 1.93 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:37.611764: step 87600, loss = 1.78 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:38.798657: step 87610, loss = 2.01 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:43:39.953482: step 87620, loss = 1.91 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:43:41.117658: step 87630, loss = 2.01 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:42.274829: step 87640, loss = 1.91 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:43.452464: step 87650, loss = 1.79 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:44.619952: step 87660, loss = 2.01 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:45.769023: step 87670, loss = 1.77 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:43:46.937792: step 87680, loss = 1.99 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:48.099593: step 87690, loss = 1.94 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:49.279974: step 87700, loss = 1.77 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:50.442977: step 87710, loss = 1.97 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:51.610088: step 87720, loss = 1.93 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:52.769379: step 87730, loss = 2.14 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:53.936005: step 87740, loss = 1.87 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:55.112747: step 87750, loss = 1.96 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:56.261651: step 87760, loss = 1.94 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:43:57.430081: step 87770, loss = 1.89 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:58.583773: step 87780, loss = 2.04 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:43:59.752273: step 87790, loss = 1.93 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:00.931887: step 87800, loss = 1.71 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:44:02.093992: step 87810, loss = 1.90 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:03.266417: step 87820, loss = 1.93 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:04.422331: step 87830, loss = 1.92 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:05.587941: step 87840, loss = 1.96 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:06.747334: step 87850, loss = 2.05 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:07.916499: step 87860, loss = 1.80 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:09.093550: step 87870, loss = 1.89 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:44:10.266131: step 87880, loss = 1.94 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:11.431087: step 87890, loss = 2.00 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:12.607491: step 87900, loss = 1.99 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:44:13.763109: step 87910, loss = 2.05 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:14.930460: step 87920, loss = 1.86 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:16.092409: step 87930, loss = 2.00 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:17.254382: step 87940, loss = 1.78 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:18.406498: step 87950, loss = 1.87 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:44:19.574736: step 87960, loss = 2.06 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:20.732922: step 87970, loss = 1.85 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:21.895351: step 87980, loss = 1.89 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:23.064148: step 87990, loss = 1.83 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:24.221145: step 88000, loss = 1.93 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:25.380328: step 88010, loss = 2.06 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:26.547657: step 88020, loss = 1.82 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:27.735184: step 88030, loss = 2.05 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:44:28.920613: step 88040, loss = 2.04 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:44:30.074608: step 88050, loss = 2.01 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:44:31.254666: step 88060, loss = 2.02 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:44:32.421727: step 88070, loss = 2.03 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:33.585465: step 88080, loss = 1.86 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:34.755700: step 88090, loss = 1.83 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:35.935461: step 88100, loss = 2.11 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:44:37.199103: step 88110, loss = 1.96 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-05 00:44:38.278747: step 88120, loss = 2.12 (1185.6 examples/sec; 0.108 sec/batch)
2017-05-05 00:44:39.467418: step 88130, loss = 1.99 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:44:40.665718: step 88140, loss = 2.05 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:44:41.862024: step 88150, loss = 1.93 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:44:43.061700: step 88160, loss = 2.07 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:44:44.268843: step 88170, loss = 1.90 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:44:45.435652: step 88180, loss = 1.92 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:46.620901: step 88190, loss = 1.95 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:44:47.833688: step 88200, loss = 1.74 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:44:49.038157: step 88210, loss = 1.80 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:44:50.226539: step 88220, loss = 2.07 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:44:51.413698: step 88230, loss = 2.08 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:44:52.604063: step 88240, loss = 1.98 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:44:53.775618: step 88250, loss = 1.92 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:54.983973: step 88260, loss = 1.83 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:44:56.181302: step 88270, loss = 1.95 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:44:57.357615: step 88280, loss = 1.80 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:44:58.512923: step 88290, loss = 1.96 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:59.703006: step 88300, loss = 1.91 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:45:00.898772: step 88310, loss = 1.83 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:45:02.076624: step 88320, loss = 1.90 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:03.291439: step 88330, loss = 1.96 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:45:04.485540: step 88340, loss = 1.84 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:45:05.664361: step 88350, loss = 1.99 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:06.858014: step 88360, loss = 2.02 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:45:08.039886: step 88370, loss = 1.92 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:09.218210: step 88380, loss = 1.93 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:10.395897: step 88390, loss = 2.04 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:11.562150: step 88400, loss = 1.95 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:12.739698: step 88410, loss = 2.04 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:13.903952: step 88420, loss = 1.97 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:15.083445: step 88430, loss = 1.79 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:16.254132: step 88440, loss = 1.98 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:17.427440: step 88450, loss = 1.99 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:18.603272: step 88460, loss = 1.90 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:19.795081: step 88470, loss = 2.01 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:45:20.972096: step 88480, loss = 2.05 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:22.130567: step 88490, loss = 1.92 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:23.287867: step 88500, loss = 1.82 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:24.451001: step 88510, loss = 1.81 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:25.608679: step 88520, loss = 1.93 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:26.783813: step 88530, loss = 1.93 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:27.974108: step 88540, loss = 1.84 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:45:29.130868: step 88550, loss = 1.97 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:30.288420: step 88560, loss = 1.98 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:31.458047: step 88570, loss = 1.98 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:32.626464: step 88580, loss = 1.91 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:33.801516: step 88590, loss = 2.01 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:34.973367: step 88600, loss = 2.03 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:36.174820: step 88610, loss = 1.87 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:45:37.346090: step 88620, loss = 1.91 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:38.528796: step 88630, loss = 1.85 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:39.710854: step 88640, loss = 1.84 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:40.885739: step 88650, loss = 1.83 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:42.055454: step 88660, loss = 1.85 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:43.232681: step 88670, loss = 1.96 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:44.409647: step 88680, loss = 1.79 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:45.569802: step 88690, loss = 1.94 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:46.730810: step 88700, loss = 2.09 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:47.905794: step 88710, loss = 1.85 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:49.071358: step 88720, loss = 1.92 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:50.216391: step 88730, loss = 1.89 (1117.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:45:51.388003: step 88740, loss = 2.02 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:52.580667: step 88750, loss = 2.15 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:45:53.723059: step 88760, loss = 2.02 (1120.5 examples/sec; 0.114 sec/batch)
2017-05-05 00:45:54.872950: step 88770, loss = 2.00 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:45:56.035302: step 88780, loss = 1.90 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:57.211608: step 88790, loss = 2.05 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:58.368527: step 88800, loss = 1.93 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:59.554904: step 88810, loss = 1.94 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:46:00.724309: step 88820, loss = 1.92 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:01.881490: step 88830, loss = 2.06 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:03.037793: step 88840, loss = 2.00 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:04.206027: step 88850, loss = 1.87 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:05.372970: step 88860, loss = 1.89 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:06.539003: step 88870, loss = 2.14 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:07.700408: step 88880, loss = 1.86 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:08.890919: step 88890, loss = 1.90 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:46:10.064497: step 88900, loss = 2.04 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:11.236466: step 88910, loss = 1.94 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:12.405404: step 88920, loss = 1.88 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:13.572839: step 88930, loss = 2.05 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:14.739972: step 88940, loss = 1.79 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:15.903852: step 88950, loss = 1.73 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:17.072164: step 88960, loss = 2.05 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:18.216043: step 88970, loss = 2.07 (1119.0 examples/sec; 0.114 sec/batch)
2017-05-05 00:46:19.385918: step 88980, loss = 1.89 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:20.545984: step 88990, loss = 1.97 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:21.705665: step 89000, loss = 2.05 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:22.882682: step 89010, loss = 2.02 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:46:24.045293: step 89020, loss = 1.91 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:25.232943: step 89030, loss = 2.00 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:46:26.402308: step 89040, loss = 1.89 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:27.590293: step 89050, loss = 1.91 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:46:28.773850: step 89060, loss = 1.90 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:46:29.969548: step 89070, loss = 1.94 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:46:31.161924: step 89080, loss = 1.95 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:46:32.356351: step 89090, loss = 1.94 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:46:33.633178: step 89100, loss = 1.92 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-05 00:46:34.746015: step 89110, loss = 2.04 (1150.2 examples/sec; 0.111 sec/batch)
2017-05-05 00:46:35.948960: step 89120, loss = 2.09 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:46:37.150814: step 89130, loss = 2.03 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:46:38.354315: step 89140, loss = 1.87 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:46:39.535079: step 89150, loss = 1.97 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:46:40.707603: step 89160, loss = 2.11 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:41.876454: step 89170, loss = 1.97 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:43.050690: step 89180, loss = 1.84 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:44.249607: step 89190, loss = 1.98 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:46:45.434801: step 89200, loss = 2.04 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:46:46.611677: step 89210, loss = 1.93 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:46:47.780047: step 89220, loss = 1.81 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:48.963670: step 89230, loss = 1.93 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:46:50.128493: step 89240, loss = 1.97 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:51.295488: step 89250, loss = 1.84 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:52.466205: step 89260, loss = 1.88 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:53.636392: step 89270, loss = 2.08 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:54.822557: step 89280, loss = 1.97 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:46:55.999104: step 89290, loss = 1.80 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:46:57.194274: step 89300, loss = 2.07 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:46:58.366313: step 89310, loss = 2.01 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:59.547191: step 89320, loss = 2.03 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:47:00.747802: step 89330, loss = 1.95 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:47:01.907509: step 89340, loss = 1.95 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:03.065659: step 89350, loss = 1.89 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:04.235970: step 89360, loss = 1.94 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:05.395050: step 89370, loss = 1.90 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:06.564314: step 89380, loss = 2.06 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:07.712455: step 89390, loss = 1.94 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:47:08.884577: step 89400, loss = 1.82 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:10.046384: step 89410, loss = 1.92 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:11.204811: step 89420, loss = 2.01 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:12.364440: step 89430, loss = 1.91 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:13.512726: step 89440, loss = 1.89 (1114.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:47:14.663478: step 89450, loss = 2.04 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:47:15.829023: step 89460, loss = 1.98 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:16.989316: step 89470, loss = 2.07 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:18.134510: step 89480, loss = 2.00 (1117.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:47:19.298560: step 89490, loss = 1.91 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:20.470558: step 89500, loss = 1.84 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:21.627002: step 89510, loss = 2.03 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:22.800814: step 89520, loss = 1.84 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:23.962216: step 89530, loss = 1.95 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:25.160953: step 89540, loss = 1.91 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:47:26.310579: step 89550, loss = 2.11 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:47:27.474510: step 89560, loss = 1.86 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:28.634221: step 89570, loss = 1.89 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:29.814768: step 89580, loss = 1.87 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:47:30.971466: step 89590, loss = 1.91 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:32.123304: step 89600, loss = 1.92 (1111.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:47:33.309088: step 89610, loss = 1.72 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:47:34.467399: step 89620, loss = 1.93 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:35.650323: step 89630, loss = 1.95 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:47:36.842747: step 89640, loss = 2.04 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:47:37.996968: step 89650, loss = 1.83 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:47:39.151982: step 89660, loss = 1.81 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:40.310643: step 89670, loss = 1.99 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:41.484177: step 89680, loss = 2.02 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:42.638806: step 89690, loss = 1.72 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:47:43.814535: step 89700, loss = 1.94 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:47:44.967217: step 89710, loss = 1.83 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:47:46.110686: step 89720, loss = 2.02 (1119.4 examples/sec; 0.114 sec/batch)
2017-05-05 00:47:47.287113: step 89730, loss = 2.13 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:47:48.466920: step 89740, loss = 1.91 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:47:49.648145: step 89750, loss = 1.94 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:47:50.816813: step 89760, loss = 2.08 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:51.980291: step 89770, loss = 1.89 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:53.146981: step 89780, loss = 1.94 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:54.315726: step 89790, loss = 1.96 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:55.493000: step 89800, loss = 2.10 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:47:56.655383: step 89810, loss = 2.07 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:57.813693: step 89820, loss = 2.00 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:58.987772: step 89830, loss = 1.85 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:00.154818: step 89840, loss = 1.86 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:01.321582: step 89850, loss = 2.02 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:02.489198: step 89860, loss = 2.00 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:03.668973: step 89870, loss = 1.92 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:04.855090: step 89880, loss = 2.02 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:48:06.033484: step 89890, loss = 1.98 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:07.204405: step 89900, loss = 1.91 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:08.372191: step 89910, loss = 1.82 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:09.536413: step 89920, loss = 1.94 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:48:10.708199: step 89930, loss = 2.00 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:11.882445: step 89940, loss = 1.88 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:13.065380: step 89950, loss = 1.91 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:14.244416: step 89960, loss = 1.99 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:15.420318: step 89970, loss = 2.15 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:16.577675: step 89980, loss = 2.03 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:48:17.758937: step 89990, loss = 1.98 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:18.962522: step 90000, loss = 1.89 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:48:20.142506: step 90010, loss = 1.90 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:21.322759: step 90020, loss = 1.92 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:22.518859: step 90030, loss = 1.86 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:48:23.703700: step 90040, loss = 1.79 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:24.912607: step 90050, loss = 1.90 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:48:26.113283: step 90060, loss = 2.07 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:48:27.309601: step 90070, loss = 1.88 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:48:28.495621: step 90080, loss = 1.84 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:48:29.782735: step 90090, loss = 1.80 (994.5 examples/sec; 0.129 sec/batch)
2017-05-05 00:48:30.882484: step 90100, loss = 1.94 (1163.9 examples/sec; 0.110 sec/batch)
2017-05-05 00:48:32.082120: step 90110, loss = 1.92 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:48:33.264129: step 90120, loss = 1.83 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:34.448057: step 90130, loss = 2.01 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:35.617884: step 90140, loss = 1.87 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:36.781658: step 90150, loss = 1.92 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:48:37.946414: step 90160, loss = 2.03 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:48:39.113319: step 90170, loss = 1.97 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:40.268294: step 90180, loss = 1.87 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:48:41.424760: step 90190, loss = 1.83 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:48:42.567943: step 90200, loss = 1.92 (1119.7 examples/sec; 0.114 sec/batch)
2017-05-05 00:48:43.729410: step 90210, loss = 2.01 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:48:44.928523: step 90220, loss = 2.05 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:48:46.084325: step 90230, loss = 1.97 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:48:47.278271: step 90240, loss = 1.95 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:48:48.459505: step 90250, loss = 2.00 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:49.630266: step 90260, loss = 1.94 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:50.801144: step 90270, loss = 2.01 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:51.964558: step 90280, loss = 1.84 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:48:53.143268: step 90290, loss = 2.00 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:54.368649: step 90300, loss = 1.94 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:48:55.571802: step 90310, loss = 1.88 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:48:56.774594: step 90320, loss = 1.88 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:48:57.959302: step 90330, loss = 1.98 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:59.172479: step 90340, loss = 1.92 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:00.397994: step 90350, loss = 2.02 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:01.609693: step 90360, loss = 1.81 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:02.854732: step 90370, loss = 1.89 (1028.1 examples/sec; 0.125 sec/batch)
2017-05-05 00:49:04.077628: step 90380, loss = 1.80 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:05.291493: step 90390, loss = 1.89 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:06.506359: step 90400, loss = 1.94 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:07.712910: step 90410, loss = 1.85 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:08.960661: step 90420, loss = 1.99 (1025.8 examples/sec; 0.125 sec/batch)
2017-05-05 00:49:10.164167: step 90430, loss = 1.83 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:49:11.378680: step 90440, loss = 2.04 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:12.620976: step 90450, loss = 2.05 (1030.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:49:13.818138: step 90460, loss = 1.76 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:49:15.042922: step 90470, loss = 1.85 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:16.253825: step 90480, loss = 1.91 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:17.476697: step 90490, loss = 2.05 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:18.692374: step 90500, loss = 1.94 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:19.900516: step 90510, loss = 1.85 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:21.123058: step 90520, loss = 1.93 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:22.331647: step 90530, loss = 1.97 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:23.551451: step 90540, loss = 1.93 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:24.772720: step 90550, loss = 1.94 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:25.965161: step 90560, loss = 1.89 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:49:27.195556: step 90570, loss = 1.94 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:28.406985: step 90580, loss = 1.89 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:29.626372: step 90590, loss = 2.15 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:30.853909: step 90600, loss = 1.98 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:32.085290: step 90610, loss = 1.98 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:33.302453: step 90620, loss = 1.91 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:34.508909: step 90630, loss = 1.93 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:35.723704: step 90640, loss = 1.83 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:36.930485: step 90650, loss = 1.86 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:38.127500: step 90660, loss = 1.89 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:49:39.354200: step 90670, loss = 1.91 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:40.567538: step 90680, loss = 1.94 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:41.765446: step 90690, loss = 2.06 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:49:42.981386: step 90700, loss = 1.90 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:44.201405: step 90710, loss = 1.99 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:45.420021: step 90720, loss = 2.03 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:46.625919: step 90730, loss = 1.88 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:47.868280: step 90740, loss = 2.24 (1030.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:49:49.087882: step 90750, loss = 1.70 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:50.296985: step 90760, loss = 1.92 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:51.512288: step 90770, loss = 1.95 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:52.741925: step 90780, loss = 1.94 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:53.951594: step 90790, loss = 1.94 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:55.186277: step 90800, loss = 1.74 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:56.411513: step 90810, loss = 2.02 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:57.628212: step 90820, loss = 1.82 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:58.834218: step 90830, loss = 1.98 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:00.052129: step 90840, loss = 1.90 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:01.265494: step 90850, loss = 1.94 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:02.479072: step 90860, loss = 1.90 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:03.691729: step 90870, loss = 2.00 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:04.923966: step 90880, loss = 1.86 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:50:06.101891: step 90890, loss = 1.88 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:50:07.315458: step 90900, loss = 1.77 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:08.536544: step 90910, loss = 1.79 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:09.736336: step 90920, loss = 1.92 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:50:10.960889: step 90930, loss = 1.92 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:12.193205: step 90940, loss = 1.84 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:50:13.412804: step 90950, loss = 1.92 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:14.609604: step 90960, loss = 1.85 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:50:15.835724: step 90970, loss = 2.05 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:50:17.061222: step 90980, loss = 1.93 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:50:18.270893: step 90990, loss = 1.85 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:19.492223: step 91000, loss = 1.88 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:20.698835: step 91010, loss = 1.81 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:21.900660: step 91020, loss = 1.99 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:50:23.103437: step 91030, loss = 1.82 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:50:24.320485: step 91040, loss = 1.83 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:25.521113: step 91050, loss = 1.82 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:50:26.745217: step 91060, loss = 1.99 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:27.953537: step 91070, loss = 2.09 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:29.297821: step 91080, loss = 1.87 (952.2 examples/sec; 0.134 sec/batch)
2017-05-05 00:50:30.361379: step 91090, loss = 2.00 (1203.5 examples/sec; 0.106 sec/batch)
2017-05-05 00:50:31.570122: step 91100, loss = 2.05 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:32.764571: step 91110, loss = 1.88 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:50:33.956082: step 91120, loss = 2.00 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:50:35.153929: step 91130, loss = 1.86 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:50:36.367752: step 91140, loss = 1.97 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:37.586659: step 91150, loss = 1.84 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:38.803580: step 91160, loss = 2.02 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:40.025804: step 91170, loss = 1.91 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:41.257416: step 91180, loss = 1.98 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:50:42.465695: step 91190, loss = 1.87 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:43.688766: step 91200, loss = 1.85 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:44.903713: step 91210, loss = 1.91 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:46.127648: step 91220, loss = 2.07 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:47.340527: step 91230, loss = 1.87 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:48.558618: step 91240, loss = 1.83 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:49.756457: step 91250, loss = 2.06 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:50:50.983336: step 91260, loss = 2.00 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:50:52.193680: step 91270, loss = 1.74 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:53.399308: step 91280, loss = 1.99 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:54.616759: step 91290, loss = 1.99 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:55.844047: step 91300, loss = 1.96 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:50:57.069411: step 91310, loss = 1.94 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:50:58.279153: step 91320, loss = 1.94 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:59.486045: step 91330, loss = 1.84 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:00.711613: step 91340, loss = 1.76 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:51:01.918747: step 91350, loss = 1.85 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:03.109836: step 91360, loss = 1.91 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:51:04.319663: step 91370, loss = 2.02 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:05.531193: step 91380, loss = 1.86 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:06.757547: step 91390, loss = 1.88 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:51:07.984653: step 91400, loss = 1.85 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:51:09.229309: step 91410, loss = 1.79 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:51:10.419040: step 91420, loss = 1.87 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:51:11.636160: step 91430, loss = 2.01 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:12.887361: step 91440, loss = 1.91 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-05 00:51:14.098634: step 91450, loss = 2.01 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:15.319177: step 91460, loss = 1.86 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:16.539698: step 91470, loss = 1.98 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:17.755174: step 91480, loss = 1.85 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:18.987193: step 91490, loss = 1.81 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:51:20.201378: step 91500, loss = 1.85 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:21.415188: step 91510, loss = 1.83 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:22.609539: step 91520, loss = 2.00 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:51:23.829996: step 91530, loss = 1.83 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:25.055819: step 91540, loss = 1.80 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:51:26.295655: step 91550, loss = 1.94 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:51:27.531932: step 91560, loss = 1.90 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:51:28.737543: step 91570, loss = 2.07 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:29.945193: step 91580, loss = 1.92 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:31.204207: step 91590, loss = 1.84 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-05 00:51:32.413061: step 91600, loss = 1.89 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:33.627202: step 91610, loss = 1.94 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:34.849896: step 91620, loss = 1.90 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:36.051738: step 91630, loss = 1.88 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:51:37.276285: step 91640, loss = 1.86 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:38.477842: step 91650, loss = 2.00 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:51:39.707703: step 91660, loss = 1.90 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:51:40.920147: step 91670, loss = 2.11 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:42.091740: step 91680, loss = 1.99 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:51:43.313447: step 91690, loss = 2.01 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:44.544706: step 91700, loss = 1.91 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:51:45.764200: step 91710, loss = 1.81 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:46.963784: step 91720, loss = 2.00 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:51:48.189977: step 91730, loss = 1.78 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:51:49.416506: step 91740, loss = 1.95 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:51:50.622825: step 91750, loss = 1.82 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:51.845410: step 91760, loss = 1.97 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:53.055879: step 91770, loss = 1.92 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:54.250285: step 91780, loss = 1.87 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:51:55.464449: step 91790, loss = 1.84 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:56.662469: step 91800, loss = 2.00 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:51:57.859133: step 91810, loss = 1.91 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:51:59.079540: step 91820, loss = 2.02 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:00.286570: step 91830, loss = 2.11 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:01.483069: step 91840, loss = 2.01 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:52:02.680371: step 91850, loss = 1.85 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:52:03.884524: step 91860, loss = 1.93 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:52:05.074976: step 91870, loss = 1.74 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:52:06.285899: step 91880, loss = 1.93 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:07.517742: step 91890, loss = 1.99 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:52:08.742187: step 91900, loss = 2.06 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:09.943468: step 91910, loss = 2.01 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:52:11.154074: step 91920, loss = 1.84 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:12.379414: step 91930, loss = 1.92 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:52:13.592611: step 91940, loss = 1.99 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:14.798999: step 91950, loss = 1.88 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:16.022896: step 91960, loss = 1.90 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:17.255933: step 91970, loss = 1.91 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:52:18.471304: step 91980, loss = 1.91 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:19.683123: step 91990, loss = 1.88 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:20.910059: step 92000, loss = 1.81 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:52:22.124657: step 92010, loss = 2.15 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:23.344412: step 92020, loss = 2.03 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:24.568473: step 92030, loss = 2.09 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:25.776669: step 92040, loss = 1.96 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:26.987380: step 92050, loss = 2.04 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:28.216367: step 92060, loss = 1.98 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:52:29.510851: step 92070, loss = 1.99 (988.8 examples/sec; 0.129 sec/batch)
2017-05-05 00:52:30.614365: step 92080, loss = 1.99 (1159.9 examples/sec; 0.110 sec/batch)
2017-05-05 00:52:31.840677: step 92090, loss = 1.92 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:52:33.071035: step 92100, loss = 1.98 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:52:34.270801: step 92110, loss = 1.90 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:52:35.503128: step 92120, loss = 2.02 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:52:36.718265: step 92130, loss = 1.91 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:37.935338: step 92140, loss = 1.86 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:39.154478: step 92150, loss = 1.79 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:40.363308: step 92160, loss = 1.98 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:41.561633: step 92170, loss = 1.82 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:52:42.797698: step 92180, loss = 1.91 (1035.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:52:44.012802: step 92190, loss = 1.85 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:45.221387: step 92200, loss = 1.91 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:46.416104: step 92210, loss = 1.95 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:52:47.634017: step 92220, loss = 2.02 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:48.856149: step 92230, loss = 1.93 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:50.074347: step 92240, loss = 1.91 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:51.288297: step 92250, loss = 1.90 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:52.499698: step 92260, loss = 1.71 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:53.675611: step 92270, loss = 1.89 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:52:54.903428: step 92280, loss = 1.96 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:52:56.134827: step 92290, loss = 1.94 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:52:57.360481: step 92300, loss = 1.90 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:52:58.563885: step 92310, loss = 2.01 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:52:59.772530: step 92320, loss = 1.90 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:01.010186: step 92330, loss = 2.00 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:53:02.212689: step 92340, loss = 1.83 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:53:03.438278: step 92350, loss = 2.03 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:53:04.658372: step 92360, loss = 1.83 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:05.870658: step 92370, loss = 1.85 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:07.077517: step 92380, loss = 2.00 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:08.277634: step 92390, loss = 1.90 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:53:09.502092: step 92400, loss = 1.98 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:10.713812: step 92410, loss = 1.89 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:11.944821: step 92420, loss = 2.18 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:53:13.163637: step 92430, loss = 1.95 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:14.364391: step 92440, loss = 1.80 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:53:15.574864: step 92450, loss = 1.87 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:16.795124: step 92460, loss = 1.81 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:17.966687: step 92470, loss = 1.82 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:53:19.194134: step 92480, loss = 2.11 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:53:20.425166: step 92490, loss = 1.82 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:53:21.632028: step 92500, loss = 1.95 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:22.855655: step 92510, loss = 1.92 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:24.062022: step 92520, loss = 1.88 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:25.281658: step 92530, loss = 1.85 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:26.491945: step 92540, loss = 2.00 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:27.695170: step 92550, loss = 1.95 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:53:28.927635: step 92560, loss = 1.91 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:53:30.150846: step 92570, loss = 2.05 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:31.361120: step 92580, loss = 1.96 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:32.573477: step 92590, loss = 1.86 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:33.774943: step 92600, loss = 1.99 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:53:34.991774: step 92610, loss = 1.87 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:36.211398: step 92620, loss = 1.82 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:37.415421: step 92630, loss = 1.88 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:53:38.638973: step 92640, loss = 1.97 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:39.854394: step 92650, loss = 1.99 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:41.066717: step 92660, loss = 1.99 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:42.258800: step 92670, loss = 1.92 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:53:43.487462: step 92680, loss = 1.80 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:53:44.712961: step 92690, loss = 1.98 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:53:45.923886: step 92700, loss = 1.84 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:47.154042: step 92710, loss = 1.89 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:53:48.370817: step 92720, loss = 2.04 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:49.587814: step 92730, loss = 1.88 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:50.804923: step 92740, loss = 1.81 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:52.010013: step 92750, loss = 1.87 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:53.239653: step 92760, loss = 2.00 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:53:54.454390: step 92770, loss = 1.87 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:55.664198: step 92780, loss = 1.91 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:56.887144: step 92790, loss = 1.90 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:58.099209: step 92800, loss = 1.90 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:59.330187: step 92810, loss = 1.93 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:54:00.520849: step 92820, loss = 1.79 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:54:01.752131: step 92830, loss = 1.91 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:54:02.983735: step 92840, loss = 1.86 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:54:04.213403: step 92850, loss = 1.86 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:54:05.443447: step 92860, loss = 1.99 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:54:06.615330: step 92870, loss = 2.02 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:54:07.843924: step 92880, loss = 2.00 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:54:09.065017: step 92890, loss = 1.92 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:10.279164: step 92900, loss = 1.93 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:54:11.503000: step 92910, loss = 1.78 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:12.728373: step 92920, loss = 1.80 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:54:13.930487: step 92930, loss = 2.14 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:54:15.147038: step 92940, loss = 1.92 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:16.365443: step 92950, loss = 1.87 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:17.579571: step 92960, loss = 1.83 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:54:18.808321: step 92970, loss = 1.96 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:54:20.035463: step 92980, loss = 1.88 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:54:21.252458: step 92990, loss = 1.92 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:22.468815: step 93000, loss = 2.05 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:23.699100: step 93010, loss = 1.96 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:54:24.921637: step 93020, loss = 1.84 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:26.125919: step 93030, loss = 1.88 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:54:27.342401: step 93040, loss = 1.90 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:28.553225: step 93050, loss = 1.85 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:54:29.841696: step 93060, loss = 1.89 (993.4 examples/sec; 0.129 sec/batch)
2017-05-05 00:54:30.936199: step 93070, loss = 2.13 (1169.5 examples/sec; 0.109 sec/batch)
2017-05-05 00:54:32.149993: step 93080, loss = 1.95 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:54:33.341776: step 93090, loss = 1.83 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:54:34.538524: step 93100, loss = 1.89 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:54:35.730164: step 93110, loss = 1.96 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:54:36.914206: step 93120, loss = 1.99 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:54:38.114930: step 93130, loss = 2.03 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:54:39.306199: step 93140, loss = 1.87 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:54:40.491035: step 93150, loss = 1.99 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:54:41.666521: step 93160, loss = 2.02 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:54:42.834866: step 93170, loss = 1.77 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:54:44.027063: step 93180, loss = 1.96 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:54:45.202590: step 93190, loss = 2.02 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:54:46.385306: step 93200, loss = 2.06 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:54:47.566951: step 93210, loss = 1.81 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:54:48.745679: step 93220, loss = 1.85 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:54:49.893315: step 93230, loss = 2.03 (1115.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:54:51.075336: step 93240, loss = 1.82 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:54:52.235951: step 93250, loss = 1.99 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:54:53.390477: step 93260, loss = 1.91 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:54:54.546481: step 93270, loss = 1.88 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:54:55.717948: step 93280, loss = 1.90 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:54:56.856423: step 93290, loss = 1.90 (1124.3 examples/sec; 0.114 sec/batch)
2017-05-05 00:54:58.031192: step 93300, loss = 2.09 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:54:59.188975: step 93310, loss = 2.01 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:55:00.352029: step 93320, loss = 1.83 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:55:01.517757: step 93330, loss = 1.96 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:55:02.716773: step 93340, loss = 1.97 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:55:03.886774: step 93350, loss = 2.03 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:55:05.063151: step 93360, loss = 1.92 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:55:06.233803: step 93370, loss = 1.97 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:55:07.408032: step 93380, loss = 2.04 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:55:08.586582: step 93390, loss = 2.04 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:55:09.778573: step 93400, loss = 1.87 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:55:10.967855: step 93410, loss = 1.96 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:55:12.150067: step 93420, loss = 1.82 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:55:13.328634: step 93430, loss = 1.92 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:55:14.520744: step 93440, loss = 1.88 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:55:15.718995: step 93450, loss = 1.89 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:55:16.906326: step 93460, loss = 1.94 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:55:18.105928: step 93470, loss = 2.00 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:55:19.298161: step 93480, loss = 1.82 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:55:20.508102: step 93490, loss = 1.92 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:55:21.708816: step 93500, loss = 1.97 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:55:22.909427: step 93510, loss = 2.04 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:55:24.130315: step 93520, loss = 2.00 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:55:25.345534: step 93530, loss = 1.92 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:55:26.554499: step 93540, loss = 1.85 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:55:27.786765: step 93550, loss = 1.90 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:55:28.983596: step 93560, loss = 1.88 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:55:30.194458: step 93570, loss = 1.84 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:55:31.422177: step 93580, loss = 1.91 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:55:32.638412: step 93590, loss = 1.88 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:55:33.828786: step 93600, loss = 2.03 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:55:35.031835: step 93610, loss = 1.90 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:55:36.241976: step 93620, loss = 1.83 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:55:37.424283: step 93630, loss = 2.03 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:55:38.606850: step 93640, loss = 2.02 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:55:39.808958: step 93650, loss = 2.02 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:55:41.014083: step 93660, loss = 1.77 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:55:42.171189: step 93670, loss = 1.91 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:55:43.363268: step 93680, loss = 1.84 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:55:44.549706: step 93690, loss = 1.84 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:55:45.736100: step 93700, loss = 1.99 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:55:46.921748: step 93710, loss = 1.94 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:55:48.094637: step 93720, loss = 2.06 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:55:49.261458: step 93730, loss = 2.17 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:55:50.414152: step 93740, loss = 1.83 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:55:51.589344: step 93750, loss = 1.86 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:55:52.753063: step 93760, loss = 2.10 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:55:53.901064: step 93770, loss = 1.93 (1115.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:55:55.067544: step 93780, loss = 1.91 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:55:56.227453: step 93790, loss = 1.93 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:55:57.390718: step 93800, loss = 1.78 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:55:58.553260: step 93810, loss = 1.87 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:55:59.732606: step 93820, loss = 2.06 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:00.903473: step 93830, loss = 1.94 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:02.043112: step 93840, loss = 2.02 (1123.2 examples/sec; 0.114 sec/batch)
2017-05-05 00:56:03.211665: step 93850, loss = 1.92 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:04.389804: step 93860, loss = 1.98 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:05.531800: step 93870, loss = 1.96 (1120.8 examples/sec; 0.114 sec/batch)
2017-05-05 00:56:06.690028: step 93880, loss = 1.86 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:56:07.868806: step 93890, loss = 2.04 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:09.045925: step 93900, loss = 1.92 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:10.213056: step 93910, loss = 1.73 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:11.388743: step 93920, loss = 1.88 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:12.563786: step 93930, loss = 1.89 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:13.723149: step 93940, loss = 1.90 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:56:14.885909: step 93950, loss = 1.88 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:56:16.052212: step 93960, loss = 2.16 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:17.213941: step 93970, loss = 1.99 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:56:18.372952: step 93980, loss = 2.01 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:56:19.528500: step 93990, loss = 1.97 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:56:20.702822: step 94000, loss = 1.85 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:21.859708: step 94010, loss = 2.08 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:56:23.056606: step 94020, loss = 1.93 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:56:24.220852: step 94030, loss = 1.86 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:56:25.383999: step 94040, loss = 1.93 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:56:26.646196: step 94050, loss = 2.19 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-05 00:56:27.737502: step 94060, loss = 1.78 (1172.9 examples/sec; 0.109 sec/batch)
2017-05-05 00:56:28.925389: step 94070, loss = 1.89 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:56:30.115308: step 94080, loss = 1.91 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:56:31.327125: step 94090, loss = 1.95 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:56:32.536274: step 94100, loss = 1.83 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:56:33.696015: step 94110, loss = 2.03 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:56:34.881168: step 94120, loss = 2.01 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:56:36.058937: step 94130, loss = 1.91 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:37.232597: step 94140, loss = 2.18 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:38.389859: step 94150, loss = 1.96 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:56:39.567817: step 94160, loss = 1.98 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:40.737486: step 94170, loss = 2.08 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:41.925060: step 94180, loss = 1.89 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:56:43.130370: step 94190, loss = 1.92 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:56:44.329365: step 94200, loss = 1.92 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:56:45.499206: step 94210, loss = 2.22 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:46.684420: step 94220, loss = 1.93 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:56:47.887398: step 94230, loss = 1.94 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:56:49.091088: step 94240, loss = 1.90 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:56:50.357725: step 94250, loss = 1.96 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-05 00:56:51.502640: step 94260, loss = 1.99 (1118.0 examples/sec; 0.114 sec/batch)
2017-05-05 00:56:52.720033: step 94270, loss = 1.92 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:56:53.937347: step 94280, loss = 2.05 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:56:55.174922: step 94290, loss = 1.89 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:56:56.378603: step 94300, loss = 1.87 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:56:57.612425: step 94310, loss = 1.92 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:56:58.836904: step 94320, loss = 1.93 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:57:00.022796: step 94330, loss = 2.01 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:01.241333: step 94340, loss = 2.13 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:57:02.459169: step 94350, loss = 1.95 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:57:03.655029: step 94360, loss = 2.08 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:04.847713: step 94370, loss = 1.86 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:06.037429: step 94380, loss = 1.89 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:07.246839: step 94390, loss = 1.90 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:57:08.463199: step 94400, loss = 1.87 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:57:09.677089: step 94410, loss = 1.93 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:57:10.893151: step 94420, loss = 1.85 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:57:12.097713: step 94430, loss = 1.99 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:13.291178: step 94440, loss = 1.93 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:14.481161: step 94450, loss = 1.88 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:15.676929: step 94460, loss = 2.00 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:16.871147: step 94470, loss = 1.98 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:18.037881: step 94480, loss = 2.05 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:57:19.218348: step 94490, loss = 1.87 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:20.412770: step 94500, loss = 1.81 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:21.590131: step 94510, loss = 1.83 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:22.784017: step 94520, loss = 1.98 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:23.991370: step 94530, loss = 1.89 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:57:25.177986: step 94540, loss = 1.98 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:26.361322: step 94550, loss = 2.03 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:27.564199: step 94560, loss = 1.92 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:28.757110: step 94570, loss = 1.77 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:29.951551: step 94580, loss = 1.79 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:31.142300: step 94590, loss = 1.92 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:32.327981: step 94600, loss = 2.01 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:33.509189: step 94610, loss = 1.87 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:34.721023: step 94620, loss = 1.91 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:57:35.917585: step 94630, loss = 1.99 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:37.141411: step 94640, loss = 2.00 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:57:38.320293: step 94650, loss = 1.87 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:39.499880: step 94660, loss = 1.99 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:40.683544: step 94670, loss = 1.84 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:41.856695: step 94680, loss = 1.87 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:57:43.070051: step 94690, loss = 1.96 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:57:44.249033: step 94700, loss = 1.95 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:45.429772: step 94710, loss = 1.82 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:46.602693: step 94720, loss = 1.99 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:57:47.777328: step 94730, loss = 1.89 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:57:48.930192: step 94740, loss = 1.94 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:57:50.087730: step 94750, loss = 1.89 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:57:51.268166: step 94760, loss = 1.77 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:52.423153: step 94770, loss = 1.92 (1108.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:57:53.577130: step 94780, loss = 1.89 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:57:54.752546: step 94790, loss = 1.98 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:55.901172: step 94800, loss = 2.11 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:57:57.068623: step 94810, loss = 1.91 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:57:58.211292: step 94820, loss = 2.07 (1120.2 examples/sec; 0.114 sec/batch)
2017-05-05 00:57:59.393425: step 94830, loss = 1.92 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:00.543625: step 94840, loss = 1.95 (1112.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:58:01.724050: step 94850, loss = 2.03 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:02.902897: step 94860, loss = 1.96 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:04.065700: step 94870, loss = 1.93 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:05.231753: step 94880, loss = 2.03 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:06.414767: step 94890, loss = 2.04 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:07.574001: step 94900, loss = 1.89 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:08.751554: step 94910, loss = 2.13 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:09.904993: step 94920, loss = 1.91 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:58:11.077165: step 94930, loss = 1.93 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:12.250199: step 94940, loss = 1.90 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:13.427712: step 94950, loss = 1.86 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:14.601643: step 94960, loss = 1.99 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:15.775485: step 94970, loss = 1.87 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:16.945222: step 94980, loss = 1.95 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:18.109781: step 94990, loss = 1.72 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:19.285399: step 95000, loss = 1.95 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:20.459080: step 95010, loss = 1.89 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:21.623798: step 95020, loss = 2.00 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:22.785110: step 95030, loss = 1.93 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:24.059466: step 95040, loss = 1.91 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-05 00:58:25.127983: step 95050, loss = 2.09 (1197.9 examples/sec; 0.107 sec/batch)
2017-05-05 00:58:26.281055: step 95060, loss = 1.90 (1110.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:58:27.455866: step 95070, loss = 2.00 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:28.615918: step 95080, loss = 1.88 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:29.771795: step 95090, loss = 1.89 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:30.948527: step 95100, loss = 1.84 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:32.111711: step 95110, loss = 2.04 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:33.282739: step 95120, loss = 1.91 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:34.444022: step 95130, loss = 1.82 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:35.599636: step 95140, loss = 1.76 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:36.764886: step 95150, loss = 1.86 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:37.933847: step 95160, loss = 2.00 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:39.118715: step 95170, loss = 1.79 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:40.299925: step 95180, loss = 1.78 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:41.485435: step 95190, loss = 2.24 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:58:42.633552: step 95200, loss = 1.90 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:58:43.813983: step 95210, loss = 2.07 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:44.986463: step 95220, loss = 1.90 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:46.136587: step 95230, loss = 2.03 (1112.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:58:47.331711: step 95240, loss = 1.91 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:58:48.524486: step 95250, loss = 1.87 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:58:49.712584: step 95260, loss = 1.85 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:58:50.894850: step 95270, loss = 2.05 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:52.081205: step 95280, loss = 1.98 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:58:53.278494: step 95290, loss = 1.88 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:58:54.476836: step 95300, loss = 1.82 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:58:55.682877: step 95310, loss = 1.98 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:58:56.864505: step 95320, loss = 1.91 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:58.050988: step 95330, loss = 1.87 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:58:59.248782: step 95340, loss = 1.76 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:59:00.449805: step 95350, loss = 1.81 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:59:01.628678: step 95360, loss = 1.99 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:59:02.851282: step 95370, loss = 1.88 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:59:04.061398: step 95380, loss = 2.11 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:59:05.287980: step 95390, loss = 1.90 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:59:06.496694: step 95400, loss = 1.91 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:59:07.713302: step 95410, loss = 1.88 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:59:08.908751: step 95420, loss = 2.03 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:59:10.068651: step 95430, loss = 1.72 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:11.241382: step 95440, loss = 2.03 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:12.407513: step 95450, loss = 2.17 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:13.572437: step 95460, loss = 1.95 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:14.726474: step 95470, loss = 1.98 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:59:15.894278: step 95480, loss = 1.93 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:17.068045: step 95490, loss = 1.90 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:18.234058: step 95500, loss = 1.99 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:19.403884: step 95510, loss = 1.89 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:20.566454: step 95520, loss = 1.98 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:21.717798: step 95530, loss = 2.13 (1111.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:59:22.892175: step 95540, loss = 2.00 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:24.056318: step 95550, loss = 1.97 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:25.212926: step 95560, loss = 1.89 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:26.366335: step 95570, loss = 1.86 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 00:59:27.534280: step 95580, loss = 1.86 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:28.709610: step 95590, loss = 2.10 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:59:29.873653: step 95600, loss = 1.92 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:31.043140: step 95610, loss = 1.84 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:32.194411: step 95620, loss = 2.02 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-05 00:59:33.373924: step 95630, loss = 1.83 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:59:34.532403: step 95640, loss = 1.96 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:35.694798: step 95650, loss = 1.93 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:36.859319: step 95660, loss = 1.68 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:38.007931: step 95670, loss = 1.96 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:59:39.163074: step 95680, loss = 1.93 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:40.329709: step 95690, loss = 1.91 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:41.474962: step 95700, loss = 1.92 (1117.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:59:42.645012: step 95710, loss = 1.86 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:43.816183: step 95720, loss = 1.91 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:44.984879: step 95730, loss = 1.90 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:46.167085: step 95740, loss = 1.74 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:59:47.336320: step 95750, loss = 1.92 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:48.507393: step 95760, loss = 2.02 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:49.690316: step 95770, loss = 2.10 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:59:50.870906: step 95780, loss = 1.76 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:59:52.067710: step 95790, loss = 1.89 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:59:53.246657: step 95800, loss = 1.84 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:59:54.427068: step 95810, loss = 1.89 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:59:55.633692: step 95820, loss = 2.01 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:59:56.840319: step 95830, loss = 1.90 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:59:58.030290: step 95840, loss = 1.98 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:59:59.241852: step 95850, loss = 1.95 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:00:00.440711: step 95860, loss = 1.78 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:00:01.636781: step 95870, loss = 1.77 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:00:02.832138: step 95880, loss = 2.00 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:00:04.017524: step 95890, loss = 1.82 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:00:05.226837: step 95900, loss = 1.97 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:00:06.410773: step 95910, loss = 2.00 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:00:07.585606: step 95920, loss = 1.83 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:08.770178: step 95930, loss = 2.11 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:00:09.938897: step 95940, loss = 1.95 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:11.098719: step 95950, loss = 1.95 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:00:12.270309: step 95960, loss = 1.88 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:13.431821: step 95970, loss = 1.94 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:00:14.603786: step 95980, loss = 1.94 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:15.786693: step 95990, loss = 1.82 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:00:16.937971: step 96000, loss = 2.03 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:00:18.096474: step 96010, loss = 2.07 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:00:19.257630: step 96020, loss = 1.87 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:00:20.526274: step 96030, loss = 1.92 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-05 01:00:21.602426: step 96040, loss = 2.14 (1189.4 examples/sec; 0.108 sec/batch)
2017-05-05 01:00:22.767156: step 96050, loss = 1.94 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:00:23.922660: step 96060, loss = 2.14 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:00:25.099749: step 96070, loss = 1.96 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:00:26.262193: step 96080, loss = 1.96 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:00:27.453183: step 96090, loss = 1.93 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:00:28.627169: step 96100, loss = 1.89 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:29.782857: step 96110, loss = 2.04 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:00:30.953877: step 96120, loss = 1.87 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:32.119775: step 96130, loss = 1.90 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:33.305923: step 96140, loss = 1.90 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:00:34.475333: step 96150, loss = 1.87 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:35.654176: step 96160, loss = 1.99 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:00:36.808146: step 96170, loss = 2.16 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:00:37.982272: step 96180, loss = 1.97 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:39.150535: step 96190, loss = 1.93 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:40.326302: step 96200, loss = 2.05 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:00:41.496683: step 96210, loss = 2.07 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:42.660181: step 96220, loss = 1.82 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:00:43.827546: step 96230, loss = 1.98 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:44.989023: step 96240, loss = 1.94 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:00:46.151718: step 96250, loss = 1.98 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:00:47.342602: step 96260, loss = 1.93 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:00:48.537647: step 96270, loss = 1.88 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:00:49.706830: step 96280, loss = 1.98 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:50.876067: step 96290, loss = 1.85 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:52.069078: step 96300, loss = 1.92 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:00:53.253479: step 96310, loss = 2.08 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:00:54.439640: step 96320, loss = 2.07 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:00:55.643250: step 96330, loss = 2.00 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:00:56.854859: step 96340, loss = 1.95 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:00:58.046611: step 96350, loss = 1.98 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:00:59.258792: step 96360, loss = 1.93 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:01:00.469273: step 96370, loss = 1.80 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:01:01.687366: step 96380, loss = 1.95 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:01:02.900798: step 96390, loss = 1.88 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:01:04.086818: step 96400, loss = 1.87 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:01:05.293949: step 96410, loss = 2.12 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:01:06.486646: step 96420, loss = 1.85 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:01:07.671801: step 96430, loss = 1.97 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:01:08.869653: step 96440, loss = 1.89 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:01:10.045885: step 96450, loss = 2.09 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:11.225437: step 96460, loss = 1.84 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:12.405105: step 96470, loss = 1.87 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:13.559962: step 96480, loss = 1.87 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:01:14.743119: step 96490, loss = 1.94 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:15.919499: step 96500, loss = 1.80 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:17.084379: step 96510, loss = 2.01 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:18.244383: step 96520, loss = 1.83 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:19.404275: step 96530, loss = 1.84 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:20.564504: step 96540, loss = 1.78 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:21.717472: step 96550, loss = 1.96 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:01:22.872590: step 96560, loss = 2.01 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:24.071382: step 96570, loss = 2.09 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:01:25.235600: step 96580, loss = 2.07 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:26.393362: step 96590, loss = 1.89 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:27.567346: step 96600, loss = 1.87 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:01:28.727171: step 96610, loss = 1.82 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:29.885916: step 96620, loss = 1.92 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:31.055145: step 96630, loss = 1.94 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:01:32.211194: step 96640, loss = 1.98 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:33.369424: step 96650, loss = 2.03 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:34.551365: step 96660, loss = 1.87 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:35.717226: step 96670, loss = 2.04 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:01:36.882665: step 96680, loss = 1.88 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:01:38.045394: step 96690, loss = 1.89 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:39.218947: step 96700, loss = 2.08 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:01:40.358450: step 96710, loss = 2.08 (1123.3 examples/sec; 0.114 sec/batch)
2017-05-05 01:01:41.532208: step 96720, loss = 1.93 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:01:42.694100: step 96730, loss = 2.04 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:43.846853: step 96740, loss = 2.00 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:01:45.035930: step 96750, loss = 1.82 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:01:46.190438: step 96760, loss = 1.93 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:01:47.353596: step 96770, loss = 1.93 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:48.522652: step 96780, loss = 1.94 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:01:49.705595: step 96790, loss = 1.88 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:50.893389: step 96800, loss = 1.74 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:01:52.097632: step 96810, loss = 1.83 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:01:53.297432: step 96820, loss = 2.07 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:01:54.476854: step 96830, loss = 1.90 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:55.652372: step 96840, loss = 1.87 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:56.832319: step 96850, loss = 1.86 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:58.013754: step 96860, loss = 1.95 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:59.172608: step 96870, loss = 2.05 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:02:00.328145: step 96880, loss = 1.97 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:02:01.505296: step 96890, loss = 1.94 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:02.685434: step 96900, loss = 2.00 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:03.873535: step 96910, loss = 1.88 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:05.033262: step 96920, loss = 1.94 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:02:06.204519: step 96930, loss = 1.79 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:02:07.387780: step 96940, loss = 2.00 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:08.556340: step 96950, loss = 1.90 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:02:09.720578: step 96960, loss = 1.70 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:02:10.897409: step 96970, loss = 2.02 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:12.084560: step 96980, loss = 1.73 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:13.283670: step 96990, loss = 1.80 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:02:14.466445: step 97000, loss = 1.84 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:15.679849: step 97010, loss = 1.95 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:02:16.953607: step 97020, loss = 1.92 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-05 01:02:18.042076: step 97030, loss = 1.79 (1176.0 examples/sec; 0.109 sec/batch)
2017-05-05 01:02:19.235815: step 97040, loss = 1.94 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:20.419140: step 97050, loss = 2.23 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:21.633777: step 97060, loss = 1.94 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:02:22.826185: step 97070, loss = 1.92 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:24.039912: step 97080, loss = 1.85 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:02:25.247385: step 97090, loss = 1.89 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:02:26.452127: step 97100, loss = 1.85 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:02:27.668002: step 97110, loss = 2.01 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:02:28.860902: step 97120, loss = 1.86 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:30.056986: step 97130, loss = 1.97 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:02:31.257357: step 97140, loss = 1.94 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:02:32.468685: step 97150, loss = 1.93 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:02:33.653036: step 97160, loss = 2.02 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:34.841940: step 97170, loss = 1.97 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:36.024711: step 97180, loss = 2.03 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:37.210951: step 97190, loss = 1.98 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:38.412545: step 97200, loss = 1.93 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:02:39.632293: step 97210, loss = 2.01 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:02:40.820489: step 97220, loss = 1.85 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:41.995865: step 97230, loss = 1.80 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:43.198256: step 97240, loss = 1.92 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:02:44.407585: step 97250, loss = 2.02 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:02:45.621428: step 97260, loss = 2.00 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:02:46.808588: step 97270, loss = 2.06 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:47.983172: step 97280, loss = 1.98 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:02:49.152466: step 97290, loss = 1.96 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:02:50.341790: step 97300, loss = 1.95 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:51.522161: step 97310, loss = 1.93 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:52.710609: step 97320, loss = 2.03 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:53.899538: step 97330, loss = 1.82 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:55.077650: step 97340, loss = 1.81 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:56.287317: step 97350, loss = 1.78 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:02:57.472795: step 97360, loss = 1.90 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:58.680411: step 97370, loss = 1.98 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:02:59.871940: step 97380, loss = 2.13 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:01.056905: step 97390, loss = 1.86 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:03:02.230836: step 97400, loss = 1.73 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:03:03.416404: step 97410, loss = 1.97 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:04.603822: step 97420, loss = 1.78 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:05.794463: step 97430, loss = 1.91 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:06.977492: step 97440, loss = 1.88 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:03:08.176558: step 97450, loss = 1.90 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:09.368410: step 97460, loss = 1.96 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:10.541408: step 97470, loss = 1.95 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:03:11.719980: step 97480, loss = 2.03 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:03:12.898037: step 97490, loss = 1.85 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:03:14.070687: step 97500, loss = 2.03 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:03:15.250871: step 97510, loss = 1.86 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:03:16.423783: step 97520, loss = 1.95 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:03:17.598883: step 97530, loss = 1.83 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:03:18.789520: step 97540, loss = 1.82 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:19.972459: step 97550, loss = 1.98 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:03:21.153682: step 97560, loss = 1.77 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:03:22.320688: step 97570, loss = 1.97 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:03:23.512763: step 97580, loss = 1.86 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:24.742632: step 97590, loss = 1.97 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:03:25.940343: step 97600, loss = 1.97 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:27.154558: step 97610, loss = 2.02 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:03:28.372987: step 97620, loss = 2.04 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:03:29.583525: step 97630, loss = 2.11 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:03:30.791398: step 97640, loss = 1.71 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:03:32.013898: step 97650, loss = 1.87 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:03:33.229805: step 97660, loss = 2.09 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:03:34.434087: step 97670, loss = 1.88 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:35.638035: step 97680, loss = 1.97 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:36.836595: step 97690, loss = 2.01 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:38.039056: step 97700, loss = 1.91 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:39.253303: step 97710, loss = 1.90 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:03:40.469894: step 97720, loss = 2.14 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:03:41.667366: step 97730, loss = 1.93 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:42.848684: step 97740, loss = 1.87 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:03:44.045550: step 97750, loss = 1.92 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:45.238802: step 97760, loss = 1.87 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:46.425888: step 97770, loss = 2.00 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:47.630843: step 97780, loss = 1.94 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:48.826615: step 97790, loss = 1.77 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:49.997504: step 97800, loss = 1.88 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:03:51.208124: step 97810, loss = 1.91 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:03:52.406855: step 97820, loss = 1.74 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:53.607631: step 97830, loss = 1.95 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:54.804424: step 97840, loss = 1.74 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:55.998340: step 97850, loss = 1.93 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:57.172482: step 97860, loss = 1.96 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:03:58.334837: step 97870, loss = 1.91 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:03:59.513794: step 97880, loss = 1.94 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:04:00.695383: step 97890, loss = 1.82 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:04:01.869611: step 97900, loss = 1.96 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:04:03.045272: step 97910, loss = 1.88 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:04:04.224814: step 97920, loss = 1.89 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:04:05.385681: step 97930, loss = 1.91 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:04:06.550283: step 97940, loss = 1.85 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:04:07.735737: step 97950, loss = 1.90 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:08.890646: step 97960, loss = 1.86 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:04:10.051761: step 97970, loss = 1.88 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:04:11.207980: step 97980, loss = 1.89 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:04:12.371705: step 97990, loss = 1.89 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:04:13.522219: step 98000, loss = 1.88 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:04:14.809558: step 98010, loss = 1.84 (994.3 examples/sec; 0.129 sec/batch)
2017-05-05 01:04:15.882446: step 98020, loss = 2.15 (1193.0 examples/sec; 0.107 sec/batch)
2017-05-05 01:04:17.058751: step 98030, loss = 1.90 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:04:18.209364: step 98040, loss = 1.96 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:04:19.392421: step 98050, loss = 1.98 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:04:20.588954: step 98060, loss = 1.93 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:04:21.759146: step 98070, loss = 2.01 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:04:22.963794: step 98080, loss = 2.01 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:04:24.181407: step 98090, loss = 1.88 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:04:25.375547: step 98100, loss = 1.89 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:26.583869: step 98110, loss = 2.06 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:04:27.809783: step 98120, loss = 1.84 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:04:29.005645: step 98130, loss = 1.76 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:04:30.191583: step 98140, loss = 1.97 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:31.413183: step 98150, loss = 1.81 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:04:32.612062: step 98160, loss = 1.91 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:04:33.805073: step 98170, loss = 1.86 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:34.990174: step 98180, loss = 1.96 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:36.179297: step 98190, loss = 1.97 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:37.360802: step 98200, loss = 1.95 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:04:38.566805: step 98210, loss = 1.95 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:04:39.748863: step 98220, loss = 2.11 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:04:40.930073: step 98230, loss = 1.94 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:04:42.072987: step 98240, loss = 1.92 (1119.9 examples/sec; 0.114 sec/batch)
2017-05-05 01:04:43.266006: step 98250, loss = 1.88 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:44.466927: step 98260, loss = 1.99 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:04:45.661412: step 98270, loss = 1.96 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:46.851463: step 98280, loss = 1.96 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:48.049023: step 98290, loss = 1.90 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:04:49.256906: step 98300, loss = 2.02 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:04:50.465119: step 98310, loss = 2.12 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:04:51.672154: step 98320, loss = 1.91 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:04:52.890944: step 98330, loss = 1.93 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:04:54.112994: step 98340, loss = 1.79 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:04:55.338139: step 98350, loss = 1.93 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:04:56.563986: step 98360, loss = 2.00 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:04:57.781347: step 98370, loss = 1.89 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:04:59.014778: step 98380, loss = 1.85 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:05:00.241869: step 98390, loss = 1.87 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:05:01.460325: step 98400, loss = 1.93 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:02.638395: step 98410, loss = 1.93 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:05:03.857812: step 98420, loss = 1.93 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:05.080478: step 98430, loss = 1.90 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:06.295749: step 98440, loss = 1.82 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:07.522566: step 98450, loss = 1.91 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-05 01:05:08.757473: step 98460, loss = 1.94 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:05:09.948222: step 98470, loss = 1.95 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:05:11.175217: step 98480, loss = 1.81 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:05:12.392871: step 98490, loss = 1.94 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:13.606311: step 98500, loss = 1.82 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:14.815682: step 98510, loss = 1.92 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:16.033820: step 98520, loss = 2.06 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:17.249863: step 98530, loss = 2.09 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:18.472685: step 98540, loss = 1.88 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:19.678363: step 98550, loss = 1.86 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:20.894110: step 98560, loss = 2.10 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:22.098182: step 98570, loss = 1.97 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:05:23.312907: step 98580, loss = 2.02 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:24.524904: step 98590, loss = 1.85 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:25.725576: step 98600, loss = 2.02 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:05:26.916107: step 98610, loss = 1.98 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:05:28.129048: step 98620, loss = 1.87 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:29.346007: step 98630, loss = 1.82 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:30.570128: step 98640, loss = 1.81 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:31.791914: step 98650, loss = 2.19 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:33.019899: step 98660, loss = 2.23 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:05:34.248648: step 98670, loss = 1.84 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:05:35.474042: step 98680, loss = 1.82 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:05:36.676784: step 98690, loss = 1.93 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:05:37.883753: step 98700, loss = 1.96 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:39.106996: step 98710, loss = 1.94 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:40.320956: step 98720, loss = 1.89 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:41.538053: step 98730, loss = 1.79 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:42.727709: step 98740, loss = 1.80 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:05:43.957780: step 98750, loss = 1.90 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:05:45.168400: step 98760, loss = 1.92 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:46.388050: step 98770, loss = 1.85 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:47.603449: step 98780, loss = 1.91 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:48.849599: step 98790, loss = 1.80 (1027.2 examples/sec; 0.125 sec/batch)
2017-05-05 01:05:50.055679: step 98800, loss = 1.99 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:51.231915: step 98810, loss = 1.88 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:05:52.448470: step 98820, loss = 1.97 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:53.655028: step 98830, loss = 1.82 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:54.876561: step 98840, loss = 1.84 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:56.080622: step 98850, loss = 2.06 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:05:57.292956: step 98860, loss = 1.96 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:58.492183: step 98870, loss = 1.92 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:05:59.717307: step 98880, loss = 2.01 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:06:00.920202: step 98890, loss = 1.83 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:02.137092: step 98900, loss = 1.88 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:06:03.357713: step 98910, loss = 1.97 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:06:04.573099: step 98920, loss = 1.93 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:06:05.791919: step 98930, loss = 1.90 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:06:06.990296: step 98940, loss = 1.94 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:08.196904: step 98950, loss = 1.76 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:06:09.391277: step 98960, loss = 1.89 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:06:10.578054: step 98970, loss = 1.89 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:06:11.760734: step 98980, loss = 1.89 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:06:12.942439: step 98990, loss = 1.90 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:06:14.210318: step 99000, loss = 1.89 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-05 01:06:15.329164: step 99010, loss = 1.91 (1144.0 examples/sec; 0.112 sec/batch)
2017-05-05 01:06:16.543360: step 99020, loss = 1.82 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:06:17.736093: step 99030, loss = 1.90 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:06:18.943927: step 99040, loss = 1.94 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:06:20.146014: step 99050, loss = 2.03 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:21.337539: step 99060, loss = 1.94 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:06:22.522221: step 99070, loss = 1.94 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:06:23.742504: step 99080, loss = 1.87 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:06:24.936726: step 99090, loss = 1.92 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:06:26.125190: step 99100, loss = 1.95 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:06:27.350707: step 99110, loss = 1.89 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:06:28.564173: step 99120, loss = 2.23 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:06:29.768120: step 99130, loss = 1.81 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:30.968997: step 99140, loss = 1.78 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:32.168183: step 99150, loss = 1.84 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:33.384393: step 99160, loss = 1.96 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:06:34.555687: step 99170, loss = 1.90 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:06:35.750732: step 99180, loss = 2.02 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:36.932435: step 99190, loss = 1.88 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:06:38.129639: step 99200, loss = 1.93 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:39.346028: step 99210, loss = 1.86 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:06:40.547640: step 99220, loss = 1.92 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:41.719457: step 99230, loss = 2.05 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:06:42.901138: step 99240, loss = 2.00 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:06:44.090596: step 99250, loss = 2.05 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:06:45.258961: step 99260, loss = 1.87 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:06:46.442308: step 99270, loss = 1.87 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:06:47.622538: step 99280, loss = 2.03 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:06:48.789914: step 99290, loss = 2.09 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:06:49.945302: step 99300, loss = 1.93 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:06:51.102886: step 99310, loss = 1.93 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:06:52.276323: step 99320, loss = 1.88 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:06:53.445984: step 99330, loss = 1.84 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:06:54.595639: step 99340, loss = 1.95 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:06:55.769917: step 99350, loss = 2.01 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:06:56.947928: step 99360, loss = 1.80 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:06:58.116743: step 99370, loss = 1.94 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:06:59.288624: step 99380, loss = 1.86 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:07:00.434403: step 99390, loss = 1.95 (1117.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:07:01.592800: step 99400, loss = 1.91 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:07:02.754019: step 99410, loss = 2.01 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:07:03.941468: step 99420, loss = 1.89 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:07:05.123508: step 99430, loss = 1.93 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:07:06.281873: step 99440, loss = 2.08 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:07:07.468133: step 99450, loss = 2.07 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:07:08.654972: step 99460, loss = 1.89 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:07:09.823900: step 99470, loss = 1.82 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:07:11.002890: step 99480, loss = 2.15 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:07:12.161273: step 99490, loss = 1.96 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:07:13.338696: step 99500, loss = 2.01 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:07:14.510021: step 99510, loss = 1.96 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:07:15.689394: step 99520, loss = 1.98 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:07:16.886096: step 99530, loss = 1.88 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:07:18.059659: step 99540, loss = 1.95 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:07:19.232007: step 99550, loss = 2.17 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:07:20.393152: step 99560, loss = 2.11 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:07:21.586916: step 99570, loss = 2.04 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:07:22.761751: step 99580, loss = 1.91 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:07:23.975526: step 99590, loss = 2.13 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:07:25.181484: step 99600, loss = 1.91 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:07:26.397954: step 99610, loss = 2.12 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:27.609698: step 99620, loss = 2.06 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:07:28.836039: step 99630, loss = 1.79 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:07:30.041032: step 99640, loss = 1.84 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:07:31.263629: step 99650, loss = 1.89 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:32.484689: step 99660, loss = 1.94 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:33.698342: step 99670, loss = 1.99 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:07:34.910851: step 99680, loss = 2.00 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:07:36.115276: step 99690, loss = 1.91 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:07:37.333270: step 99700, loss = 2.00 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:38.546276: step 99710, loss = 1.96 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:07:39.768126: step 99720, loss = 1.98 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:41.015529: step 99730, loss = 1.96 (1026.1 examples/sec; 0.125 sec/batch)
2017-05-05 01:07:42.244388: step 99740, loss = 2.00 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:07:43.469934: step 99750, loss = 1.95 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:07:44.695027: step 99760, loss = 1.96 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:07:45.901699: step 99770, loss = 1.89 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:07:47.115795: step 99780, loss = 1.97 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:07:48.325901: step 99790, loss = 1.78 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:07:49.506756: step 99800, loss = 2.07 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:07:50.737975: step 99810, loss = 1.87 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:07:51.958732: step 99820, loss = 2.13 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:53.158755: step 99830, loss = 2.06 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:07:54.358900: step 99840, loss = 1.94 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:07:55.585692: step 99850, loss = 1.93 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:07:56.799800: step 99860, loss = 1.92 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:07:58.002090: step 99870, loss = 2.08 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:07:59.232452: step 99880, loss = 2.00 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-05 01:08:00.450928: step 99890, loss = 1.94 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:01.654514: step 99900, loss = 1.92 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:08:02.870202: step 99910, loss = 1.90 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:04.095565: step 99920, loss = 2.01 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:08:05.301287: step 99930, loss = 1.92 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:06.496538: step 99940, loss = 1.94 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:08:07.726045: step 99950, loss = 1.85 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:08:08.953690: step 99960, loss = 1.97 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:08:10.161562: step 99970, loss = 1.93 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:11.350942: step 99980, loss = 1.79 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:08:12.640429: step 99990, loss = 1.98 (992.6 examples/sec; 0.129 sec/batch)
2017-05-05 01:08:13.750560: step 100000, loss = 1.72 (1153.0 examples/sec; 0.111 sec/batch)
2017-05-05 01:08:14.963072: step 100010, loss = 1.91 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:16.179345: step 100020, loss = 1.84 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:17.391209: step 100030, loss = 1.87 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:18.607978: step 100040, loss = 1.85 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:19.829023: step 100050, loss = 1.95 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:21.044575: step 100060, loss = 2.11 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:22.258858: step 100070, loss = 1.93 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:23.489961: step 100080, loss = 2.01 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:08:24.696940: step 100090, loss = 2.03 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:25.911357: step 100100, loss = 1.96 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:27.141634: step 100110, loss = 1.90 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:08:28.342662: step 100120, loss = 1.91 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:08:29.548187: step 100130, loss = 1.90 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:30.769914: step 100140, loss = 1.80 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:31.980682: step 100150, loss = 1.84 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:33.184648: step 100160, loss = 2.04 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:08:34.404017: step 100170, loss = 2.00 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:35.615536: step 100180, loss = 2.08 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:36.801504: step 100190, loss = 1.88 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:08:38.031380: step 100200, loss = 1.94 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:08:39.249772: step 100210, loss = 1.89 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:40.477148: step 100220, loss = 1.96 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:08:41.683264: step 100230, loss = 1.83 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:42.895913: step 100240, loss = 2.11 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:44.107103: step 100250, loss = 2.16 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:45.325076: step 100260, loss = 1.86 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:46.533559: step 100270, loss = 1.91 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:47.747883: step 100280, loss = 2.17 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:48.951655: step 100290, loss = 1.86 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:08:50.159403: step 100300, loss = 1.95 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:51.369338: step 100310, loss = 1.98 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:52.581309: step 100320, loss = 1.91 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:53.767714: step 100330, loss = 2.20 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:08:54.991398: step 100340, loss = 1.80 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:56.221655: step 100350, loss = 2.12 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:08:57.439216: step 100360, loss = 1.94 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:58.652729: step 100370, loss = 1.91 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:59.881136: step 100380, loss = 2.05 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:09:01.089970: step 100390, loss = 1.81 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:02.293071: step 100400, loss = 1.78 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:03.502041: step 100410, loss = 1.91 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:04.705688: step 100420, loss = 1.80 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:05.929220: step 100430, loss = 1.87 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:07.167363: step 100440, loss = 1.86 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-05 01:09:08.381683: step 100450, loss = 1.94 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:09.599877: step 100460, loss = 1.95 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:10.816448: step 100470, loss = 1.87 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:12.046584: step 100480, loss = 1.94 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:09:13.277308: step 100490, loss = 1.99 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:09:14.483702: step 100500, loss = 1.90 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:15.702877: step 100510, loss = 1.82 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:16.911621: step 100520, loss = 1.86 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:18.115420: step 100530, loss = 1.95 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:19.335682: step 100540, loss = 1.99 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:20.544269: step 100550, loss = 1.96 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:21.762411: step 100560, loss = 1.96 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:22.985807: step 100570, loss = 1.92 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:24.186926: step 100580, loss = 1.79 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:25.395068: step 100590, loss = 1.91 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:26.606539: step 100600, loss = 1.99 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:27.826639: step 100610, loss = 1.96 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:29.038944: step 100620, loss = 1.92 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:30.239336: step 100630, loss = 1.90 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:31.444060: step 100640, loss = 1.98 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:32.659327: step 100650, loss = 1.80 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:33.846419: step 100660, loss = 1.86 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:09:35.053674: step 100670, loss = 1.90 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:36.258003: step 100680, loss = 1.91 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:37.455990: step 100690, loss = 2.07 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:38.653010: step 100700, loss = 1.91 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:39.842317: step 100710, loss = 1.97 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:09:41.065043: step 100720, loss = 2.07 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:42.270422: step 100730, loss = 2.14 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:43.481298: step 100740, loss = 1.89 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:44.680905: step 100750, loss = 1.86 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:45.867270: step 100760, loss = 1.89 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:09:47.114471: step 100770, loss = 1.94 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-05 01:09:48.332867: step 100780, loss = 1.97 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:49.543945: step 100790, loss = 1.88 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:50.764605: step 100800, loss = 1.90 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:51.997551: step 100810, loss = 1.83 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:09:53.211967: step 100820, loss = 1.83 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:54.419309: step 100830, loss = 1.80 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:55.617383: step 100840, loss = 1.90 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:56.858078: step 100850, loss = 1.84 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-05 01:09:58.081644: step 100860, loss = 1.81 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:59.313320: step 100870, loss = 1.94 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:10:00.521072: step 100880, loss = 1.98 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:01.726107: step 100890, loss = 2.11 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:02.953508: step 100900, loss = 1.87 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:10:04.152714: step 100910, loss = 1.78 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:05.358606: step 100920, loss = 2.01 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:06.557612: step 100930, loss = 1.99 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:07.762575: step 100940, loss = 1.89 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:08.968041: step 100950, loss = 1.82 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:10.183404: step 100960, loss = 1.98 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:11.393220: step 100970, loss = 1.89 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:12.689200: step 100980, loss = 1.99 (987.7 examples/sec; 0.130 sec/batch)
2017-05-05 01:10:13.794115: step 100990, loss = 1.90 (1158.5 examples/sec; 0.110 sec/batch)
2017-05-05 01:10:15.014676: step 101000, loss = 1.85 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:16.218322: step 101010, loss = 1.94 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:17.410773: step 101020, loss = 1.87 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:10:18.609449: step 101030, loss = 2.03 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:19.812649: step 101040, loss = 1.90 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:20.992848: step 101050, loss = 1.76 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:10:22.179359: step 101060, loss = 2.01 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:10:23.382715: step 101070, loss = 1.78 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:24.600592: step 101080, loss = 1.96 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:25.783681: step 101090, loss = 1.74 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:10:26.992702: step 101100, loss = 1.97 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:28.191514: step 101110, loss = 1.95 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:29.395817: step 101120, loss = 1.80 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:30.592427: step 101130, loss = 1.91 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:31.814434: step 101140, loss = 1.91 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:33.032267: step 101150, loss = 1.97 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:34.244973: step 101160, loss = 1.83 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:35.469133: step 101170, loss = 1.92 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:36.672804: step 101180, loss = 2.06 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:37.882208: step 101190, loss = 1.84 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:39.106639: step 101200, loss = 1.97 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:40.332667: step 101210, loss = 1.85 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:10:41.533649: step 101220, loss = 1.88 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:42.743555: step 101230, loss = 1.92 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:43.969597: step 101240, loss = 1.96 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:10:45.197309: step 101250, loss = 1.87 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:10:46.392917: step 101260, loss = 1.98 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:47.598174: step 101270, loss = 1.94 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:48.818345: step 101280, loss = 2.00 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:50.017013: step 101290, loss = 1.94 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:51.233186: step 101300, loss = 2.03 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:52.445662: step 101310, loss = 1.94 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:53.661182: step 101320, loss = 1.88 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:54.883381: step 101330, loss = 2.07 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:56.093052: step 101340, loss = 1.95 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:57.305319: step 101350, loss = 1.85 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:58.513871: step 101360, loss = 1.99 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:59.727065: step 101370, loss = 1.97 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:00.922798: step 101380, loss = 1.81 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:11:02.111490: step 101390, loss = 1.97 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:11:03.347812: step 101400, loss = 1.96 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-05 01:11:04.562895: step 101410, loss = 1.91 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:05.749159: step 101420, loss = 1.89 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:11:06.972417: step 101430, loss = 1.99 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:08.217076: step 101440, loss = 1.90 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-05 01:11:09.424737: step 101450, loss = 1.90 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:10.641394: step 101460, loss = 2.02 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:11.866465: step 101470, loss = 1.90 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:13.093547: step 101480, loss = 1.97 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:14.292296: step 101490, loss = 2.01 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:11:15.521660: step 101500, loss = 1.84 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:16.749796: step 101510, loss = 1.88 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:17.976257: step 101520, loss = 1.83 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:19.185328: step 101530, loss = 2.01 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:20.412514: step 101540, loss = 1.96 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:21.610360: step 101550, loss = 1.95 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:11:22.827428: step 101560, loss = 1.78 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:24.032638: step 101570, loss = 2.02 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:25.245381: step 101580, loss = 2.04 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:26.452661: step 101590, loss = 1.92 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:27.662422: step 101600, loss = 1.94 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:28.882048: step 101610, loss = 2.03 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:30.089539: step 101620, loss = 1.91 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:31.319105: step 101630, loss = 1.79 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:32.542343: step 101640, loss = 2.04 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:33.748996: step 101650, loss = 1.88 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:34.975998: step 101660, loss = 1.93 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:36.199130: step 101670, loss = 1.85 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:37.408078: step 101680, loss = 1.82 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:38.617541: step 101690, loss = 1.91 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:39.821713: step 101700, loss = 1.89 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:11:41.054637: step 101710, loss = 1.85 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:42.267305: step 101720, loss = 1.87 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:43.490851: step 101730, loss = 1.81 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:44.717620: step 101740, loss = 1.77 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:45.935722: step 101750, loss = 1.84 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:47.165096: step 101760, loss = 1.76 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:48.377934: step 101770, loss = 1.99 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:49.548317: step 101780, loss = 1.87 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:11:50.787860: step 101790, loss = 2.07 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-05 01:11:52.002472: step 101800, loss = 1.94 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:53.235357: step 101810, loss = 1.89 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:54.443079: step 101820, loss = 1.96 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:55.668679: step 101830, loss = 2.05 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:56.893569: step 101840, loss = 1.90 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:58.088617: step 101850, loss = 1.92 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:11:59.322020: step 101860, loss = 1.88 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:00.550432: step 101870, loss = 1.98 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:01.764144: step 101880, loss = 1.93 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:02.974503: step 101890, loss = 1.91 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:04.181884: step 101900, loss = 1.93 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:05.416404: step 101910, loss = 2.12 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:06.639117: step 101920, loss = 1.88 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:07.853368: step 101930, loss = 1.97 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:09.063254: step 101940, loss = 2.04 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:10.281856: step 101950, loss = 1.83 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:11.511610: step 101960, loss = 1.74 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:12.807157: step 101970, loss = 1.73 (988.0 examples/sec; 0.130 sec/batch)
2017-05-05 01:12:13.880200: step 101980, loss = 1.91 (1192.9 examples/sec; 0.107 sec/batch)
2017-05-05 01:12:15.107677: step 101990, loss = 2.14 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:16.331450: step 102000, loss = 1.84 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:17.533143: step 102010, loss = 2.07 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:12:18.771731: step 102020, loss = 1.98 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-05 01:12:19.997613: step 102030, loss = 1.72 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:21.223420: step 102040, loss = 1.79 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:22.446531: step 102050, loss = 2.09 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:23.677584: step 102060, loss = 2.08 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:24.894458: step 102070, loss = 1.95 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:26.106350: step 102080, loss = 1.93 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:27.320058: step 102090, loss = 1.86 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:28.519861: step 102100, loss = 1.71 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:12:29.733975: step 102110, loss = 1.94 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:30.966406: step 102120, loss = 2.03 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:32.193711: step 102130, loss = 1.82 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:33.427797: step 102140, loss = 1.95 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:34.638472: step 102150, loss = 1.93 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:35.851217: step 102160, loss = 1.98 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:37.025393: step 102170, loss = 1.94 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:12:38.224731: step 102180, loss = 1.87 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:12:39.435409: step 102190, loss = 1.88 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:40.647354: step 102200, loss = 1.90 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:41.863057: step 102210, loss = 2.03 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:43.096439: step 102220, loss = 1.86 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:44.318866: step 102230, loss = 1.93 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:45.536965: step 102240, loss = 2.06 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:46.743773: step 102250, loss = 1.94 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:47.962075: step 102260, loss = 1.85 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:49.192082: step 102270, loss = 1.87 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:50.386151: step 102280, loss = 1.91 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:12:51.607014: step 102290, loss = 1.81 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:52.837325: step 102300, loss = 2.00 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:54.045825: step 102310, loss = 1.64 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:55.259612: step 102320, loss = 2.00 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:56.474433: step 102330, loss = 1.98 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:57.693622: step 102340, loss = 1.83 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:58.916977: step 102350, loss = 1.99 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:00.133607: step 102360, loss = 1.75 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:01.340415: step 102370, loss = 1.83 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:13:02.546062: step 102380, loss = 1.87 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:13:03.767024: step 102390, loss = 1.99 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:04.980336: step 102400, loss = 1.94 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:13:06.181508: step 102410, loss = 2.02 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:13:07.407778: step 102420, loss = 2.13 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:13:08.627167: step 102430, loss = 1.84 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:09.845545: step 102440, loss = 2.00 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:11.068346: step 102450, loss = 1.95 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:12.291999: step 102460, loss = 1.93 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:13.501105: step 102470, loss = 2.00 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:13:14.719683: step 102480, loss = 1.91 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:15.962411: step 102490, loss = 1.81 (1030.0 examples/sec; 0.124 sec/batch)
2017-05-05 01:13:17.190613: step 102500, loss = 1.85 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:13:18.381348: step 102510, loss = 2.00 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:13:19.599426: step 102520, loss = 1.79 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:20.820575: step 102530, loss = 1.81 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:22.028623: step 102540, loss = 2.02 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:13:23.243494: step 102550, loss = 1.94 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:13:24.448805: step 102560, loss = 1.87 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:13:25.623143: step 102570, loss = 1.90 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:13:26.844724: step 102580, loss = 1.83 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:28.066994: step 102590, loss = 1.99 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:29.287708: step 102600, loss = 1.75 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:30.495608: step 102610, loss = 1.92 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:13:31.718176: step 102620, loss = 2.18 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:32.945397: step 102630, loss = 1.88 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:13:34.132366: step 102640, loss = 1.78 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:13:35.364870: step 102650, loss = 1.90 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:13:36.584211: step 102660, loss = 1.99 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:37.798646: step 102670, loss = 1.97 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:13:39.022053: step 102680, loss = 1.84 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:40.231498: step 102690, loss = 1.83 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:13:41.453022: step 102700, loss = 1.90 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:42.671336: step 102710, loss = 1.97 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:43.900372: step 102720, loss = 1.95 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:13:45.125599: step 102730, loss = 1.86 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:13:46.319495: step 102740, loss = 1.90 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:13:47.546800: step 102750, loss = 1.91 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:13:48.771439: step 102760, loss = 1.86 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:49.933387: step 102770, loss = 2.08 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:13:51.160998: step 102780, loss = 2.10 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:13:52.376561: step 102790, loss = 1.96 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:53.582895: step 102800, loss = 1.92 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:13:54.802668: step 102810, loss = 1.95 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:56.044867: step 102820, loss = 1.87 (1030.4 examples/sec; 0.124 sec/batch)
2017-05-05 01:13:57.260536: step 102830, loss = 2.05 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:58.476404: step 102840, loss = 2.03 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:59.693226: step 102850, loss = 1.98 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:14:00.906354: step 102860, loss = 1.95 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:14:02.113658: step 102870, loss = 1.91 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:14:03.340301: step 102880, loss = 1.92 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:14:04.556221: step 102890, loss = 1.85 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:14:05.762261: step 102900, loss = 1.94 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:14:06.977301: step 102910, loss = 2.08 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:14:08.201536: step 102920, loss = 1.92 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:14:09.424104: step 102930, loss = 2.03 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:14:10.636397: step 102940, loss = 1.93 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:14:11.842188: step 102950, loss = 1.82 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:14:13.136284: step 102960, loss = 1.99 (989.1 examples/sec; 0.129 sec/batch)
2017-05-05 01:14:14.234826: step 102970, loss = 1.83 (1165.2 examples/sec; 0.110 sec/batch)
2017-05-05 01:14:15.434073: step 102980, loss = 1.78 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:14:16.624705: step 102990, loss = 1.87 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:14:17.817391: step 103000, loss = 2.01 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:14:19.001695: step 103010, loss = 1.92 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:20.190955: step 103020, loss = 1.98 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:14:21.376781: step 103030, loss = 1.93 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:14:22.554315: step 103040, loss = 1.92 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:23.780313: step 103050, loss = 1.95 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:14:24.997983: step 103060, loss = 1.85 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:14:26.203735: step 103070, loss = 1.91 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:14:27.415436: step 103080, loss = 2.06 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:14:28.625315: step 103090, loss = 1.96 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:14:29.826883: step 103100, loss = 1.88 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:14:31.043782: step 103110, loss = 1.86 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:14:32.232874: step 103120, loss = 2.02 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:14:33.421764: step 103130, loss = 1.93 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:14:34.603151: step 103140, loss = 1.81 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:35.782540: step 103150, loss = 1.90 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:36.972092: step 103160, loss = 1.92 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:14:38.141545: step 103170, loss = 1.83 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:14:39.316880: step 103180, loss = 1.69 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:40.499132: step 103190, loss = 1.92 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:41.675336: step 103200, loss = 2.04 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:42.853614: step 103210, loss = 1.95 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:44.039629: step 103220, loss = 1.88 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:14:45.208015: step 103230, loss = 1.88 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:14:46.371501: step 103240, loss = 2.04 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:14:47.544593: step 103250, loss = 1.87 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:14:48.727098: step 103260, loss = 1.95 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:49.892004: step 103270, loss = 2.03 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:14:51.069437: step 103280, loss = 1.94 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:52.242275: step 103290, loss = 1.97 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:14:53.414496: step 103300, loss = 2.01 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:14:54.607075: step 103310, loss = 1.93 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:14:55.782106: step 103320, loss = 1.94 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:56.971506: step 103330, loss = 2.03 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:14:58.159025: step 103340, loss = 1.77 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:14:59.349366: step 103350, loss = 1.89 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:15:00.541469: step 103360, loss = 1.86 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:15:01.701120: step 103370, loss = 1.77 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:15:02.913375: step 103380, loss = 1.82 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:15:04.107363: step 103390, loss = 1.86 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:15:05.287614: step 103400, loss = 1.91 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:06.476456: step 103410, loss = 2.02 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:15:07.684830: step 103420, loss = 1.83 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:15:08.879472: step 103430, loss = 1.83 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:15:10.075683: step 103440, loss = 1.85 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:15:11.264281: step 103450, loss = 1.81 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:15:12.445274: step 103460, loss = 1.93 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:13.611018: step 103470, loss = 1.88 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:14.780366: step 103480, loss = 1.75 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:15.962037: step 103490, loss = 1.86 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:17.143121: step 103500, loss = 2.02 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:18.303703: step 103510, loss = 1.79 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:15:19.497386: step 103520, loss = 2.03 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:15:20.679828: step 103530, loss = 1.88 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:21.850565: step 103540, loss = 1.94 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:23.054035: step 103550, loss = 1.87 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:15:24.226676: step 103560, loss = 2.05 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:25.394003: step 103570, loss = 1.78 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:26.542560: step 103580, loss = 1.88 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:15:27.711898: step 103590, loss = 1.91 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:28.886025: step 103600, loss = 1.90 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:30.050357: step 103610, loss = 1.78 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:15:31.229173: step 103620, loss = 1.92 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:32.399715: step 103630, loss = 1.76 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:33.561206: step 103640, loss = 1.73 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:15:34.736331: step 103650, loss = 1.99 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:35.895793: step 103660, loss = 1.81 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:15:37.063099: step 103670, loss = 1.83 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:38.234492: step 103680, loss = 1.89 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:39.406887: step 103690, loss = 1.88 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:40.576670: step 103700, loss = 2.07 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:41.755553: step 103710, loss = 1.92 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:42.937042: step 103720, loss = 2.06 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:44.087332: step 103730, loss = 1.88 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:15:45.256158: step 103740, loss = 1.87 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:46.415854: step 103750, loss = 2.32 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:15:47.580867: step 103760, loss = 1.87 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:48.772906: step 103770, loss = 1.88 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:15:49.968632: step 103780, loss = 1.86 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:15:51.163302: step 103790, loss = 1.88 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:15:52.377448: step 103800, loss = 1.89 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:15:53.600956: step 103810, loss = 2.07 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:15:54.811329: step 103820, loss = 1.80 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:15:56.018491: step 103830, loss = 2.13 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:15:57.236126: step 103840, loss = 1.82 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:15:58.444448: step 103850, loss = 1.93 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:15:59.642297: step 103860, loss = 1.85 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:16:00.825196: step 103870, loss = 1.85 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:02.012450: step 103880, loss = 1.75 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:16:03.210794: step 103890, loss = 1.94 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:16:04.393419: step 103900, loss = 1.83 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:05.552952: step 103910, loss = 1.83 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:16:06.728905: step 103920, loss = 1.99 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:07.923745: step 103930, loss = 1.88 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:16:09.099041: step 103940, loss = 2.01 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:10.367862: step 103950, loss = 1.87 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-05 01:16:11.406749: step 103960, loss = 1.86 (1232.1 examples/sec; 0.104 sec/batch)
2017-05-05 01:16:12.582130: step 103970, loss = 1.93 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:13.742630: step 103980, loss = 1.85 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:16:14.904074: step 103990, loss = 1.93 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:16:16.070326: step 104000, loss = 1.93 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:17.242145: step 104010, loss = 1.88 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:18.410484: step 104020, loss = 1.96 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:19.585162: step 104030, loss = 1.77 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:20.744988: step 104040, loss = 1.88 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:16:21.911914: step 104050, loss = 1.94 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:23.079397: step 104060, loss = 1.84 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:24.241161: step 104070, loss = 1.85 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:16:25.399385: step 104080, loss = 2.12 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:16:26.579971: step 104090, loss = 1.90 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:27.760409: step 104100, loss = 1.90 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:28.946987: step 104110, loss = 1.91 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:16:30.125471: step 104120, loss = 2.06 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:31.306584: step 104130, loss = 1.86 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:32.487710: step 104140, loss = 1.90 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:33.667043: step 104150, loss = 1.99 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:34.864297: step 104160, loss = 1.86 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:16:36.054472: step 104170, loss = 1.90 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:16:37.239425: step 104180, loss = 1.96 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:38.402920: step 104190, loss = 1.96 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:16:39.570364: step 104200, loss = 1.89 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:40.743495: step 104210, loss = 1.89 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:41.909614: step 104220, loss = 1.92 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:43.096112: step 104230, loss = 2.09 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:16:44.266502: step 104240, loss = 2.06 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:45.420320: step 104250, loss = 1.82 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:16:46.585040: step 104260, loss = 1.87 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:16:47.744940: step 104270, loss = 1.96 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:16:48.898572: step 104280, loss = 1.94 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:16:50.070996: step 104290, loss = 1.91 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:51.239200: step 104300, loss = 2.07 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:52.420700: step 104310, loss = 1.90 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:53.577763: step 104320, loss = 1.87 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:16:54.758411: step 104330, loss = 1.93 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:55.951515: step 104340, loss = 1.85 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:16:57.136180: step 104350, loss = 1.93 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:58.326575: step 104360, loss = 2.06 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:16:59.532998: step 104370, loss = 1.91 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:17:00.730018: step 104380, loss = 1.84 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:17:01.905331: step 104390, loss = 1.92 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:03.095116: step 104400, loss = 1.93 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:04.281304: step 104410, loss = 1.84 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:05.465957: step 104420, loss = 1.87 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:06.656451: step 104430, loss = 1.97 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:07.875515: step 104440, loss = 1.86 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:17:09.106351: step 104450, loss = 1.77 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:17:10.299153: step 104460, loss = 1.95 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:11.485918: step 104470, loss = 1.91 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:12.667763: step 104480, loss = 2.09 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:13.854361: step 104490, loss = 2.01 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:15.056001: step 104500, loss = 1.92 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:17:16.245360: step 104510, loss = 1.88 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:17.433313: step 104520, loss = 1.95 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:18.608837: step 104530, loss = 2.03 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:19.801389: step 104540, loss = 1.91 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:20.970605: step 104550, loss = 1.90 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:22.150123: step 104560, loss = 1.89 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:23.332648: step 104570, loss = 2.05 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:24.525153: step 104580, loss = 1.92 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:25.691123: step 104590, loss = 1.88 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:26.861540: step 104600, loss = 1.82 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:28.024557: step 104610, loss = 1.93 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:29.179470: step 104620, loss = 1.97 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:17:30.331332: step 104630, loss = 1.95 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:17:31.485288: step 104640, loss = 2.00 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:17:32.656426: step 104650, loss = 1.85 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:33.812308: step 104660, loss = 1.93 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:34.989863: step 104670, loss = 1.98 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:36.157727: step 104680, loss = 1.87 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:37.313551: step 104690, loss = 2.04 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:38.480927: step 104700, loss = 1.91 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:39.663868: step 104710, loss = 1.96 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:40.842398: step 104720, loss = 1.96 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:42.007038: step 104730, loss = 1.81 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:43.184050: step 104740, loss = 1.84 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:44.343526: step 104750, loss = 1.94 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:45.545858: step 104760, loss = 1.87 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:17:46.702379: step 104770, loss = 2.05 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:47.850785: step 104780, loss = 1.86 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:17:49.018020: step 104790, loss = 1.79 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:50.183199: step 104800, loss = 1.94 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:51.329349: step 104810, loss = 1.86 (1116.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:17:52.502048: step 104820, loss = 1.98 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:53.658078: step 104830, loss = 1.73 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:54.821906: step 104840, loss = 1.84 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:55.993691: step 104850, loss = 1.86 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:57.177462: step 104860, loss = 1.88 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:58.334568: step 104870, loss = 1.77 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:59.513265: step 104880, loss = 1.87 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:18:00.691992: step 104890, loss = 1.99 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:18:01.861746: step 104900, loss = 1.91 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:03.029425: step 104910, loss = 1.98 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:04.194047: step 104920, loss = 1.95 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:05.367576: step 104930, loss = 1.85 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:06.666978: step 104940, loss = 1.97 (985.1 examples/sec; 0.130 sec/batch)
2017-05-05 01:18:07.678897: step 104950, loss = 1.95 (1264.9 examples/sec; 0.101 sec/batch)
2017-05-05 01:18:08.849807: step 104960, loss = 1.92 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:10.006710: step 104970, loss = 1.91 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:11.165208: step 104980, loss = 2.00 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:12.323417: step 104990, loss = 2.09 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:13.467347: step 105000, loss = 2.02 (1119.0 examples/sec; 0.114 sec/batch)
2017-05-05 01:18:14.654986: step 105010, loss = 1.90 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:18:15.827583: step 105020, loss = 2.07 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:16.983823: step 105030, loss = 1.91 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:18.168776: step 105040, loss = 1.98 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:18:19.327900: step 105050, loss = 1.78 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:20.496860: step 105060, loss = 2.01 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:21.658449: step 105070, loss = 1.95 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:22.829883: step 105080, loss = 1.90 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:23.995158: step 105090, loss = 1.89 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:25.177147: step 105100, loss = 1.85 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:18:26.337643: step 105110, loss = 1.88 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:27.492720: step 105120, loss = 2.06 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:28.691970: step 105130, loss = 1.99 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:18:29.855291: step 105140, loss = 1.89 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:31.030443: step 105150, loss = 1.77 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:18:32.199077: step 105160, loss = 1.96 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:33.363195: step 105170, loss = 1.92 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:34.512582: step 105180, loss = 1.93 (1113.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:18:35.670598: step 105190, loss = 1.73 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:36.809265: step 105200, loss = 2.04 (1124.1 examples/sec; 0.114 sec/batch)
2017-05-05 01:18:37.975593: step 105210, loss = 1.95 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:39.131127: step 105220, loss = 2.02 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:40.301195: step 105230, loss = 1.90 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:41.455031: step 105240, loss = 1.93 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:18:42.617020: step 105250, loss = 2.01 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:43.771773: step 105260, loss = 1.80 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:18:44.937139: step 105270, loss = 1.85 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:46.091274: step 105280, loss = 2.00 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:18:47.252856: step 105290, loss = 1.83 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:48.422469: step 105300, loss = 2.02 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:49.568913: step 105310, loss = 1.96 (1116.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:18:50.739424: step 105320, loss = 2.02 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:51.891958: step 105330, loss = 1.84 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:18:53.080808: step 105340, loss = 1.99 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:18:54.355142: step 105350, loss = 1.93 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-05 01:18:55.450582: step 105360, loss = 1.94 (1168.5 examples/sec; 0.110 sec/batch)
2017-05-05 01:18:56.612714: step 105370, loss = 1.82 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:57.767089: step 105380, loss = 1.98 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:18:58.947734: step 105390, loss = 1.86 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:00.112038: step 105400, loss = 2.00 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:01.267556: step 105410, loss = 1.86 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:02.404821: step 105420, loss = 1.91 (1125.5 examples/sec; 0.114 sec/batch)
2017-05-05 01:19:03.588215: step 105430, loss = 1.92 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:04.742795: step 105440, loss = 2.03 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:19:05.897532: step 105450, loss = 1.93 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:19:07.067725: step 105460, loss = 1.89 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:08.223134: step 105470, loss = 1.90 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:09.408955: step 105480, loss = 1.91 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:19:10.552316: step 105490, loss = 1.86 (1119.5 examples/sec; 0.114 sec/batch)
2017-05-05 01:19:11.711466: step 105500, loss = 1.84 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:12.875951: step 105510, loss = 1.74 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:14.053187: step 105520, loss = 1.91 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:15.224556: step 105530, loss = 2.14 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:16.393266: step 105540, loss = 1.83 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:17.557952: step 105550, loss = 1.86 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:18.709691: step 105560, loss = 2.00 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:19:19.891835: step 105570, loss = 1.82 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:21.067119: step 105580, loss = 1.83 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:22.213665: step 105590, loss = 2.09 (1116.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:19:23.393525: step 105600, loss = 1.90 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:24.569826: step 105610, loss = 2.08 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:25.750067: step 105620, loss = 1.87 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:26.912349: step 105630, loss = 1.91 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:28.075254: step 105640, loss = 1.91 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:29.225652: step 105650, loss = 2.00 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:19:30.403455: step 105660, loss = 1.97 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:31.543839: step 105670, loss = 2.00 (1122.4 examples/sec; 0.114 sec/batch)
2017-05-05 01:19:32.715891: step 105680, loss = 1.75 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:33.854456: step 105690, loss = 1.98 (1124.2 examples/sec; 0.114 sec/batch)
2017-05-05 01:19:35.017435: step 105700, loss = 1.97 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:36.203416: step 105710, loss = 2.04 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:19:37.364653: step 105720, loss = 1.98 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:38.518587: step 105730, loss = 1.85 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:19:39.706303: step 105740, loss = 1.93 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:19:40.855015: step 105750, loss = 1.98 (1114.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:19:42.011973: step 105760, loss = 1.84 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:43.175875: step 105770, loss = 1.92 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:44.323671: step 105780, loss = 2.12 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:19:45.481171: step 105790, loss = 1.79 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:46.666157: step 105800, loss = 2.07 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:47.824344: step 105810, loss = 1.88 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:49.008540: step 105820, loss = 1.77 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:50.172503: step 105830, loss = 1.93 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:51.353140: step 105840, loss = 1.91 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:52.518519: step 105850, loss = 1.82 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:53.660692: step 105860, loss = 2.09 (1120.7 examples/sec; 0.114 sec/batch)
2017-05-05 01:19:54.823825: step 105870, loss = 1.85 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:55.995660: step 105880, loss = 2.00 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:57.172820: step 105890, loss = 1.94 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:58.325311: step 105900, loss = 2.00 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:19:59.489019: step 105910, loss = 1.94 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:20:00.660910: step 105920, loss = 1.97 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:20:01.941366: step 105930, loss = 1.92 (999.6 examples/sec; 0.128 sec/batch)
2017-05-05 01:20:03.033131: step 105940, loss = 1.81 (1172.4 examples/sec; 0.109 sec/batch)
2017-05-05 01:20:04.230347: step 105950, loss = 1.82 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:05.405343: step 105960, loss = 2.00 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:20:06.603469: step 105970, loss = 1.90 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:07.801942: step 105980, loss = 1.98 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:08.984566: step 105990, loss = 2.08 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:10.178704: step 106000, loss = 1.90 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:11.395928: step 106010, loss = 1.74 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:20:12.622308: step 106020, loss = 1.83 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:20:13.805130: step 106030, loss = 1.95 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:14.994627: step 106040, loss = 1.93 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:16.192700: step 106050, loss = 2.02 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:17.379438: step 106060, loss = 2.12 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:18.570761: step 106070, loss = 1.84 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:19.774988: step 106080, loss = 1.95 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:20.956745: step 106090, loss = 1.98 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:22.129519: step 106100, loss = 1.99 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:20:23.334940: step 106110, loss = 1.82 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:20:24.534675: step 106120, loss = 1.75 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:25.841075: step 106130, loss = 1.85 (979.8 examples/sec; 0.131 sec/batch)
2017-05-05 01:20:26.934625: step 106140, loss = 1.92 (1170.5 examples/sec; 0.109 sec/batch)
2017-05-05 01:20:28.151943: step 106150, loss = 1.75 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:20:29.349642: step 106160, loss = 1.93 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:30.535846: step 106170, loss = 2.07 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:31.724856: step 106180, loss = 1.97 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:32.921311: step 106190, loss = 1.98 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:34.111804: step 106200, loss = 1.87 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:35.296710: step 106210, loss = 1.85 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:36.494273: step 106220, loss = 2.00 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:37.696171: step 106230, loss = 1.95 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:38.892471: step 106240, loss = 1.75 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:40.061347: step 106250, loss = 1.89 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:20:41.256080: step 106260, loss = 1.71 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:42.411090: step 106270, loss = 1.87 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:20:43.592318: step 106280, loss = 2.03 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:44.775872: step 106290, loss = 1.89 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:45.926655: step 106300, loss = 1.94 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:20:47.098666: step 106310, loss = 1.86 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:20:48.277656: step 106320, loss = 1.92 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:49.465604: step 106330, loss = 1.92 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:50.621128: step 106340, loss = 2.00 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:20:51.792295: step 106350, loss = 1.85 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:20:52.968538: step 106360, loss = 1.96 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:54.127466: step 106370, loss = 1.90 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:20:55.296439: step 106380, loss = 1.71 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:20:56.463179: step 106390, loss = 1.82 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:20:57.612108: step 106400, loss = 2.01 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:20:58.783460: step 106410, loss = 1.93 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:20:59.953924: step 106420, loss = 2.00 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:01.126465: step 106430, loss = 1.91 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:02.268743: step 106440, loss = 1.83 (1120.6 examples/sec; 0.114 sec/batch)
2017-05-05 01:21:03.431662: step 106450, loss = 1.86 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:21:04.608539: step 106460, loss = 1.96 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:05.768441: step 106470, loss = 1.82 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:21:06.940604: step 106480, loss = 1.82 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:08.126746: step 106490, loss = 1.87 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:21:09.311678: step 106500, loss = 1.92 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:10.474706: step 106510, loss = 1.99 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:21:11.657930: step 106520, loss = 1.74 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:12.849285: step 106530, loss = 1.98 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:21:14.025744: step 106540, loss = 1.96 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:15.229897: step 106550, loss = 1.85 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:21:16.422682: step 106560, loss = 1.90 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:21:17.620776: step 106570, loss = 1.98 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:21:18.806733: step 106580, loss = 1.80 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:21:20.002834: step 106590, loss = 1.92 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:21:21.212326: step 106600, loss = 1.85 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:21:22.419715: step 106610, loss = 1.83 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:21:23.627827: step 106620, loss = 1.93 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:21:24.830690: step 106630, loss = 1.84 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:21:26.005847: step 106640, loss = 1.93 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:27.201478: step 106650, loss = 1.91 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:21:28.395361: step 106660, loss = 2.11 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:21:29.578498: step 106670, loss = 1.78 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:30.754006: step 106680, loss = 1.91 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:31.930113: step 106690, loss = 1.95 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:33.108572: step 106700, loss = 1.86 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:34.291989: step 106710, loss = 1.93 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:35.458379: step 106720, loss = 2.10 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:36.620954: step 106730, loss = 1.97 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:21:37.787782: step 106740, loss = 2.04 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:38.969789: step 106750, loss = 2.08 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:40.155299: step 106760, loss = 2.01 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:21:41.315001: step 106770, loss = 1.94 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:21:42.475442: step 106780, loss = 2.09 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:21:43.627849: step 106790, loss = 1.91 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:21:44.806798: step 106800, loss = 1.78 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:45.958257: step 106810, loss = 1.87 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:21:47.112985: step 106820, loss = 1.79 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:21:48.284400: step 106830, loss = 1.94 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:49.443143: step 106840, loss = 1.82 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:21:50.611297: step 106850, loss = 1.88 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:51.767431: step 106860, loss = 2.06 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:21:52.945367: step 106870, loss = 2.05 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:54.109061: step 106880, loss = 1.96 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:21:55.279079: step 106890, loss = 1.93 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:56.452324: step 106900, loss = 1.88 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:57.630011: step 106910, loss = 1.85 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:58.904158: step 106920, loss = 1.85 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-05 01:21:59.974831: step 106930, loss = 1.84 (1195.5 examples/sec; 0.107 sec/batch)
2017-05-05 01:22:01.149191: step 106940, loss = 1.93 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:02.305540: step 106950, loss = 1.90 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:22:03.495628: step 106960, loss = 1.89 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:22:04.655558: step 106970, loss = 1.96 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:22:05.815666: step 106980, loss = 2.02 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:22:07.000082: step 106990, loss = 1.85 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:08.169919: step 107000, loss = 1.87 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:09.344538: step 107010, loss = 1.90 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:10.514574: step 107020, loss = 1.90 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:11.687585: step 107030, loss = 2.00 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:12.863540: step 107040, loss = 1.90 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:14.038702: step 107050, loss = 2.04 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:15.237970: step 107060, loss = 2.11 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:22:16.444930: step 107070, loss = 1.91 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:22:17.612473: step 107080, loss = 1.86 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:18.802490: step 107090, loss = 2.04 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:22:20.012326: step 107100, loss = 2.07 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:22:21.204435: step 107110, loss = 2.19 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:22:22.385676: step 107120, loss = 1.99 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:23.585573: step 107130, loss = 1.85 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:22:24.762224: step 107140, loss = 1.97 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:25.924627: step 107150, loss = 1.88 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:22:27.099600: step 107160, loss = 1.88 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:28.273434: step 107170, loss = 2.00 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:29.439145: step 107180, loss = 1.85 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:30.608734: step 107190, loss = 1.86 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:31.774809: step 107200, loss = 2.01 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:32.945672: step 107210, loss = 2.01 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:34.104035: step 107220, loss = 2.10 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:22:35.270544: step 107230, loss = 1.82 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:36.427691: step 107240, loss = 1.76 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:22:37.593716: step 107250, loss = 1.92 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:38.755494: step 107260, loss = 1.91 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:22:39.927563: step 107270, loss = 1.96 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:41.108048: step 107280, loss = 1.93 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:42.277285: step 107290, loss = 1.83 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:43.461170: step 107300, loss = 1.93 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:44.642823: step 107310, loss = 1.93 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:45.824722: step 107320, loss = 1.77 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:47.021441: step 107330, loss = 1.77 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:22:48.227445: step 107340, loss = 2.08 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:22:49.431801: step 107350, loss = 2.11 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:22:50.632013: step 107360, loss = 1.86 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:22:51.838388: step 107370, loss = 1.98 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:22:53.046414: step 107380, loss = 1.87 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:22:54.237440: step 107390, loss = 1.95 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:22:55.432318: step 107400, loss = 1.83 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:22:56.602501: step 107410, loss = 1.98 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:57.785421: step 107420, loss = 2.05 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:58.987404: step 107430, loss = 1.99 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:23:00.192542: step 107440, loss = 1.95 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:23:01.354645: step 107450, loss = 1.91 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:02.529494: step 107460, loss = 2.02 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:03.728187: step 107470, loss = 2.02 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:23:04.934531: step 107480, loss = 1.86 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:23:06.118199: step 107490, loss = 1.82 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:07.292896: step 107500, loss = 1.85 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:08.476040: step 107510, loss = 2.13 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:09.643732: step 107520, loss = 1.87 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:10.826839: step 107530, loss = 1.96 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:11.999124: step 107540, loss = 1.83 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:13.163408: step 107550, loss = 1.99 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:14.315504: step 107560, loss = 1.94 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:23:15.501825: step 107570, loss = 1.92 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:23:16.694602: step 107580, loss = 1.98 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:23:17.850368: step 107590, loss = 1.92 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:19.016800: step 107600, loss = 1.99 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:20.199225: step 107610, loss = 1.94 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:21.368497: step 107620, loss = 1.82 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:22.532696: step 107630, loss = 1.88 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:23.703828: step 107640, loss = 1.89 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:24.873867: step 107650, loss = 1.97 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:26.016880: step 107660, loss = 1.82 (1119.8 examples/sec; 0.114 sec/batch)
2017-05-05 01:23:27.191418: step 107670, loss = 1.93 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:28.369588: step 107680, loss = 1.94 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:29.539763: step 107690, loss = 1.93 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:30.694882: step 107700, loss = 1.88 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:31.862134: step 107710, loss = 2.00 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:33.032742: step 107720, loss = 2.09 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:34.167548: step 107730, loss = 1.87 (1128.0 examples/sec; 0.113 sec/batch)
2017-05-05 01:23:35.334154: step 107740, loss = 1.91 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:36.506258: step 107750, loss = 2.00 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:37.655767: step 107760, loss = 1.93 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:23:38.823809: step 107770, loss = 2.11 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:39.976147: step 107780, loss = 1.79 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:23:41.157364: step 107790, loss = 1.91 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:42.324820: step 107800, loss = 1.91 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:43.491686: step 107810, loss = 1.73 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:44.672855: step 107820, loss = 1.83 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:45.807853: step 107830, loss = 1.94 (1127.7 examples/sec; 0.114 sec/batch)
2017-05-05 01:23:46.966647: step 107840, loss = 1.86 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:48.126355: step 107850, loss = 1.92 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:49.301083: step 107860, loss = 1.86 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:50.445865: step 107870, loss = 1.76 (1118.1 examples/sec; 0.114 sec/batch)
2017-05-05 01:23:51.621325: step 107880, loss = 1.80 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:52.784637: step 107890, loss = 1.86 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:53.962491: step 107900, loss = 1.91 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:55.228465: step 107910, loss = 2.17 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-05 01:23:56.296486: step 107920, loss = 2.12 (1198.5 examples/sec; 0.107 sec/batch)
2017-05-05 01:23:57.465622: step 107930, loss = 1.68 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:58.643223: step 107940, loss = 1.75 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:59.821211: step 107950, loss = 1.99 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:24:00.996396: step 107960, loss = 1.94 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:24:02.158970: step 107970, loss = 2.04 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:24:03.337813: step 107980, loss = 1.86 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:24:04.508510: step 107990, loss = 2.15 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:05.670253: step 108000, loss = 2.00 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:24:06.834909: step 108010, loss = 1.88 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:24:07.987564: step 108020, loss = 1.83 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:24:09.159023: step 108030, loss = 2.03 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:10.332191: step 108040, loss = 1.91 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:11.526176: step 108050, loss = 2.07 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:12.707797: step 108060, loss = 1.88 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:24:13.861757: step 108070, loss = 1.89 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:24:15.036595: step 108080, loss = 2.00 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:16.201616: step 108090, loss = 1.74 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:17.359066: step 108100, loss = 1.95 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:24:18.527214: step 108110, loss = 1.98 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:19.675310: step 108120, loss = 2.00 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:24:20.864069: step 108130, loss = 1.87 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:22.038103: step 108140, loss = 2.01 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:23.229449: step 108150, loss = 2.06 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:24.397533: step 108160, loss = 1.84 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:25.599661: step 108170, loss = 2.02 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:24:26.766765: step 108180, loss = 1.80 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:27.946179: step 108190, loss = 1.84 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:24:29.120572: step 108200, loss = 1.86 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:30.286508: step 108210, loss = 2.05 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:31.474322: step 108220, loss = 1.91 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:32.654958: step 108230, loss = 1.89 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:24:33.821528: step 108240, loss = 1.92 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:35.040045: step 108250, loss = 2.04 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:24:36.226547: step 108260, loss = 1.79 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:37.444773: step 108270, loss = 1.87 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:24:38.656553: step 108280, loss = 1.77 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:24:39.843947: step 108290, loss = 1.86 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:41.073182: step 108300, loss = 2.06 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-05 01:24:42.258498: step 108310, loss = 2.08 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:43.479505: step 108320, loss = 1.87 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:24:44.706913: step 108330, loss = 1.93 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:24:45.900915: step 108340, loss = 1.96 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:47.126917: step 108350, loss = 1.82 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:24:48.339435: step 108360, loss = 1.80 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:24:49.545580: step 108370, loss = 1.99 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:24:50.760707: step 108380, loss = 2.00 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:24:51.978423: step 108390, loss = 2.01 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:24:53.223657: step 108400, loss = 1.90 (1027.9 examples/sec; 0.125 sec/batch)
2017-05-05 01:24:54.424167: step 108410, loss = 1.92 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:24:55.623881: step 108420, loss = 1.96 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:24:56.807135: step 108430, loss = 1.88 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:24:58.013034: step 108440, loss = 2.00 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:24:59.227612: step 108450, loss = 1.94 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:25:00.433081: step 108460, loss = 1.82 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:25:01.648532: step 108470, loss = 1.96 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:25:02.868054: step 108480, loss = 1.81 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:25:04.073115: step 108490, loss = 1.85 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:25:05.265819: step 108500, loss = 2.01 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:25:06.444337: step 108510, loss = 1.75 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:25:07.647900: step 108520, loss = 1.90 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:25:08.856814: step 108530, loss = 1.98 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:25:10.047381: step 108540, loss = 1.90 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:25:11.261119: step 108550, loss = 1.80 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:25:12.450117: step 108560, loss = 1.86 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:25:13.624269: step 108570, loss = 1.95 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:14.805535: step 108580, loss = 1.87 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:25:15.967727: step 108590, loss = 1.83 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:17.142402: step 108600, loss = 1.95 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:18.298940: step 108610, loss = 1.94 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:19.499025: step 108620, loss = 2.03 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:25:20.675670: step 108630, loss = 1.85 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:25:21.839989: step 108640, loss = 1.95 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:22.996041: step 108650, loss = 1.95 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:24.164931: step 108660, loss = 2.17 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:25.362173: step 108670, loss = 1.89 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:25:26.516019: step 108680, loss = 1.88 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:25:27.699432: step 108690, loss = 1.90 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:25:28.886632: step 108700, loss = 1.99 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:25:30.041209: step 108710, loss = 1.74 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:25:31.206292: step 108720, loss = 1.83 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:32.350821: step 108730, loss = 2.00 (1118.4 examples/sec; 0.114 sec/batch)
2017-05-05 01:25:33.525103: step 108740, loss = 2.03 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:34.700553: step 108750, loss = 1.94 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:25:35.871938: step 108760, loss = 2.01 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:37.030425: step 108770, loss = 1.94 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:38.203733: step 108780, loss = 1.90 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:39.351657: step 108790, loss = 2.09 (1115.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:25:40.521214: step 108800, loss = 1.76 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:41.678007: step 108810, loss = 2.10 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:42.850111: step 108820, loss = 1.92 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:44.023878: step 108830, loss = 1.87 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:45.193222: step 108840, loss = 1.92 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:46.357852: step 108850, loss = 1.80 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:47.522415: step 108860, loss = 1.78 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:48.701657: step 108870, loss = 1.88 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:25:49.866698: step 108880, loss = 1.90 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:51.024466: step 108890, loss = 1.81 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:52.303134: step 108900, loss = 2.04 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-05 01:25:53.377025: step 108910, loss = 1.86 (1191.9 examples/sec; 0.107 sec/batch)
2017-05-05 01:25:54.532713: step 108920, loss = 1.75 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:55.698619: step 108930, loss = 1.87 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:56.874398: step 108940, loss = 1.97 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:25:58.045545: step 108950, loss = 1.99 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:59.205556: step 108960, loss = 1.92 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:00.376286: step 108970, loss = 1.83 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:01.546050: step 108980, loss = 1.89 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:02.712910: step 108990, loss = 1.79 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:03.881259: step 109000, loss = 2.08 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:05.052283: step 109010, loss = 1.81 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:06.208777: step 109020, loss = 1.77 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:07.379007: step 109030, loss = 2.03 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:08.547058: step 109040, loss = 1.90 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:09.708993: step 109050, loss = 1.85 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:10.884781: step 109060, loss = 1.93 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:12.065079: step 109070, loss = 1.95 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:13.220981: step 109080, loss = 1.86 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:14.392270: step 109090, loss = 1.80 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:15.568635: step 109100, loss = 1.85 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:16.720279: step 109110, loss = 1.90 (1111.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:26:17.888316: step 109120, loss = 1.90 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:19.030086: step 109130, loss = 1.82 (1121.1 examples/sec; 0.114 sec/batch)
2017-05-05 01:26:20.207062: step 109140, loss = 1.85 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:21.366948: step 109150, loss = 1.85 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:22.538737: step 109160, loss = 1.93 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:23.714781: step 109170, loss = 2.04 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:24.910192: step 109180, loss = 2.03 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:26:26.075723: step 109190, loss = 1.85 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:27.258174: step 109200, loss = 2.00 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:28.422449: step 109210, loss = 1.91 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:29.580937: step 109220, loss = 1.91 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:30.755739: step 109230, loss = 2.05 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:31.904633: step 109240, loss = 1.95 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:26:33.072050: step 109250, loss = 1.95 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:34.224758: step 109260, loss = 1.87 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:26:35.388279: step 109270, loss = 1.91 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:36.562451: step 109280, loss = 1.81 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:37.729096: step 109290, loss = 2.10 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:38.898809: step 109300, loss = 1.94 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:40.082405: step 109310, loss = 1.89 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:41.286029: step 109320, loss = 2.24 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:26:42.441589: step 109330, loss = 1.87 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:43.623468: step 109340, loss = 1.76 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:44.787295: step 109350, loss = 1.88 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:45.938938: step 109360, loss = 1.90 (1111.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:26:47.113535: step 109370, loss = 1.90 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:48.276548: step 109380, loss = 1.86 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:49.450875: step 109390, loss = 1.93 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:50.611006: step 109400, loss = 1.90 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:51.777247: step 109410, loss = 1.95 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:52.943817: step 109420, loss = 2.06 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:54.095119: step 109430, loss = 2.09 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:26:55.271265: step 109440, loss = 1.70 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:56.440794: step 109450, loss = 1.89 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:57.585782: step 109460, loss = 1.90 (1117.9 examples/sec; 0.114 sec/batch)
2017-05-05 01:26:58.735080: step 109470, loss = 1.81 (1113.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:26:59.883117: step 109480, loss = 1.95 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:27:01.045896: step 109490, loss = 1.93 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:02.203181: step 109500, loss = 1.95 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:03.390146: step 109510, loss = 1.84 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:27:04.562192: step 109520, loss = 1.89 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:27:05.720034: step 109530, loss = 1.84 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:06.878497: step 109540, loss = 1.96 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:08.057332: step 109550, loss = 1.79 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:09.227557: step 109560, loss = 1.91 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:27:10.370150: step 109570, loss = 1.91 (1120.3 examples/sec; 0.114 sec/batch)
2017-05-05 01:27:11.529297: step 109580, loss = 2.02 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:12.697674: step 109590, loss = 1.80 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:27:13.869891: step 109600, loss = 1.96 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:27:15.052355: step 109610, loss = 2.13 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:16.228401: step 109620, loss = 1.88 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:17.412280: step 109630, loss = 1.86 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:18.605909: step 109640, loss = 1.92 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:27:19.798014: step 109650, loss = 1.94 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:27:20.981449: step 109660, loss = 2.03 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:22.159658: step 109670, loss = 1.91 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:23.370274: step 109680, loss = 1.71 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:27:24.578068: step 109690, loss = 1.90 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:27:25.760331: step 109700, loss = 1.88 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:26.958600: step 109710, loss = 1.97 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:27:28.140223: step 109720, loss = 1.95 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:29.330463: step 109730, loss = 1.96 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:27:30.511333: step 109740, loss = 2.21 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:31.715372: step 109750, loss = 1.99 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:27:32.913837: step 109760, loss = 1.93 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:27:34.060014: step 109770, loss = 1.78 (1116.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:27:35.237963: step 109780, loss = 1.96 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:36.390090: step 109790, loss = 1.84 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:27:37.571922: step 109800, loss = 1.90 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:38.762672: step 109810, loss = 1.92 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:27:39.954500: step 109820, loss = 2.07 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:27:41.131649: step 109830, loss = 1.87 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:42.296068: step 109840, loss = 2.11 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:43.486989: step 109850, loss = 1.89 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:27:44.654833: step 109860, loss = 1.90 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:27:45.808339: step 109870, loss = 1.91 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:27:46.972708: step 109880, loss = 1.84 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:48.256347: step 109890, loss = 1.92 (997.2 examples/sec; 0.128 sec/batch)
2017-05-05 01:27:49.330764: step 109900, loss = 1.96 (1191.3 examples/sec; 0.107 sec/batch)
2017-05-05 01:27:50.479539: step 109910, loss = 1.79 (1114.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:27:51.659523: step 109920, loss = 1.95 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:52.824822: step 109930, loss = 1.94 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:27:53.968418: step 109940, loss = 1.89 (1119.3 examples/sec; 0.114 sec/batch)
2017-05-05 01:27:55.131942: step 109950, loss = 1.93 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:56.300471: step 109960, loss = 2.08 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:27:57.481436: step 109970, loss = 1.84 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:58.663381: step 109980, loss = 1.93 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:59.851352: step 109990, loss = 1.96 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:28:01.027061: step 110000, loss = 2.14 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:02.239859: step 110010, loss = 2.00 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:28:03.410060: step 110020, loss = 1.83 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:04.578786: step 110030, loss = 1.70 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:05.720942: step 110040, loss = 1.99 (1120.7 examples/sec; 0.114 sec/batch)
2017-05-05 01:28:06.890984: step 110050, loss = 1.95 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:08.071372: step 110060, loss = 1.90 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:09.234856: step 110070, loss = 2.02 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:10.382075: step 110080, loss = 2.25 (1115.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:28:11.546598: step 110090, loss = 1.94 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:12.710263: step 110100, loss = 1.97 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:13.886208: step 110110, loss = 1.77 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:15.045746: step 110120, loss = 2.02 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:16.207911: step 110130, loss = 2.01 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:17.360681: step 110140, loss = 2.08 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:28:18.512638: step 110150, loss = 1.72 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:28:19.687631: step 110160, loss = 1.80 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:20.850119: step 110170, loss = 1.91 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:22.008865: step 110180, loss = 1.98 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:23.191360: step 110190, loss = 1.84 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:24.363195: step 110200, loss = 1.73 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:25.517931: step 110210, loss = 1.94 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:28:26.678499: step 110220, loss = 1.84 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:27.855183: step 110230, loss = 2.11 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:29.028698: step 110240, loss = 1.94 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:30.207574: step 110250, loss = 1.92 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:31.391794: step 110260, loss = 1.79 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:32.595054: step 110270, loss = 1.88 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:28:33.764897: step 110280, loss = 1.79 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:34.960180: step 110290, loss = 1.83 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:28:36.129667: step 110300, loss = 1.88 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:37.294467: step 110310, loss = 2.02 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:38.467222: step 110320, loss = 1.95 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:39.651350: step 110330, loss = 1.90 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:40.849992: step 110340, loss = 1.90 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:28:42.035657: step 110350, loss = 1.91 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:28:43.224435: step 110360, loss = 1.86 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:28:44.415727: step 110370, loss = 1.87 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:28:45.618028: step 110380, loss = 1.89 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:28:46.825456: step 110390, loss = 1.95 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:28:48.021874: step 110400, loss = 1.87 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:28:49.217899: step 110410, loss = 1.87 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:28:50.395605: step 110420, loss = 1.84 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:51.610906: step 110430, loss = 1.90 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:28:52.796853: step 110440, loss = 1.90 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:28:53.973217: step 110450, loss = 2.09 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:55.181203: step 110460, loss = 1.85 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:28:56.338263: step 110470, loss = 1.97 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:57.503689: step 110480, loss = 1.90 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:58.678414: step 110490, loss = 2.05 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:59.840623: step 110500, loss = 2.05 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:29:01.003590: step 110510, loss = 2.06 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:29:02.174013: step 110520, loss = 1.96 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:29:03.351041: step 110530, loss = 1.70 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:29:04.533667: step 110540, loss = 2.16 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:29:05.680538: step 110550, loss = 1.75 (1116.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:29:06.850198: step 110560, loss = 1.96 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:29:08.031823: step 110570, loss = 1.89 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:29:09.179091: step 110580, loss = 1.99 (1115.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:29:10.336991: step 110590, loss = 1.83 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:29:11.508114: step 110600, loss = 1.95 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:29:12.670386: step 110610, loss = 1.98 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:29:13.828480: step 110620, loss = 1.79 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:29:14.989275: step 110630, loss = 1.88 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:29:16.146657: step 110640, loss = 1.76 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:29:17.311436: step 110650, loss = 2.01 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:29:18.476572: step 110660, loss = 1.83 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:29:19.644540: step 110670, loss = 1.93 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:29:20.819877: step 110680, loss = 1.84 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:29:22.010466: step 110690, loss = 1.88 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:29:23.186311: step 110700, loss = 1.91 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:29:24.356708: step 110710, loss = 2.00 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:29:25.534425: step 110720, loss = 1.99 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:29:26.730213: step 110730, loss = 2.05 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:29:27.953466: step 110740, loss = 1.90 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:29:29.147153: step 110750, loss = 2.05 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:29:30.335559: step 110760, loss = 1.78 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:29:31.556190: step 110770, loss = 1.91 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:29:32.752910: step 110780, loss = 1.97 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:29:33.946746: step 110790, loss = 2.03 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:29:35.164729: step 110800, loss = 1.91 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:29:36.354533: step 110810, loss = 2.09 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:29:37.563030: step 110820, loss = 1.85 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:29:38.744051: step 110830, loss = 1.99 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:29:39.958269: step 110840, loss = 1.85 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:29:41.170100: step 110850, loss = 1.87 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:29:42.373147: step 110860, loss = 1.90 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:29:43.601989: step 110870, loss = 1.85 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:29:44.907064: step 110880, loss = 1.97 (980.8 examples/sec; 0.131 sec/batch)
2017-05-05 01:29:45.997383: step 110890, loss = 2.05 (1174.0 examples/sec; 0.109 sec/batch)
2017-05-05 01:29:47.220080: step 110900, loss = 2.06 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:29:48.433719: step 110910, loss = 2.02 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:29:49.640852: step 110920, loss = 2.00 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:29:50.856361: step 110930, loss = 1.85 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:29:52.077371: step 110940, loss = 1.92 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:29:53.311562: step 110950, loss = 1.97 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:29:54.550976: step 110960, loss = 1.83 (1032.8 examples/sec; 0.124 sec/batch)
2017-05-05 01:29:55.797886: step 110970, loss = 1.81 (1026.5 examples/sec; 0.125 sec/batch)
2017-05-05 01:29:57.018511: step 110980, loss = 1.96 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:29:58.240445: step 110990, loss = 1.82 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:29:59.438555: step 111000, loss = 1.97 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:30:00.669056: step 111010, loss = 1.95 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:30:01.895119: step 111020, loss = 2.00 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:30:03.113367: step 111030, loss = 1.93 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:04.335897: step 111040, loss = 1.81 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:05.531366: step 111050, loss = 1.93 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:30:06.742147: step 111060, loss = 1.82 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:07.971123: step 111070, loss = 1.92 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:30:09.243741: step 111080, loss = 1.94 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-05 01:30:10.356441: step 111090, loss = 1.94 (1150.4 examples/sec; 0.111 sec/batch)
2017-05-05 01:30:11.559940: step 111100, loss = 1.89 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:30:12.793555: step 111110, loss = 1.80 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:30:13.998516: step 111120, loss = 1.85 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:30:15.236102: step 111130, loss = 1.89 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-05 01:30:16.448190: step 111140, loss = 1.93 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:17.637662: step 111150, loss = 1.79 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:30:18.865005: step 111160, loss = 1.89 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:30:20.084338: step 111170, loss = 1.92 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:21.297834: step 111180, loss = 1.96 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:22.525153: step 111190, loss = 1.74 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:30:23.741107: step 111200, loss = 1.80 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:24.937091: step 111210, loss = 1.94 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:30:26.127604: step 111220, loss = 1.85 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:30:27.352712: step 111230, loss = 1.95 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:30:28.575030: step 111240, loss = 1.90 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:29.766151: step 111250, loss = 2.03 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:30:31.002993: step 111260, loss = 1.90 (1034.9 examples/sec; 0.124 sec/batch)
2017-05-05 01:30:32.220105: step 111270, loss = 2.08 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:33.396811: step 111280, loss = 1.94 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:30:34.610737: step 111290, loss = 1.82 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:35.827624: step 111300, loss = 1.93 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:37.037207: step 111310, loss = 1.91 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:38.227601: step 111320, loss = 1.95 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:30:39.436393: step 111330, loss = 1.86 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:40.649901: step 111340, loss = 1.92 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:41.867026: step 111350, loss = 1.99 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:43.109772: step 111360, loss = 1.95 (1030.0 examples/sec; 0.124 sec/batch)
2017-05-05 01:30:44.330045: step 111370, loss = 1.89 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:45.537750: step 111380, loss = 1.94 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:46.770650: step 111390, loss = 1.98 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:30:47.993224: step 111400, loss = 1.97 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:49.212871: step 111410, loss = 1.93 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:50.408627: step 111420, loss = 1.89 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:30:51.634017: step 111430, loss = 1.84 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:30:52.838545: step 111440, loss = 1.71 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:30:54.063634: step 111450, loss = 1.81 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:30:55.276149: step 111460, loss = 1.91 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:56.487773: step 111470, loss = 1.91 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:57.682395: step 111480, loss = 1.81 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:30:58.900257: step 111490, loss = 1.81 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:31:00.116734: step 111500, loss = 1.99 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:31:01.353731: step 111510, loss = 1.94 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-05 01:31:02.551646: step 111520, loss = 1.96 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:31:03.789183: step 111530, loss = 1.94 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-05 01:31:04.999990: step 111540, loss = 1.77 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:06.206597: step 111550, loss = 1.99 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:07.429509: step 111560, loss = 1.79 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:31:08.655379: step 111570, loss = 1.88 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:31:09.868476: step 111580, loss = 1.90 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:11.101018: step 111590, loss = 1.85 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:31:12.305715: step 111600, loss = 1.99 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:31:13.511794: step 111610, loss = 1.86 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:14.719134: step 111620, loss = 1.92 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:15.971373: step 111630, loss = 1.78 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-05 01:31:17.183530: step 111640, loss = 1.94 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:18.397319: step 111650, loss = 1.86 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:19.605859: step 111660, loss = 1.86 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:20.830387: step 111670, loss = 1.97 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:31:21.994992: step 111680, loss = 1.82 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:31:23.229689: step 111690, loss = 1.92 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:31:24.445435: step 111700, loss = 1.91 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:31:25.650409: step 111710, loss = 1.84 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:31:26.866022: step 111720, loss = 1.95 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:31:28.094597: step 111730, loss = 1.84 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:31:29.297405: step 111740, loss = 1.96 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:31:30.486485: step 111750, loss = 1.86 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:31:31.691932: step 111760, loss = 1.97 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:32.877627: step 111770, loss = 2.00 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:31:34.078125: step 111780, loss = 1.76 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:31:35.275067: step 111790, loss = 1.94 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:31:36.476961: step 111800, loss = 1.80 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:31:37.675734: step 111810, loss = 1.97 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:31:38.856937: step 111820, loss = 1.92 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:31:40.036617: step 111830, loss = 1.95 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:31:41.207009: step 111840, loss = 2.05 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:31:42.373395: step 111850, loss = 1.89 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:31:43.527124: step 111860, loss = 1.80 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:31:44.812314: step 111870, loss = 1.86 (996.0 examples/sec; 0.129 sec/batch)
2017-05-05 01:31:45.879534: step 111880, loss = 1.98 (1199.4 examples/sec; 0.107 sec/batch)
2017-05-05 01:31:47.049737: step 111890, loss = 1.96 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:31:48.233129: step 111900, loss = 1.93 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:31:49.381065: step 111910, loss = 1.86 (1115.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:31:50.541327: step 111920, loss = 1.87 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:31:51.724386: step 111930, loss = 1.93 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:31:52.898600: step 111940, loss = 2.02 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:31:54.047759: step 111950, loss = 1.82 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:31:55.215560: step 111960, loss = 1.87 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:31:56.367364: step 111970, loss = 1.91 (1111.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:31:57.539072: step 111980, loss = 2.02 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:31:58.704833: step 111990, loss = 2.03 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:31:59.882960: step 112000, loss = 2.01 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:01.028061: step 112010, loss = 1.96 (1117.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:32:02.203972: step 112020, loss = 1.93 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:03.364444: step 112030, loss = 1.98 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:32:04.532075: step 112040, loss = 1.87 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:05.687690: step 112050, loss = 1.89 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:32:06.861777: step 112060, loss = 1.87 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:08.026167: step 112070, loss = 2.00 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:32:09.199500: step 112080, loss = 1.81 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:10.356883: step 112090, loss = 1.96 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:32:11.511177: step 112100, loss = 1.82 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:32:12.695800: step 112110, loss = 2.00 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:13.852958: step 112120, loss = 1.83 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:32:15.026949: step 112130, loss = 1.84 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:16.202836: step 112140, loss = 1.96 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:17.369444: step 112150, loss = 1.94 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:18.533555: step 112160, loss = 1.89 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:32:19.702253: step 112170, loss = 1.91 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:20.881958: step 112180, loss = 1.81 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:22.026136: step 112190, loss = 1.89 (1118.7 examples/sec; 0.114 sec/batch)
2017-05-05 01:32:23.179562: step 112200, loss = 1.89 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:32:24.356609: step 112210, loss = 2.00 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:25.492901: step 112220, loss = 1.88 (1126.5 examples/sec; 0.114 sec/batch)
2017-05-05 01:32:26.652284: step 112230, loss = 1.85 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:32:27.834553: step 112240, loss = 1.93 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:29.018848: step 112250, loss = 1.91 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:30.204127: step 112260, loss = 1.84 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:32:31.401828: step 112270, loss = 1.81 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:32:32.618149: step 112280, loss = 2.00 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:32:33.810271: step 112290, loss = 1.91 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:32:35.004645: step 112300, loss = 2.08 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:32:36.222704: step 112310, loss = 1.90 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:32:37.418741: step 112320, loss = 1.80 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:32:38.596549: step 112330, loss = 1.92 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:39.792950: step 112340, loss = 1.84 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:32:40.973476: step 112350, loss = 1.86 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:42.141383: step 112360, loss = 1.89 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:43.306686: step 112370, loss = 1.77 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:44.477058: step 112380, loss = 2.01 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:45.645298: step 112390, loss = 1.82 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:46.814587: step 112400, loss = 1.86 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:48.005674: step 112410, loss = 2.01 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:32:49.195648: step 112420, loss = 1.94 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:32:50.375143: step 112430, loss = 1.96 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:51.577108: step 112440, loss = 1.86 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:32:52.763763: step 112450, loss = 1.97 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:32:53.954494: step 112460, loss = 2.01 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:32:55.155535: step 112470, loss = 1.88 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:32:56.347631: step 112480, loss = 1.98 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:32:57.519149: step 112490, loss = 1.92 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:58.700133: step 112500, loss = 1.98 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:59.918631: step 112510, loss = 1.92 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:01.132465: step 112520, loss = 1.88 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:02.345976: step 112530, loss = 1.92 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:03.563906: step 112540, loss = 1.80 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:04.782533: step 112550, loss = 1.94 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:05.983356: step 112560, loss = 1.89 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:33:07.200776: step 112570, loss = 1.76 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:08.371664: step 112580, loss = 1.94 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:33:09.524348: step 112590, loss = 1.79 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:33:10.687499: step 112600, loss = 1.94 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:33:11.860505: step 112610, loss = 1.94 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:33:13.047357: step 112620, loss = 1.88 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:33:14.213593: step 112630, loss = 1.99 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:33:15.399863: step 112640, loss = 1.96 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:33:16.584755: step 112650, loss = 2.03 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:33:17.783339: step 112660, loss = 1.85 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:33:19.000454: step 112670, loss = 2.00 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:20.220569: step 112680, loss = 1.85 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:21.430522: step 112690, loss = 1.83 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:22.631690: step 112700, loss = 1.84 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:33:23.846487: step 112710, loss = 1.88 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:25.046645: step 112720, loss = 1.89 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:33:26.242634: step 112730, loss = 2.10 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:33:27.462377: step 112740, loss = 1.86 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:28.685082: step 112750, loss = 1.84 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:29.889158: step 112760, loss = 1.93 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:33:31.100281: step 112770, loss = 1.98 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:32.321650: step 112780, loss = 2.17 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:33.502077: step 112790, loss = 1.77 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:33:34.713704: step 112800, loss = 1.89 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:35.924438: step 112810, loss = 1.89 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:37.143385: step 112820, loss = 1.93 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:38.351546: step 112830, loss = 1.95 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:39.556554: step 112840, loss = 2.05 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:40.780053: step 112850, loss = 1.99 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:42.071955: step 112860, loss = 1.93 (990.8 examples/sec; 0.129 sec/batch)
2017-05-05 01:33:43.177327: step 112870, loss = 1.84 (1158.0 examples/sec; 0.111 sec/batch)
2017-05-05 01:33:44.399994: step 112880, loss = 1.96 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:45.618703: step 112890, loss = 1.76 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:46.834474: step 112900, loss = 1.88 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:48.048393: step 112910, loss = 1.89 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:49.248428: step 112920, loss = 2.06 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:33:50.481920: step 112930, loss = 1.89 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:33:51.717710: step 112940, loss = 1.85 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-05 01:33:52.926392: step 112950, loss = 1.93 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:54.131335: step 112960, loss = 1.88 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:33:55.344295: step 112970, loss = 1.96 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:56.558650: step 112980, loss = 1.85 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:57.765629: step 112990, loss = 1.90 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:58.975255: step 113000, loss = 2.13 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:00.186566: step 113010, loss = 1.99 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:01.370823: step 113020, loss = 1.91 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:34:02.566576: step 113030, loss = 1.96 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:34:03.746988: step 113040, loss = 1.91 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:34:04.947504: step 113050, loss = 1.81 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:34:06.120507: step 113060, loss = 1.89 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:34:07.310090: step 113070, loss = 2.00 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:34:08.502696: step 113080, loss = 1.93 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:34:09.693523: step 113090, loss = 2.06 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:34:10.897817: step 113100, loss = 1.80 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:34:12.106711: step 113110, loss = 1.91 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:13.311843: step 113120, loss = 2.06 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:14.515102: step 113130, loss = 1.89 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:34:15.721996: step 113140, loss = 2.06 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:16.933593: step 113150, loss = 2.05 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:18.132596: step 113160, loss = 2.00 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:34:19.351024: step 113170, loss = 1.95 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:34:20.566341: step 113180, loss = 1.88 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:34:21.777146: step 113190, loss = 1.93 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:22.990310: step 113200, loss = 1.91 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:24.216798: step 113210, loss = 2.00 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:34:25.431510: step 113220, loss = 2.01 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:26.625067: step 113230, loss = 1.87 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:34:27.829011: step 113240, loss = 1.67 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:34:29.040026: step 113250, loss = 1.87 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:30.234155: step 113260, loss = 1.88 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:34:31.428050: step 113270, loss = 1.90 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:34:32.624690: step 113280, loss = 1.80 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:34:33.802748: step 113290, loss = 1.79 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:34:34.977080: step 113300, loss = 1.89 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:34:36.172033: step 113310, loss = 1.77 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:34:37.346288: step 113320, loss = 2.06 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:34:38.547901: step 113330, loss = 1.99 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:34:39.708748: step 113340, loss = 1.93 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:34:40.867270: step 113350, loss = 1.99 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:34:42.018020: step 113360, loss = 2.01 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:34:43.171668: step 113370, loss = 2.01 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:34:44.343115: step 113380, loss = 1.96 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:34:45.503101: step 113390, loss = 1.86 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:34:46.671042: step 113400, loss = 2.00 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:34:47.849517: step 113410, loss = 1.78 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:34:49.024868: step 113420, loss = 1.91 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:34:50.204690: step 113430, loss = 1.76 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:34:51.384676: step 113440, loss = 1.83 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:34:52.549327: step 113450, loss = 1.90 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:34:53.709914: step 113460, loss = 1.76 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:34:54.883000: step 113470, loss = 2.01 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:34:56.049637: step 113480, loss = 1.81 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:34:57.214635: step 113490, loss = 1.95 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:34:58.362265: step 113500, loss = 1.70 (1115.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:34:59.528955: step 113510, loss = 1.92 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:00.703744: step 113520, loss = 2.08 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:01.886348: step 113530, loss = 1.87 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:03.068376: step 113540, loss = 1.79 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:04.244414: step 113550, loss = 1.91 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:05.408062: step 113560, loss = 2.02 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:06.576691: step 113570, loss = 1.87 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:07.737601: step 113580, loss = 1.90 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:08.898271: step 113590, loss = 2.13 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:10.058348: step 113600, loss = 1.80 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:11.223184: step 113610, loss = 1.96 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:12.396641: step 113620, loss = 1.86 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:13.553745: step 113630, loss = 1.94 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:14.697997: step 113640, loss = 1.80 (1118.6 examples/sec; 0.114 sec/batch)
2017-05-05 01:35:15.852543: step 113650, loss = 1.74 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:35:17.020434: step 113660, loss = 1.83 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:18.178087: step 113670, loss = 1.90 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:19.335260: step 113680, loss = 1.90 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:20.505791: step 113690, loss = 1.99 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:21.662086: step 113700, loss = 1.99 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:22.846908: step 113710, loss = 2.05 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:24.018947: step 113720, loss = 1.86 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:25.208947: step 113730, loss = 1.87 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:35:26.375435: step 113740, loss = 1.87 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:27.570219: step 113750, loss = 1.99 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:35:28.764053: step 113760, loss = 1.94 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:35:29.919515: step 113770, loss = 1.87 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:31.102942: step 113780, loss = 1.98 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:32.270989: step 113790, loss = 1.81 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:33.442052: step 113800, loss = 1.83 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:34.598375: step 113810, loss = 1.73 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:35.769146: step 113820, loss = 1.90 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:36.968785: step 113830, loss = 1.84 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:35:38.103568: step 113840, loss = 1.90 (1128.0 examples/sec; 0.113 sec/batch)
2017-05-05 01:35:39.425567: step 113850, loss = 1.95 (968.2 examples/sec; 0.132 sec/batch)
2017-05-05 01:35:40.457669: step 113860, loss = 1.79 (1240.2 examples/sec; 0.103 sec/batch)
2017-05-05 01:35:41.635958: step 113870, loss = 1.95 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:42.797076: step 113880, loss = 1.87 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:43.964993: step 113890, loss = 1.85 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:45.149607: step 113900, loss = 1.80 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:46.307890: step 113910, loss = 1.91 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:47.482381: step 113920, loss = 1.83 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:48.660603: step 113930, loss = 2.05 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:49.838210: step 113940, loss = 1.90 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:51.014506: step 113950, loss = 1.88 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:52.190177: step 113960, loss = 1.78 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:53.350572: step 113970, loss = 1.81 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:54.544041: step 113980, loss = 1.92 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:35:55.715016: step 113990, loss = 1.86 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:56.869440: step 114000, loss = 1.90 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:35:58.016456: step 114010, loss = 1.95 (1115.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:35:59.175444: step 114020, loss = 2.06 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:00.337967: step 114030, loss = 1.90 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:01.482911: step 114040, loss = 1.95 (1118.0 examples/sec; 0.114 sec/batch)
2017-05-05 01:36:02.651891: step 114050, loss = 1.74 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:03.815029: step 114060, loss = 1.85 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:04.984936: step 114070, loss = 2.00 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:06.146842: step 114080, loss = 1.94 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:07.309263: step 114090, loss = 1.77 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:08.478932: step 114100, loss = 1.86 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:09.664887: step 114110, loss = 1.86 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:36:10.837751: step 114120, loss = 2.01 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:11.990713: step 114130, loss = 1.95 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:36:13.169147: step 114140, loss = 1.93 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:36:14.317213: step 114150, loss = 1.90 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:36:15.483895: step 114160, loss = 1.77 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:16.640906: step 114170, loss = 1.79 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:17.802159: step 114180, loss = 1.88 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:18.975958: step 114190, loss = 1.89 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:20.130866: step 114200, loss = 1.89 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:36:21.306132: step 114210, loss = 1.81 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:36:22.473878: step 114220, loss = 1.98 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:23.666757: step 114230, loss = 1.98 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:36:24.824469: step 114240, loss = 1.91 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:25.982128: step 114250, loss = 1.94 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:27.154433: step 114260, loss = 1.83 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:28.325676: step 114270, loss = 2.02 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:29.489173: step 114280, loss = 1.97 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:30.636771: step 114290, loss = 1.86 (1115.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:36:31.814666: step 114300, loss = 1.83 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:36:32.985850: step 114310, loss = 1.90 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:34.149368: step 114320, loss = 1.99 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:35.321980: step 114330, loss = 1.98 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:36.481507: step 114340, loss = 1.96 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:37.662755: step 114350, loss = 1.87 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:36:38.811425: step 114360, loss = 2.13 (1114.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:36:39.968752: step 114370, loss = 1.76 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:41.139207: step 114380, loss = 1.76 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:42.300040: step 114390, loss = 1.95 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:43.457503: step 114400, loss = 1.91 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:44.632491: step 114410, loss = 2.03 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:45.809413: step 114420, loss = 1.84 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:36:46.984773: step 114430, loss = 1.95 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:36:48.164297: step 114440, loss = 1.94 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:36:49.333978: step 114450, loss = 1.85 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:50.505066: step 114460, loss = 1.91 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:51.692022: step 114470, loss = 1.89 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:36:52.905298: step 114480, loss = 1.88 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:36:54.106344: step 114490, loss = 1.76 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:36:55.297041: step 114500, loss = 1.93 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:36:56.534372: step 114510, loss = 1.96 (1034.5 examples/sec; 0.124 sec/batch)
2017-05-05 01:36:57.738297: step 114520, loss = 2.02 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:36:58.968208: step 114530, loss = 1.91 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:37:00.156317: step 114540, loss = 1.81 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:37:01.348267: step 114550, loss = 2.03 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:37:02.557945: step 114560, loss = 1.94 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:37:03.756412: step 114570, loss = 1.78 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:37:04.961386: step 114580, loss = 1.98 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:37:06.139909: step 114590, loss = 1.85 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:07.319570: step 114600, loss = 1.91 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:08.496515: step 114610, loss = 1.98 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:09.674742: step 114620, loss = 1.84 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:10.844669: step 114630, loss = 2.12 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:12.031799: step 114640, loss = 1.97 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:37:13.226599: step 114650, loss = 1.97 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:37:14.389294: step 114660, loss = 1.81 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:37:15.540338: step 114670, loss = 1.79 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:37:16.716816: step 114680, loss = 1.95 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:17.874882: step 114690, loss = 2.03 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:37:19.045479: step 114700, loss = 1.94 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:20.219437: step 114710, loss = 2.18 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:21.366289: step 114720, loss = 1.95 (1116.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:37:22.520074: step 114730, loss = 1.98 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:37:23.697180: step 114740, loss = 1.92 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:24.847888: step 114750, loss = 1.92 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:37:26.015395: step 114760, loss = 1.95 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:27.178490: step 114770, loss = 1.90 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:37:28.350886: step 114780, loss = 1.80 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:29.520154: step 114790, loss = 2.02 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:30.713547: step 114800, loss = 1.94 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:37:31.873917: step 114810, loss = 2.00 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:37:33.050396: step 114820, loss = 1.87 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:34.219101: step 114830, loss = 1.92 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:35.475761: step 114840, loss = 1.89 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-05 01:37:36.526765: step 114850, loss = 1.88 (1217.9 examples/sec; 0.105 sec/batch)
2017-05-05 01:37:37.683351: step 114860, loss = 1.79 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:37:38.872745: step 114870, loss = 1.78 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:37:40.048163: step 114880, loss = 1.83 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:41.194633: step 114890, loss = 1.77 (1116.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:37:42.347858: step 114900, loss = 1.93 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:37:43.506411: step 114910, loss = 1.80 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:37:44.674319: step 114920, loss = 1.99 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:45.841889: step 114930, loss = 1.89 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:47.011106: step 114940, loss = 1.97 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:48.184129: step 114950, loss = 2.02 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:49.356883: step 114960, loss = 1.95 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:50.532835: step 114970, loss = 2.01 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:51.717005: step 114980, loss = 1.82 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:52.883505: step 114990, loss = 1.85 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:54.045917: step 115000, loss = 1.94 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:37:55.229310: step 115010, loss = 2.03 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:56.383163: step 115020, loss = 1.86 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:37:57.546282: step 115030, loss = 1.98 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:37:58.726962: step 115040, loss = 1.95 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:59.887352: step 115050, loss = 1.94 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:38:01.059536: step 115060, loss = 1.78 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:38:02.232957: step 115070, loss = 1.87 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:38:03.427060: step 115080, loss = 2.04 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:38:04.611679: step 115090, loss = 1.97 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:05.784936: step 115100, loss = 1.83 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:38:06.967534: step 115110, loss = 1.96 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:08.148915: step 115120, loss = 1.94 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:09.347682: step 115130, loss = 1.90 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:38:10.545185: step 115140, loss = 1.83 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:38:11.748403: step 115150, loss = 2.02 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:38:12.956372: step 115160, loss = 1.82 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:38:14.137910: step 115170, loss = 1.76 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:15.308090: step 115180, loss = 1.81 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:38:16.494347: step 115190, loss = 1.84 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:38:17.650274: step 115200, loss = 1.82 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:38:18.827471: step 115210, loss = 1.84 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:20.006447: step 115220, loss = 1.86 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:21.171579: step 115230, loss = 1.80 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:38:22.314342: step 115240, loss = 1.77 (1120.1 examples/sec; 0.114 sec/batch)
2017-05-05 01:38:23.482054: step 115250, loss = 1.91 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:38:24.656395: step 115260, loss = 1.95 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:38:25.814948: step 115270, loss = 1.92 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:38:27.009846: step 115280, loss = 1.84 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:38:28.177436: step 115290, loss = 2.02 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:38:29.358322: step 115300, loss = 2.16 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:30.529025: step 115310, loss = 1.87 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:38:31.726054: step 115320, loss = 1.93 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:38:32.918926: step 115330, loss = 1.87 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:38:34.088427: step 115340, loss = 1.84 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:38:35.259014: step 115350, loss = 1.78 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:38:36.442628: step 115360, loss = 1.90 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:37.656274: step 115370, loss = 1.90 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:38:38.878148: step 115380, loss = 1.84 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:38:40.117052: step 115390, loss = 1.85 (1033.2 examples/sec; 0.124 sec/batch)
2017-05-05 01:38:41.307848: step 115400, loss = 1.86 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:38:42.509597: step 115410, loss = 1.83 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:38:43.708577: step 115420, loss = 1.82 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:38:44.920317: step 115430, loss = 1.87 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:38:46.131015: step 115440, loss = 1.90 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:38:47.369478: step 115450, loss = 1.97 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-05 01:38:48.575700: step 115460, loss = 1.98 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:38:49.791403: step 115470, loss = 1.90 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:38:51.002995: step 115480, loss = 1.90 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:38:52.234037: step 115490, loss = 1.86 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:38:53.438158: step 115500, loss = 1.95 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:38:54.639169: step 115510, loss = 1.86 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:38:55.845586: step 115520, loss = 2.11 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:38:57.080467: step 115530, loss = 1.89 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:38:58.268993: step 115540, loss = 1.86 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:38:59.472848: step 115550, loss = 1.74 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:39:00.703329: step 115560, loss = 1.88 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:39:01.901991: step 115570, loss = 1.99 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:39:03.111248: step 115580, loss = 2.16 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:39:04.332092: step 115590, loss = 1.87 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:39:05.550022: step 115600, loss = 1.81 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:39:06.730424: step 115610, loss = 1.94 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:07.927633: step 115620, loss = 1.97 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:39:09.131687: step 115630, loss = 2.00 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:39:10.319508: step 115640, loss = 2.01 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:39:11.512385: step 115650, loss = 1.80 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:39:12.701046: step 115660, loss = 2.17 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:39:13.874614: step 115670, loss = 1.78 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:15.047154: step 115680, loss = 1.95 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:16.234508: step 115690, loss = 2.08 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:39:17.415605: step 115700, loss = 1.87 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:18.565461: step 115710, loss = 1.85 (1113.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:39:19.730290: step 115720, loss = 2.02 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:39:20.906105: step 115730, loss = 1.92 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:22.065797: step 115740, loss = 1.89 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:39:23.236858: step 115750, loss = 1.86 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:24.410713: step 115760, loss = 1.74 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:25.579490: step 115770, loss = 1.88 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:26.767051: step 115780, loss = 1.79 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:39:27.956586: step 115790, loss = 1.98 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:39:29.154814: step 115800, loss = 1.94 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:39:30.331335: step 115810, loss = 1.85 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:31.508420: step 115820, loss = 1.96 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:32.839597: step 115830, loss = 1.86 (961.6 examples/sec; 0.133 sec/batch)
2017-05-05 01:39:33.888132: step 115840, loss = 1.93 (1220.7 examples/sec; 0.105 sec/batch)
2017-05-05 01:39:35.089033: step 115850, loss = 1.82 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:39:36.309089: step 115860, loss = 1.83 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:39:37.522756: step 115870, loss = 1.78 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:39:38.708332: step 115880, loss = 1.80 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:39:39.935884: step 115890, loss = 1.76 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:39:41.143412: step 115900, loss = 2.14 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:39:42.321714: step 115910, loss = 1.82 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:43.529630: step 115920, loss = 1.89 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:39:44.712738: step 115930, loss = 1.78 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:45.883591: step 115940, loss = 1.92 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:47.051187: step 115950, loss = 1.92 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:48.225590: step 115960, loss = 1.95 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:49.394147: step 115970, loss = 1.81 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:50.554851: step 115980, loss = 1.87 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:39:51.710216: step 115990, loss = 1.76 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:39:52.872878: step 116000, loss = 1.87 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:39:54.029821: step 116010, loss = 2.11 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:39:55.221271: step 116020, loss = 2.00 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:39:56.410943: step 116030, loss = 1.89 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:39:57.560390: step 116040, loss = 1.81 (1113.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:39:58.731069: step 116050, loss = 1.84 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:59.907470: step 116060, loss = 1.99 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:01.103020: step 116070, loss = 1.85 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:40:02.288031: step 116080, loss = 1.76 (1080.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:40:03.490978: step 116090, loss = 2.03 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:40:04.671267: step 116100, loss = 1.84 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:05.842203: step 116110, loss = 1.79 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:40:07.017881: step 116120, loss = 1.99 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:08.196414: step 116130, loss = 1.96 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:09.351253: step 116140, loss = 1.88 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:40:10.520101: step 116150, loss = 1.84 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:40:11.704751: step 116160, loss = 1.80 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:12.889143: step 116170, loss = 1.96 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:14.051621: step 116180, loss = 1.68 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:40:15.234433: step 116190, loss = 1.77 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:16.404875: step 116200, loss = 1.97 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:40:17.563536: step 116210, loss = 1.94 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:40:18.728596: step 116220, loss = 1.90 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:40:19.879348: step 116230, loss = 1.87 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:40:21.035557: step 116240, loss = 1.92 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:40:22.209121: step 116250, loss = 1.85 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:40:23.393461: step 116260, loss = 1.96 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:24.562362: step 116270, loss = 1.93 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:40:25.725795: step 116280, loss = 1.84 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:40:26.903428: step 116290, loss = 1.94 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:28.074413: step 116300, loss = 1.98 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:40:29.255849: step 116310, loss = 2.02 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:30.416196: step 116320, loss = 1.74 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:40:31.609687: step 116330, loss = 1.83 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:40:32.792565: step 116340, loss = 1.83 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:33.962629: step 116350, loss = 1.88 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:40:35.157311: step 116360, loss = 1.92 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:40:36.322852: step 116370, loss = 2.12 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:40:37.471661: step 116380, loss = 1.95 (1114.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:40:38.668783: step 116390, loss = 1.96 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:40:39.873273: step 116400, loss = 1.93 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:40:41.052466: step 116410, loss = 1.81 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:42.229500: step 116420, loss = 1.92 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:43.444803: step 116430, loss = 1.91 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:40:44.657563: step 116440, loss = 1.92 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:40:45.842043: step 116450, loss = 1.91 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:47.043001: step 116460, loss = 1.88 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:40:48.253207: step 116470, loss = 2.02 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:40:49.456713: step 116480, loss = 1.99 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:40:50.672690: step 116490, loss = 1.81 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:40:51.878605: step 116500, loss = 1.82 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:40:53.111346: step 116510, loss = 2.03 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-05 01:40:54.300376: step 116520, loss = 1.81 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:40:55.509264: step 116530, loss = 1.85 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:40:56.714536: step 116540, loss = 1.87 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:40:57.901262: step 116550, loss = 1.92 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:40:59.106494: step 116560, loss = 1.86 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:41:00.299564: step 116570, loss = 1.84 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:41:01.490955: step 116580, loss = 1.85 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:41:02.704641: step 116590, loss = 1.88 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:41:03.913853: step 116600, loss = 2.05 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:41:05.141006: step 116610, loss = 1.82 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:41:06.370423: step 116620, loss = 1.91 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:41:07.579841: step 116630, loss = 1.92 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:41:08.814637: step 116640, loss = 1.78 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:41:10.014061: step 116650, loss = 1.80 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:11.239715: step 116660, loss = 1.99 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-05 01:41:12.462615: step 116670, loss = 1.91 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:41:13.667273: step 116680, loss = 1.76 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:14.906956: step 116690, loss = 1.97 (1032.5 examples/sec; 0.124 sec/batch)
2017-05-05 01:41:16.143611: step 116700, loss = 1.88 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-05 01:41:17.355262: step 116710, loss = 1.89 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:41:18.570483: step 116720, loss = 1.88 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:41:19.787197: step 116730, loss = 1.88 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:41:21.013279: step 116740, loss = 1.81 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:41:22.221642: step 116750, loss = 1.87 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:41:23.448598: step 116760, loss = 1.86 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:41:24.686431: step 116770, loss = 1.89 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-05 01:41:25.886503: step 116780, loss = 1.83 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:27.122335: step 116790, loss = 2.03 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-05 01:41:28.351982: step 116800, loss = 1.95 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:41:29.563394: step 116810, loss = 1.90 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:41:30.853857: step 116820, loss = 1.71 (991.9 examples/sec; 0.129 sec/batch)
2017-05-05 01:41:31.957743: step 116830, loss = 1.89 (1159.5 examples/sec; 0.110 sec/batch)
2017-05-05 01:41:33.176441: step 116840, loss = 1.85 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:41:34.368288: step 116850, loss = 2.08 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:41:35.559817: step 116860, loss = 1.85 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:41:36.759470: step 116870, loss = 1.89 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:37.969245: step 116880, loss = 2.04 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:41:39.160462: step 116890, loss = 1.77 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:41:40.344303: step 116900, loss = 2.02 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:41:41.528874: step 116910, loss = 2.00 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:41:42.728137: step 116920, loss = 2.03 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:43.923967: step 116930, loss = 2.00 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:45.110633: step 116940, loss = 1.91 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:41:46.304492: step 116950, loss = 2.01 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:41:47.492893: step 116960, loss = 1.89 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:41:48.703670: step 116970, loss = 1.96 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:41:49.884385: step 116980, loss = 1.97 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:41:51.081710: step 116990, loss = 1.77 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:52.278482: step 117000, loss = 1.96 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:53.457040: step 117010, loss = 1.87 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:41:54.640587: step 117020, loss = 1.92 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:41:55.840921: step 117030, loss = 1.84 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:57.025517: step 117040, loss = 1.93 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:41:58.195604: step 117050, loss = 1.87 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:41:59.388826: step 117060, loss = 1.80 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:00.567636: step 117070, loss = 2.03 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:01.741718: step 117080, loss = 2.10 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:02.931798: step 117090, loss = 1.93 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:04.116157: step 117100, loss = 1.99 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:05.293660: step 117110, loss = 1.94 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:06.471635: step 117120, loss = 1.86 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:07.684964: step 117130, loss = 1.89 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:42:08.882001: step 117140, loss = 2.03 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:42:10.062545: step 117150, loss = 1.95 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:11.252003: step 117160, loss = 1.90 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:12.465415: step 117170, loss = 1.80 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:42:13.674431: step 117180, loss = 1.90 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:42:14.883831: step 117190, loss = 2.00 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:42:16.083447: step 117200, loss = 2.03 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:42:17.296090: step 117210, loss = 2.01 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:42:18.467317: step 117220, loss = 1.88 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:19.672453: step 117230, loss = 1.84 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:42:20.858845: step 117240, loss = 2.11 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:22.047383: step 117250, loss = 1.91 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:23.248513: step 117260, loss = 1.80 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:42:24.445827: step 117270, loss = 1.95 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:42:25.633123: step 117280, loss = 2.12 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:26.807107: step 117290, loss = 1.78 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:27.966641: step 117300, loss = 1.85 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:42:29.149505: step 117310, loss = 1.84 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:30.315496: step 117320, loss = 1.82 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:31.469720: step 117330, loss = 1.89 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:42:32.659746: step 117340, loss = 1.90 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:33.828091: step 117350, loss = 1.83 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:34.994846: step 117360, loss = 2.06 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:36.166857: step 117370, loss = 1.99 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:37.322708: step 117380, loss = 2.03 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:42:38.498338: step 117390, loss = 1.95 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:39.684073: step 117400, loss = 2.05 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:40.854480: step 117410, loss = 1.78 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:42.023939: step 117420, loss = 1.83 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:43.171024: step 117430, loss = 1.93 (1115.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:42:44.326682: step 117440, loss = 1.88 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:42:45.490467: step 117450, loss = 1.91 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:42:46.638291: step 117460, loss = 1.82 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:42:47.807791: step 117470, loss = 1.85 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:48.956371: step 117480, loss = 2.02 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:42:50.109989: step 117490, loss = 2.00 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:42:51.290300: step 117500, loss = 2.08 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:52.458855: step 117510, loss = 1.82 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:53.604276: step 117520, loss = 1.95 (1117.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:42:54.767191: step 117530, loss = 1.84 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:42:55.933569: step 117540, loss = 2.06 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:57.113724: step 117550, loss = 1.87 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:58.262803: step 117560, loss = 1.85 (1113.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:42:59.427640: step 117570, loss = 1.94 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:43:00.612661: step 117580, loss = 1.90 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:01.797887: step 117590, loss = 1.91 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:02.961344: step 117600, loss = 1.81 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:43:04.123493: step 117610, loss = 1.83 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:43:05.294382: step 117620, loss = 1.80 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:06.457890: step 117630, loss = 1.88 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:43:07.625147: step 117640, loss = 1.99 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:08.809002: step 117650, loss = 1.97 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:43:09.977353: step 117660, loss = 1.95 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:11.168934: step 117670, loss = 1.96 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:12.371546: step 117680, loss = 1.88 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:43:13.590361: step 117690, loss = 1.94 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:43:14.805509: step 117700, loss = 1.97 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:43:16.016819: step 117710, loss = 1.94 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:43:17.229724: step 117720, loss = 1.77 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:43:18.439115: step 117730, loss = 2.01 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:43:19.675852: step 117740, loss = 1.90 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-05 01:43:20.874308: step 117750, loss = 1.70 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:43:22.045297: step 117760, loss = 1.76 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:23.236351: step 117770, loss = 1.86 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:24.432423: step 117780, loss = 1.97 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:43:25.620935: step 117790, loss = 1.95 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:26.836490: step 117800, loss = 1.98 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:43:28.151522: step 117810, loss = 1.82 (973.4 examples/sec; 0.132 sec/batch)
2017-05-05 01:43:29.247590: step 117820, loss = 2.08 (1167.8 examples/sec; 0.110 sec/batch)
2017-05-05 01:43:30.457987: step 117830, loss = 1.96 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:43:31.663780: step 117840, loss = 1.92 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:43:32.856489: step 117850, loss = 2.08 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:34.036054: step 117860, loss = 1.86 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:43:35.250198: step 117870, loss = 1.79 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:43:36.446122: step 117880, loss = 1.86 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:43:37.633448: step 117890, loss = 1.94 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:38.835518: step 117900, loss = 2.00 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:43:40.029095: step 117910, loss = 1.82 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:41.247968: step 117920, loss = 1.85 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:43:42.412379: step 117930, loss = 1.98 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:43:43.592436: step 117940, loss = 1.80 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:43:44.775580: step 117950, loss = 1.98 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:43:45.944371: step 117960, loss = 1.86 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:47.128514: step 117970, loss = 1.91 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:43:48.322735: step 117980, loss = 1.96 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:49.502200: step 117990, loss = 2.04 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:43:50.650287: step 118000, loss = 1.84 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:43:51.851707: step 118010, loss = 1.88 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:43:53.017898: step 118020, loss = 2.00 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:54.188701: step 118030, loss = 1.91 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:55.362389: step 118040, loss = 1.86 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:56.517563: step 118050, loss = 1.83 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:43:57.682899: step 118060, loss = 1.92 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:58.850493: step 118070, loss = 1.96 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:00.019694: step 118080, loss = 1.92 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:01.184375: step 118090, loss = 1.86 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:02.337298: step 118100, loss = 1.81 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:03.501608: step 118110, loss = 1.87 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:04.668776: step 118120, loss = 2.11 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:05.834877: step 118130, loss = 1.99 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:07.009793: step 118140, loss = 2.01 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:08.173703: step 118150, loss = 1.81 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:09.345971: step 118160, loss = 1.94 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:10.496817: step 118170, loss = 1.88 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:11.652189: step 118180, loss = 1.93 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:12.814602: step 118190, loss = 1.87 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:13.987525: step 118200, loss = 1.85 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:15.158692: step 118210, loss = 1.78 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:16.342784: step 118220, loss = 1.99 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:44:17.527797: step 118230, loss = 1.86 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:44:18.693350: step 118240, loss = 1.88 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:19.854607: step 118250, loss = 1.98 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:20.989816: step 118260, loss = 1.85 (1127.5 examples/sec; 0.114 sec/batch)
2017-05-05 01:44:22.149082: step 118270, loss = 1.79 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:23.326625: step 118280, loss = 1.90 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:44:24.476116: step 118290, loss = 2.05 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:25.630406: step 118300, loss = 1.74 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:26.814399: step 118310, loss = 2.05 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:44:27.992217: step 118320, loss = 1.90 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:44:29.156489: step 118330, loss = 1.80 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:30.316789: step 118340, loss = 1.98 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:31.497250: step 118350, loss = 1.89 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:44:32.676727: step 118360, loss = 1.85 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:44:33.843687: step 118370, loss = 1.94 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:35.004972: step 118380, loss = 2.00 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:36.163819: step 118390, loss = 1.90 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:37.328557: step 118400, loss = 1.99 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:38.475162: step 118410, loss = 1.88 (1116.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:39.628507: step 118420, loss = 1.94 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:40.793623: step 118430, loss = 1.93 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:41.979451: step 118440, loss = 2.01 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:44:43.134659: step 118450, loss = 1.75 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:44.294817: step 118460, loss = 1.90 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:45.442315: step 118470, loss = 1.90 (1115.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:46.633696: step 118480, loss = 1.96 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:44:47.815339: step 118490, loss = 1.95 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:44:48.978894: step 118500, loss = 1.96 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:50.133582: step 118510, loss = 1.99 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:51.318450: step 118520, loss = 1.99 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:44:52.489822: step 118530, loss = 1.84 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:53.654023: step 118540, loss = 1.76 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:54.806963: step 118550, loss = 2.04 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:55.965863: step 118560, loss = 1.93 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:57.126480: step 118570, loss = 1.83 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:58.280777: step 118580, loss = 1.82 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:59.453345: step 118590, loss = 1.85 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:00.649894: step 118600, loss = 1.85 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:45:01.828024: step 118610, loss = 1.96 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:45:02.994278: step 118620, loss = 1.90 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:04.161511: step 118630, loss = 1.84 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:05.342329: step 118640, loss = 1.90 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:45:06.494908: step 118650, loss = 1.87 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:45:07.647901: step 118660, loss = 1.88 (1110.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:45:08.818271: step 118670, loss = 2.04 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:09.965311: step 118680, loss = 2.00 (1116.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:45:11.150021: step 118690, loss = 1.90 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:45:12.310351: step 118700, loss = 1.84 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:45:13.483293: step 118710, loss = 1.87 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:14.643287: step 118720, loss = 2.09 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:45:15.815959: step 118730, loss = 1.93 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:16.979708: step 118740, loss = 1.92 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:45:18.134794: step 118750, loss = 2.02 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:45:19.306579: step 118760, loss = 1.93 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:20.458208: step 118770, loss = 1.88 (1111.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:45:21.580430: step 118780, loss = 2.08 (1140.6 examples/sec; 0.112 sec/batch)
2017-05-05 01:45:22.750982: step 118790, loss = 1.85 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:24.000188: step 118800, loss = 1.78 (1024.7 examples/sec; 0.125 sec/batch)
2017-05-05 01:45:25.067966: step 118810, loss = 1.78 (1198.8 examples/sec; 0.107 sec/batch)
2017-05-05 01:45:26.224420: step 118820, loss = 1.85 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:45:27.384516: step 118830, loss = 1.80 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:45:28.555697: step 118840, loss = 2.00 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:29.716318: step 118850, loss = 1.89 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:45:30.914438: step 118860, loss = 1.96 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:45:32.097785: step 118870, loss = 1.97 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:45:33.286181: step 118880, loss = 1.89 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:45:34.497408: step 118890, loss = 1.94 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:45:35.703356: step 118900, loss = 1.76 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:45:36.939330: step 118910, loss = 1.95 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-05 01:45:38.117050: step 118920, loss = 2.04 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:45:39.368382: step 118930, loss = 2.01 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-05 01:45:40.574943: step 118940, loss = 1.82 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:45:41.789455: step 118950, loss = 1.85 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:45:42.996839: step 118960, loss = 1.88 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:45:44.223081: step 118970, loss = 2.10 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:45:45.433750: step 118980, loss = 2.06 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:45:46.641942: step 118990, loss = 1.91 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:45:47.940567: step 119000, loss = 1.85 (985.7 examples/sec; 0.130 sec/batch)
2017-05-05 01:45:49.061041: step 119010, loss = 1.99 (1142.4 examples/sec; 0.112 sec/batch)
2017-05-05 01:45:50.259844: step 119020, loss = 1.87 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:45:51.493789: step 119030, loss = 2.01 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-05 01:45:52.689969: step 119040, loss = 1.90 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:45:53.897043: step 119050, loss = 1.85 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:45:55.106763: step 119060, loss = 1.79 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:45:56.338643: step 119070, loss = 1.99 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:45:57.548085: step 119080, loss = 2.06 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:45:58.774015: step 119090, loss = 1.89 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:45:59.991217: step 119100, loss = 1.98 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:46:01.191793: step 119110, loss = 1.90 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:02.427500: step 119120, loss = 2.11 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-05 01:46:03.629863: step 119130, loss = 1.85 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:04.844066: step 119140, loss = 1.83 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:06.033552: step 119150, loss = 1.80 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:46:07.226574: step 119160, loss = 1.83 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:46:08.437329: step 119170, loss = 1.82 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:09.650388: step 119180, loss = 1.79 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:10.884121: step 119190, loss = 1.95 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:46:12.081396: step 119200, loss = 1.93 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:13.318590: step 119210, loss = 1.95 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-05 01:46:14.509022: step 119220, loss = 1.79 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:46:15.741216: step 119230, loss = 1.85 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:46:16.941566: step 119240, loss = 1.94 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:18.150166: step 119250, loss = 1.71 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:19.362005: step 119260, loss = 1.86 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:20.562520: step 119270, loss = 1.97 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:21.774426: step 119280, loss = 1.73 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:23.004471: step 119290, loss = 1.99 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:46:24.208197: step 119300, loss = 2.09 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:25.426431: step 119310, loss = 1.88 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:46:26.628881: step 119320, loss = 1.98 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:27.851032: step 119330, loss = 2.03 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:46:29.065102: step 119340, loss = 1.85 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:30.265872: step 119350, loss = 1.94 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:31.464199: step 119360, loss = 1.94 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:32.659332: step 119370, loss = 2.00 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:33.856702: step 119380, loss = 1.95 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:35.071159: step 119390, loss = 2.03 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:36.267534: step 119400, loss = 1.99 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:37.455526: step 119410, loss = 1.92 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:46:38.626098: step 119420, loss = 1.81 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:46:39.815209: step 119430, loss = 1.94 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:46:41.039342: step 119440, loss = 2.11 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:46:42.231840: step 119450, loss = 2.00 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:46:43.422276: step 119460, loss = 1.92 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:46:44.598982: step 119470, loss = 1.94 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:46:45.777498: step 119480, loss = 1.86 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:46:46.974220: step 119490, loss = 1.84 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:48.143782: step 119500, loss = 2.17 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:46:49.324914: step 119510, loss = 1.97 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:46:50.491454: step 119520, loss = 1.77 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:46:51.674394: step 119530, loss = 1.93 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:46:52.845548: step 119540, loss = 1.86 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:46:54.019569: step 119550, loss = 1.90 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:46:55.201760: step 119560, loss = 1.91 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:46:56.401725: step 119570, loss = 1.80 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:57.582621: step 119580, loss = 2.03 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:46:58.775307: step 119590, loss = 1.96 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:46:59.963331: step 119600, loss = 1.89 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:01.184592: step 119610, loss = 1.90 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:47:02.378356: step 119620, loss = 1.91 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:03.591451: step 119630, loss = 1.97 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:47:04.807260: step 119640, loss = 1.81 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:47:05.991471: step 119650, loss = 1.92 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:07.180132: step 119660, loss = 2.02 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:08.368060: step 119670, loss = 1.99 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:09.555399: step 119680, loss = 1.77 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:10.726558: step 119690, loss = 1.81 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:47:11.900478: step 119700, loss = 1.91 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:47:13.087741: step 119710, loss = 1.80 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:14.271629: step 119720, loss = 2.06 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:15.462450: step 119730, loss = 1.95 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:16.636859: step 119740, loss = 1.84 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:47:17.813966: step 119750, loss = 1.95 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:18.989623: step 119760, loss = 1.92 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:20.172998: step 119770, loss = 1.83 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:21.373924: step 119780, loss = 1.92 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:47:22.644038: step 119790, loss = 1.86 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-05 01:47:23.741428: step 119800, loss = 1.97 (1166.4 examples/sec; 0.110 sec/batch)
2017-05-05 01:47:24.909012: step 119810, loss = 1.74 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:47:26.073090: step 119820, loss = 1.96 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:47:27.255122: step 119830, loss = 2.00 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:28.439745: step 119840, loss = 1.84 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:29.625067: step 119850, loss = 1.84 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:30.807840: step 119860, loss = 1.88 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:31.996561: step 119870, loss = 1.98 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:33.177540: step 119880, loss = 1.92 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:34.359319: step 119890, loss = 1.81 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:35.538362: step 119900, loss = 1.98 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:36.728834: step 119910, loss = 1.98 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:37.887004: step 119920, loss = 1.70 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:47:39.099470: step 119930, loss = 1.93 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:47:40.306584: step 119940, loss = 1.81 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:47:41.495922: step 119950, loss = 2.01 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:42.692110: step 119960, loss = 1.89 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:47:43.905894: step 119970, loss = 1.84 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:47:45.115417: step 119980, loss = 1.93 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:47:46.376900: step 119990, loss = 1.79 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-05 01:47:47.534802: step 120000, loss = 1.84 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:47:48.730847: step 120010, loss = 1.86 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:47:49.928479: step 120020, loss = 1.93 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:47:51.117429: step 120030, loss = 1.79 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:52.295337: step 120040, loss = 1.92 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:53.471349: step 120050, loss = 1.95 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:54.626607: step 120060, loss = 1.91 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:47:55.801648: step 120070, loss = 1.99 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:56.980081: step 120080, loss = 1.92 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:58.139630: step 120090, loss = 1.89 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:47:59.309595: step 120100, loss = 1.91 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:00.480016: step 120110, loss = 1.90 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:01.656066: step 120120, loss = 1.99 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:02.811421: step 120130, loss = 1.89 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:03.969386: step 120140, loss = 2.09 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:05.127416: step 120150, loss = 2.00 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:06.295436: step 120160, loss = 1.88 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:07.467315: step 120170, loss = 2.00 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:08.662404: step 120180, loss = 1.92 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:48:09.845444: step 120190, loss = 1.96 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:11.039079: step 120200, loss = 2.00 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:48:12.225817: step 120210, loss = 1.81 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:48:13.405280: step 120220, loss = 1.87 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:14.561887: step 120230, loss = 1.89 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:15.723011: step 120240, loss = 1.79 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:16.882734: step 120250, loss = 1.89 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:18.040880: step 120260, loss = 2.00 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:19.223713: step 120270, loss = 1.97 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:20.419931: step 120280, loss = 1.90 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:48:21.585360: step 120290, loss = 1.92 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:22.763879: step 120300, loss = 1.89 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:23.920829: step 120310, loss = 1.84 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:25.086193: step 120320, loss = 1.71 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:26.280122: step 120330, loss = 1.88 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:48:27.464893: step 120340, loss = 1.78 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:28.634475: step 120350, loss = 1.90 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:29.789668: step 120360, loss = 1.97 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:30.949166: step 120370, loss = 2.12 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:32.127145: step 120380, loss = 2.01 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:33.274574: step 120390, loss = 1.90 (1115.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:48:34.436536: step 120400, loss = 1.83 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:35.609514: step 120410, loss = 1.78 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:36.770401: step 120420, loss = 1.85 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:37.936014: step 120430, loss = 1.86 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:39.103881: step 120440, loss = 1.71 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:40.279471: step 120450, loss = 1.87 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:41.444831: step 120460, loss = 2.00 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:42.630660: step 120470, loss = 1.98 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:48:43.802421: step 120480, loss = 1.97 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:44.982455: step 120490, loss = 1.94 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:46.144076: step 120500, loss = 1.92 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:47.285580: step 120510, loss = 1.90 (1121.3 examples/sec; 0.114 sec/batch)
2017-05-05 01:48:48.454655: step 120520, loss = 1.90 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:49.619156: step 120530, loss = 1.88 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:50.779856: step 120540, loss = 1.89 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:51.949017: step 120550, loss = 1.98 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:53.105899: step 120560, loss = 1.97 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:54.264498: step 120570, loss = 1.87 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:55.464149: step 120580, loss = 1.85 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:48:56.648183: step 120590, loss = 1.85 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:57.797070: step 120600, loss = 1.86 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:48:58.969487: step 120610, loss = 1.91 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:00.137411: step 120620, loss = 2.00 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:01.293845: step 120630, loss = 2.11 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:02.453886: step 120640, loss = 2.01 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:03.632978: step 120650, loss = 1.70 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:49:04.810947: step 120660, loss = 1.91 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:49:05.970367: step 120670, loss = 1.98 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:07.123285: step 120680, loss = 1.70 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:08.295872: step 120690, loss = 1.96 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:09.445416: step 120700, loss = 1.94 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:10.595450: step 120710, loss = 1.89 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:11.783658: step 120720, loss = 1.79 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:49:12.933673: step 120730, loss = 1.79 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:14.093778: step 120740, loss = 1.94 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:15.260371: step 120750, loss = 1.94 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:16.435287: step 120760, loss = 1.76 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:17.590820: step 120770, loss = 1.80 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:18.869897: step 120780, loss = 1.90 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-05 01:49:19.948163: step 120790, loss = 1.99 (1187.1 examples/sec; 0.108 sec/batch)
2017-05-05 01:49:21.111074: step 120800, loss = 2.01 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:22.266273: step 120810, loss = 1.87 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:23.446114: step 120820, loss = 1.75 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:49:24.618688: step 120830, loss = 1.78 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:25.783422: step 120840, loss = 1.88 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:26.953564: step 120850, loss = 1.96 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:28.144200: step 120860, loss = 2.00 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:49:29.311971: step 120870, loss = 1.81 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:30.473417: step 120880, loss = 1.95 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:31.630296: step 120890, loss = 1.98 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:32.767806: step 120900, loss = 1.94 (1125.3 examples/sec; 0.114 sec/batch)
2017-05-05 01:49:33.931071: step 120910, loss = 1.88 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:35.103402: step 120920, loss = 1.85 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:36.257436: step 120930, loss = 1.88 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:37.401872: step 120940, loss = 1.97 (1118.5 examples/sec; 0.114 sec/batch)
2017-05-05 01:49:38.557440: step 120950, loss = 1.99 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:39.709892: step 120960, loss = 1.85 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:40.884573: step 120970, loss = 1.83 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:42.056012: step 120980, loss = 1.93 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:43.245939: step 120990, loss = 1.88 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:49:44.431101: step 121000, loss = 1.92 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:49:45.569647: step 121010, loss = 2.00 (1124.2 examples/sec; 0.114 sec/batch)
2017-05-05 01:49:46.739451: step 121020, loss = 1.80 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:47.909771: step 121030, loss = 1.87 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:49.076731: step 121040, loss = 2.07 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:50.238689: step 121050, loss = 1.94 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:51.381414: step 121060, loss = 1.82 (1120.1 examples/sec; 0.114 sec/batch)
2017-05-05 01:49:52.543867: step 121070, loss = 1.90 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:53.693772: step 121080, loss = 1.94 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:54.855456: step 121090, loss = 1.97 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:56.030032: step 121100, loss = 1.76 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:57.208370: step 121110, loss = 1.85 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:49:58.365643: step 121120, loss = 1.87 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:59.549744: step 121130, loss = 1.82 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:50:00.724541: step 121140, loss = 1.98 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:01.898104: step 121150, loss = 1.90 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:03.060135: step 121160, loss = 1.97 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:04.232686: step 121170, loss = 1.99 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:05.382861: step 121180, loss = 1.95 (1112.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:50:06.544721: step 121190, loss = 1.92 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:07.707556: step 121200, loss = 1.83 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:08.857449: step 121210, loss = 1.89 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:50:10.019002: step 121220, loss = 1.89 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:11.183263: step 121230, loss = 1.94 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:12.345402: step 121240, loss = 1.74 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:13.516015: step 121250, loss = 1.96 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:14.664665: step 121260, loss = 1.87 (1114.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:50:15.823928: step 121270, loss = 1.95 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:16.983828: step 121280, loss = 1.99 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:18.148767: step 121290, loss = 1.88 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:19.312110: step 121300, loss = 1.82 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:20.487612: step 121310, loss = 1.87 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:50:21.644463: step 121320, loss = 1.96 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:22.824360: step 121330, loss = 1.90 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:50:24.000435: step 121340, loss = 1.83 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:50:25.168977: step 121350, loss = 1.93 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:26.326070: step 121360, loss = 1.80 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:27.517678: step 121370, loss = 1.91 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:50:28.686195: step 121380, loss = 1.78 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:29.856071: step 121390, loss = 1.98 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:31.048510: step 121400, loss = 2.06 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:50:32.204356: step 121410, loss = 1.97 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:33.366427: step 121420, loss = 1.81 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:34.533398: step 121430, loss = 1.83 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:35.702196: step 121440, loss = 1.85 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:36.861577: step 121450, loss = 1.85 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:38.024517: step 121460, loss = 1.98 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:39.200221: step 121470, loss = 2.04 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:50:40.359539: step 121480, loss = 1.80 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:41.514544: step 121490, loss = 1.99 (1108.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:50:42.690643: step 121500, loss = 1.87 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:50:43.877664: step 121510, loss = 1.96 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:50:45.065255: step 121520, loss = 1.87 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:50:46.230474: step 121530, loss = 1.91 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:47.391573: step 121540, loss = 1.85 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:48.566489: step 121550, loss = 1.92 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:49.708509: step 121560, loss = 1.80 (1120.8 examples/sec; 0.114 sec/batch)
2017-05-05 01:50:50.872141: step 121570, loss = 2.08 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:52.058923: step 121580, loss = 1.86 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:50:53.215857: step 121590, loss = 1.81 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:54.370441: step 121600, loss = 1.98 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:50:55.529673: step 121610, loss = 2.01 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:56.683666: step 121620, loss = 1.86 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:50:57.846262: step 121630, loss = 2.06 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:59.018839: step 121640, loss = 1.95 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:00.196487: step 121650, loss = 1.90 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:51:01.355537: step 121660, loss = 1.97 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:02.496713: step 121670, loss = 2.05 (1121.6 examples/sec; 0.114 sec/batch)
2017-05-05 01:51:03.659310: step 121680, loss = 2.01 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:04.819609: step 121690, loss = 1.70 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:05.970493: step 121700, loss = 1.90 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:51:07.167433: step 121710, loss = 1.89 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:51:08.331947: step 121720, loss = 1.84 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:09.475053: step 121730, loss = 1.86 (1119.8 examples/sec; 0.114 sec/batch)
2017-05-05 01:51:10.638843: step 121740, loss = 1.83 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:11.830736: step 121750, loss = 1.93 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:51:12.988720: step 121760, loss = 1.76 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:14.226400: step 121770, loss = 1.95 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-05 01:51:15.302134: step 121780, loss = 1.76 (1189.9 examples/sec; 0.108 sec/batch)
2017-05-05 01:51:16.471311: step 121790, loss = 1.95 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:17.631665: step 121800, loss = 1.87 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:18.786113: step 121810, loss = 1.80 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:51:19.934280: step 121820, loss = 1.89 (1114.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:51:21.095390: step 121830, loss = 1.96 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:22.248002: step 121840, loss = 1.88 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:51:23.416791: step 121850, loss = 1.83 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:24.583918: step 121860, loss = 1.84 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:25.754691: step 121870, loss = 1.90 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:26.918706: step 121880, loss = 1.90 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:28.103700: step 121890, loss = 2.03 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:51:29.283601: step 121900, loss = 1.82 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:51:30.449495: step 121910, loss = 1.88 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:31.596605: step 121920, loss = 1.95 (1115.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:51:32.764999: step 121930, loss = 1.95 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:33.926923: step 121940, loss = 1.78 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:35.116470: step 121950, loss = 1.81 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:51:36.274066: step 121960, loss = 1.81 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:37.446583: step 121970, loss = 1.88 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:38.598885: step 121980, loss = 1.81 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:51:39.773166: step 121990, loss = 1.78 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:40.913707: step 122000, loss = 1.95 (1122.3 examples/sec; 0.114 sec/batch)
2017-05-05 01:51:42.070127: step 122010, loss = 1.99 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:43.243087: step 122020, loss = 1.75 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:44.413987: step 122030, loss = 1.89 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:45.575565: step 122040, loss = 1.81 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:46.736661: step 122050, loss = 1.92 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:47.898836: step 122060, loss = 1.81 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:49.071085: step 122070, loss = 1.75 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:50.238628: step 122080, loss = 2.03 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:51.402485: step 122090, loss = 1.89 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:52.571221: step 122100, loss = 1.84 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:53.719862: step 122110, loss = 2.12 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:51:54.885725: step 122120, loss = 1.89 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:56.065079: step 122130, loss = 1.81 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:51:57.237044: step 122140, loss = 1.88 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:58.375027: step 122150, loss = 2.02 (1124.8 examples/sec; 0.114 sec/batch)
2017-05-05 01:51:59.544995: step 122160, loss = 1.89 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:00.713809: step 122170, loss = 1.86 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:01.862794: step 122180, loss = 1.98 (1114.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:52:03.024807: step 122190, loss = 1.86 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:52:04.179110: step 122200, loss = 1.89 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:52:05.336178: step 122210, loss = 1.84 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:52:06.498523: step 122220, loss = 1.91 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:52:07.672949: step 122230, loss = 1.77 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:08.853400: step 122240, loss = 1.88 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:10.027729: step 122250, loss = 1.88 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:11.232052: step 122260, loss = 1.93 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:52:12.415703: step 122270, loss = 2.06 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:13.575578: step 122280, loss = 1.97 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:52:14.747296: step 122290, loss = 1.74 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:15.928216: step 122300, loss = 1.85 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:17.091144: step 122310, loss = 1.79 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:52:18.273503: step 122320, loss = 2.02 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:19.465270: step 122330, loss = 1.71 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:20.662779: step 122340, loss = 1.95 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:52:21.837663: step 122350, loss = 1.77 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:23.027532: step 122360, loss = 1.97 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:24.200623: step 122370, loss = 1.80 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:25.388954: step 122380, loss = 1.78 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:26.567423: step 122390, loss = 2.03 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:27.752589: step 122400, loss = 1.78 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:28.946100: step 122410, loss = 1.98 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:30.099354: step 122420, loss = 1.89 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:52:31.291438: step 122430, loss = 1.78 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:32.464417: step 122440, loss = 1.77 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:33.624019: step 122450, loss = 1.82 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:52:34.813157: step 122460, loss = 1.89 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:36.013735: step 122470, loss = 1.83 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:52:37.207474: step 122480, loss = 1.86 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:38.395849: step 122490, loss = 1.75 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:39.603872: step 122500, loss = 1.83 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:52:40.806154: step 122510, loss = 2.00 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:52:41.986515: step 122520, loss = 2.01 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:43.178920: step 122530, loss = 2.19 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:44.377924: step 122540, loss = 1.90 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:52:45.560882: step 122550, loss = 1.77 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:46.737031: step 122560, loss = 1.79 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:47.918676: step 122570, loss = 1.79 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:49.084897: step 122580, loss = 1.94 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:50.265781: step 122590, loss = 1.77 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:51.437990: step 122600, loss = 1.90 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:52.623656: step 122610, loss = 1.87 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:53.790222: step 122620, loss = 2.01 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:54.967332: step 122630, loss = 1.85 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:56.131428: step 122640, loss = 2.09 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:52:57.289305: step 122650, loss = 1.89 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:52:58.460544: step 122660, loss = 1.88 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:59.609504: step 122670, loss = 1.90 (1114.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:53:00.782424: step 122680, loss = 1.94 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:53:01.926768: step 122690, loss = 1.87 (1118.5 examples/sec; 0.114 sec/batch)
2017-05-05 01:53:03.084819: step 122700, loss = 1.90 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:53:04.253361: step 122710, loss = 1.92 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:53:05.411688: step 122720, loss = 1.85 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:53:06.581736: step 122730, loss = 2.00 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:53:07.739616: step 122740, loss = 2.01 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:53:08.884301: step 122750, loss = 1.85 (1118.2 examples/sec; 0.114 sec/batch)
2017-05-05 01:53:10.145830: step 122760, loss = 1.98 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-05 01:53:11.229618: step 122770, loss = 1.78 (1181.0 examples/sec; 0.108 sec/batch)
2017-05-05 01:53:12.404974: step 122780, loss = 1.99 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:53:13.576948: step 122790, loss = 2.00 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:53:14.766315: step 122800, loss = 1.86 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:53:15.967964: step 122810, loss = 2.03 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:53:17.156532: step 122820, loss = 1.87 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:53:18.345612: step 122830, loss = 1.90 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:53:19.548358: step 122840, loss = 2.02 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:53:20.752590: step 122850, loss = 1.82 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:53:21.947546: step 122860, loss = 1.91 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:53:23.186983: step 122870, loss = 1.98 (1032.7 examples/sec; 0.124 sec/batch)
2017-05-05 01:53:24.392790: step 122880, loss = 1.87 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:53:25.601998: step 122890, loss = 2.04 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:53:26.825994: step 122900, loss = 1.92 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:28.024154: step 122910, loss = 1.97 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:53:29.245260: step 122920, loss = 1.85 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:30.468095: step 122930, loss = 1.85 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:31.695770: step 122940, loss = 1.95 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:53:32.927144: step 122950, loss = 2.11 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:53:34.194656: step 122960, loss = 2.08 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-05 01:53:35.320494: step 122970, loss = 1.84 (1136.9 examples/sec; 0.113 sec/batch)
2017-05-05 01:53:36.548533: step 122980, loss = 1.85 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-05 01:53:37.770648: step 122990, loss = 1.81 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:38.987330: step 123000, loss = 1.84 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:40.222553: step 123010, loss = 1.81 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-05 01:53:41.433675: step 123020, loss = 1.86 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:53:42.644940: step 123030, loss = 1.98 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:53:43.886004: step 123040, loss = 1.91 (1031.4 examples/sec; 0.124 sec/batch)
2017-05-05 01:53:45.101913: step 123050, loss = 1.75 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:46.319382: step 123060, loss = 1.92 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:47.542894: step 123070, loss = 1.82 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:48.759450: step 123080, loss = 2.12 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:49.978418: step 123090, loss = 1.93 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:51.187033: step 123100, loss = 1.90 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:53:52.424451: step 123110, loss = 2.04 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-05 01:53:53.641054: step 123120, loss = 1.94 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:54.863765: step 123130, loss = 2.06 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:56.063496: step 123140, loss = 2.15 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:53:57.300019: step 123150, loss = 1.94 (1035.2 examples/sec; 0.124 sec/batch)
2017-05-05 01:53:58.482880: step 123160, loss = 1.84 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:53:59.701030: step 123170, loss = 1.88 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:00.913012: step 123180, loss = 1.91 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:02.108876: step 123190, loss = 1.92 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:54:03.338356: step 123200, loss = 1.88 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:54:04.575069: step 123210, loss = 1.91 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-05 01:54:05.772144: step 123220, loss = 1.98 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:54:06.980765: step 123230, loss = 1.88 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:08.182722: step 123240, loss = 1.87 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:54:09.410214: step 123250, loss = 1.89 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:54:10.628110: step 123260, loss = 1.85 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:11.841786: step 123270, loss = 1.98 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:13.043773: step 123280, loss = 2.00 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:54:14.267376: step 123290, loss = 1.78 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:15.494958: step 123300, loss = 1.92 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:54:16.732922: step 123310, loss = 2.10 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-05 01:54:17.930685: step 123320, loss = 2.00 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:54:19.155286: step 123330, loss = 1.97 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:20.385268: step 123340, loss = 2.00 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:54:21.607977: step 123350, loss = 1.89 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:22.792169: step 123360, loss = 1.85 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:54:24.007351: step 123370, loss = 1.94 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:25.213362: step 123380, loss = 1.82 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:26.441183: step 123390, loss = 2.02 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:54:27.643062: step 123400, loss = 1.95 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:54:28.862575: step 123410, loss = 1.97 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:30.073613: step 123420, loss = 1.82 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:31.291113: step 123430, loss = 1.88 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:32.523910: step 123440, loss = 1.90 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-05 01:54:33.723275: step 123450, loss = 1.87 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:54:34.960566: step 123460, loss = 1.93 (1034.5 examples/sec; 0.124 sec/batch)
2017-05-05 01:54:36.160303: step 123470, loss = 1.78 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:54:37.385511: step 123480, loss = 1.88 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:54:38.612841: step 123490, loss = 1.86 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:54:39.828700: step 123500, loss = 1.94 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:41.038035: step 123510, loss = 1.94 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:42.237805: step 123520, loss = 1.88 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:54:43.468420: step 123530, loss = 1.98 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:54:44.691514: step 123540, loss = 1.82 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:45.882716: step 123550, loss = 2.07 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:54:47.075821: step 123560, loss = 1.71 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:54:48.277442: step 123570, loss = 1.96 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:54:49.529385: step 123580, loss = 1.95 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-05 01:54:50.744696: step 123590, loss = 1.76 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:51.984941: step 123600, loss = 1.97 (1032.1 examples/sec; 0.124 sec/batch)
2017-05-05 01:54:53.204786: step 123610, loss = 1.83 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:54.415547: step 123620, loss = 1.88 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:55.635182: step 123630, loss = 1.99 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:56.858623: step 123640, loss = 1.92 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:58.073697: step 123650, loss = 1.90 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:59.291921: step 123660, loss = 1.90 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:00.517028: step 123670, loss = 1.90 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:01.735422: step 123680, loss = 1.82 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:02.987760: step 123690, loss = 2.00 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-05 01:55:04.206612: step 123700, loss = 1.98 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:05.427481: step 123710, loss = 2.04 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:06.607533: step 123720, loss = 1.75 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:55:07.842654: step 123730, loss = 1.79 (1036.3 examples/sec; 0.124 sec/batch)
2017-05-05 01:55:09.060852: step 123740, loss = 1.99 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:10.377573: step 123750, loss = 2.07 (972.1 examples/sec; 0.132 sec/batch)
2017-05-05 01:55:11.436080: step 123760, loss = 1.98 (1209.2 examples/sec; 0.106 sec/batch)
2017-05-05 01:55:12.662685: step 123770, loss = 2.02 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:55:13.865595: step 123780, loss = 1.86 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:55:15.083815: step 123790, loss = 2.03 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:16.298267: step 123800, loss = 2.02 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:55:17.497857: step 123810, loss = 1.92 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:55:18.687283: step 123820, loss = 1.81 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:55:19.925382: step 123830, loss = 1.81 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-05 01:55:21.137614: step 123840, loss = 1.87 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:55:22.336880: step 123850, loss = 2.00 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:55:23.557017: step 123860, loss = 1.90 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:24.781868: step 123870, loss = 1.74 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:25.977900: step 123880, loss = 1.71 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:55:27.213295: step 123890, loss = 1.83 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-05 01:55:28.412600: step 123900, loss = 1.96 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:55:29.630658: step 123910, loss = 1.85 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:30.852924: step 123920, loss = 1.97 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:32.063359: step 123930, loss = 1.74 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:55:33.268577: step 123940, loss = 1.95 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:55:34.488875: step 123950, loss = 1.83 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:35.631471: step 123960, loss = 2.13 (1120.3 examples/sec; 0.114 sec/batch)
2017-05-05 01:55:36.838565: step 123970, loss = 1.74 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:55:37.997428: step 123980, loss = 1.92 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:55:39.177644: step 123990, loss = 2.01 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:55:40.378116: step 124000, loss = 1.83 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:55:41.556893: step 124010, loss = 1.97 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:55:42.732794: step 124020, loss = 1.91 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:55:43.918123: step 124030, loss = 1.97 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:55:45.093426: step 124040, loss = 2.00 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:55:46.265609: step 124050, loss = 1.73 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:55:47.456102: step 124060, loss = 1.79 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:55:48.625023: step 124070, loss = 1.86 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:55:49.791066: step 124080, loss = 1.87 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:55:50.973108: step 124090, loss = 1.86 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:55:52.110798: step 124100, loss = 1.96 (1125.1 examples/sec; 0.114 sec/batch)
2017-05-05 01:55:53.282555: step 124110, loss = 1.85 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:55:54.422096: step 124120, loss = 1.86 (1123.3 examples/sec; 0.114 sec/batch)
2017-05-05 01:55:55.600686: step 124130, loss = 1.86 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:55:56.772893: step 124140, loss = 1.86 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:55:57.937454: step 124150, loss = 2.08 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:55:59.074322: step 124160, loss = 2.07 (1125.9 examples/sec; 0.114 sec/batch)
2017-05-05 01:56:00.257417: step 124170, loss = 1.88 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:01.410121: step 124180, loss = 1.94 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:56:02.588521: step 124190, loss = 1.97 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:03.767740: step 124200, loss = 1.95 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:04.953907: step 124210, loss = 1.95 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:56:06.115878: step 124220, loss = 1.82 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:07.298346: step 124230, loss = 1.92 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:08.468812: step 124240, loss = 2.00 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:09.634296: step 124250, loss = 1.81 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:10.786531: step 124260, loss = 1.87 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:56:11.961510: step 124270, loss = 1.78 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:13.112834: step 124280, loss = 1.95 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:56:14.275256: step 124290, loss = 1.86 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:15.453443: step 124300, loss = 1.89 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:16.605823: step 124310, loss = 1.87 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:56:17.770740: step 124320, loss = 1.80 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:18.915854: step 124330, loss = 1.97 (1117.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:56:20.085990: step 124340, loss = 1.92 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:21.256741: step 124350, loss = 2.04 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:22.438805: step 124360, loss = 1.90 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:23.633100: step 124370, loss = 1.98 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:56:24.838663: step 124380, loss = 1.75 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:56:26.031183: step 124390, loss = 1.81 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:56:27.214058: step 124400, loss = 1.98 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:28.385186: step 124410, loss = 1.92 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:29.566166: step 124420, loss = 1.92 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:30.755719: step 124430, loss = 2.01 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:56:31.963690: step 124440, loss = 1.93 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:56:33.152186: step 124450, loss = 1.97 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:56:34.323156: step 124460, loss = 1.92 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:35.496603: step 124470, loss = 1.95 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:36.664915: step 124480, loss = 1.79 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:37.810464: step 124490, loss = 1.96 (1117.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:56:38.970697: step 124500, loss = 1.85 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:40.140969: step 124510, loss = 1.89 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:41.318615: step 124520, loss = 1.94 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:42.472831: step 124530, loss = 1.91 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:56:43.633725: step 124540, loss = 1.95 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:44.825723: step 124550, loss = 1.89 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:56:45.977213: step 124560, loss = 1.88 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:56:47.148214: step 124570, loss = 1.89 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:48.330998: step 124580, loss = 1.93 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:49.501816: step 124590, loss = 1.80 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:50.655210: step 124600, loss = 1.76 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:56:51.830733: step 124610, loss = 1.95 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:52.975713: step 124620, loss = 2.06 (1117.9 examples/sec; 0.114 sec/batch)
2017-05-05 01:56:54.147516: step 124630, loss = 1.90 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:55.315682: step 124640, loss = 1.85 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:56.485390: step 124650, loss = 1.96 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:57.650111: step 124660, loss = 1.79 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:58.809852: step 124670, loss = 1.87 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:59.976717: step 124680, loss = 2.16 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:01.136004: step 124690, loss = 2.00 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:02.311881: step 124700, loss = 1.78 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:03.488413: step 124710, loss = 1.76 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:04.673552: step 124720, loss = 1.91 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:57:05.859985: step 124730, loss = 1.85 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:57:07.112043: step 124740, loss = 1.91 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-05 01:57:08.188875: step 124750, loss = 1.83 (1188.7 examples/sec; 0.108 sec/batch)
2017-05-05 01:57:09.365694: step 124760, loss = 1.91 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:10.533211: step 124770, loss = 1.96 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:11.707802: step 124780, loss = 2.03 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:12.888827: step 124790, loss = 2.03 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:14.048855: step 124800, loss = 1.98 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:15.215757: step 124810, loss = 1.99 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:16.375277: step 124820, loss = 1.89 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:17.542127: step 124830, loss = 1.89 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:18.709925: step 124840, loss = 1.91 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:19.875987: step 124850, loss = 1.97 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:21.043917: step 124860, loss = 1.79 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:22.197913: step 124870, loss = 1.80 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:57:23.355295: step 124880, loss = 1.86 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:24.517785: step 124890, loss = 1.92 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:25.679422: step 124900, loss = 1.93 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:26.854493: step 124910, loss = 1.80 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:28.027289: step 124920, loss = 1.78 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:29.187268: step 124930, loss = 1.97 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:30.333507: step 124940, loss = 1.90 (1116.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:57:31.503850: step 124950, loss = 1.96 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:32.670009: step 124960, loss = 1.86 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:33.839304: step 124970, loss = 1.86 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:35.020519: step 124980, loss = 1.97 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:36.186602: step 124990, loss = 1.81 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:37.340449: step 125000, loss = 1.91 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:57:38.497838: step 125010, loss = 2.04 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:39.676814: step 125020, loss = 1.94 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:40.838171: step 125030, loss = 1.91 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:42.012097: step 125040, loss = 1.87 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:43.184270: step 125050, loss = 1.93 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:44.357836: step 125060, loss = 1.86 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:45.511687: step 125070, loss = 2.12 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:57:46.672866: step 125080, loss = 1.94 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:47.853934: step 125090, loss = 1.82 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:49.020736: step 125100, loss = 1.93 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:50.164420: step 125110, loss = 1.96 (1119.2 examples/sec; 0.114 sec/batch)
2017-05-05 01:57:51.344620: step 125120, loss = 1.92 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:52.545576: step 125130, loss = 2.02 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:57:53.743039: step 125140, loss = 2.05 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:57:54.944588: step 125150, loss = 2.00 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:57:56.136771: step 125160, loss = 1.99 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:57:57.323990: step 125170, loss = 1.82 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:57:58.501890: step 125180, loss = 2.02 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:59.692622: step 125190, loss = 1.84 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:58:00.874604: step 125200, loss = 1.90 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:02.025985: step 125210, loss = 1.91 (1111.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:58:03.216595: step 125220, loss = 2.01 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:58:04.365841: step 125230, loss = 1.76 (1113.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:58:05.521114: step 125240, loss = 1.93 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:06.679836: step 125250, loss = 1.84 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:07.868413: step 125260, loss = 1.88 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:58:09.053013: step 125270, loss = 1.90 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:10.201073: step 125280, loss = 1.93 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:58:11.371034: step 125290, loss = 2.04 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:12.536718: step 125300, loss = 1.91 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:13.703159: step 125310, loss = 1.87 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:14.867515: step 125320, loss = 1.89 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:16.027806: step 125330, loss = 1.89 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:17.196649: step 125340, loss = 1.96 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:18.338606: step 125350, loss = 1.92 (1120.9 examples/sec; 0.114 sec/batch)
2017-05-05 01:58:19.518902: step 125360, loss = 1.92 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:20.700652: step 125370, loss = 1.86 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:21.849145: step 125380, loss = 1.88 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:58:23.036069: step 125390, loss = 1.98 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:58:24.199126: step 125400, loss = 1.92 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:25.369722: step 125410, loss = 1.79 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:26.516201: step 125420, loss = 1.83 (1116.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:58:27.705798: step 125430, loss = 1.79 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:58:28.866445: step 125440, loss = 1.79 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:30.014257: step 125450, loss = 1.89 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:58:31.185891: step 125460, loss = 1.92 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:32.324484: step 125470, loss = 1.87 (1124.2 examples/sec; 0.114 sec/batch)
2017-05-05 01:58:33.503984: step 125480, loss = 1.79 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:34.644336: step 125490, loss = 1.93 (1122.5 examples/sec; 0.114 sec/batch)
2017-05-05 01:58:35.825851: step 125500, loss = 1.97 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:36.975462: step 125510, loss = 1.90 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:58:38.135864: step 125520, loss = 2.07 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:39.307291: step 125530, loss = 1.97 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:40.467803: step 125540, loss = 1.80 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:41.632133: step 125550, loss = 1.73 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:42.811279: step 125560, loss = 1.98 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:44.000627: step 125570, loss = 1.91 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:58:45.182929: step 125580, loss = 1.92 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:46.358651: step 125590, loss = 1.91 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:47.538578: step 125600, loss = 1.81 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:48.710139: step 125610, loss = 1.81 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:49.887527: step 125620, loss = 1.96 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:51.055995: step 125630, loss = 1.86 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:52.231568: step 125640, loss = 1.85 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:53.392940: step 125650, loss = 1.86 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:54.583529: step 125660, loss = 1.77 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:58:55.754709: step 125670, loss = 1.91 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:56.911210: step 125680, loss = 1.87 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:58.076185: step 125690, loss = 1.80 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:59.254132: step 125700, loss = 1.95 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:00.421642: step 125710, loss = 1.84 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:01.597809: step 125720, loss = 1.79 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:02.856598: step 125730, loss = 1.86 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-05 01:59:03.924841: step 125740, loss = 1.81 (1198.2 examples/sec; 0.107 sec/batch)
2017-05-05 01:59:05.086873: step 125750, loss = 2.01 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:06.249446: step 125760, loss = 1.89 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:07.429216: step 125770, loss = 1.85 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:08.596441: step 125780, loss = 1.87 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:09.757754: step 125790, loss = 1.97 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:10.934406: step 125800, loss = 1.73 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:12.116475: step 125810, loss = 2.08 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:13.308510: step 125820, loss = 1.93 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:59:14.479150: step 125830, loss = 2.01 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:15.658521: step 125840, loss = 1.95 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:16.820735: step 125850, loss = 2.01 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:18.006970: step 125860, loss = 2.02 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:59:19.196140: step 125870, loss = 1.92 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:59:20.385188: step 125880, loss = 1.94 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:59:21.568661: step 125890, loss = 1.85 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:22.738396: step 125900, loss = 1.91 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:23.913335: step 125910, loss = 1.83 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:25.083501: step 125920, loss = 1.93 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:26.235671: step 125930, loss = 1.84 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:59:27.409418: step 125940, loss = 1.84 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:28.562568: step 125950, loss = 1.77 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:59:29.725033: step 125960, loss = 1.84 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:30.900586: step 125970, loss = 1.99 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:32.071297: step 125980, loss = 1.90 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:33.233034: step 125990, loss = 1.84 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:34.388530: step 126000, loss = 1.75 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:35.565226: step 126010, loss = 1.91 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:36.724690: step 126020, loss = 1.86 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:37.887187: step 126030, loss = 1.97 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:39.042936: step 126040, loss = 1.89 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:40.223698: step 126050, loss = 1.86 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:41.386936: step 126060, loss = 1.90 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:42.540416: step 126070, loss = 1.92 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:59:43.700453: step 126080, loss = 1.95 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:44.894955: step 126090, loss = 1.88 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:59:46.038525: step 126100, loss = 1.89 (1119.3 examples/sec; 0.114 sec/batch)
2017-05-05 01:59:47.205017: step 126110, loss = 2.02 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:48.385153: step 126120, loss = 1.78 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:49.561591: step 126130, loss = 1.90 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:50.727731: step 126140, loss = 1.97 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:51.918222: step 126150, loss = 1.92 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:59:53.094696: step 126160, loss = 1.89 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:54.255661: step 126170, loss = 1.88 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:55.416346: step 126180, loss = 1.84 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:56.598803: step 126190, loss = 1.88 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:57.783371: step 126200, loss = 1.82 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:58.956789: step 126210, loss = 1.86 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:00.142909: step 126220, loss = 1.80 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:00:01.320182: step 126230, loss = 1.91 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:02.466935: step 126240, loss = 1.81 (1116.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:00:03.631462: step 126250, loss = 1.93 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:04.807754: step 126260, loss = 1.85 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:05.969865: step 126270, loss = 1.87 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:07.137067: step 126280, loss = 1.87 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:08.298514: step 126290, loss = 1.94 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:09.477010: step 126300, loss = 1.87 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:10.627467: step 126310, loss = 1.82 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:00:11.793570: step 126320, loss = 1.97 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:12.983393: step 126330, loss = 1.77 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:00:14.147067: step 126340, loss = 1.90 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:15.305037: step 126350, loss = 2.01 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:16.498859: step 126360, loss = 1.90 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:00:17.652774: step 126370, loss = 1.97 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:00:18.838616: step 126380, loss = 1.81 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:00:19.990431: step 126390, loss = 1.84 (1111.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:00:21.167492: step 126400, loss = 2.04 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:22.322647: step 126410, loss = 1.76 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:23.495813: step 126420, loss = 1.85 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:24.665131: step 126430, loss = 1.96 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:25.799141: step 126440, loss = 1.91 (1128.7 examples/sec; 0.113 sec/batch)
2017-05-05 02:00:26.991672: step 126450, loss = 1.80 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:00:28.149091: step 126460, loss = 1.95 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:29.320595: step 126470, loss = 1.93 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:30.486267: step 126480, loss = 1.83 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:31.635349: step 126490, loss = 1.96 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:00:32.815443: step 126500, loss = 1.85 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:33.952900: step 126510, loss = 1.94 (1125.3 examples/sec; 0.114 sec/batch)
2017-05-05 02:00:35.114538: step 126520, loss = 1.92 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:36.279925: step 126530, loss = 1.93 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:37.438861: step 126540, loss = 1.79 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:38.624345: step 126550, loss = 1.98 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:00:39.776984: step 126560, loss = 1.84 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:00:40.944103: step 126570, loss = 2.02 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:42.110825: step 126580, loss = 1.98 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:43.289326: step 126590, loss = 1.90 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:44.452063: step 126600, loss = 1.89 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:45.621464: step 126610, loss = 2.00 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:46.811177: step 126620, loss = 1.83 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:00:47.989975: step 126630, loss = 1.85 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:49.157939: step 126640, loss = 1.84 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:50.322906: step 126650, loss = 1.95 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:51.484724: step 126660, loss = 1.92 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:52.679298: step 126670, loss = 1.85 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:00:53.841539: step 126680, loss = 1.86 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:55.001915: step 126690, loss = 1.85 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:56.155259: step 126700, loss = 1.86 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:00:57.334527: step 126710, loss = 1.89 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:58.585838: step 126720, loss = 1.97 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-05 02:00:59.668906: step 126730, loss = 1.84 (1181.8 examples/sec; 0.108 sec/batch)
2017-05-05 02:01:00.847064: step 126740, loss = 1.88 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:02.044420: step 126750, loss = 1.93 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:01:03.221183: step 126760, loss = 2.01 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:04.377355: step 126770, loss = 1.89 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:01:05.553171: step 126780, loss = 1.78 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:06.718428: step 126790, loss = 1.79 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:01:07.874605: step 126800, loss = 1.83 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:01:09.043863: step 126810, loss = 1.86 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:01:10.196050: step 126820, loss = 1.68 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:01:11.362144: step 126830, loss = 1.82 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:01:12.512673: step 126840, loss = 1.88 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:01:13.661630: step 126850, loss = 1.97 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:01:14.827221: step 126860, loss = 1.88 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:01:16.004533: step 126870, loss = 1.94 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:17.168487: step 126880, loss = 1.87 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:01:18.336782: step 126890, loss = 1.93 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:01:19.503650: step 126900, loss = 2.05 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:01:20.653539: step 126910, loss = 2.14 (1113.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:01:21.841848: step 126920, loss = 1.90 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:23.017265: step 126930, loss = 1.81 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:24.195126: step 126940, loss = 1.99 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:25.366955: step 126950, loss = 1.94 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:01:26.550483: step 126960, loss = 1.92 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:27.739612: step 126970, loss = 1.89 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:28.944710: step 126980, loss = 1.95 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:01:30.134987: step 126990, loss = 1.85 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:31.334142: step 127000, loss = 1.78 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:01:32.518722: step 127010, loss = 2.05 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:33.699850: step 127020, loss = 2.01 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:34.908026: step 127030, loss = 1.88 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:01:36.097749: step 127040, loss = 2.01 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:37.281625: step 127050, loss = 1.86 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:38.474069: step 127060, loss = 1.84 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:39.661124: step 127070, loss = 1.85 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:40.851589: step 127080, loss = 1.97 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:42.043168: step 127090, loss = 1.91 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:43.219753: step 127100, loss = 1.85 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:44.428037: step 127110, loss = 1.89 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:01:45.638502: step 127120, loss = 1.86 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:01:46.820501: step 127130, loss = 1.99 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:48.019788: step 127140, loss = 1.97 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:01:49.198147: step 127150, loss = 2.06 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:50.400510: step 127160, loss = 1.78 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:01:51.597500: step 127170, loss = 2.14 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:01:52.798637: step 127180, loss = 2.00 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:01:53.974064: step 127190, loss = 2.00 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:55.185034: step 127200, loss = 1.94 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:01:56.386437: step 127210, loss = 1.80 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:01:57.571542: step 127220, loss = 1.79 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:58.763937: step 127230, loss = 1.96 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:59.963574: step 127240, loss = 2.01 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:01.170415: step 127250, loss = 2.03 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:02:02.356530: step 127260, loss = 1.97 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:03.565104: step 127270, loss = 1.95 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:02:04.757995: step 127280, loss = 1.86 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:05.942553: step 127290, loss = 2.02 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:07.131103: step 127300, loss = 1.77 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:08.329062: step 127310, loss = 1.83 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:09.510041: step 127320, loss = 1.88 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:10.696941: step 127330, loss = 2.19 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:11.899929: step 127340, loss = 1.89 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:13.100899: step 127350, loss = 1.93 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:14.292604: step 127360, loss = 2.06 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:15.483183: step 127370, loss = 1.95 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:16.668885: step 127380, loss = 1.84 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:17.844702: step 127390, loss = 1.90 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:19.027496: step 127400, loss = 1.89 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:20.214475: step 127410, loss = 1.82 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:21.388220: step 127420, loss = 1.87 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:02:22.580624: step 127430, loss = 1.97 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:23.776470: step 127440, loss = 1.89 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:24.968700: step 127450, loss = 1.66 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:26.145346: step 127460, loss = 1.95 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:27.341700: step 127470, loss = 1.90 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:28.521488: step 127480, loss = 1.89 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:29.696774: step 127490, loss = 1.79 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:30.877345: step 127500, loss = 1.95 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:32.069364: step 127510, loss = 1.95 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:33.253535: step 127520, loss = 1.87 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:34.444489: step 127530, loss = 1.98 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:35.634200: step 127540, loss = 1.92 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:36.825678: step 127550, loss = 1.96 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:38.000964: step 127560, loss = 2.03 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:39.209398: step 127570, loss = 2.04 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:02:40.409298: step 127580, loss = 1.77 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:41.594206: step 127590, loss = 1.81 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:42.820095: step 127600, loss = 1.72 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-05 02:02:44.015513: step 127610, loss = 2.01 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:45.231625: step 127620, loss = 1.85 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:02:46.429731: step 127630, loss = 1.92 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:47.624594: step 127640, loss = 2.13 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:48.795863: step 127650, loss = 2.04 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:02:49.966200: step 127660, loss = 1.85 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:02:51.168259: step 127670, loss = 1.91 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:52.348711: step 127680, loss = 1.87 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:53.520780: step 127690, loss = 1.88 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:02:54.695032: step 127700, loss = 1.79 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:02:55.955529: step 127710, loss = 1.96 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-05 02:02:57.027648: step 127720, loss = 1.95 (1193.9 examples/sec; 0.107 sec/batch)
2017-05-05 02:02:58.185307: step 127730, loss = 1.91 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:02:59.353186: step 127740, loss = 1.73 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:00.528523: step 127750, loss = 1.85 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:01.672624: step 127760, loss = 1.80 (1118.8 examples/sec; 0.114 sec/batch)
2017-05-05 02:03:02.832557: step 127770, loss = 1.78 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:04.016537: step 127780, loss = 1.89 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:05.191653: step 127790, loss = 1.97 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:06.337102: step 127800, loss = 1.85 (1117.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:03:07.499738: step 127810, loss = 1.85 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:08.642352: step 127820, loss = 1.80 (1120.2 examples/sec; 0.114 sec/batch)
2017-05-05 02:03:09.803726: step 127830, loss = 2.00 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:10.969309: step 127840, loss = 1.98 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:12.141561: step 127850, loss = 1.90 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:13.317659: step 127860, loss = 1.91 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:14.465432: step 127870, loss = 1.90 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:03:15.629384: step 127880, loss = 1.90 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:16.799569: step 127890, loss = 1.85 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:17.965438: step 127900, loss = 1.92 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:19.161148: step 127910, loss = 1.86 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:03:20.310297: step 127920, loss = 1.84 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:03:21.461124: step 127930, loss = 1.80 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:03:22.617915: step 127940, loss = 1.86 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:23.802966: step 127950, loss = 1.76 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:03:24.987461: step 127960, loss = 1.87 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:26.137868: step 127970, loss = 2.12 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:03:27.313416: step 127980, loss = 1.71 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:28.488581: step 127990, loss = 1.93 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:29.663933: step 128000, loss = 1.86 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:30.824745: step 128010, loss = 2.05 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:32.023670: step 128020, loss = 1.97 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:03:33.197961: step 128030, loss = 1.80 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:34.373365: step 128040, loss = 1.75 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:35.523369: step 128050, loss = 1.95 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:03:36.689545: step 128060, loss = 1.98 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:37.839695: step 128070, loss = 1.83 (1112.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:03:39.008392: step 128080, loss = 2.02 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:40.213503: step 128090, loss = 1.91 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:03:41.379820: step 128100, loss = 1.92 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:42.546705: step 128110, loss = 1.85 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:43.727851: step 128120, loss = 1.85 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:44.894749: step 128130, loss = 2.04 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:46.068211: step 128140, loss = 1.91 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:47.229428: step 128150, loss = 1.76 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:48.393671: step 128160, loss = 1.92 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:49.541663: step 128170, loss = 1.70 (1115.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:03:50.692503: step 128180, loss = 1.96 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:03:51.861384: step 128190, loss = 1.90 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:53.015417: step 128200, loss = 1.86 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:03:54.184786: step 128210, loss = 2.02 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:55.343388: step 128220, loss = 1.76 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:56.506561: step 128230, loss = 2.01 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:57.681041: step 128240, loss = 1.97 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:58.856654: step 128250, loss = 1.80 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:04:00.025904: step 128260, loss = 1.91 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:01.215991: step 128270, loss = 1.83 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:04:02.348146: step 128280, loss = 1.86 (1130.6 examples/sec; 0.113 sec/batch)
2017-05-05 02:04:03.507187: step 128290, loss = 1.96 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:04.675186: step 128300, loss = 1.74 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:05.809172: step 128310, loss = 1.94 (1128.8 examples/sec; 0.113 sec/batch)
2017-05-05 02:04:06.991886: step 128320, loss = 1.91 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:04:08.163446: step 128330, loss = 1.79 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:09.342594: step 128340, loss = 1.80 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:04:10.502385: step 128350, loss = 1.85 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:11.690655: step 128360, loss = 1.92 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:04:12.835194: step 128370, loss = 1.98 (1118.4 examples/sec; 0.114 sec/batch)
2017-05-05 02:04:13.994389: step 128380, loss = 1.93 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:15.164477: step 128390, loss = 2.05 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:16.307847: step 128400, loss = 1.90 (1119.5 examples/sec; 0.114 sec/batch)
2017-05-05 02:04:17.476278: step 128410, loss = 1.92 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:18.646261: step 128420, loss = 1.78 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:19.818096: step 128430, loss = 1.81 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:20.988979: step 128440, loss = 1.92 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:22.173938: step 128450, loss = 1.77 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:04:23.334286: step 128460, loss = 1.83 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:24.519032: step 128470, loss = 1.80 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:04:25.669772: step 128480, loss = 1.87 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:04:26.848858: step 128490, loss = 1.80 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:04:28.006533: step 128500, loss = 1.84 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:29.178984: step 128510, loss = 1.95 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:30.322371: step 128520, loss = 1.88 (1119.5 examples/sec; 0.114 sec/batch)
2017-05-05 02:04:31.479301: step 128530, loss = 1.88 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:32.649266: step 128540, loss = 1.84 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:33.806022: step 128550, loss = 1.86 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:34.977320: step 128560, loss = 2.10 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:36.148301: step 128570, loss = 1.88 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:37.308543: step 128580, loss = 1.82 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:38.464477: step 128590, loss = 1.95 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:39.650563: step 128600, loss = 1.80 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:04:40.800456: step 128610, loss = 2.15 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:04:41.962370: step 128620, loss = 2.03 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:43.134454: step 128630, loss = 1.97 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:44.289843: step 128640, loss = 1.90 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:45.462008: step 128650, loss = 2.01 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:46.604273: step 128660, loss = 1.94 (1120.6 examples/sec; 0.114 sec/batch)
2017-05-05 02:04:47.767207: step 128670, loss = 1.92 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:48.926368: step 128680, loss = 2.06 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:50.075925: step 128690, loss = 1.95 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:04:51.359065: step 128700, loss = 1.87 (997.6 examples/sec; 0.128 sec/batch)
2017-05-05 02:04:52.440137: step 128710, loss = 1.74 (1184.0 examples/sec; 0.108 sec/batch)
2017-05-05 02:04:53.616623: step 128720, loss = 1.79 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:04:54.800287: step 128730, loss = 1.98 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:04:55.982834: step 128740, loss = 2.05 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:04:57.172606: step 128750, loss = 1.89 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:04:58.297645: step 128760, loss = 1.77 (1137.7 examples/sec; 0.113 sec/batch)
2017-05-05 02:04:59.458022: step 128770, loss = 1.86 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:00.618281: step 128780, loss = 2.10 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:01.790621: step 128790, loss = 1.96 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:02.959930: step 128800, loss = 1.82 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:04.132329: step 128810, loss = 1.97 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:05.316856: step 128820, loss = 1.88 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:05:06.467334: step 128830, loss = 1.80 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:05:07.635295: step 128840, loss = 1.85 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:08.815307: step 128850, loss = 2.09 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:05:09.949953: step 128860, loss = 1.92 (1128.1 examples/sec; 0.113 sec/batch)
2017-05-05 02:05:11.130278: step 128870, loss = 1.80 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:05:12.302479: step 128880, loss = 1.88 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:13.470254: step 128890, loss = 1.85 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:14.632631: step 128900, loss = 2.13 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:15.810426: step 128910, loss = 1.83 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:05:16.956891: step 128920, loss = 1.91 (1116.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:05:18.108627: step 128930, loss = 1.84 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:05:19.282323: step 128940, loss = 1.88 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:20.431626: step 128950, loss = 1.81 (1113.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:05:21.605392: step 128960, loss = 1.86 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:22.764632: step 128970, loss = 1.98 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:23.929752: step 128980, loss = 1.91 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:25.085272: step 128990, loss = 1.95 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:26.253433: step 129000, loss = 1.85 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:27.428708: step 129010, loss = 1.86 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:05:28.613993: step 129020, loss = 1.91 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:05:29.786199: step 129030, loss = 1.98 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:30.941391: step 129040, loss = 1.85 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:32.101223: step 129050, loss = 2.02 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:33.260546: step 129060, loss = 1.87 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:34.416131: step 129070, loss = 1.88 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:35.588610: step 129080, loss = 1.83 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:36.760155: step 129090, loss = 1.89 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:37.902649: step 129100, loss = 1.91 (1120.4 examples/sec; 0.114 sec/batch)
2017-05-05 02:05:39.073438: step 129110, loss = 1.85 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:40.227740: step 129120, loss = 1.87 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:05:41.400450: step 129130, loss = 1.92 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:42.558861: step 129140, loss = 1.88 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:43.744450: step 129150, loss = 1.80 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:05:44.910357: step 129160, loss = 1.87 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:46.061056: step 129170, loss = 1.89 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:05:47.223198: step 129180, loss = 1.81 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:48.401501: step 129190, loss = 1.96 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:05:49.586643: step 129200, loss = 1.97 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:05:50.763190: step 129210, loss = 1.94 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:05:51.914146: step 129220, loss = 1.88 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:05:53.091883: step 129230, loss = 1.92 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:05:54.277433: step 129240, loss = 2.01 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:05:55.476207: step 129250, loss = 2.04 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:05:56.665451: step 129260, loss = 1.85 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:05:57.849591: step 129270, loss = 1.90 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:05:59.043054: step 129280, loss = 1.82 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:00.224191: step 129290, loss = 1.85 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:01.404105: step 129300, loss = 1.82 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:02.584085: step 129310, loss = 1.81 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:03.785587: step 129320, loss = 1.96 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:06:04.976077: step 129330, loss = 1.83 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:06.157632: step 129340, loss = 1.90 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:07.356884: step 129350, loss = 1.80 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:06:08.550556: step 129360, loss = 1.97 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:09.726588: step 129370, loss = 1.92 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:10.897098: step 129380, loss = 1.77 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:12.105483: step 129390, loss = 1.94 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:06:13.297982: step 129400, loss = 1.94 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:14.469300: step 129410, loss = 1.79 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:15.646847: step 129420, loss = 1.76 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:16.849025: step 129430, loss = 1.98 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:06:18.027987: step 129440, loss = 1.77 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:19.223158: step 129450, loss = 1.85 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:06:20.385214: step 129460, loss = 1.92 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:06:21.552338: step 129470, loss = 1.88 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:22.719705: step 129480, loss = 1.95 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:23.875842: step 129490, loss = 2.03 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:06:25.035950: step 129500, loss = 1.95 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:06:26.187943: step 129510, loss = 1.98 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:06:27.372688: step 129520, loss = 1.79 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:28.552370: step 129530, loss = 1.81 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:29.693471: step 129540, loss = 1.81 (1121.7 examples/sec; 0.114 sec/batch)
2017-05-05 02:06:30.858811: step 129550, loss = 1.93 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:32.048406: step 129560, loss = 1.89 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:33.234475: step 129570, loss = 1.84 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:34.403068: step 129580, loss = 1.85 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:35.567412: step 129590, loss = 1.84 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:06:36.746334: step 129600, loss = 1.85 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:37.898184: step 129610, loss = 1.85 (1111.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:06:39.079820: step 129620, loss = 1.98 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:40.248707: step 129630, loss = 1.85 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:41.432110: step 129640, loss = 1.92 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:42.596951: step 129650, loss = 1.91 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:06:43.779726: step 129660, loss = 1.94 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:44.954301: step 129670, loss = 1.73 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:46.153118: step 129680, loss = 1.96 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:06:47.448714: step 129690, loss = 1.93 (988.0 examples/sec; 0.130 sec/batch)
2017-05-05 02:06:48.562550: step 129700, loss = 2.07 (1149.2 examples/sec; 0.111 sec/batch)
2017-05-05 02:06:49.746851: step 129710, loss = 2.17 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:50.935513: step 129720, loss = 1.89 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:52.137633: step 129730, loss = 1.90 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:06:53.339080: step 129740, loss = 2.05 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:06:54.517667: step 129750, loss = 2.04 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:55.708331: step 129760, loss = 1.74 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:56.908921: step 129770, loss = 1.88 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:06:58.078405: step 129780, loss = 1.79 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:59.262212: step 129790, loss = 1.98 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:00.440618: step 129800, loss = 1.80 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:01.618356: step 129810, loss = 1.84 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:02.794439: step 129820, loss = 2.08 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:03.949553: step 129830, loss = 2.01 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:05.109117: step 129840, loss = 1.78 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:06.274494: step 129850, loss = 2.03 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:07.446978: step 129860, loss = 1.74 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:08.646474: step 129870, loss = 2.05 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:07:09.850734: step 129880, loss = 1.87 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:07:11.068107: step 129890, loss = 2.04 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:07:12.278341: step 129900, loss = 1.98 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:07:13.494561: step 129910, loss = 1.86 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:07:14.707348: step 129920, loss = 1.73 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:07:15.902159: step 129930, loss = 1.93 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:07:17.100208: step 129940, loss = 2.03 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:07:18.274754: step 129950, loss = 1.96 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:19.458043: step 129960, loss = 1.91 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:20.643602: step 129970, loss = 1.99 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:07:21.815776: step 129980, loss = 2.16 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:23.006821: step 129990, loss = 1.77 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:07:24.174743: step 130000, loss = 1.95 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:25.324360: step 130010, loss = 1.85 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:07:26.482520: step 130020, loss = 1.75 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:27.645807: step 130030, loss = 1.91 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:28.834950: step 130040, loss = 2.00 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:07:29.992224: step 130050, loss = 1.74 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:31.155093: step 130060, loss = 1.90 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:32.320560: step 130070, loss = 1.93 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:33.489557: step 130080, loss = 2.03 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:34.667708: step 130090, loss = 1.97 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:35.842120: step 130100, loss = 1.80 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:37.010843: step 130110, loss = 1.99 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:38.189451: step 130120, loss = 1.79 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:39.344911: step 130130, loss = 2.03 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:40.511212: step 130140, loss = 1.92 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:41.674219: step 130150, loss = 1.84 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:42.843069: step 130160, loss = 1.92 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:44.039059: step 130170, loss = 2.00 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:07:45.204611: step 130180, loss = 1.87 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:46.356058: step 130190, loss = 2.06 (1111.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:07:47.520716: step 130200, loss = 1.78 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:48.702483: step 130210, loss = 1.99 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:49.855355: step 130220, loss = 1.89 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:07:51.031850: step 130230, loss = 1.85 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:52.194708: step 130240, loss = 1.76 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:53.356664: step 130250, loss = 2.02 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:54.513847: step 130260, loss = 1.75 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:55.668711: step 130270, loss = 1.82 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:07:56.835028: step 130280, loss = 1.81 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:57.994744: step 130290, loss = 1.99 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:59.178916: step 130300, loss = 1.95 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:08:00.355440: step 130310, loss = 1.95 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:08:01.519531: step 130320, loss = 1.98 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:02.698694: step 130330, loss = 1.86 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:08:03.887985: step 130340, loss = 1.95 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:08:05.061367: step 130350, loss = 1.86 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:06.220192: step 130360, loss = 1.88 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:07.386075: step 130370, loss = 1.89 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:08.563959: step 130380, loss = 1.86 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:08:09.730109: step 130390, loss = 1.80 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:10.882100: step 130400, loss = 1.80 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:08:12.049599: step 130410, loss = 2.02 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:13.211066: step 130420, loss = 1.80 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:14.352611: step 130430, loss = 1.89 (1121.3 examples/sec; 0.114 sec/batch)
2017-05-05 02:08:15.533030: step 130440, loss = 1.92 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:08:16.718129: step 130450, loss = 1.96 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:08:17.860266: step 130460, loss = 1.89 (1120.7 examples/sec; 0.114 sec/batch)
2017-05-05 02:08:19.032165: step 130470, loss = 1.71 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:20.217986: step 130480, loss = 1.78 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:08:21.385119: step 130490, loss = 1.99 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:22.554572: step 130500, loss = 1.97 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:23.724348: step 130510, loss = 1.84 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:24.888558: step 130520, loss = 1.77 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:26.036354: step 130530, loss = 2.00 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:08:27.187818: step 130540, loss = 1.76 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:08:28.380929: step 130550, loss = 1.90 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:08:29.536428: step 130560, loss = 1.83 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:30.695932: step 130570, loss = 1.79 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:31.862263: step 130580, loss = 2.00 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:33.058818: step 130590, loss = 1.74 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:08:34.209010: step 130600, loss = 1.87 (1112.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:08:35.355839: step 130610, loss = 1.93 (1116.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:08:36.519078: step 130620, loss = 1.88 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:37.713919: step 130630, loss = 1.87 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:08:38.890664: step 130640, loss = 1.86 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:08:40.069674: step 130650, loss = 1.97 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:08:41.231207: step 130660, loss = 1.99 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:42.375803: step 130670, loss = 1.92 (1118.3 examples/sec; 0.114 sec/batch)
2017-05-05 02:08:43.635644: step 130680, loss = 1.96 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-05 02:08:44.716969: step 130690, loss = 1.92 (1183.7 examples/sec; 0.108 sec/batch)
2017-05-05 02:08:45.858826: step 130700, loss = 1.99 (1121.0 examples/sec; 0.114 sec/batch)
2017-05-05 02:08:47.007788: step 130710, loss = 1.83 (1114.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:08:48.179124: step 130720, loss = 1.88 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:49.365613: step 130730, loss = 1.91 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:08:50.533934: step 130740, loss = 1.79 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:51.697958: step 130750, loss = 1.92 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:52.876098: step 130760, loss = 1.95 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:08:54.031602: step 130770, loss = 1.86 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:55.253415: step 130780, loss = 2.00 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:08:56.427007: step 130790, loss = 1.85 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:57.597471: step 130800, loss = 1.91 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:58.751878: step 130810, loss = 1.90 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:08:59.913154: step 130820, loss = 1.99 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:01.071490: step 130830, loss = 1.81 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:02.229938: step 130840, loss = 1.85 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:03.422128: step 130850, loss = 1.79 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:09:04.594345: step 130860, loss = 1.81 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:05.740822: step 130870, loss = 1.83 (1116.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:09:06.917209: step 130880, loss = 1.88 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:09:08.085013: step 130890, loss = 1.82 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:09.245012: step 130900, loss = 1.85 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:10.401257: step 130910, loss = 1.86 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:11.558645: step 130920, loss = 1.85 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:12.724404: step 130930, loss = 1.88 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:13.871643: step 130940, loss = 1.94 (1115.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:09:15.054849: step 130950, loss = 1.99 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:09:16.208321: step 130960, loss = 1.99 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:09:17.365803: step 130970, loss = 1.86 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:18.531476: step 130980, loss = 1.96 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:19.700514: step 130990, loss = 1.91 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:20.847080: step 131000, loss = 2.02 (1116.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:09:22.002171: step 131010, loss = 1.77 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:23.168164: step 131020, loss = 1.94 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:24.345563: step 131030, loss = 1.76 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:09:25.503228: step 131040, loss = 1.89 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:26.671188: step 131050, loss = 1.92 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:27.827605: step 131060, loss = 1.80 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:28.999702: step 131070, loss = 1.83 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:30.178891: step 131080, loss = 1.92 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:09:31.344178: step 131090, loss = 2.00 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:32.533337: step 131100, loss = 1.95 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:09:33.707605: step 131110, loss = 1.97 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:34.868071: step 131120, loss = 1.90 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:36.039963: step 131130, loss = 1.90 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:37.201808: step 131140, loss = 1.99 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:38.366527: step 131150, loss = 1.76 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:39.536375: step 131160, loss = 1.92 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:40.707731: step 131170, loss = 1.80 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:41.873499: step 131180, loss = 1.82 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:43.056026: step 131190, loss = 1.83 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:09:44.216908: step 131200, loss = 1.77 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:45.385390: step 131210, loss = 1.97 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:46.537867: step 131220, loss = 2.00 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:09:47.701331: step 131230, loss = 1.83 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:48.870143: step 131240, loss = 1.81 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:50.022647: step 131250, loss = 1.87 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:09:51.192419: step 131260, loss = 1.84 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:52.353757: step 131270, loss = 1.78 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:53.520717: step 131280, loss = 1.92 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:54.706617: step 131290, loss = 1.88 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:09:55.908847: step 131300, loss = 1.79 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:09:57.093747: step 131310, loss = 1.82 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:09:58.256565: step 131320, loss = 1.88 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:59.442180: step 131330, loss = 1.82 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:00.625084: step 131340, loss = 1.91 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:01.803735: step 131350, loss = 2.02 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:02.975858: step 131360, loss = 1.86 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:04.147964: step 131370, loss = 1.87 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:05.301742: step 131380, loss = 1.88 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:10:06.481420: step 131390, loss = 1.84 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:07.661154: step 131400, loss = 1.81 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:08.832758: step 131410, loss = 1.88 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:09.998373: step 131420, loss = 1.92 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:11.180573: step 131430, loss = 1.78 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:12.362584: step 131440, loss = 1.89 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:13.532634: step 131450, loss = 1.81 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:14.710179: step 131460, loss = 1.79 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:15.911737: step 131470, loss = 1.76 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:10:17.098490: step 131480, loss = 1.89 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:18.282435: step 131490, loss = 1.97 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:19.463727: step 131500, loss = 1.87 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:20.615727: step 131510, loss = 1.77 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:10:21.784618: step 131520, loss = 2.18 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:22.956662: step 131530, loss = 2.10 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:24.159971: step 131540, loss = 1.82 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:10:25.363905: step 131550, loss = 1.80 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:10:26.544401: step 131560, loss = 1.85 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:27.726042: step 131570, loss = 2.02 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:28.928176: step 131580, loss = 1.98 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:10:30.120931: step 131590, loss = 1.85 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:31.306340: step 131600, loss = 1.91 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:32.505084: step 131610, loss = 2.05 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:10:33.696426: step 131620, loss = 1.99 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:34.892802: step 131630, loss = 1.84 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:10:36.083878: step 131640, loss = 1.85 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:37.281014: step 131650, loss = 1.90 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:10:38.498952: step 131660, loss = 1.78 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:10:39.786662: step 131670, loss = 1.88 (994.0 examples/sec; 0.129 sec/batch)
2017-05-05 02:10:40.858741: step 131680, loss = 1.91 (1193.9 examples/sec; 0.107 sec/batch)
2017-05-05 02:10:42.034957: step 131690, loss = 1.72 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:43.223940: step 131700, loss = 1.89 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:44.402435: step 131710, loss = 2.10 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:45.567664: step 131720, loss = 1.88 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:46.741871: step 131730, loss = 1.93 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:47.914080: step 131740, loss = 1.89 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:49.085144: step 131750, loss = 1.86 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:50.263343: step 131760, loss = 1.96 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:51.447733: step 131770, loss = 2.10 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:52.611953: step 131780, loss = 2.05 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:10:53.768431: step 131790, loss = 1.85 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:10:54.943606: step 131800, loss = 1.87 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:56.113609: step 131810, loss = 1.98 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:57.282119: step 131820, loss = 1.75 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:58.450717: step 131830, loss = 1.84 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:59.634938: step 131840, loss = 1.82 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:00.801321: step 131850, loss = 2.07 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:01.958400: step 131860, loss = 2.04 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:03.134097: step 131870, loss = 1.82 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:04.297471: step 131880, loss = 1.86 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:05.476106: step 131890, loss = 1.92 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:06.630815: step 131900, loss = 1.95 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:11:07.788056: step 131910, loss = 1.81 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:08.969462: step 131920, loss = 1.78 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:10.118012: step 131930, loss = 1.78 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:11:11.302085: step 131940, loss = 1.85 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:12.475913: step 131950, loss = 1.83 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:13.654371: step 131960, loss = 1.97 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:14.809982: step 131970, loss = 1.78 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:15.995275: step 131980, loss = 1.81 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:11:17.172775: step 131990, loss = 1.71 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:18.352907: step 132000, loss = 1.82 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:19.523468: step 132010, loss = 1.98 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:20.691778: step 132020, loss = 1.83 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:21.853371: step 132030, loss = 1.76 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:23.014265: step 132040, loss = 1.85 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:24.192627: step 132050, loss = 1.88 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:25.358275: step 132060, loss = 1.90 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:26.508725: step 132070, loss = 1.74 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:11:27.668373: step 132080, loss = 1.74 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:28.823115: step 132090, loss = 1.88 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:11:29.984761: step 132100, loss = 1.86 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:31.180050: step 132110, loss = 1.91 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:11:32.348878: step 132120, loss = 1.94 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:33.515059: step 132130, loss = 1.72 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:34.669810: step 132140, loss = 1.90 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:11:35.844539: step 132150, loss = 1.80 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:37.014728: step 132160, loss = 2.03 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:38.174195: step 132170, loss = 2.08 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:39.342860: step 132180, loss = 1.88 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:40.501057: step 132190, loss = 1.78 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:41.677397: step 132200, loss = 1.92 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:42.834301: step 132210, loss = 1.90 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:43.980204: step 132220, loss = 1.78 (1117.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:11:45.141125: step 132230, loss = 1.96 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:46.307443: step 132240, loss = 1.93 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:47.480538: step 132250, loss = 1.79 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:48.637479: step 132260, loss = 1.93 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:49.795715: step 132270, loss = 1.88 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:50.975700: step 132280, loss = 1.86 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:52.155220: step 132290, loss = 1.87 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:53.353864: step 132300, loss = 1.85 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:11:54.541342: step 132310, loss = 1.98 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:11:55.737066: step 132320, loss = 1.98 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:11:56.932851: step 132330, loss = 2.02 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:11:58.140980: step 132340, loss = 1.82 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:11:59.338335: step 132350, loss = 1.78 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:12:00.512296: step 132360, loss = 1.90 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:01.703826: step 132370, loss = 2.04 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:12:02.910299: step 132380, loss = 1.87 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:12:04.122477: step 132390, loss = 1.89 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:12:05.315116: step 132400, loss = 1.79 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:12:06.518361: step 132410, loss = 2.15 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:12:07.717689: step 132420, loss = 1.93 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:12:08.885993: step 132430, loss = 1.88 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:10.057133: step 132440, loss = 2.11 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:11.245894: step 132450, loss = 1.79 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:12:12.432776: step 132460, loss = 1.84 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:12:13.610190: step 132470, loss = 1.85 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:12:14.792469: step 132480, loss = 1.72 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:12:15.995750: step 132490, loss = 1.77 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:12:17.178674: step 132500, loss = 1.88 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:12:18.332470: step 132510, loss = 1.92 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:12:19.498505: step 132520, loss = 1.92 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:20.711683: step 132530, loss = 1.85 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:12:21.871626: step 132540, loss = 1.99 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:12:23.047712: step 132550, loss = 1.89 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:12:24.219607: step 132560, loss = 1.89 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:25.381729: step 132570, loss = 1.86 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:12:26.559044: step 132580, loss = 1.84 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:12:27.727157: step 132590, loss = 1.96 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:28.883940: step 132600, loss = 1.79 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:12:30.048445: step 132610, loss = 1.84 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:12:31.220927: step 132620, loss = 1.93 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:32.386709: step 132630, loss = 1.94 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:33.552291: step 132640, loss = 2.00 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:34.719997: step 132650, loss = 1.91 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:35.967538: step 132660, loss = 1.99 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-05 02:12:37.049361: step 132670, loss = 1.96 (1183.2 examples/sec; 0.108 sec/batch)
2017-05-05 02:12:38.213470: step 132680, loss = 1.84 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:12:39.383952: step 132690, loss = 2.00 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:40.565674: step 132700, loss = 1.78 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:12:41.717296: step 132710, loss = 1.77 (1111.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:12:42.888111: step 132720, loss = 1.95 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:44.058804: step 132730, loss = 1.75 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:45.250490: step 132740, loss = 1.76 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:12:46.407205: step 132750, loss = 1.97 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:12:47.589400: step 132760, loss = 1.83 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:12:48.771895: step 132770, loss = 1.96 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:12:49.933286: step 132780, loss = 1.88 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:12:51.125296: step 132790, loss = 2.00 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:12:52.303321: step 132800, loss = 1.87 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:12:53.494546: step 132810, loss = 1.75 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:12:54.704792: step 132820, loss = 1.78 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:12:55.917094: step 132830, loss = 2.00 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:12:57.141517: step 132840, loss = 2.00 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:12:58.343563: step 132850, loss = 1.80 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:12:59.552825: step 132860, loss = 1.90 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:00.737033: step 132870, loss = 1.72 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:13:01.931200: step 132880, loss = 1.89 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:13:03.132484: step 132890, loss = 1.81 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:13:04.341797: step 132900, loss = 2.00 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:05.540961: step 132910, loss = 1.93 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:13:06.744889: step 132920, loss = 1.89 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:13:07.965815: step 132930, loss = 1.94 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:13:09.165177: step 132940, loss = 1.80 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:13:10.378828: step 132950, loss = 1.83 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:11.583677: step 132960, loss = 2.04 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:13:12.788487: step 132970, loss = 1.89 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:13:13.994563: step 132980, loss = 1.95 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:15.199717: step 132990, loss = 2.00 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:16.419358: step 133000, loss = 2.05 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:13:17.612846: step 133010, loss = 1.95 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:13:18.819272: step 133020, loss = 1.88 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:20.039871: step 133030, loss = 1.92 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:13:21.263798: step 133040, loss = 1.87 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:13:22.471071: step 133050, loss = 1.86 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:23.655419: step 133060, loss = 1.93 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:13:24.867294: step 133070, loss = 1.97 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:26.061599: step 133080, loss = 2.01 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:13:27.277710: step 133090, loss = 1.97 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:13:28.510023: step 133100, loss = 1.98 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-05 02:13:29.716418: step 133110, loss = 1.81 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:30.932567: step 133120, loss = 1.97 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:13:32.141338: step 133130, loss = 1.87 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:33.327935: step 133140, loss = 1.93 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:13:34.532470: step 133150, loss = 1.95 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:13:35.706910: step 133160, loss = 1.83 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:13:36.899487: step 133170, loss = 1.94 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:13:38.060706: step 133180, loss = 1.80 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:13:39.211543: step 133190, loss = 1.88 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:13:40.372323: step 133200, loss = 1.85 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:13:41.535739: step 133210, loss = 1.93 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:13:42.699160: step 133220, loss = 1.77 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:13:43.855634: step 133230, loss = 1.97 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:13:45.021171: step 133240, loss = 1.79 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:13:46.171285: step 133250, loss = 1.88 (1112.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:13:47.355488: step 133260, loss = 1.83 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:13:48.540659: step 133270, loss = 1.97 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:13:49.702442: step 133280, loss = 1.78 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:13:50.866834: step 133290, loss = 1.77 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:13:52.031706: step 133300, loss = 1.76 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:13:53.213370: step 133310, loss = 1.93 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:13:54.375548: step 133320, loss = 1.87 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:13:55.534427: step 133330, loss = 1.95 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:13:56.698044: step 133340, loss = 1.85 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:13:57.863994: step 133350, loss = 1.96 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:13:59.021514: step 133360, loss = 1.76 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:00.181677: step 133370, loss = 2.03 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:01.329398: step 133380, loss = 2.08 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:14:02.491993: step 133390, loss = 1.90 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:03.649169: step 133400, loss = 2.12 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:04.834675: step 133410, loss = 1.94 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:14:06.000329: step 133420, loss = 1.87 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:07.153676: step 133430, loss = 1.90 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:14:08.326365: step 133440, loss = 1.85 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:09.499954: step 133450, loss = 1.87 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:10.668302: step 133460, loss = 1.81 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:11.842036: step 133470, loss = 1.86 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:13.024880: step 133480, loss = 1.74 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:14.186678: step 133490, loss = 1.85 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:15.352610: step 133500, loss = 1.90 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:16.515533: step 133510, loss = 1.92 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:17.682544: step 133520, loss = 1.88 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:18.847405: step 133530, loss = 1.89 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:20.023925: step 133540, loss = 2.14 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:21.203112: step 133550, loss = 1.96 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:22.375133: step 133560, loss = 2.03 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:23.550157: step 133570, loss = 2.01 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:24.730511: step 133580, loss = 1.80 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:25.904083: step 133590, loss = 1.92 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:27.086807: step 133600, loss = 1.87 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:28.239763: step 133610, loss = 1.83 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:14:29.407102: step 133620, loss = 1.90 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:30.579596: step 133630, loss = 1.92 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:31.758882: step 133640, loss = 1.80 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:33.066541: step 133650, loss = 2.05 (978.8 examples/sec; 0.131 sec/batch)
2017-05-05 02:14:34.123846: step 133660, loss = 1.95 (1210.6 examples/sec; 0.106 sec/batch)
2017-05-05 02:14:35.333356: step 133670, loss = 1.84 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:14:36.543123: step 133680, loss = 2.02 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:14:37.722664: step 133690, loss = 1.88 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:38.918573: step 133700, loss = 1.73 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:14:40.130413: step 133710, loss = 1.87 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:14:41.335724: step 133720, loss = 2.03 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:14:42.551570: step 133730, loss = 2.00 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:14:43.764472: step 133740, loss = 1.88 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:14:44.969440: step 133750, loss = 1.95 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:14:46.156833: step 133760, loss = 1.78 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:14:47.337870: step 133770, loss = 1.89 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:48.528356: step 133780, loss = 1.90 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:14:49.692100: step 133790, loss = 1.79 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:50.873429: step 133800, loss = 1.90 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:52.064843: step 133810, loss = 1.79 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:14:53.254586: step 133820, loss = 1.77 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:14:54.435207: step 133830, loss = 1.83 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:55.630779: step 133840, loss = 1.89 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:14:56.807504: step 133850, loss = 1.88 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:58.019078: step 133860, loss = 1.87 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:14:59.243022: step 133870, loss = 1.92 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:15:00.434187: step 133880, loss = 1.89 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:15:01.609655: step 133890, loss = 2.06 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:02.816575: step 133900, loss = 1.90 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:15:04.014493: step 133910, loss = 1.92 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:05.209564: step 133920, loss = 1.91 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:06.390397: step 133930, loss = 1.90 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:07.575172: step 133940, loss = 1.90 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:08.777123: step 133950, loss = 1.92 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:09.960736: step 133960, loss = 1.88 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:11.164001: step 133970, loss = 1.90 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:12.367852: step 133980, loss = 1.87 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:13.569042: step 133990, loss = 1.82 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:14.751719: step 134000, loss = 2.01 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:15.947213: step 134010, loss = 2.07 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:17.159911: step 134020, loss = 1.76 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:15:18.367710: step 134030, loss = 2.02 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:15:19.554474: step 134040, loss = 1.83 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:15:20.745968: step 134050, loss = 2.03 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:15:21.912160: step 134060, loss = 2.01 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:23.085792: step 134070, loss = 1.82 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:24.286405: step 134080, loss = 1.88 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:25.472017: step 134090, loss = 2.09 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:15:26.648508: step 134100, loss = 1.84 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:27.811345: step 134110, loss = 1.72 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:15:28.979828: step 134120, loss = 2.00 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:30.127778: step 134130, loss = 1.80 (1115.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:15:31.290894: step 134140, loss = 2.07 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:15:32.447034: step 134150, loss = 1.88 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:15:33.620795: step 134160, loss = 1.89 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:34.782265: step 134170, loss = 1.83 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:15:35.953838: step 134180, loss = 2.03 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:37.140706: step 134190, loss = 1.83 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:15:38.309710: step 134200, loss = 1.81 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:39.479753: step 134210, loss = 1.78 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:40.661437: step 134220, loss = 1.78 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:41.842152: step 134230, loss = 1.81 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:43.037675: step 134240, loss = 1.98 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:44.204614: step 134250, loss = 1.83 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:45.387311: step 134260, loss = 2.05 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:46.542423: step 134270, loss = 1.85 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:15:47.729882: step 134280, loss = 1.79 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:15:48.920140: step 134290, loss = 1.93 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:15:50.102745: step 134300, loss = 1.92 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:51.262001: step 134310, loss = 1.91 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:15:52.437278: step 134320, loss = 1.86 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:53.583101: step 134330, loss = 1.90 (1117.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:15:54.758885: step 134340, loss = 2.00 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:55.927065: step 134350, loss = 1.97 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:57.098641: step 134360, loss = 1.76 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:58.256017: step 134370, loss = 1.79 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:15:59.415859: step 134380, loss = 1.83 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:00.584645: step 134390, loss = 2.02 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:01.748204: step 134400, loss = 1.90 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:02.928722: step 134410, loss = 1.90 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:04.106070: step 134420, loss = 1.92 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:05.271184: step 134430, loss = 1.90 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:06.432429: step 134440, loss = 1.94 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:07.613605: step 134450, loss = 1.86 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:08.791954: step 134460, loss = 1.88 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:09.959476: step 134470, loss = 1.85 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:11.128663: step 134480, loss = 1.86 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:12.300557: step 134490, loss = 1.75 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:13.481132: step 134500, loss = 1.88 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:14.674447: step 134510, loss = 1.93 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:16:15.830275: step 134520, loss = 1.86 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:17.000728: step 134530, loss = 1.94 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:18.170149: step 134540, loss = 1.89 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:19.364901: step 134550, loss = 1.83 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:16:20.533760: step 134560, loss = 1.91 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:21.708779: step 134570, loss = 1.92 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:22.871785: step 134580, loss = 2.00 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:24.051202: step 134590, loss = 1.95 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:25.240767: step 134600, loss = 1.86 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:16:26.393976: step 134610, loss = 2.01 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:16:27.565026: step 134620, loss = 1.83 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:28.737790: step 134630, loss = 1.82 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:29.983274: step 134640, loss = 1.91 (1027.8 examples/sec; 0.125 sec/batch)
2017-05-05 02:16:31.064936: step 134650, loss = 1.76 (1183.3 examples/sec; 0.108 sec/batch)
2017-05-05 02:16:32.223043: step 134660, loss = 1.84 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:33.394949: step 134670, loss = 1.95 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:34.568669: step 134680, loss = 2.07 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:35.753571: step 134690, loss = 1.97 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:36.923591: step 134700, loss = 1.84 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:38.065038: step 134710, loss = 1.92 (1121.4 examples/sec; 0.114 sec/batch)
2017-05-05 02:16:39.237674: step 134720, loss = 1.76 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:40.399050: step 134730, loss = 1.84 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:41.584369: step 134740, loss = 1.96 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:16:42.760830: step 134750, loss = 1.98 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:43.953589: step 134760, loss = 1.82 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:16:45.117452: step 134770, loss = 2.00 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:46.263614: step 134780, loss = 1.85 (1116.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:16:47.450998: step 134790, loss = 1.93 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:16:48.613642: step 134800, loss = 1.93 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:49.777948: step 134810, loss = 1.98 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:50.949621: step 134820, loss = 1.86 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:52.120903: step 134830, loss = 1.82 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:53.280409: step 134840, loss = 1.90 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:54.441866: step 134850, loss = 1.80 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:55.612004: step 134860, loss = 1.96 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:56.783717: step 134870, loss = 1.82 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:57.944437: step 134880, loss = 2.01 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:59.114896: step 134890, loss = 1.82 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:00.297964: step 134900, loss = 1.99 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:01.467141: step 134910, loss = 1.85 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:02.646642: step 134920, loss = 1.92 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:03.828009: step 134930, loss = 1.90 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:04.996116: step 134940, loss = 1.92 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:06.169828: step 134950, loss = 1.95 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:07.357719: step 134960, loss = 1.79 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:17:08.548858: step 134970, loss = 1.88 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:17:09.742603: step 134980, loss = 2.07 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:17:10.929357: step 134990, loss = 2.16 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:17:12.123980: step 135000, loss = 1.75 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:17:13.304894: step 135010, loss = 2.01 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:14.492538: step 135020, loss = 1.86 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:17:15.690402: step 135030, loss = 1.79 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:17:16.870419: step 135040, loss = 1.78 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:18.048439: step 135050, loss = 1.87 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:19.221941: step 135060, loss = 1.86 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:20.401209: step 135070, loss = 1.81 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:21.553909: step 135080, loss = 1.88 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:17:22.715436: step 135090, loss = 1.79 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:17:23.882903: step 135100, loss = 1.94 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:25.052517: step 135110, loss = 1.98 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:26.211015: step 135120, loss = 2.05 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:17:27.351108: step 135130, loss = 2.05 (1122.7 examples/sec; 0.114 sec/batch)
2017-05-05 02:17:28.508222: step 135140, loss = 1.93 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:17:29.686802: step 135150, loss = 1.94 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:30.882957: step 135160, loss = 1.83 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:17:32.060947: step 135170, loss = 1.86 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:33.235947: step 135180, loss = 1.93 (1089.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:34.403954: step 135190, loss = 1.86 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:35.585908: step 135200, loss = 1.99 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:36.737773: step 135210, loss = 1.98 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:17:37.885964: step 135220, loss = 1.94 (1114.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:17:39.060594: step 135230, loss = 1.87 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:40.242182: step 135240, loss = 1.97 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:41.407543: step 135250, loss = 2.01 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:42.574013: step 135260, loss = 1.85 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:43.748071: step 135270, loss = 1.92 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:44.926780: step 135280, loss = 1.83 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:46.065667: step 135290, loss = 1.95 (1123.9 examples/sec; 0.114 sec/batch)
2017-05-05 02:17:47.228515: step 135300, loss = 1.96 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:17:48.399828: step 135310, loss = 1.96 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:49.561691: step 135320, loss = 1.83 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:17:50.721901: step 135330, loss = 1.80 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:17:51.888179: step 135340, loss = 2.09 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:53.070612: step 135350, loss = 2.00 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:54.213955: step 135360, loss = 1.94 (1119.5 examples/sec; 0.114 sec/batch)
2017-05-05 02:17:55.375681: step 135370, loss = 2.00 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:17:56.545105: step 135380, loss = 1.81 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:57.698940: step 135390, loss = 2.04 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:17:58.864321: step 135400, loss = 1.90 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:00.049159: step 135410, loss = 2.00 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:01.223173: step 135420, loss = 1.95 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:02.382292: step 135430, loss = 1.99 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:03.554947: step 135440, loss = 1.89 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:04.746735: step 135450, loss = 1.91 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:18:05.905144: step 135460, loss = 1.88 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:07.073544: step 135470, loss = 1.88 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:08.255641: step 135480, loss = 1.82 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:09.417266: step 135490, loss = 1.80 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:10.573797: step 135500, loss = 1.83 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:11.740156: step 135510, loss = 2.00 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:12.904738: step 135520, loss = 1.97 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:14.075068: step 135530, loss = 1.88 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:15.236684: step 135540, loss = 2.01 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:16.394504: step 135550, loss = 1.86 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:17.548805: step 135560, loss = 1.91 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:18:18.722623: step 135570, loss = 1.95 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:19.896926: step 135580, loss = 1.70 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:21.074867: step 135590, loss = 1.86 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:22.241918: step 135600, loss = 1.94 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:23.388290: step 135610, loss = 1.82 (1116.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:18:24.566282: step 135620, loss = 1.80 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:25.850092: step 135630, loss = 1.92 (997.0 examples/sec; 0.128 sec/batch)
2017-05-05 02:18:26.897877: step 135640, loss = 1.92 (1221.6 examples/sec; 0.105 sec/batch)
2017-05-05 02:18:28.068222: step 135650, loss = 1.74 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:29.258202: step 135660, loss = 1.81 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:18:30.406010: step 135670, loss = 1.96 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:18:31.567863: step 135680, loss = 1.83 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:32.766769: step 135690, loss = 1.97 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:18:33.926682: step 135700, loss = 1.83 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:35.105087: step 135710, loss = 1.93 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:36.265585: step 135720, loss = 1.93 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:37.433483: step 135730, loss = 1.95 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:38.602449: step 135740, loss = 1.93 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:39.761496: step 135750, loss = 1.95 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:40.942642: step 135760, loss = 2.14 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:42.108991: step 135770, loss = 1.87 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:43.313951: step 135780, loss = 1.95 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:18:44.523533: step 135790, loss = 1.83 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:18:45.702147: step 135800, loss = 1.84 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:46.890046: step 135810, loss = 1.86 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:18:48.067869: step 135820, loss = 1.84 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:49.274559: step 135830, loss = 1.87 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:18:50.415165: step 135840, loss = 1.98 (1122.2 examples/sec; 0.114 sec/batch)
2017-05-05 02:18:51.598499: step 135850, loss = 1.81 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:52.761317: step 135860, loss = 1.88 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:53.938682: step 135870, loss = 1.87 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:55.160523: step 135880, loss = 1.90 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:18:56.321974: step 135890, loss = 1.87 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:57.471928: step 135900, loss = 1.85 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:18:58.649923: step 135910, loss = 1.87 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:59.797114: step 135920, loss = 1.95 (1115.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:19:00.970031: step 135930, loss = 2.01 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:19:02.137407: step 135940, loss = 1.84 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:19:03.313174: step 135950, loss = 1.84 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:19:04.492760: step 135960, loss = 1.75 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:19:05.655275: step 135970, loss = 1.81 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:19:06.843807: step 135980, loss = 1.85 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:19:08.038954: step 135990, loss = 1.88 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:19:09.246343: step 136000, loss = 1.84 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:10.447768: step 136010, loss = 1.82 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:19:11.682696: step 136020, loss = 1.98 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-05 02:19:12.897818: step 136030, loss = 1.89 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:14.119790: step 136040, loss = 1.90 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:15.333782: step 136050, loss = 1.84 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:16.541365: step 136060, loss = 1.86 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:17.754971: step 136070, loss = 1.87 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:18.969573: step 136080, loss = 1.82 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:20.184493: step 136090, loss = 2.14 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:21.404180: step 136100, loss = 1.79 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:22.600996: step 136110, loss = 1.88 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:19:23.819222: step 136120, loss = 1.83 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:25.023262: step 136130, loss = 1.91 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:19:26.211217: step 136140, loss = 1.82 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:19:27.430772: step 136150, loss = 1.90 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:28.650816: step 136160, loss = 1.94 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:29.856615: step 136170, loss = 1.99 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:31.074678: step 136180, loss = 1.85 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:32.289707: step 136190, loss = 1.76 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:33.498082: step 136200, loss = 1.92 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:34.717403: step 136210, loss = 1.90 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:35.933865: step 136220, loss = 1.77 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:37.138582: step 136230, loss = 1.98 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:19:38.350504: step 136240, loss = 2.00 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:39.566296: step 136250, loss = 1.83 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:40.790958: step 136260, loss = 1.96 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:42.001708: step 136270, loss = 1.85 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:43.230506: step 136280, loss = 1.83 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 02:19:44.464160: step 136290, loss = 1.99 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-05 02:19:45.680616: step 136300, loss = 1.91 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:46.894755: step 136310, loss = 1.99 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:48.105873: step 136320, loss = 1.67 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:49.331003: step 136330, loss = 1.88 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:19:50.557942: step 136340, loss = 1.83 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:19:51.764134: step 136350, loss = 1.88 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:52.967561: step 136360, loss = 1.91 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:19:54.160810: step 136370, loss = 1.94 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:19:55.393491: step 136380, loss = 1.92 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-05 02:19:56.603798: step 136390, loss = 1.90 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:57.800975: step 136400, loss = 1.79 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:19:59.040548: step 136410, loss = 1.82 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-05 02:20:00.248921: step 136420, loss = 1.83 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:01.453014: step 136430, loss = 2.03 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:20:02.667432: step 136440, loss = 2.04 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:03.871072: step 136450, loss = 2.00 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:20:05.098027: step 136460, loss = 1.82 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:06.303066: step 136470, loss = 1.90 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:07.539357: step 136480, loss = 1.88 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-05 02:20:08.747373: step 136490, loss = 1.75 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:09.948979: step 136500, loss = 1.89 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:20:11.181321: step 136510, loss = 1.78 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:12.398707: step 136520, loss = 1.88 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:13.605456: step 136530, loss = 1.92 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:14.806414: step 136540, loss = 1.87 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:20:16.028066: step 136550, loss = 1.82 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:17.259769: step 136560, loss = 1.82 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:18.459223: step 136570, loss = 1.77 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:20:19.669673: step 136580, loss = 1.79 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:20.893956: step 136590, loss = 1.95 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:22.086296: step 136600, loss = 1.82 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:20:23.311568: step 136610, loss = 1.90 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:24.601191: step 136620, loss = 1.83 (992.5 examples/sec; 0.129 sec/batch)
2017-05-05 02:20:25.702812: step 136630, loss = 1.88 (1161.9 examples/sec; 0.110 sec/batch)
2017-05-05 02:20:26.937135: step 136640, loss = 1.98 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:28.142288: step 136650, loss = 2.00 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:29.372335: step 136660, loss = 1.81 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:30.580867: step 136670, loss = 1.90 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:31.820978: step 136680, loss = 1.91 (1032.2 examples/sec; 0.124 sec/batch)
2017-05-05 02:20:33.047425: step 136690, loss = 1.71 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:34.262730: step 136700, loss = 2.01 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:35.471150: step 136710, loss = 1.82 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:36.686868: step 136720, loss = 1.87 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:37.879886: step 136730, loss = 1.92 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:20:39.084601: step 136740, loss = 1.91 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:20:40.302891: step 136750, loss = 1.83 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:41.507411: step 136760, loss = 1.83 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:20:42.725665: step 136770, loss = 1.88 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:43.957709: step 136780, loss = 1.89 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:45.184151: step 136790, loss = 1.88 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:46.412865: step 136800, loss = 1.97 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:47.616057: step 136810, loss = 1.92 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:20:48.802659: step 136820, loss = 1.86 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:20:50.013594: step 136830, loss = 1.74 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:51.256388: step 136840, loss = 1.79 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-05 02:20:52.457741: step 136850, loss = 1.99 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:20:53.672071: step 136860, loss = 1.97 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:54.883447: step 136870, loss = 1.90 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:56.100534: step 136880, loss = 1.92 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:57.321450: step 136890, loss = 1.90 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:58.521855: step 136900, loss = 1.98 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:20:59.739639: step 136910, loss = 2.00 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:00.967636: step 136920, loss = 1.90 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-05 02:21:02.176050: step 136930, loss = 1.96 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:21:03.394267: step 136940, loss = 2.07 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:04.594599: step 136950, loss = 1.92 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:21:05.787565: step 136960, loss = 1.92 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:21:07.000933: step 136970, loss = 1.79 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:21:08.234710: step 136980, loss = 1.87 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-05 02:21:09.459318: step 136990, loss = 1.75 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:10.677584: step 137000, loss = 1.77 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:11.894507: step 137010, loss = 1.89 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:13.108551: step 137020, loss = 1.92 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:21:14.313059: step 137030, loss = 1.73 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:21:15.529663: step 137040, loss = 1.82 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:16.744463: step 137050, loss = 1.79 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:21:17.933844: step 137060, loss = 1.85 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:21:19.130991: step 137070, loss = 1.84 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:21:20.357283: step 137080, loss = 1.95 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:21:21.576182: step 137090, loss = 1.96 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:22.792288: step 137100, loss = 1.94 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:24.011986: step 137110, loss = 1.87 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:25.223571: step 137120, loss = 1.86 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:21:26.417984: step 137130, loss = 2.00 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:21:27.618530: step 137140, loss = 1.76 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:21:28.818183: step 137150, loss = 1.81 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:21:30.006331: step 137160, loss = 1.95 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:21:31.191095: step 137170, loss = 1.80 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:21:32.382770: step 137180, loss = 1.98 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:21:33.551465: step 137190, loss = 2.02 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:21:34.758107: step 137200, loss = 1.96 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:21:35.973817: step 137210, loss = 1.78 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:37.183291: step 137220, loss = 1.91 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:21:38.369187: step 137230, loss = 1.92 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:21:39.553474: step 137240, loss = 1.86 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:21:40.737671: step 137250, loss = 1.95 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:21:41.899552: step 137260, loss = 2.06 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:21:43.077350: step 137270, loss = 1.91 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:21:44.263221: step 137280, loss = 1.77 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:21:45.426706: step 137290, loss = 1.90 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:21:46.581955: step 137300, loss = 1.92 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:21:47.757562: step 137310, loss = 1.96 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:21:48.931787: step 137320, loss = 1.88 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:21:50.091037: step 137330, loss = 1.87 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:21:51.261544: step 137340, loss = 1.80 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:21:52.431320: step 137350, loss = 1.82 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:21:53.585139: step 137360, loss = 1.78 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:21:54.760391: step 137370, loss = 1.93 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:21:55.940530: step 137380, loss = 1.95 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:21:57.112913: step 137390, loss = 1.89 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:21:58.257031: step 137400, loss = 1.95 (1118.8 examples/sec; 0.114 sec/batch)
2017-05-05 02:21:59.428890: step 137410, loss = 1.91 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:00.587107: step 137420, loss = 1.89 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:22:01.746030: step 137430, loss = 1.94 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:22:02.908958: step 137440, loss = 2.06 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:22:04.069932: step 137450, loss = 1.98 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:22:05.263437: step 137460, loss = 1.87 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:22:06.421277: step 137470, loss = 1.95 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:22:07.577841: step 137480, loss = 1.90 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:22:08.732884: step 137490, loss = 1.88 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:22:09.889678: step 137500, loss = 1.98 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:22:11.055541: step 137510, loss = 1.91 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:12.234556: step 137520, loss = 1.99 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:13.403040: step 137530, loss = 2.01 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:14.575999: step 137540, loss = 1.82 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:15.741603: step 137550, loss = 1.91 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:16.910626: step 137560, loss = 2.08 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:18.054554: step 137570, loss = 1.90 (1118.9 examples/sec; 0.114 sec/batch)
2017-05-05 02:22:19.231593: step 137580, loss = 1.93 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:20.379447: step 137590, loss = 1.76 (1115.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:22:21.538446: step 137600, loss = 1.82 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:22:22.809124: step 137610, loss = 1.89 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-05 02:22:23.886707: step 137620, loss = 1.76 (1187.8 examples/sec; 0.108 sec/batch)
2017-05-05 02:22:25.074194: step 137630, loss = 1.91 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:22:26.243058: step 137640, loss = 1.81 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:27.417453: step 137650, loss = 1.84 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:28.617161: step 137660, loss = 1.79 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:22:29.792511: step 137670, loss = 1.84 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:30.993910: step 137680, loss = 1.79 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:22:32.183462: step 137690, loss = 1.97 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:22:33.382276: step 137700, loss = 1.98 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:22:34.566971: step 137710, loss = 1.97 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:35.764042: step 137720, loss = 1.85 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:22:36.961023: step 137730, loss = 1.93 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:22:38.143974: step 137740, loss = 1.91 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:39.363309: step 137750, loss = 1.86 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:22:40.581768: step 137760, loss = 1.91 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:22:41.765895: step 137770, loss = 2.03 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:42.963351: step 137780, loss = 1.96 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:22:44.151583: step 137790, loss = 2.01 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:22:45.359708: step 137800, loss = 1.78 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:22:46.581637: step 137810, loss = 1.88 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:22:47.718630: step 137820, loss = 1.91 (1125.8 examples/sec; 0.114 sec/batch)
2017-05-05 02:22:48.894791: step 137830, loss = 1.89 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:50.067126: step 137840, loss = 1.84 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:51.273452: step 137850, loss = 1.79 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:22:52.448801: step 137860, loss = 1.93 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:53.646879: step 137870, loss = 1.91 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:22:54.821492: step 137880, loss = 2.05 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:55.996955: step 137890, loss = 1.84 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:57.185464: step 137900, loss = 1.86 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:22:58.350125: step 137910, loss = 1.80 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:22:59.523564: step 137920, loss = 1.93 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:00.699586: step 137930, loss = 2.06 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:01.846014: step 137940, loss = 1.79 (1116.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:23:03.022001: step 137950, loss = 2.02 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:04.186399: step 137960, loss = 1.88 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:23:05.361516: step 137970, loss = 1.81 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:06.539070: step 137980, loss = 1.87 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:07.724627: step 137990, loss = 1.89 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:23:08.892784: step 138000, loss = 1.88 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:10.036154: step 138010, loss = 1.74 (1119.5 examples/sec; 0.114 sec/batch)
2017-05-05 02:23:11.199650: step 138020, loss = 1.99 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:23:12.364897: step 138030, loss = 2.00 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:13.536204: step 138040, loss = 1.90 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:14.706502: step 138050, loss = 1.97 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:15.876056: step 138060, loss = 2.01 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:17.061505: step 138070, loss = 1.79 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:23:18.219831: step 138080, loss = 1.76 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:23:19.412699: step 138090, loss = 2.10 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:23:20.593675: step 138100, loss = 1.79 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:21.755950: step 138110, loss = 2.01 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:23:22.920419: step 138120, loss = 1.91 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:23:24.118976: step 138130, loss = 1.94 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:23:25.305491: step 138140, loss = 1.93 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:23:26.485280: step 138150, loss = 1.83 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:27.674335: step 138160, loss = 2.04 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:23:28.855588: step 138170, loss = 1.95 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:30.035203: step 138180, loss = 1.94 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:31.211577: step 138190, loss = 1.87 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:32.384392: step 138200, loss = 1.82 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:33.559908: step 138210, loss = 1.92 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:34.714715: step 138220, loss = 1.90 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:23:35.896316: step 138230, loss = 1.85 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:37.076103: step 138240, loss = 1.93 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:38.238261: step 138250, loss = 1.81 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:23:39.415920: step 138260, loss = 1.85 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:40.582675: step 138270, loss = 1.97 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:41.753072: step 138280, loss = 1.89 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:42.930063: step 138290, loss = 1.96 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:44.108665: step 138300, loss = 1.77 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:45.277755: step 138310, loss = 1.76 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:46.455016: step 138320, loss = 2.04 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:47.621143: step 138330, loss = 1.78 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:48.818957: step 138340, loss = 1.99 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:23:49.971277: step 138350, loss = 1.94 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:23:51.125413: step 138360, loss = 1.88 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:23:52.298169: step 138370, loss = 2.03 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:53.448045: step 138380, loss = 1.89 (1113.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:23:54.585570: step 138390, loss = 1.92 (1125.3 examples/sec; 0.114 sec/batch)
2017-05-05 02:23:55.760344: step 138400, loss = 1.91 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:56.920736: step 138410, loss = 1.91 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:23:58.070274: step 138420, loss = 1.85 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:23:59.241542: step 138430, loss = 1.95 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:00.410037: step 138440, loss = 1.91 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:01.580080: step 138450, loss = 1.72 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:02.745896: step 138460, loss = 1.91 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:03.924873: step 138470, loss = 1.87 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:05.089893: step 138480, loss = 1.84 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:06.249507: step 138490, loss = 1.75 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:24:07.439427: step 138500, loss = 1.95 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:24:08.621583: step 138510, loss = 1.87 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:09.771555: step 138520, loss = 1.81 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:24:10.944294: step 138530, loss = 1.78 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:12.115498: step 138540, loss = 1.79 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:13.291134: step 138550, loss = 1.99 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:14.458556: step 138560, loss = 1.83 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:15.623797: step 138570, loss = 1.85 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:16.810024: step 138580, loss = 1.70 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:24:17.966404: step 138590, loss = 1.78 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:24:19.246690: step 138600, loss = 1.87 (999.8 examples/sec; 0.128 sec/batch)
2017-05-05 02:24:20.309007: step 138610, loss = 1.90 (1204.9 examples/sec; 0.106 sec/batch)
2017-05-05 02:24:21.483070: step 138620, loss = 1.82 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:22.647729: step 138630, loss = 1.70 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:24:23.840022: step 138640, loss = 1.71 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:24:24.999430: step 138650, loss = 1.82 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:24:26.158127: step 138660, loss = 1.95 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:24:27.341766: step 138670, loss = 1.73 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:28.540568: step 138680, loss = 1.88 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:24:29.714082: step 138690, loss = 1.98 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:30.895003: step 138700, loss = 1.99 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:32.103400: step 138710, loss = 1.70 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:24:33.291132: step 138720, loss = 1.70 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:24:34.500167: step 138730, loss = 1.89 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:24:35.678776: step 138740, loss = 1.96 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:36.857035: step 138750, loss = 1.92 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:38.039713: step 138760, loss = 1.92 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:39.242023: step 138770, loss = 1.97 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:24:40.425643: step 138780, loss = 2.02 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:41.606521: step 138790, loss = 1.91 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:42.799457: step 138800, loss = 1.78 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:24:43.997857: step 138810, loss = 1.97 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:24:45.177561: step 138820, loss = 1.91 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:46.345704: step 138830, loss = 1.93 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:47.541924: step 138840, loss = 1.84 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:24:48.755858: step 138850, loss = 1.97 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:24:49.953146: step 138860, loss = 2.01 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:24:51.154716: step 138870, loss = 1.97 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:24:52.366469: step 138880, loss = 1.84 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:24:53.554621: step 138890, loss = 1.79 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:24:54.734605: step 138900, loss = 1.88 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:55.922518: step 138910, loss = 1.82 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:24:57.123053: step 138920, loss = 1.95 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:24:58.307384: step 138930, loss = 1.86 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:59.516127: step 138940, loss = 1.76 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:00.734833: step 138950, loss = 1.75 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:01.957464: step 138960, loss = 1.81 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:03.153899: step 138970, loss = 1.93 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:04.346297: step 138980, loss = 1.78 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:25:05.528013: step 138990, loss = 1.99 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:25:06.736062: step 139000, loss = 1.82 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:07.961701: step 139010, loss = 1.94 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-05 02:25:09.201724: step 139020, loss = 1.90 (1032.2 examples/sec; 0.124 sec/batch)
2017-05-05 02:25:10.426573: step 139030, loss = 1.90 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:11.668360: step 139040, loss = 1.80 (1030.8 examples/sec; 0.124 sec/batch)
2017-05-05 02:25:12.885505: step 139050, loss = 1.91 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:14.092791: step 139060, loss = 1.90 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:15.324348: step 139070, loss = 1.92 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:25:16.537844: step 139080, loss = 1.88 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:17.752984: step 139090, loss = 1.94 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:18.960365: step 139100, loss = 1.92 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:20.191739: step 139110, loss = 1.71 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 02:25:21.404693: step 139120, loss = 1.92 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:22.608659: step 139130, loss = 1.85 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:23.837872: step 139140, loss = 1.77 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:25:25.059782: step 139150, loss = 1.92 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:26.262682: step 139160, loss = 1.75 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:27.474401: step 139170, loss = 2.06 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:28.688576: step 139180, loss = 1.73 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:29.884703: step 139190, loss = 1.95 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:31.066061: step 139200, loss = 1.79 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:25:32.284015: step 139210, loss = 1.96 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:33.491490: step 139220, loss = 1.85 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:34.702083: step 139230, loss = 1.91 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:35.938119: step 139240, loss = 1.78 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-05 02:25:37.166871: step 139250, loss = 2.11 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 02:25:38.382783: step 139260, loss = 1.87 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:39.600077: step 139270, loss = 1.76 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:40.810626: step 139280, loss = 2.12 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:42.005261: step 139290, loss = 1.91 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:25:43.216815: step 139300, loss = 1.87 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:44.428562: step 139310, loss = 1.88 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:45.625209: step 139320, loss = 1.96 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:46.818633: step 139330, loss = 1.89 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:25:48.036131: step 139340, loss = 1.87 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:49.257071: step 139350, loss = 1.88 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:50.453479: step 139360, loss = 1.91 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:51.671321: step 139370, loss = 1.89 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:52.886435: step 139380, loss = 1.81 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:54.102980: step 139390, loss = 1.99 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:55.306929: step 139400, loss = 1.88 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:56.517414: step 139410, loss = 1.86 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:57.721431: step 139420, loss = 1.94 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:58.941933: step 139430, loss = 1.85 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:00.160826: step 139440, loss = 1.92 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:01.355763: step 139450, loss = 1.80 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:26:02.561306: step 139460, loss = 1.91 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:03.788029: step 139470, loss = 2.04 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 02:26:05.004979: step 139480, loss = 1.82 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:06.202605: step 139490, loss = 2.03 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:07.420350: step 139500, loss = 1.85 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:08.648619: step 139510, loss = 1.84 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-05 02:26:09.845905: step 139520, loss = 1.78 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:11.070931: step 139530, loss = 1.80 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:26:12.289059: step 139540, loss = 1.85 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:13.508902: step 139550, loss = 2.09 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:14.729598: step 139560, loss = 1.89 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:15.939678: step 139570, loss = 1.76 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:17.137042: step 139580, loss = 1.79 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:18.420130: step 139590, loss = 1.96 (997.6 examples/sec; 0.128 sec/batch)
2017-05-05 02:26:19.546535: step 139600, loss = 1.90 (1136.4 examples/sec; 0.113 sec/batch)
2017-05-05 02:26:20.771847: step 139610, loss = 2.03 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 02:26:21.970598: step 139620, loss = 1.92 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:23.185817: step 139630, loss = 1.84 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:24.383028: step 139640, loss = 1.73 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:25.580830: step 139650, loss = 1.90 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:26.796091: step 139660, loss = 1.91 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:28.036692: step 139670, loss = 2.07 (1031.8 examples/sec; 0.124 sec/batch)
2017-05-05 02:26:29.247335: step 139680, loss = 2.06 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:30.441701: step 139690, loss = 1.78 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:26:31.648088: step 139700, loss = 1.85 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:32.873310: step 139710, loss = 2.05 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 02:26:34.075517: step 139720, loss = 2.00 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:35.291921: step 139730, loss = 1.82 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:36.503387: step 139740, loss = 1.92 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:37.722879: step 139750, loss = 1.72 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:38.920679: step 139760, loss = 2.08 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:40.142971: step 139770, loss = 1.93 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:41.363878: step 139780, loss = 1.94 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:42.549106: step 139790, loss = 1.85 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:26:43.770702: step 139800, loss = 1.97 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:44.980325: step 139810, loss = 1.91 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:46.189771: step 139820, loss = 2.05 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:47.407606: step 139830, loss = 2.08 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:48.610319: step 139840, loss = 1.84 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:49.823224: step 139850, loss = 1.76 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:51.064207: step 139860, loss = 2.01 (1031.4 examples/sec; 0.124 sec/batch)
2017-05-05 02:26:52.274947: step 139870, loss = 1.99 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:53.487071: step 139880, loss = 1.99 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:54.701747: step 139890, loss = 1.83 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:55.914528: step 139900, loss = 1.92 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:57.156549: step 139910, loss = 1.81 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-05 02:26:58.358797: step 139920, loss = 1.80 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:59.591194: step 139930, loss = 1.93 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-05 02:27:00.807665: step 139940, loss = 1.94 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:02.005844: step 139950, loss = 1.71 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:27:03.215198: step 139960, loss = 1.94 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:04.433602: step 139970, loss = 1.88 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:05.630298: step 139980, loss = 1.67 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:27:06.847101: step 139990, loss = 1.88 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:08.051585: step 140000, loss = 2.05 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:27:09.275446: step 140010, loss = 1.89 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:10.480993: step 140020, loss = 2.03 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:11.712182: step 140030, loss = 1.98 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-05 02:27:12.924710: step 140040, loss = 1.83 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:14.128791: step 140050, loss = 1.99 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:27:15.355872: step 140060, loss = 1.97 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 02:27:16.573682: step 140070, loss = 1.88 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:17.781699: step 140080, loss = 1.83 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:18.995681: step 140090, loss = 2.00 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:20.211457: step 140100, loss = 1.94 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:21.431110: step 140110, loss = 1.93 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:22.638787: step 140120, loss = 1.88 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:23.855868: step 140130, loss = 1.87 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:25.077806: step 140140, loss = 1.83 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:26.277425: step 140150, loss = 1.89 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:27:27.510583: step 140160, loss = 1.86 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-05 02:27:28.722433: step 140170, loss = 1.84 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:29.930593: step 140180, loss = 1.97 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:31.125680: step 140190, loss = 1.88 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:27:32.339551: step 140200, loss = 1.77 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:33.560308: step 140210, loss = 1.70 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:34.765456: step 140220, loss = 1.86 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:35.980621: step 140230, loss = 2.09 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:37.194918: step 140240, loss = 1.83 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:38.384030: step 140250, loss = 1.74 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:27:39.602771: step 140260, loss = 1.88 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:40.798709: step 140270, loss = 1.96 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:27:41.980834: step 140280, loss = 1.99 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:27:43.183524: step 140290, loss = 1.83 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:27:44.376854: step 140300, loss = 1.85 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:27:45.603437: step 140310, loss = 2.02 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 02:27:46.825672: step 140320, loss = 1.77 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:48.045084: step 140330, loss = 1.84 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:49.249095: step 140340, loss = 1.95 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:27:50.432917: step 140350, loss = 1.85 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:27:51.615783: step 140360, loss = 2.03 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:27:52.808897: step 140370, loss = 1.85 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:27:53.984786: step 140380, loss = 1.88 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:27:55.160653: step 140390, loss = 1.90 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:27:56.344091: step 140400, loss = 1.85 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:27:57.502960: step 140410, loss = 1.84 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:27:58.693972: step 140420, loss = 1.88 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:27:59.858966: step 140430, loss = 1.78 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:01.022272: step 140440, loss = 1.77 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:02.202455: step 140450, loss = 1.76 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:03.389547: step 140460, loss = 1.87 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:28:04.557738: step 140470, loss = 1.76 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:05.723725: step 140480, loss = 2.00 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:06.886598: step 140490, loss = 1.85 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:08.063290: step 140500, loss = 1.86 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:09.227041: step 140510, loss = 2.01 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:10.403046: step 140520, loss = 2.11 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:11.585628: step 140530, loss = 1.86 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:12.762916: step 140540, loss = 1.70 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:13.923311: step 140550, loss = 1.67 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:15.089163: step 140560, loss = 1.88 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:16.273026: step 140570, loss = 1.82 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:17.533070: step 140580, loss = 1.94 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-05 02:28:18.620936: step 140590, loss = 1.79 (1176.6 examples/sec; 0.109 sec/batch)
2017-05-05 02:28:19.802590: step 140600, loss = 1.90 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:20.967084: step 140610, loss = 1.69 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:22.133794: step 140620, loss = 1.73 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:23.303529: step 140630, loss = 1.79 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:24.479213: step 140640, loss = 1.87 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:25.674191: step 140650, loss = 1.89 (1071.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:28:26.856956: step 140660, loss = 1.86 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:28.043637: step 140670, loss = 1.90 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:28:29.211477: step 140680, loss = 1.93 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:30.371150: step 140690, loss = 1.95 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:31.539150: step 140700, loss = 1.89 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:32.710895: step 140710, loss = 1.86 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:33.875535: step 140720, loss = 2.00 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:35.033777: step 140730, loss = 1.92 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:36.183280: step 140740, loss = 1.95 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:28:37.353427: step 140750, loss = 1.71 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:38.531120: step 140760, loss = 1.82 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:39.698771: step 140770, loss = 1.79 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:40.878404: step 140780, loss = 1.98 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:42.040899: step 140790, loss = 1.91 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:43.219489: step 140800, loss = 1.93 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:44.402808: step 140810, loss = 1.94 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:45.552136: step 140820, loss = 1.83 (1113.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:28:46.717588: step 140830, loss = 1.94 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:47.890459: step 140840, loss = 1.90 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:49.067745: step 140850, loss = 1.90 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:50.232425: step 140860, loss = 1.84 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:51.422508: step 140870, loss = 1.93 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:28:52.594192: step 140880, loss = 1.90 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:53.737902: step 140890, loss = 1.85 (1119.2 examples/sec; 0.114 sec/batch)
2017-05-05 02:28:54.976600: step 140900, loss = 1.92 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-05 02:28:56.127843: step 140910, loss = 1.85 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:28:57.291543: step 140920, loss = 2.01 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:58.445393: step 140930, loss = 1.90 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:28:59.599615: step 140940, loss = 1.97 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:29:00.756786: step 140950, loss = 1.85 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:01.918026: step 140960, loss = 1.78 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:03.067078: step 140970, loss = 1.87 (1114.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:29:04.265726: step 140980, loss = 1.75 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:29:05.407379: step 140990, loss = 1.91 (1121.2 examples/sec; 0.114 sec/batch)
2017-05-05 02:29:06.559806: step 141000, loss = 1.91 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:29:07.730171: step 141010, loss = 1.85 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:08.897064: step 141020, loss = 1.88 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:10.073289: step 141030, loss = 1.90 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:29:11.229576: step 141040, loss = 1.96 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:12.404105: step 141050, loss = 1.90 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:13.558766: step 141060, loss = 1.94 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:29:14.709474: step 141070, loss = 1.77 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:29:15.875792: step 141080, loss = 1.88 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:17.043539: step 141090, loss = 1.82 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:18.200845: step 141100, loss = 1.94 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:19.381752: step 141110, loss = 1.87 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:29:20.545851: step 141120, loss = 1.89 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:21.719144: step 141130, loss = 1.75 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:22.879510: step 141140, loss = 1.76 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:24.071954: step 141150, loss = 1.84 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:29:25.225155: step 141160, loss = 1.99 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:29:26.381617: step 141170, loss = 1.85 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:27.554482: step 141180, loss = 1.92 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:28.731606: step 141190, loss = 1.97 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:29:29.901622: step 141200, loss = 1.79 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:31.071626: step 141210, loss = 2.12 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:32.241164: step 141220, loss = 1.94 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:33.420618: step 141230, loss = 1.93 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:29:34.554581: step 141240, loss = 1.93 (1128.8 examples/sec; 0.113 sec/batch)
2017-05-05 02:29:35.712333: step 141250, loss = 1.82 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:36.880136: step 141260, loss = 1.89 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:38.039309: step 141270, loss = 2.00 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:39.199365: step 141280, loss = 1.98 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:40.376135: step 141290, loss = 1.83 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:29:41.535897: step 141300, loss = 1.92 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:42.691725: step 141310, loss = 1.85 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:43.862507: step 141320, loss = 1.97 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:45.028784: step 141330, loss = 1.82 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:46.188282: step 141340, loss = 1.94 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:47.374966: step 141350, loss = 1.84 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:29:48.542896: step 141360, loss = 1.92 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:49.698092: step 141370, loss = 1.98 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:50.868331: step 141380, loss = 1.91 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:52.014696: step 141390, loss = 2.09 (1116.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:29:53.187307: step 141400, loss = 2.01 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:54.346127: step 141410, loss = 1.84 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:55.518542: step 141420, loss = 1.96 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:56.672481: step 141430, loss = 2.03 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:29:57.836668: step 141440, loss = 1.90 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:59.007089: step 141450, loss = 2.00 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:00.166008: step 141460, loss = 2.00 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:01.317913: step 141470, loss = 1.96 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:30:02.495745: step 141480, loss = 1.88 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:30:03.692998: step 141490, loss = 1.90 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:30:04.868957: step 141500, loss = 1.99 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:30:06.040885: step 141510, loss = 1.78 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:07.207865: step 141520, loss = 1.76 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:08.394432: step 141530, loss = 1.91 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:30:09.558622: step 141540, loss = 1.85 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:10.735642: step 141550, loss = 1.92 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:30:11.922310: step 141560, loss = 1.92 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:30:13.187793: step 141570, loss = 1.93 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-05 02:30:14.260348: step 141580, loss = 1.78 (1193.4 examples/sec; 0.107 sec/batch)
2017-05-05 02:30:15.424942: step 141590, loss = 1.82 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:16.589952: step 141600, loss = 1.83 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:17.739420: step 141610, loss = 1.85 (1113.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:30:18.893761: step 141620, loss = 1.80 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:30:20.067527: step 141630, loss = 1.88 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:21.242888: step 141640, loss = 1.96 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:30:22.416980: step 141650, loss = 1.76 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:23.583788: step 141660, loss = 1.79 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:24.748202: step 141670, loss = 1.79 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:25.919669: step 141680, loss = 1.94 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:27.084163: step 141690, loss = 1.97 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:28.260142: step 141700, loss = 1.92 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:30:29.417517: step 141710, loss = 1.86 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:30.577932: step 141720, loss = 1.95 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:31.746936: step 141730, loss = 1.79 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:32.926482: step 141740, loss = 1.89 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:30:34.102179: step 141750, loss = 2.04 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:30:35.314649: step 141760, loss = 1.79 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:30:36.588190: step 141770, loss = 1.88 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-05 02:30:37.705311: step 141780, loss = 1.83 (1145.8 examples/sec; 0.112 sec/batch)
2017-05-05 02:30:38.921972: step 141790, loss = 1.86 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:30:40.133144: step 141800, loss = 1.79 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:30:41.357049: step 141810, loss = 1.77 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:30:42.568744: step 141820, loss = 1.93 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:30:43.782704: step 141830, loss = 1.93 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:30:44.982175: step 141840, loss = 1.91 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:30:46.182796: step 141850, loss = 1.93 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:30:47.391384: step 141860, loss = 1.86 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:30:48.599061: step 141870, loss = 1.87 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:30:49.782033: step 141880, loss = 1.89 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:30:50.981660: step 141890, loss = 1.95 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:30:52.164389: step 141900, loss = 1.95 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:30:53.326145: step 141910, loss = 1.93 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:54.496610: step 141920, loss = 1.85 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:55.687464: step 141930, loss = 1.91 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:30:56.862251: step 141940, loss = 1.82 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:58.033263: step 141950, loss = 1.80 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:59.192164: step 141960, loss = 1.75 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:00.380221: step 141970, loss = 1.88 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:31:01.551594: step 141980, loss = 2.01 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:02.712657: step 141990, loss = 1.85 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:03.886380: step 142000, loss = 1.73 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:05.059784: step 142010, loss = 1.88 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:06.210145: step 142020, loss = 1.99 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:31:07.394004: step 142030, loss = 1.89 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:31:08.572558: step 142040, loss = 1.79 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:31:09.725811: step 142050, loss = 1.87 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:31:10.882766: step 142060, loss = 1.93 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:12.057773: step 142070, loss = 1.93 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:31:13.237418: step 142080, loss = 1.81 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:31:14.400380: step 142090, loss = 1.78 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:15.562269: step 142100, loss = 1.93 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:16.723870: step 142110, loss = 1.86 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:17.863996: step 142120, loss = 1.85 (1122.7 examples/sec; 0.114 sec/batch)
2017-05-05 02:31:19.042074: step 142130, loss = 1.87 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:31:20.214404: step 142140, loss = 1.80 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:21.371967: step 142150, loss = 1.87 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:22.530852: step 142160, loss = 1.94 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:23.694110: step 142170, loss = 2.00 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:24.857769: step 142180, loss = 1.91 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:26.005122: step 142190, loss = 1.78 (1115.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:31:27.182861: step 142200, loss = 1.76 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:31:28.354416: step 142210, loss = 1.84 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:29.498636: step 142220, loss = 1.99 (1118.7 examples/sec; 0.114 sec/batch)
2017-05-05 02:31:30.667380: step 142230, loss = 1.89 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:31.839842: step 142240, loss = 1.80 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:33.010391: step 142250, loss = 2.00 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:34.151734: step 142260, loss = 1.93 (1121.5 examples/sec; 0.114 sec/batch)
2017-05-05 02:31:35.320460: step 142270, loss = 2.05 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:36.462577: step 142280, loss = 1.87 (1120.7 examples/sec; 0.114 sec/batch)
2017-05-05 02:31:37.621506: step 142290, loss = 1.94 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:38.781798: step 142300, loss = 1.97 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:39.946818: step 142310, loss = 1.83 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:41.121300: step 142320, loss = 1.84 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:42.307681: step 142330, loss = 1.78 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:31:43.502996: step 142340, loss = 1.90 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:31:44.707422: step 142350, loss = 1.82 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:31:45.896834: step 142360, loss = 1.86 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:31:47.106088: step 142370, loss = 1.85 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:31:48.314988: step 142380, loss = 1.94 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:31:49.526896: step 142390, loss = 1.94 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:31:50.716904: step 142400, loss = 1.86 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:31:51.928470: step 142410, loss = 2.05 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:31:53.122551: step 142420, loss = 1.88 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:31:54.318972: step 142430, loss = 1.89 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:31:55.529684: step 142440, loss = 1.99 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:31:56.746518: step 142450, loss = 2.01 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:31:57.931219: step 142460, loss = 1.89 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:31:59.148285: step 142470, loss = 1.84 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:00.350052: step 142480, loss = 1.89 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:32:01.565143: step 142490, loss = 1.93 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:02.782501: step 142500, loss = 1.94 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:04.011049: step 142510, loss = 1.94 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:32:05.238512: step 142520, loss = 1.96 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:32:06.448420: step 142530, loss = 1.81 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:07.643847: step 142540, loss = 1.97 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:32:08.853459: step 142550, loss = 1.77 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:10.169175: step 142560, loss = 1.83 (972.8 examples/sec; 0.132 sec/batch)
2017-05-05 02:32:11.270213: step 142570, loss = 1.96 (1162.5 examples/sec; 0.110 sec/batch)
2017-05-05 02:32:12.475535: step 142580, loss = 1.97 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:13.682034: step 142590, loss = 1.79 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:14.894530: step 142600, loss = 1.84 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:16.121443: step 142610, loss = 1.93 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:32:17.330882: step 142620, loss = 1.89 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:18.565508: step 142630, loss = 2.01 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:32:19.783753: step 142640, loss = 1.71 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:21.003999: step 142650, loss = 1.91 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:22.203505: step 142660, loss = 1.97 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:32:23.452440: step 142670, loss = 1.82 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-05 02:32:24.650289: step 142680, loss = 1.72 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:32:25.855291: step 142690, loss = 1.77 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:27.090816: step 142700, loss = 1.94 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-05 02:32:28.314571: step 142710, loss = 1.89 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:29.519015: step 142720, loss = 1.85 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:32:30.749731: step 142730, loss = 2.03 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-05 02:32:31.953609: step 142740, loss = 1.81 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:32:33.181677: step 142750, loss = 1.93 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:32:34.444282: step 142760, loss = 2.00 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-05 02:32:35.546333: step 142770, loss = 1.85 (1161.5 examples/sec; 0.110 sec/batch)
2017-05-05 02:32:36.781289: step 142780, loss = 1.89 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-05 02:32:37.987793: step 142790, loss = 1.84 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:39.194018: step 142800, loss = 2.03 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:40.435570: step 142810, loss = 1.87 (1031.0 examples/sec; 0.124 sec/batch)
2017-05-05 02:32:41.627654: step 142820, loss = 1.96 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:32:42.849214: step 142830, loss = 1.93 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:44.061462: step 142840, loss = 2.09 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:45.299721: step 142850, loss = 1.92 (1033.7 examples/sec; 0.124 sec/batch)
2017-05-05 02:32:46.507810: step 142860, loss = 1.79 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:47.717811: step 142870, loss = 1.92 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:48.917185: step 142880, loss = 1.89 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:32:50.141093: step 142890, loss = 1.91 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:51.349327: step 142900, loss = 2.04 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:52.591977: step 142910, loss = 1.76 (1030.1 examples/sec; 0.124 sec/batch)
2017-05-05 02:32:53.796203: step 142920, loss = 1.91 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:32:54.995194: step 142930, loss = 1.82 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:32:56.190737: step 142940, loss = 1.91 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:32:57.396485: step 142950, loss = 1.73 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:58.581680: step 142960, loss = 1.83 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:32:59.795045: step 142970, loss = 1.91 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:01.024964: step 142980, loss = 1.97 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-05 02:33:02.215948: step 142990, loss = 1.94 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:33:03.421647: step 143000, loss = 1.96 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:04.647160: step 143010, loss = 2.01 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 02:33:05.852404: step 143020, loss = 1.96 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:07.083144: step 143030, loss = 1.76 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-05 02:33:08.306725: step 143040, loss = 1.91 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:09.538679: step 143050, loss = 1.77 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-05 02:33:10.711786: step 143060, loss = 1.89 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:33:11.940396: step 143070, loss = 2.03 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:33:13.136719: step 143080, loss = 1.80 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:33:14.367146: step 143090, loss = 1.89 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:33:15.577841: step 143100, loss = 1.84 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:16.808990: step 143110, loss = 1.92 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-05 02:33:18.000497: step 143120, loss = 1.93 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:33:19.231697: step 143130, loss = 1.79 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-05 02:33:20.429579: step 143140, loss = 1.79 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:33:21.645786: step 143150, loss = 1.86 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:22.867134: step 143160, loss = 1.91 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:24.075602: step 143170, loss = 1.82 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:25.287121: step 143180, loss = 1.84 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:26.483100: step 143190, loss = 1.95 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:33:27.712451: step 143200, loss = 1.99 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-05 02:33:28.923607: step 143210, loss = 1.78 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:30.125496: step 143220, loss = 1.96 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:33:31.347339: step 143230, loss = 1.80 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:32.565684: step 143240, loss = 1.91 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:33.774744: step 143250, loss = 1.95 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:34.960258: step 143260, loss = 2.07 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:33:36.192954: step 143270, loss = 2.01 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-05 02:33:37.403030: step 143280, loss = 1.94 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:38.587100: step 143290, loss = 1.88 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:33:39.795981: step 143300, loss = 1.93 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:41.008451: step 143310, loss = 1.91 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:42.204494: step 143320, loss = 1.75 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:33:43.448080: step 143330, loss = 1.98 (1029.3 examples/sec; 0.124 sec/batch)
2017-05-05 02:33:44.639218: step 143340, loss = 1.98 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:33:45.853901: step 143350, loss = 1.91 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:47.066128: step 143360, loss = 1.94 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:48.292348: step 143370, loss = 1.88 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:33:49.508142: step 143380, loss = 1.81 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:50.715648: step 143390, loss = 1.89 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:51.905642: step 143400, loss = 1.93 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:33:53.138220: step 143410, loss = 1.85 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 02:33:54.339313: step 143420, loss = 1.91 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:33:55.566698: step 143430, loss = 1.98 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:33:56.776698: step 143440, loss = 1.88 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:57.998374: step 143450, loss = 1.76 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:59.205822: step 143460, loss = 1.84 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:00.411103: step 143470, loss = 1.94 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:01.631342: step 143480, loss = 1.96 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:02.837479: step 143490, loss = 2.00 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:04.054270: step 143500, loss = 1.88 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:05.266651: step 143510, loss = 1.77 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:06.470666: step 143520, loss = 1.75 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:34:07.702750: step 143530, loss = 1.90 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:34:08.909034: step 143540, loss = 2.03 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:10.213478: step 143550, loss = 1.93 (981.3 examples/sec; 0.130 sec/batch)
2017-05-05 02:34:11.322134: step 143560, loss = 1.77 (1154.6 examples/sec; 0.111 sec/batch)
2017-05-05 02:34:12.541698: step 143570, loss = 1.81 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:13.729088: step 143580, loss = 1.88 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:34:14.940298: step 143590, loss = 1.82 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:16.149073: step 143600, loss = 1.79 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:17.374574: step 143610, loss = 1.95 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 02:34:18.583870: step 143620, loss = 2.07 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:19.810175: step 143630, loss = 1.99 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:34:21.024212: step 143640, loss = 1.74 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:22.240388: step 143650, loss = 1.85 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:23.436597: step 143660, loss = 1.87 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:34:24.664416: step 143670, loss = 2.00 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 02:34:25.874330: step 143680, loss = 1.86 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:27.116564: step 143690, loss = 1.93 (1030.4 examples/sec; 0.124 sec/batch)
2017-05-05 02:34:28.314392: step 143700, loss = 1.96 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:34:29.556890: step 143710, loss = 1.87 (1030.2 examples/sec; 0.124 sec/batch)
2017-05-05 02:34:30.755857: step 143720, loss = 2.02 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:34:31.984445: step 143730, loss = 1.86 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:34:33.194959: step 143740, loss = 1.86 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:34.484564: step 143750, loss = 1.97 (992.6 examples/sec; 0.129 sec/batch)
2017-05-05 02:34:35.579514: step 143760, loss = 1.77 (1169.0 examples/sec; 0.109 sec/batch)
2017-05-05 02:34:36.804012: step 143770, loss = 1.80 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:38.013671: step 143780, loss = 1.85 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:39.253401: step 143790, loss = 1.98 (1032.5 examples/sec; 0.124 sec/batch)
2017-05-05 02:34:40.466454: step 143800, loss = 1.95 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:41.672296: step 143810, loss = 1.92 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:42.897715: step 143820, loss = 1.90 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 02:34:44.136596: step 143830, loss = 1.82 (1033.2 examples/sec; 0.124 sec/batch)
2017-05-05 02:34:45.321270: step 143840, loss = 1.82 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:34:46.552903: step 143850, loss = 1.97 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:34:47.767946: step 143860, loss = 1.88 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:48.984264: step 143870, loss = 1.93 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:50.189768: step 143880, loss = 1.97 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:51.412140: step 143890, loss = 1.96 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:52.622904: step 143900, loss = 1.86 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:53.809394: step 143910, loss = 2.01 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:34:55.016183: step 143920, loss = 2.00 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:56.222345: step 143930, loss = 1.84 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:57.410866: step 143940, loss = 1.82 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:34:58.594299: step 143950, loss = 1.82 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:34:59.801034: step 143960, loss = 1.88 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:35:00.974881: step 143970, loss = 1.89 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:02.140640: step 143980, loss = 1.83 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:03.326210: step 143990, loss = 1.77 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:35:04.520316: step 144000, loss = 1.74 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:35:05.702168: step 144010, loss = 1.84 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:06.893260: step 144020, loss = 2.07 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:35:08.063299: step 144030, loss = 1.98 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:09.235503: step 144040, loss = 1.89 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:10.413537: step 144050, loss = 1.89 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:11.604944: step 144060, loss = 1.88 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:35:12.810928: step 144070, loss = 1.90 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:35:13.984926: step 144080, loss = 1.96 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:15.190508: step 144090, loss = 1.82 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:35:16.371341: step 144100, loss = 1.87 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:17.550461: step 144110, loss = 1.83 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:18.735909: step 144120, loss = 1.92 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:35:19.913878: step 144130, loss = 1.87 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:21.101878: step 144140, loss = 1.86 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:35:22.270383: step 144150, loss = 1.97 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:23.436162: step 144160, loss = 1.89 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:24.585164: step 144170, loss = 1.81 (1114.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:35:25.753295: step 144180, loss = 1.87 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:26.922938: step 144190, loss = 1.89 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:28.087917: step 144200, loss = 1.91 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:29.256012: step 144210, loss = 1.81 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:30.411988: step 144220, loss = 1.84 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:31.576732: step 144230, loss = 1.93 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:32.741326: step 144240, loss = 1.81 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:33.911799: step 144250, loss = 1.87 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:35.090070: step 144260, loss = 1.82 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:36.260848: step 144270, loss = 1.85 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:37.420867: step 144280, loss = 2.05 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:38.590223: step 144290, loss = 2.03 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:39.780096: step 144300, loss = 1.82 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:35:40.960588: step 144310, loss = 1.87 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:42.151513: step 144320, loss = 1.95 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:35:43.325321: step 144330, loss = 1.86 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:44.503133: step 144340, loss = 1.98 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:45.689394: step 144350, loss = 1.91 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:35:46.849180: step 144360, loss = 1.79 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:48.022083: step 144370, loss = 1.95 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:49.193389: step 144380, loss = 1.98 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:50.340610: step 144390, loss = 1.78 (1115.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:35:51.514419: step 144400, loss = 1.82 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:52.686341: step 144410, loss = 1.95 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:53.843396: step 144420, loss = 1.83 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:55.016602: step 144430, loss = 1.94 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:56.190644: step 144440, loss = 1.98 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:57.354257: step 144450, loss = 1.87 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:58.512970: step 144460, loss = 1.75 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:59.690449: step 144470, loss = 1.85 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:00.838772: step 144480, loss = 1.82 (1114.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:36:01.987219: step 144490, loss = 1.87 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:36:03.170025: step 144500, loss = 1.78 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:04.317468: step 144510, loss = 1.86 (1115.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:36:05.484951: step 144520, loss = 1.90 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:06.644024: step 144530, loss = 1.93 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:07.910644: step 144540, loss = 1.76 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-05 02:36:08.984612: step 144550, loss = 1.88 (1191.8 examples/sec; 0.107 sec/batch)
2017-05-05 02:36:10.156078: step 144560, loss = 1.99 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:11.314378: step 144570, loss = 1.85 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:12.503822: step 144580, loss = 1.93 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:36:13.675309: step 144590, loss = 1.90 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:14.863311: step 144600, loss = 1.79 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:36:16.023606: step 144610, loss = 1.93 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:17.184179: step 144620, loss = 1.90 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:18.328834: step 144630, loss = 1.98 (1118.2 examples/sec; 0.114 sec/batch)
2017-05-05 02:36:19.493092: step 144640, loss = 1.94 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:20.657154: step 144650, loss = 1.94 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:21.817301: step 144660, loss = 2.14 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:23.013342: step 144670, loss = 1.87 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:36:24.174911: step 144680, loss = 1.90 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:25.335655: step 144690, loss = 1.77 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:26.496105: step 144700, loss = 1.82 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:27.664624: step 144710, loss = 1.87 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:28.827347: step 144720, loss = 1.87 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:30.000130: step 144730, loss = 1.93 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:31.192319: step 144740, loss = 1.93 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:36:32.347534: step 144750, loss = 1.96 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:33.502134: step 144760, loss = 2.00 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:36:34.661132: step 144770, loss = 1.81 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:35.842541: step 144780, loss = 1.91 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:37.026827: step 144790, loss = 1.91 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:38.188407: step 144800, loss = 1.96 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:39.372635: step 144810, loss = 2.03 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:40.562357: step 144820, loss = 1.89 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:36:41.723896: step 144830, loss = 1.93 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:42.898667: step 144840, loss = 1.90 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:44.069722: step 144850, loss = 1.78 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:45.259984: step 144860, loss = 1.74 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:36:46.423185: step 144870, loss = 2.14 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:47.607730: step 144880, loss = 2.03 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:48.806590: step 144890, loss = 2.02 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:36:49.962774: step 144900, loss = 1.91 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:51.150972: step 144910, loss = 1.84 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:36:52.324382: step 144920, loss = 1.77 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:53.504601: step 144930, loss = 1.86 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:54.700172: step 144940, loss = 2.01 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:36:55.894207: step 144950, loss = 1.81 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:36:57.082984: step 144960, loss = 2.03 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:36:58.272559: step 144970, loss = 2.03 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:36:59.468611: step 144980, loss = 1.94 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:37:00.652064: step 144990, loss = 1.82 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:37:01.829734: step 145000, loss = 2.00 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:37:03.028940: step 145010, loss = 1.91 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:37:04.229554: step 145020, loss = 1.86 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:37:05.423821: step 145030, loss = 2.09 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:06.607460: step 145040, loss = 1.87 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:37:07.822687: step 145050, loss = 1.80 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:37:09.016188: step 145060, loss = 1.69 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:10.200089: step 145070, loss = 1.72 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:37:11.403603: step 145080, loss = 1.82 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:37:12.614434: step 145090, loss = 1.78 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:37:13.810805: step 145100, loss = 2.04 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:37:15.038260: step 145110, loss = 1.96 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:37:16.240066: step 145120, loss = 1.93 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:37:17.456947: step 145130, loss = 1.89 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:37:18.635911: step 145140, loss = 1.95 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:37:19.855904: step 145150, loss = 2.14 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:37:21.053281: step 145160, loss = 1.83 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:37:22.242121: step 145170, loss = 1.87 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:23.463466: step 145180, loss = 1.77 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:37:24.669921: step 145190, loss = 1.89 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:37:25.857909: step 145200, loss = 1.76 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:27.045562: step 145210, loss = 1.85 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:28.232316: step 145220, loss = 1.86 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:29.424665: step 145230, loss = 1.78 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:30.588295: step 145240, loss = 1.92 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:37:31.770960: step 145250, loss = 1.99 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:37:32.960457: step 145260, loss = 1.93 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:34.135257: step 145270, loss = 2.07 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:37:35.330069: step 145280, loss = 1.75 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:36.524823: step 145290, loss = 1.95 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:37.672475: step 145300, loss = 1.97 (1115.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:37:38.827943: step 145310, loss = 1.91 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:37:40.010869: step 145320, loss = 1.83 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:37:41.205975: step 145330, loss = 1.97 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:37:42.380117: step 145340, loss = 1.87 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:37:43.545932: step 145350, loss = 1.83 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:37:44.713662: step 145360, loss = 1.87 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:37:45.880758: step 145370, loss = 1.89 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:37:47.044510: step 145380, loss = 1.90 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:37:48.209647: step 145390, loss = 1.87 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:37:49.383347: step 145400, loss = 1.96 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:37:50.540926: step 145410, loss = 1.94 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:37:51.694951: step 145420, loss = 2.01 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:37:52.864847: step 145430, loss = 1.81 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:37:54.029456: step 145440, loss = 1.95 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:37:55.199897: step 145450, loss = 1.93 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:37:56.354487: step 145460, loss = 1.95 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:37:57.502559: step 145470, loss = 1.87 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:37:58.678902: step 145480, loss = 1.76 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:37:59.844536: step 145490, loss = 1.90 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:01.035955: step 145500, loss = 1.84 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:38:02.186475: step 145510, loss = 1.65 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:38:03.343508: step 145520, loss = 1.96 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:04.626070: step 145530, loss = 1.97 (998.0 examples/sec; 0.128 sec/batch)
2017-05-05 02:38:05.673051: step 145540, loss = 1.77 (1222.6 examples/sec; 0.105 sec/batch)
2017-05-05 02:38:06.840430: step 145550, loss = 1.77 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:07.998051: step 145560, loss = 1.89 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:09.147537: step 145570, loss = 1.97 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:38:10.315998: step 145580, loss = 1.73 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:11.474681: step 145590, loss = 1.92 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:12.636225: step 145600, loss = 1.90 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:13.794940: step 145610, loss = 1.94 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:14.967285: step 145620, loss = 2.03 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:16.122866: step 145630, loss = 2.04 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:17.281884: step 145640, loss = 1.89 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:18.458374: step 145650, loss = 1.90 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:19.634497: step 145660, loss = 1.78 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:20.816168: step 145670, loss = 1.93 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:21.971817: step 145680, loss = 1.91 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:23.149844: step 145690, loss = 1.91 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:24.315165: step 145700, loss = 1.68 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:25.479638: step 145710, loss = 1.72 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:26.651040: step 145720, loss = 1.80 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:27.822362: step 145730, loss = 1.87 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:28.990837: step 145740, loss = 1.97 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:30.163902: step 145750, loss = 1.94 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:31.363407: step 145760, loss = 1.80 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:38:32.533617: step 145770, loss = 1.88 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:33.703037: step 145780, loss = 1.80 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:34.884417: step 145790, loss = 1.88 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:36.077682: step 145800, loss = 1.84 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:38:37.244144: step 145810, loss = 1.98 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:38.402550: step 145820, loss = 1.76 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:39.592749: step 145830, loss = 1.79 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:38:40.759024: step 145840, loss = 1.86 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:41.904809: step 145850, loss = 1.78 (1117.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:38:43.071849: step 145860, loss = 1.84 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:44.240945: step 145870, loss = 1.77 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:45.398427: step 145880, loss = 1.97 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:46.572543: step 145890, loss = 1.86 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:47.743455: step 145900, loss = 1.85 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:48.927761: step 145910, loss = 1.95 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:50.088376: step 145920, loss = 1.92 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:51.274187: step 145930, loss = 2.06 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:38:52.433818: step 145940, loss = 1.96 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:53.570155: step 145950, loss = 1.85 (1126.4 examples/sec; 0.114 sec/batch)
2017-05-05 02:38:54.772980: step 145960, loss = 2.12 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:38:55.933051: step 145970, loss = 1.84 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:57.103292: step 145980, loss = 1.78 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:58.273086: step 145990, loss = 1.75 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:59.443083: step 146000, loss = 1.94 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:00.640910: step 146010, loss = 1.84 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:39:01.831621: step 146020, loss = 1.82 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:39:03.045583: step 146030, loss = 1.87 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:39:04.262117: step 146040, loss = 1.84 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:39:05.470087: step 146050, loss = 1.75 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:39:06.659572: step 146060, loss = 1.92 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:39:07.854221: step 146070, loss = 1.91 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:39:09.041758: step 146080, loss = 1.89 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:39:10.220641: step 146090, loss = 1.81 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:39:11.405187: step 146100, loss = 1.73 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:39:12.588952: step 146110, loss = 1.96 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:39:13.745075: step 146120, loss = 1.85 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:14.898864: step 146130, loss = 1.79 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:39:16.066818: step 146140, loss = 2.17 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:17.237945: step 146150, loss = 1.84 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:18.389576: step 146160, loss = 1.88 (1111.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:39:19.563859: step 146170, loss = 1.78 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:20.738807: step 146180, loss = 2.09 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:21.920881: step 146190, loss = 1.95 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:39:23.092660: step 146200, loss = 1.84 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:24.271562: step 146210, loss = 1.90 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:39:25.414410: step 146220, loss = 1.81 (1120.0 examples/sec; 0.114 sec/batch)
2017-05-05 02:39:26.571284: step 146230, loss = 1.76 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:27.726413: step 146240, loss = 1.74 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:28.886786: step 146250, loss = 2.00 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:30.054776: step 146260, loss = 1.86 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:31.215167: step 146270, loss = 1.87 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:32.419007: step 146280, loss = 1.89 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:39:33.567330: step 146290, loss = 1.97 (1114.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:39:34.739495: step 146300, loss = 1.82 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:35.909932: step 146310, loss = 1.89 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:37.075791: step 146320, loss = 1.91 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:38.228513: step 146330, loss = 1.90 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:39:39.398402: step 146340, loss = 1.89 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:40.580288: step 146350, loss = 1.87 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:39:41.727481: step 146360, loss = 1.77 (1115.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:39:42.881706: step 146370, loss = 1.87 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:39:44.040710: step 146380, loss = 1.89 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:45.208098: step 146390, loss = 1.92 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:46.360135: step 146400, loss = 1.84 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:39:47.521565: step 146410, loss = 1.85 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:48.681481: step 146420, loss = 1.91 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:49.839384: step 146430, loss = 1.93 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:51.015008: step 146440, loss = 1.97 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:39:52.198503: step 146450, loss = 2.05 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:39:53.382321: step 146460, loss = 1.94 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:39:54.536395: step 146470, loss = 2.01 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:39:55.698036: step 146480, loss = 1.80 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:56.893629: step 146490, loss = 1.79 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:39:58.034278: step 146500, loss = 1.92 (1122.2 examples/sec; 0.114 sec/batch)
2017-05-05 02:39:59.207742: step 146510, loss = 1.85 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:00.460198: step 146520, loss = 1.91 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-05 02:40:01.518627: step 146530, loss = 1.92 (1209.3 examples/sec; 0.106 sec/batch)
2017-05-05 02:40:02.682399: step 146540, loss = 1.89 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:03.867537: step 146550, loss = 1.82 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:40:05.023618: step 146560, loss = 1.84 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:06.193540: step 146570, loss = 1.90 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:07.374592: step 146580, loss = 1.95 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:08.537290: step 146590, loss = 1.83 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:09.694833: step 146600, loss = 1.99 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:10.873469: step 146610, loss = 1.92 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:12.041550: step 146620, loss = 1.89 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:13.229045: step 146630, loss = 1.76 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:40:14.387011: step 146640, loss = 1.95 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:15.564582: step 146650, loss = 1.88 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:16.710306: step 146660, loss = 1.84 (1117.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:40:17.863144: step 146670, loss = 1.86 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:40:19.025824: step 146680, loss = 1.94 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:20.202367: step 146690, loss = 1.89 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:21.382592: step 146700, loss = 1.78 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:22.542472: step 146710, loss = 1.91 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:23.727031: step 146720, loss = 1.80 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:24.895115: step 146730, loss = 1.75 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:26.043865: step 146740, loss = 1.85 (1114.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:40:27.215165: step 146750, loss = 1.79 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:28.389742: step 146760, loss = 1.87 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:29.548921: step 146770, loss = 1.89 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:30.729114: step 146780, loss = 1.94 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:31.904163: step 146790, loss = 1.88 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:33.080179: step 146800, loss = 1.85 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:34.249038: step 146810, loss = 1.67 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:35.428187: step 146820, loss = 1.94 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:36.584848: step 146830, loss = 1.83 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:37.740326: step 146840, loss = 1.88 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:38.896675: step 146850, loss = 2.02 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:40.063484: step 146860, loss = 1.88 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:41.234187: step 146870, loss = 1.88 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:42.389747: step 146880, loss = 2.05 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:43.547071: step 146890, loss = 1.88 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:44.708284: step 146900, loss = 1.91 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:45.872281: step 146910, loss = 1.85 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:47.070851: step 146920, loss = 1.88 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:40:48.229774: step 146930, loss = 1.82 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:49.407856: step 146940, loss = 1.87 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:50.559418: step 146950, loss = 1.93 (1111.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:40:51.722870: step 146960, loss = 1.81 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:52.894181: step 146970, loss = 1.69 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:54.055510: step 146980, loss = 1.92 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:55.231668: step 146990, loss = 1.86 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:56.386445: step 147000, loss = 1.86 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:40:57.573490: step 147010, loss = 1.82 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:40:58.741046: step 147020, loss = 1.79 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:59.901332: step 147030, loss = 1.87 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:01.077954: step 147040, loss = 1.93 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:41:02.238797: step 147050, loss = 1.85 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:03.373458: step 147060, loss = 1.77 (1128.1 examples/sec; 0.113 sec/batch)
2017-05-05 02:41:04.550330: step 147070, loss = 1.91 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:41:05.725121: step 147080, loss = 1.87 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:06.905477: step 147090, loss = 1.77 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:41:08.068658: step 147100, loss = 1.92 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:09.221327: step 147110, loss = 1.80 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:41:10.387525: step 147120, loss = 1.90 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:11.555637: step 147130, loss = 1.92 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:12.712184: step 147140, loss = 1.86 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:13.878303: step 147150, loss = 1.92 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:15.026746: step 147160, loss = 1.90 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:41:16.215190: step 147170, loss = 1.95 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:41:17.377112: step 147180, loss = 1.99 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:18.563051: step 147190, loss = 1.82 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:41:19.713481: step 147200, loss = 1.95 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:41:20.890639: step 147210, loss = 1.85 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:41:22.054754: step 147220, loss = 1.76 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:23.228713: step 147230, loss = 1.90 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:24.420881: step 147240, loss = 1.89 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:41:25.569459: step 147250, loss = 2.14 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:41:26.740006: step 147260, loss = 1.84 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:27.905804: step 147270, loss = 1.90 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:29.070747: step 147280, loss = 1.86 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:30.232550: step 147290, loss = 1.83 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:31.424384: step 147300, loss = 1.79 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:41:32.606396: step 147310, loss = 1.88 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:41:33.782008: step 147320, loss = 2.01 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:41:34.989190: step 147330, loss = 1.85 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:41:36.186167: step 147340, loss = 1.95 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:41:37.371789: step 147350, loss = 1.78 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:41:38.555808: step 147360, loss = 1.93 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:41:39.729664: step 147370, loss = 1.98 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:40.931741: step 147380, loss = 1.85 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:41:42.106201: step 147390, loss = 1.75 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:43.292657: step 147400, loss = 2.10 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:41:44.498825: step 147410, loss = 2.07 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:41:45.689666: step 147420, loss = 1.86 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:41:46.875583: step 147430, loss = 1.79 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:41:48.041365: step 147440, loss = 1.90 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:49.206722: step 147450, loss = 1.83 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:50.354166: step 147460, loss = 1.75 (1115.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:41:51.522181: step 147470, loss = 1.80 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:52.686270: step 147480, loss = 1.84 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:53.841936: step 147490, loss = 1.93 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:55.006334: step 147500, loss = 1.95 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:56.251184: step 147510, loss = 1.90 (1028.2 examples/sec; 0.124 sec/batch)
2017-05-05 02:41:57.329515: step 147520, loss = 1.90 (1187.0 examples/sec; 0.108 sec/batch)
2017-05-05 02:41:58.478922: step 147530, loss = 1.85 (1113.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:41:59.651588: step 147540, loss = 1.84 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:00.819054: step 147550, loss = 1.73 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:01.982296: step 147560, loss = 1.93 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:03.155416: step 147570, loss = 2.05 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:04.326542: step 147580, loss = 1.79 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:05.488106: step 147590, loss = 1.80 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:06.635750: step 147600, loss = 1.85 (1115.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:07.816068: step 147610, loss = 1.93 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:42:08.993809: step 147620, loss = 2.09 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:42:10.170058: step 147630, loss = 2.00 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:42:11.323547: step 147640, loss = 1.89 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:12.487063: step 147650, loss = 1.79 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:13.641570: step 147660, loss = 1.69 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:14.792771: step 147670, loss = 1.74 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:15.964105: step 147680, loss = 1.97 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:17.132784: step 147690, loss = 1.98 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:18.307153: step 147700, loss = 1.86 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:19.475779: step 147710, loss = 1.74 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:20.641632: step 147720, loss = 1.83 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:21.811954: step 147730, loss = 1.79 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:22.992350: step 147740, loss = 1.87 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:42:24.156304: step 147750, loss = 1.94 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:25.327839: step 147760, loss = 1.88 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:26.475438: step 147770, loss = 1.95 (1115.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:27.634675: step 147780, loss = 2.04 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:28.795293: step 147790, loss = 1.75 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:29.967756: step 147800, loss = 1.99 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:31.139797: step 147810, loss = 1.99 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:32.293661: step 147820, loss = 1.88 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:33.463376: step 147830, loss = 1.82 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:34.622689: step 147840, loss = 1.79 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:35.800065: step 147850, loss = 1.76 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:42:36.972352: step 147860, loss = 2.01 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:38.105317: step 147870, loss = 1.87 (1129.8 examples/sec; 0.113 sec/batch)
2017-05-05 02:42:39.272282: step 147880, loss = 1.93 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:40.421371: step 147890, loss = 1.92 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:41.586144: step 147900, loss = 1.91 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:42.744191: step 147910, loss = 2.05 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:43.922900: step 147920, loss = 1.91 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:42:45.096195: step 147930, loss = 1.82 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:46.256944: step 147940, loss = 1.78 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:47.411466: step 147950, loss = 1.89 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:48.575344: step 147960, loss = 1.82 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:49.722471: step 147970, loss = 1.83 (1115.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:50.883465: step 147980, loss = 1.93 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:52.059227: step 147990, loss = 1.81 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:42:53.225792: step 148000, loss = 1.77 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:54.379880: step 148010, loss = 1.93 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:55.555347: step 148020, loss = 1.83 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:42:56.718855: step 148030, loss = 1.93 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:57.874566: step 148040, loss = 1.88 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:59.055342: step 148050, loss = 2.02 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:00.227558: step 148060, loss = 1.88 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:01.382119: step 148070, loss = 1.90 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:43:02.520861: step 148080, loss = 1.88 (1124.0 examples/sec; 0.114 sec/batch)
2017-05-05 02:43:03.696035: step 148090, loss = 1.77 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:04.877441: step 148100, loss = 1.76 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:06.021613: step 148110, loss = 1.93 (1118.7 examples/sec; 0.114 sec/batch)
2017-05-05 02:43:07.201425: step 148120, loss = 1.91 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:08.362092: step 148130, loss = 1.80 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:09.528503: step 148140, loss = 1.75 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:10.681205: step 148150, loss = 1.97 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:43:11.853043: step 148160, loss = 1.86 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:13.010602: step 148170, loss = 1.99 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:14.161119: step 148180, loss = 1.79 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:43:15.327154: step 148190, loss = 1.83 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:16.502827: step 148200, loss = 1.97 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:17.659468: step 148210, loss = 1.89 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:18.817930: step 148220, loss = 1.99 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:19.984487: step 148230, loss = 1.94 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:21.152575: step 148240, loss = 1.79 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:22.316876: step 148250, loss = 1.96 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:23.480560: step 148260, loss = 1.83 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:24.647136: step 148270, loss = 1.92 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:25.793528: step 148280, loss = 1.88 (1116.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:43:26.967945: step 148290, loss = 1.95 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:28.152543: step 148300, loss = 1.75 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:29.324798: step 148310, loss = 1.98 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:30.506794: step 148320, loss = 1.99 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:31.689417: step 148330, loss = 1.83 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:32.867352: step 148340, loss = 1.88 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:34.041696: step 148350, loss = 1.94 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:35.211024: step 148360, loss = 2.01 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:36.397121: step 148370, loss = 1.95 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:43:37.571955: step 148380, loss = 1.78 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:38.725063: step 148390, loss = 1.92 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:43:39.890869: step 148400, loss = 1.85 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:41.080922: step 148410, loss = 2.06 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:43:42.246783: step 148420, loss = 1.84 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:43.405249: step 148430, loss = 1.63 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:44.569194: step 148440, loss = 1.78 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:45.721912: step 148450, loss = 1.87 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:43:46.888219: step 148460, loss = 1.73 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:48.070417: step 148470, loss = 1.96 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:49.229867: step 148480, loss = 1.86 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:50.393559: step 148490, loss = 1.75 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:51.667160: step 148500, loss = 1.83 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-05 02:43:52.727029: step 148510, loss = 1.74 (1207.7 examples/sec; 0.106 sec/batch)
2017-05-05 02:43:53.880510: step 148520, loss = 1.75 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:43:55.057991: step 148530, loss = 1.84 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:56.230141: step 148540, loss = 1.87 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:57.403126: step 148550, loss = 1.87 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:58.578916: step 148560, loss = 1.91 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:59.741057: step 148570, loss = 1.73 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:00.923272: step 148580, loss = 2.01 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:44:02.094384: step 148590, loss = 1.95 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:03.261218: step 148600, loss = 1.84 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:04.432404: step 148610, loss = 1.79 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:05.599702: step 148620, loss = 1.94 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:06.753372: step 148630, loss = 1.84 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:44:07.928029: step 148640, loss = 1.80 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:09.097957: step 148650, loss = 1.93 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:10.254085: step 148660, loss = 1.92 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:11.433939: step 148670, loss = 1.84 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:44:12.595494: step 148680, loss = 1.95 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:13.770760: step 148690, loss = 1.91 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:44:14.941836: step 148700, loss = 1.83 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:16.101191: step 148710, loss = 1.87 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:17.273798: step 148720, loss = 1.88 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:18.426361: step 148730, loss = 1.79 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:44:19.614618: step 148740, loss = 1.85 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:44:20.792161: step 148750, loss = 2.00 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:44:21.955059: step 148760, loss = 1.85 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:23.116174: step 148770, loss = 1.80 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:24.281524: step 148780, loss = 1.80 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:25.436526: step 148790, loss = 1.85 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:26.590641: step 148800, loss = 1.89 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:44:27.772191: step 148810, loss = 1.94 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:44:28.941548: step 148820, loss = 1.82 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:30.098582: step 148830, loss = 1.89 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:31.269526: step 148840, loss = 2.09 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:32.428733: step 148850, loss = 1.80 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:33.578563: step 148860, loss = 2.01 (1113.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:44:34.738227: step 148870, loss = 1.82 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:35.904105: step 148880, loss = 1.82 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:37.069322: step 148890, loss = 1.94 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:38.228279: step 148900, loss = 1.89 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:39.387281: step 148910, loss = 1.87 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:40.539375: step 148920, loss = 1.85 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:44:41.703281: step 148930, loss = 2.03 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:42.861289: step 148940, loss = 1.85 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:44.032206: step 148950, loss = 1.89 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:45.213889: step 148960, loss = 1.88 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:44:46.362508: step 148970, loss = 1.83 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:44:47.527275: step 148980, loss = 1.86 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:48.704316: step 148990, loss = 1.91 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:44:49.879709: step 149000, loss = 1.77 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:44:51.049931: step 149010, loss = 1.76 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:52.227868: step 149020, loss = 1.91 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:44:53.386701: step 149030, loss = 1.88 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:54.549498: step 149040, loss = 2.01 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:55.727986: step 149050, loss = 1.72 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:44:56.893131: step 149060, loss = 1.93 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:58.058120: step 149070, loss = 2.00 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:59.206175: step 149080, loss = 1.87 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:45:00.351398: step 149090, loss = 1.84 (1117.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:45:01.507435: step 149100, loss = 1.80 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:02.683132: step 149110, loss = 1.87 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:03.864094: step 149120, loss = 1.88 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:05.026400: step 149130, loss = 2.02 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:06.185166: step 149140, loss = 1.79 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:07.340889: step 149150, loss = 1.91 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:08.510205: step 149160, loss = 1.80 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:09.677279: step 149170, loss = 1.84 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:10.839317: step 149180, loss = 1.98 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:12.003940: step 149190, loss = 1.74 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:13.159100: step 149200, loss = 1.93 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:14.307858: step 149210, loss = 1.87 (1114.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:45:15.468772: step 149220, loss = 2.10 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:16.640742: step 149230, loss = 1.89 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:17.816095: step 149240, loss = 1.73 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:18.960175: step 149250, loss = 1.98 (1118.8 examples/sec; 0.114 sec/batch)
2017-05-05 02:45:20.122562: step 149260, loss = 1.77 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:21.297700: step 149270, loss = 1.86 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:22.476372: step 149280, loss = 1.86 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:23.640325: step 149290, loss = 1.84 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:24.803483: step 149300, loss = 1.90 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:25.975765: step 149310, loss = 1.79 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:27.162671: step 149320, loss = 1.83 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:45:28.358889: step 149330, loss = 1.95 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:45:29.525597: step 149340, loss = 1.93 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:30.696565: step 149350, loss = 2.10 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:31.880599: step 149360, loss = 1.84 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:33.062024: step 149370, loss = 1.88 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:34.231980: step 149380, loss = 1.88 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:35.414017: step 149390, loss = 1.83 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:36.582098: step 149400, loss = 1.96 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:37.739472: step 149410, loss = 1.78 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:38.892989: step 149420, loss = 1.84 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:45:40.073745: step 149430, loss = 2.10 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:41.231354: step 149440, loss = 1.77 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:42.410881: step 149450, loss = 1.86 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:43.567249: step 149460, loss = 1.95 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:44.731582: step 149470, loss = 2.05 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:45.881509: step 149480, loss = 1.73 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:45:47.197055: step 149490, loss = 1.81 (973.0 examples/sec; 0.132 sec/batch)
2017-05-05 02:45:48.281717: step 149500, loss = 2.03 (1180.1 examples/sec; 0.108 sec/batch)
2017-05-05 02:45:49.484222: step 149510, loss = 2.02 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:45:50.661541: step 149520, loss = 1.79 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:51.870637: step 149530, loss = 1.86 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:45:53.084165: step 149540, loss = 1.87 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:45:54.273522: step 149550, loss = 1.90 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:45:55.484934: step 149560, loss = 1.98 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:45:56.693224: step 149570, loss = 1.84 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:45:57.858827: step 149580, loss = 2.08 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:59.056398: step 149590, loss = 1.83 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:00.237910: step 149600, loss = 1.84 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:46:01.435531: step 149610, loss = 1.87 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:02.607691: step 149620, loss = 1.80 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:46:03.832412: step 149630, loss = 1.80 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:46:05.034301: step 149640, loss = 1.70 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:06.238592: step 149650, loss = 1.96 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:07.441409: step 149660, loss = 1.97 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:08.656729: step 149670, loss = 1.95 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:46:09.840990: step 149680, loss = 1.90 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:46:11.097897: step 149690, loss = 1.89 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-05 02:46:12.239703: step 149700, loss = 1.86 (1121.0 examples/sec; 0.114 sec/batch)
2017-05-05 02:46:13.438950: step 149710, loss = 1.84 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:14.643786: step 149720, loss = 1.90 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:15.851112: step 149730, loss = 1.77 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:17.053874: step 149740, loss = 1.85 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:18.235869: step 149750, loss = 1.73 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:46:19.439790: step 149760, loss = 1.79 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:20.635279: step 149770, loss = 1.80 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:21.880088: step 149780, loss = 1.95 (1028.3 examples/sec; 0.124 sec/batch)
2017-05-05 02:46:23.101819: step 149790, loss = 1.94 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:46:24.318390: step 149800, loss = 1.98 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:46:25.516149: step 149810, loss = 1.95 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:26.729567: step 149820, loss = 1.85 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:27.925885: step 149830, loss = 1.96 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:29.126809: step 149840, loss = 1.95 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:30.316954: step 149850, loss = 1.86 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:46:31.515496: step 149860, loss = 1.90 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:32.723831: step 149870, loss = 1.87 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:33.948355: step 149880, loss = 1.97 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:46:35.127910: step 149890, loss = 1.98 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:46:36.356588: step 149900, loss = 2.21 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:46:37.571728: step 149910, loss = 1.92 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:46:38.778478: step 149920, loss = 1.98 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:39.973777: step 149930, loss = 1.89 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:41.198639: step 149940, loss = 1.91 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:46:42.371610: step 149950, loss = 1.90 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:46:43.582788: step 149960, loss = 1.79 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:44.788887: step 149970, loss = 1.77 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:45.990653: step 149980, loss = 1.95 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:47.197421: step 149990, loss = 2.01 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:48.387221: step 150000, loss = 1.88 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:46:49.594391: step 150010, loss = 1.96 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:50.787059: step 150020, loss = 1.95 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:46:51.966661: step 150030, loss = 1.94 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:46:53.131499: step 150040, loss = 1.87 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:46:54.310691: step 150050, loss = 1.95 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:46:55.487791: step 150060, loss = 1.91 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:46:56.671692: step 150070, loss = 1.94 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:46:57.836567: step 150080, loss = 1.87 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:46:59.015688: step 150090, loss = 1.83 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:00.181528: step 150100, loss = 1.92 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:01.356778: step 150110, loss = 1.94 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:02.527474: step 150120, loss = 2.06 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:03.718129: step 150130, loss = 1.82 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:47:04.916992: step 150140, loss = 1.81 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:47:06.106431: step 150150, loss = 1.80 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:47:07.291034: step 150160, loss = 1.84 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:08.467065: step 150170, loss = 1.92 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:09.633609: step 150180, loss = 1.96 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:10.820336: step 150190, loss = 1.92 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:47:11.991956: step 150200, loss = 1.88 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:13.179491: step 150210, loss = 1.79 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:47:14.340366: step 150220, loss = 1.90 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:15.500470: step 150230, loss = 1.86 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:16.699620: step 150240, loss = 1.96 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:47:17.851300: step 150250, loss = 1.90 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:47:19.035182: step 150260, loss = 1.83 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:20.200681: step 150270, loss = 1.86 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:21.351843: step 150280, loss = 1.88 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:47:22.521035: step 150290, loss = 1.98 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:23.706896: step 150300, loss = 1.93 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:47:24.872532: step 150310, loss = 1.89 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:26.052895: step 150320, loss = 1.75 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:27.240100: step 150330, loss = 1.94 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:47:28.405747: step 150340, loss = 1.97 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:29.575775: step 150350, loss = 1.88 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:30.733161: step 150360, loss = 1.94 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:31.895763: step 150370, loss = 1.83 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:33.060660: step 150380, loss = 1.77 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:34.203732: step 150390, loss = 1.94 (1119.8 examples/sec; 0.114 sec/batch)
2017-05-05 02:47:35.372602: step 150400, loss = 1.83 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:36.536107: step 150410, loss = 1.80 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:37.715281: step 150420, loss = 1.91 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:38.898983: step 150430, loss = 1.88 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:40.060336: step 150440, loss = 1.89 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:41.240921: step 150450, loss = 1.99 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:42.388918: step 150460, loss = 1.98 (1115.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:47:43.561013: step 150470, loss = 1.96 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:44.812386: step 150480, loss = 1.97 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-05 02:47:45.884932: step 150490, loss = 1.92 (1193.4 examples/sec; 0.107 sec/batch)
2017-05-05 02:47:47.047593: step 150500, loss = 1.98 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:48.233062: step 150510, loss = 1.85 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:47:49.391698: step 150520, loss = 1.89 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:50.546774: step 150530, loss = 1.88 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:51.709779: step 150540, loss = 1.88 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:52.866656: step 150550, loss = 1.80 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:54.025992: step 150560, loss = 1.83 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:55.189492: step 150570, loss = 1.99 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:56.368010: step 150580, loss = 2.03 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:57.545341: step 150590, loss = 2.18 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:58.702346: step 150600, loss = 2.06 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:59.880353: step 150610, loss = 1.79 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:01.064001: step 150620, loss = 1.96 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:02.225463: step 150630, loss = 1.67 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:03.404459: step 150640, loss = 1.95 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:04.566542: step 150650, loss = 1.93 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:05.731838: step 150660, loss = 1.94 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:06.884613: step 150670, loss = 1.75 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:48:08.050207: step 150680, loss = 1.84 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:09.225661: step 150690, loss = 1.79 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:10.375408: step 150700, loss = 1.84 (1113.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:48:11.538741: step 150710, loss = 1.87 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:12.707143: step 150720, loss = 1.82 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:13.865128: step 150730, loss = 1.89 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:15.030412: step 150740, loss = 1.83 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:16.204487: step 150750, loss = 1.86 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:17.368918: step 150760, loss = 1.92 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:18.513730: step 150770, loss = 1.82 (1118.1 examples/sec; 0.114 sec/batch)
2017-05-05 02:48:19.694693: step 150780, loss = 1.81 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:20.858641: step 150790, loss = 1.82 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:22.025395: step 150800, loss = 1.94 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:23.184810: step 150810, loss = 1.94 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:24.342976: step 150820, loss = 1.99 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:25.513417: step 150830, loss = 1.80 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:26.695633: step 150840, loss = 1.92 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:27.883067: step 150850, loss = 1.81 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:48:29.040893: step 150860, loss = 1.84 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:30.218463: step 150870, loss = 1.88 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:31.405103: step 150880, loss = 1.79 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:48:32.592827: step 150890, loss = 2.00 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:48:33.756611: step 150900, loss = 1.72 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:34.909788: step 150910, loss = 1.90 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:48:36.071375: step 150920, loss = 1.88 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:37.252031: step 150930, loss = 1.89 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:38.387660: step 150940, loss = 1.86 (1127.1 examples/sec; 0.114 sec/batch)
2017-05-05 02:48:39.547142: step 150950, loss = 1.80 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:40.729594: step 150960, loss = 2.06 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:41.865433: step 150970, loss = 1.90 (1126.9 examples/sec; 0.114 sec/batch)
2017-05-05 02:48:43.046409: step 150980, loss = 1.87 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:44.231349: step 150990, loss = 1.97 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:45.398010: step 151000, loss = 1.91 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:46.558171: step 151010, loss = 2.17 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:47.711928: step 151020, loss = 1.73 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:48:48.890474: step 151030, loss = 1.82 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:50.047914: step 151040, loss = 1.75 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:51.236026: step 151050, loss = 1.79 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:48:52.411526: step 151060, loss = 1.85 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:53.572013: step 151070, loss = 1.84 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:54.764369: step 151080, loss = 1.78 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:48:55.951211: step 151090, loss = 2.01 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:48:57.100878: step 151100, loss = 1.88 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:48:58.248380: step 151110, loss = 1.88 (1115.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:48:59.447662: step 151120, loss = 2.11 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:49:00.620529: step 151130, loss = 1.81 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:01.765655: step 151140, loss = 1.94 (1117.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:49:02.950107: step 151150, loss = 1.97 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:49:04.114311: step 151160, loss = 1.91 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:05.271625: step 151170, loss = 1.95 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:06.415113: step 151180, loss = 1.89 (1119.4 examples/sec; 0.114 sec/batch)
2017-05-05 02:49:07.583145: step 151190, loss = 1.81 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:08.752829: step 151200, loss = 2.11 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:09.915917: step 151210, loss = 1.95 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:11.079302: step 151220, loss = 1.88 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:12.239796: step 151230, loss = 1.82 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:13.407388: step 151240, loss = 1.96 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:14.557097: step 151250, loss = 1.96 (1113.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:49:15.723022: step 151260, loss = 1.81 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:16.873490: step 151270, loss = 2.06 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:49:18.030434: step 151280, loss = 1.67 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:19.202619: step 151290, loss = 1.84 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:20.365485: step 151300, loss = 1.85 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:21.517676: step 151310, loss = 1.91 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:49:22.708206: step 151320, loss = 2.02 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:49:23.877616: step 151330, loss = 1.71 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:25.056487: step 151340, loss = 1.88 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:49:26.219166: step 151350, loss = 1.88 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:27.398568: step 151360, loss = 1.85 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:49:28.581289: step 151370, loss = 1.81 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:49:29.747824: step 151380, loss = 1.94 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:30.955365: step 151390, loss = 1.88 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:49:32.119318: step 151400, loss = 1.79 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:33.289735: step 151410, loss = 1.82 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:34.468738: step 151420, loss = 1.92 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:49:35.658494: step 151430, loss = 1.82 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:49:36.844473: step 151440, loss = 1.92 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:49:38.017673: step 151450, loss = 1.80 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:39.189765: step 151460, loss = 1.83 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:40.458966: step 151470, loss = 1.90 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-05 02:49:41.510888: step 151480, loss = 1.96 (1216.8 examples/sec; 0.105 sec/batch)
2017-05-05 02:49:42.664678: step 151490, loss = 1.98 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:49:43.825167: step 151500, loss = 1.81 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:45.004943: step 151510, loss = 1.73 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:49:46.150616: step 151520, loss = 1.89 (1117.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:49:47.341976: step 151530, loss = 1.91 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:49:48.492985: step 151540, loss = 1.91 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:49:49.658077: step 151550, loss = 1.94 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:50.824491: step 151560, loss = 1.80 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:51.991848: step 151570, loss = 1.85 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:53.183255: step 151580, loss = 1.83 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:49:54.339961: step 151590, loss = 2.06 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:55.498122: step 151600, loss = 2.11 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:56.662875: step 151610, loss = 1.91 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:57.813806: step 151620, loss = 1.85 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:49:58.996010: step 151630, loss = 1.78 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:00.139679: step 151640, loss = 2.01 (1119.2 examples/sec; 0.114 sec/batch)
2017-05-05 02:50:01.328425: step 151650, loss = 1.83 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:50:02.495468: step 151660, loss = 1.97 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:03.652222: step 151670, loss = 1.95 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:04.797402: step 151680, loss = 1.88 (1117.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:50:05.956652: step 151690, loss = 1.86 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:07.105801: step 151700, loss = 1.92 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:50:08.280420: step 151710, loss = 2.08 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:09.454666: step 151720, loss = 1.92 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:10.628262: step 151730, loss = 1.79 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:11.809964: step 151740, loss = 1.87 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:12.994323: step 151750, loss = 1.87 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:14.149863: step 151760, loss = 1.92 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:15.337378: step 151770, loss = 1.78 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:50:16.493224: step 151780, loss = 1.94 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:17.649354: step 151790, loss = 1.91 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:18.817210: step 151800, loss = 1.90 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:19.980373: step 151810, loss = 1.84 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:21.167945: step 151820, loss = 1.93 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:50:22.327149: step 151830, loss = 1.90 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:23.490512: step 151840, loss = 1.89 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:24.662524: step 151850, loss = 2.16 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:25.857913: step 151860, loss = 1.80 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:50:27.025192: step 151870, loss = 1.97 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:28.212996: step 151880, loss = 1.97 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:50:29.377529: step 151890, loss = 1.88 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:30.532515: step 151900, loss = 1.85 (1108.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:50:31.699060: step 151910, loss = 1.91 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:32.864093: step 151920, loss = 1.88 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:34.019431: step 151930, loss = 1.79 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:35.173764: step 151940, loss = 1.88 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:50:36.323182: step 151950, loss = 1.97 (1113.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:50:37.494172: step 151960, loss = 1.89 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:38.664651: step 151970, loss = 1.81 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:39.854593: step 151980, loss = 1.84 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:50:41.060258: step 151990, loss = 1.79 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:50:42.228345: step 152000, loss = 1.76 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:43.400122: step 152010, loss = 1.83 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:44.589417: step 152020, loss = 1.98 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:50:45.751419: step 152030, loss = 2.03 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:46.918766: step 152040, loss = 1.97 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:48.103174: step 152050, loss = 1.83 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:49.271154: step 152060, loss = 1.86 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:50.431469: step 152070, loss = 1.84 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:51.613739: step 152080, loss = 1.86 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:52.799820: step 152090, loss = 1.81 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:50:53.972072: step 152100, loss = 1.78 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:55.160123: step 152110, loss = 1.94 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:50:56.349593: step 152120, loss = 1.92 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:50:57.524430: step 152130, loss = 1.79 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:58.694018: step 152140, loss = 2.03 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:59.890634: step 152150, loss = 1.80 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:51:01.064889: step 152160, loss = 1.87 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:02.257650: step 152170, loss = 1.86 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:51:03.429620: step 152180, loss = 1.72 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:04.596677: step 152190, loss = 1.84 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:05.723192: step 152200, loss = 1.84 (1136.2 examples/sec; 0.113 sec/batch)
2017-05-05 02:51:06.880671: step 152210, loss = 1.86 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:51:08.051122: step 152220, loss = 1.80 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:09.230783: step 152230, loss = 2.20 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:51:10.366516: step 152240, loss = 1.86 (1127.0 examples/sec; 0.114 sec/batch)
2017-05-05 02:51:11.534721: step 152250, loss = 1.78 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:12.728303: step 152260, loss = 1.86 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:51:13.865699: step 152270, loss = 1.72 (1125.4 examples/sec; 0.114 sec/batch)
2017-05-05 02:51:15.031095: step 152280, loss = 1.81 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:16.220216: step 152290, loss = 1.99 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:51:17.361830: step 152300, loss = 1.79 (1121.2 examples/sec; 0.114 sec/batch)
2017-05-05 02:51:18.536639: step 152310, loss = 1.82 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:19.696447: step 152320, loss = 2.12 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:51:20.866277: step 152330, loss = 1.94 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:22.016636: step 152340, loss = 1.86 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:51:23.189802: step 152350, loss = 1.96 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:24.353150: step 152360, loss = 1.87 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:51:25.506716: step 152370, loss = 1.90 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:51:26.692818: step 152380, loss = 1.88 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:51:27.877996: step 152390, loss = 1.83 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:51:29.066907: step 152400, loss = 1.97 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:51:30.274929: step 152410, loss = 1.90 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:51:31.492203: step 152420, loss = 1.88 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:51:32.680441: step 152430, loss = 1.86 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:51:33.885582: step 152440, loss = 1.99 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:51:35.112491: step 152450, loss = 1.89 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:51:36.436332: step 152460, loss = 2.00 (966.9 examples/sec; 0.132 sec/batch)
2017-05-05 02:51:37.551392: step 152470, loss = 1.81 (1147.9 examples/sec; 0.112 sec/batch)
2017-05-05 02:51:38.750291: step 152480, loss = 1.96 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:51:39.952223: step 152490, loss = 1.84 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:51:41.152354: step 152500, loss = 1.80 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:51:42.371708: step 152510, loss = 1.80 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:51:43.589647: step 152520, loss = 1.86 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:51:44.823081: step 152530, loss = 1.89 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:51:46.011931: step 152540, loss = 1.82 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:51:47.233770: step 152550, loss = 1.94 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:51:48.448713: step 152560, loss = 1.97 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:51:49.680279: step 152570, loss = 1.96 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:51:50.917675: step 152580, loss = 1.84 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-05 02:51:52.170832: step 152590, loss = 1.84 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-05 02:51:53.387002: step 152600, loss = 1.89 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:51:54.613215: step 152610, loss = 2.02 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:51:55.834034: step 152620, loss = 1.90 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:51:57.057149: step 152630, loss = 1.81 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:51:58.258893: step 152640, loss = 1.94 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:51:59.488164: step 152650, loss = 1.82 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:00.775708: step 152660, loss = 1.92 (994.1 examples/sec; 0.129 sec/batch)
2017-05-05 02:52:01.895034: step 152670, loss = 1.98 (1143.5 examples/sec; 0.112 sec/batch)
2017-05-05 02:52:03.108018: step 152680, loss = 1.80 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:04.348290: step 152690, loss = 1.80 (1032.0 examples/sec; 0.124 sec/batch)
2017-05-05 02:52:05.527207: step 152700, loss = 1.96 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:52:06.766724: step 152710, loss = 1.97 (1032.7 examples/sec; 0.124 sec/batch)
2017-05-05 02:52:07.966897: step 152720, loss = 1.99 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:52:09.185678: step 152730, loss = 2.03 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:10.364401: step 152740, loss = 1.81 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:52:11.584753: step 152750, loss = 1.94 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:12.819463: step 152760, loss = 1.84 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:14.004940: step 152770, loss = 1.88 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:52:15.211884: step 152780, loss = 2.04 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:16.439846: step 152790, loss = 1.84 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:17.637748: step 152800, loss = 1.86 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:52:18.878914: step 152810, loss = 2.11 (1031.3 examples/sec; 0.124 sec/batch)
2017-05-05 02:52:20.086669: step 152820, loss = 1.76 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:21.321551: step 152830, loss = 1.83 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:22.530718: step 152840, loss = 1.80 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:23.777153: step 152850, loss = 1.78 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-05 02:52:24.961957: step 152860, loss = 1.71 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:52:26.188442: step 152870, loss = 1.99 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:27.408140: step 152880, loss = 1.76 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:28.634842: step 152890, loss = 1.89 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:29.824692: step 152900, loss = 1.83 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:52:31.077037: step 152910, loss = 1.85 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-05 02:52:32.285325: step 152920, loss = 2.01 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:33.508544: step 152930, loss = 2.01 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:34.693712: step 152940, loss = 1.86 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:52:35.897604: step 152950, loss = 1.77 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:52:37.108548: step 152960, loss = 1.75 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:38.311768: step 152970, loss = 1.85 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:52:39.529582: step 152980, loss = 1.94 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:40.770156: step 152990, loss = 1.82 (1031.8 examples/sec; 0.124 sec/batch)
2017-05-05 02:52:41.980011: step 153000, loss = 1.86 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:43.196707: step 153010, loss = 1.93 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:44.411161: step 153020, loss = 1.92 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:45.608669: step 153030, loss = 1.94 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:52:46.835687: step 153040, loss = 1.82 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:48.061450: step 153050, loss = 1.78 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:49.236502: step 153060, loss = 1.84 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:52:50.431530: step 153070, loss = 1.89 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:52:51.648937: step 153080, loss = 1.94 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:52.882586: step 153090, loss = 1.81 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:54.084895: step 153100, loss = 2.05 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:52:55.273309: step 153110, loss = 2.02 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:52:56.497296: step 153120, loss = 1.84 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:57.677104: step 153130, loss = 1.88 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:52:58.873855: step 153140, loss = 1.79 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:53:00.052845: step 153150, loss = 2.03 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:53:01.245608: step 153160, loss = 1.92 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:02.430030: step 153170, loss = 1.95 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:53:03.611804: step 153180, loss = 1.87 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:53:04.781410: step 153190, loss = 1.87 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:05.944575: step 153200, loss = 1.96 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:07.101940: step 153210, loss = 1.88 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:08.285153: step 153220, loss = 1.80 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:53:09.459164: step 153230, loss = 1.95 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:10.646934: step 153240, loss = 1.83 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:11.813785: step 153250, loss = 1.91 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:12.970980: step 153260, loss = 1.79 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:14.137688: step 153270, loss = 1.73 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:15.330939: step 153280, loss = 1.80 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:16.479399: step 153290, loss = 1.87 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:53:17.650582: step 153300, loss = 1.90 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:18.804092: step 153310, loss = 1.95 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:53:19.966726: step 153320, loss = 1.81 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:21.157892: step 153330, loss = 2.05 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:22.329879: step 153340, loss = 1.81 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:23.496518: step 153350, loss = 1.92 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:24.662588: step 153360, loss = 1.97 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:25.810959: step 153370, loss = 1.92 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:53:26.988931: step 153380, loss = 2.00 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:53:28.147519: step 153390, loss = 1.82 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:29.332210: step 153400, loss = 1.89 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:53:30.502546: step 153410, loss = 1.82 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:31.668215: step 153420, loss = 1.90 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:32.855728: step 153430, loss = 1.78 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:34.029556: step 153440, loss = 1.92 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:35.310622: step 153450, loss = 1.89 (999.2 examples/sec; 0.128 sec/batch)
2017-05-05 02:53:36.391892: step 153460, loss = 1.91 (1183.8 examples/sec; 0.108 sec/batch)
2017-05-05 02:53:37.543266: step 153470, loss = 1.79 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:53:38.721303: step 153480, loss = 1.85 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:53:39.915494: step 153490, loss = 1.98 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:41.104669: step 153500, loss = 1.83 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:42.264490: step 153510, loss = 1.74 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:43.445666: step 153520, loss = 1.83 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:53:44.620032: step 153530, loss = 1.88 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:45.784499: step 153540, loss = 1.86 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:46.956674: step 153550, loss = 1.96 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:48.142011: step 153560, loss = 1.67 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:49.328484: step 153570, loss = 1.69 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:50.498500: step 153580, loss = 1.89 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:51.697377: step 153590, loss = 1.89 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:53:52.890062: step 153600, loss = 1.81 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:54.088161: step 153610, loss = 1.91 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:53:55.286426: step 153620, loss = 1.92 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:53:56.479894: step 153630, loss = 1.93 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:57.637656: step 153640, loss = 1.97 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:58.817815: step 153650, loss = 1.81 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:00.006613: step 153660, loss = 1.82 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:01.173838: step 153670, loss = 1.97 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:54:02.311344: step 153680, loss = 1.89 (1125.3 examples/sec; 0.114 sec/batch)
2017-05-05 02:54:03.478092: step 153690, loss = 1.79 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:54:04.670928: step 153700, loss = 1.82 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:05.820062: step 153710, loss = 1.77 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:54:06.997866: step 153720, loss = 1.87 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:08.151905: step 153730, loss = 1.71 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:54:09.310711: step 153740, loss = 1.97 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:54:10.488302: step 153750, loss = 1.80 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:11.661281: step 153760, loss = 1.94 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:54:12.812775: step 153770, loss = 1.85 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:54:13.979463: step 153780, loss = 1.98 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:54:15.165512: step 153790, loss = 1.84 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:16.337110: step 153800, loss = 1.91 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:54:17.489918: step 153810, loss = 1.79 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:54:18.656704: step 153820, loss = 1.92 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:54:19.819604: step 153830, loss = 1.91 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:54:20.972505: step 153840, loss = 1.80 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:54:22.149130: step 153850, loss = 1.98 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:23.341819: step 153860, loss = 1.76 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:24.545174: step 153870, loss = 1.95 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:54:25.732116: step 153880, loss = 2.16 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:26.924779: step 153890, loss = 1.72 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:28.123686: step 153900, loss = 1.80 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:54:29.319197: step 153910, loss = 1.76 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:54:30.485129: step 153920, loss = 1.92 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:54:31.673885: step 153930, loss = 2.00 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:32.864971: step 153940, loss = 1.99 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:34.015767: step 153950, loss = 1.78 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:54:35.192462: step 153960, loss = 2.18 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:36.384933: step 153970, loss = 1.80 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:37.557557: step 153980, loss = 2.01 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:54:38.749747: step 153990, loss = 1.73 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:39.926968: step 154000, loss = 2.17 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:41.114971: step 154010, loss = 1.91 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:42.258937: step 154020, loss = 1.96 (1118.9 examples/sec; 0.114 sec/batch)
2017-05-05 02:54:43.415353: step 154030, loss = 1.94 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:54:44.574714: step 154040, loss = 1.80 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:54:45.731206: step 154050, loss = 1.80 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:54:46.908431: step 154060, loss = 1.85 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:48.099031: step 154070, loss = 1.94 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:49.285091: step 154080, loss = 1.83 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:50.468584: step 154090, loss = 1.92 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:51.677793: step 154100, loss = 1.91 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:54:52.851380: step 154110, loss = 1.84 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:54:54.030478: step 154120, loss = 1.95 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:55.210279: step 154130, loss = 1.83 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:56.432010: step 154140, loss = 1.97 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:54:57.650332: step 154150, loss = 1.89 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:54:58.849926: step 154160, loss = 1.85 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:55:00.045569: step 154170, loss = 1.75 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:55:01.278080: step 154180, loss = 1.87 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 02:55:02.479388: step 154190, loss = 1.82 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:55:03.688375: step 154200, loss = 1.87 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:55:04.888376: step 154210, loss = 1.92 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:55:06.073931: step 154220, loss = 1.80 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:07.263300: step 154230, loss = 1.75 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:08.457557: step 154240, loss = 1.84 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:09.642398: step 154250, loss = 1.97 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:10.849814: step 154260, loss = 2.04 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:55:12.034979: step 154270, loss = 1.87 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:13.215498: step 154280, loss = 1.86 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:14.381459: step 154290, loss = 1.89 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:55:15.568969: step 154300, loss = 1.84 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:16.752596: step 154310, loss = 1.85 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:17.958436: step 154320, loss = 1.79 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:55:19.163892: step 154330, loss = 1.75 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:55:20.383126: step 154340, loss = 1.72 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:55:21.566543: step 154350, loss = 1.94 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:22.771718: step 154360, loss = 1.82 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:55:23.970352: step 154370, loss = 1.95 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:55:25.161134: step 154380, loss = 1.85 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:26.332923: step 154390, loss = 1.78 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:55:27.520063: step 154400, loss = 1.88 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:28.712276: step 154410, loss = 1.87 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:29.908937: step 154420, loss = 1.89 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:55:31.084243: step 154430, loss = 1.80 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:32.371306: step 154440, loss = 1.97 (994.5 examples/sec; 0.129 sec/batch)
2017-05-05 02:55:33.446140: step 154450, loss = 1.97 (1190.9 examples/sec; 0.107 sec/batch)
2017-05-05 02:55:34.630341: step 154460, loss = 1.75 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:35.824964: step 154470, loss = 1.96 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:37.004876: step 154480, loss = 1.86 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:38.163671: step 154490, loss = 1.90 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:55:39.334549: step 154500, loss = 1.86 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:55:40.507059: step 154510, loss = 1.88 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:55:41.650634: step 154520, loss = 1.78 (1119.3 examples/sec; 0.114 sec/batch)
2017-05-05 02:55:42.829457: step 154530, loss = 1.93 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:43.996511: step 154540, loss = 1.93 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:55:45.172883: step 154550, loss = 1.92 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:46.331316: step 154560, loss = 1.89 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:55:47.519510: step 154570, loss = 1.91 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:48.708204: step 154580, loss = 1.88 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:49.886744: step 154590, loss = 1.85 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:51.104813: step 154600, loss = 1.89 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:55:52.310539: step 154610, loss = 1.97 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:55:53.486751: step 154620, loss = 1.86 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:54.695018: step 154630, loss = 2.05 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:55:55.974216: step 154640, loss = 1.82 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-05 02:55:57.113073: step 154650, loss = 1.83 (1123.9 examples/sec; 0.114 sec/batch)
2017-05-05 02:55:58.338087: step 154660, loss = 2.02 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:55:59.538736: step 154670, loss = 1.87 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:56:00.750714: step 154680, loss = 1.78 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:56:01.964975: step 154690, loss = 1.98 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:56:03.172337: step 154700, loss = 1.84 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:56:04.392538: step 154710, loss = 1.85 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:56:05.584846: step 154720, loss = 1.95 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:56:06.778555: step 154730, loss = 1.78 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:56:07.958845: step 154740, loss = 1.89 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:56:09.138746: step 154750, loss = 1.82 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:56:10.350538: step 154760, loss = 1.97 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:56:11.549962: step 154770, loss = 1.77 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:56:12.755650: step 154780, loss = 1.79 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:56:13.965491: step 154790, loss = 1.86 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:56:15.172345: step 154800, loss = 2.02 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:56:16.371389: step 154810, loss = 2.02 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:56:17.552727: step 154820, loss = 1.81 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:56:18.759870: step 154830, loss = 1.87 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:56:19.955066: step 154840, loss = 1.85 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:56:21.128207: step 154850, loss = 1.86 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:22.292278: step 154860, loss = 1.88 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:23.498153: step 154870, loss = 1.77 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:56:24.700266: step 154880, loss = 1.86 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:56:25.882832: step 154890, loss = 1.77 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:56:27.053297: step 154900, loss = 2.01 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:28.232856: step 154910, loss = 1.94 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:56:29.406263: step 154920, loss = 1.92 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:30.555642: step 154930, loss = 2.03 (1113.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:56:31.716399: step 154940, loss = 2.09 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:32.889922: step 154950, loss = 1.80 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:34.040289: step 154960, loss = 2.05 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:56:35.204717: step 154970, loss = 1.81 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:36.373447: step 154980, loss = 1.86 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:37.557094: step 154990, loss = 1.92 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:56:38.724773: step 155000, loss = 1.72 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:39.894921: step 155010, loss = 1.94 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:41.064309: step 155020, loss = 1.82 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:42.218467: step 155030, loss = 2.06 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:56:43.380787: step 155040, loss = 1.87 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:44.566839: step 155050, loss = 1.78 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:56:45.698610: step 155060, loss = 1.79 (1131.0 examples/sec; 0.113 sec/batch)
2017-05-05 02:56:46.896647: step 155070, loss = 1.93 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:56:48.054288: step 155080, loss = 1.86 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:49.215081: step 155090, loss = 1.93 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:50.366353: step 155100, loss = 2.01 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:56:51.532906: step 155110, loss = 1.73 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:52.702131: step 155120, loss = 1.79 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:53.859390: step 155130, loss = 1.80 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:55.033105: step 155140, loss = 1.81 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:56.207647: step 155150, loss = 2.03 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:57.392165: step 155160, loss = 1.88 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:56:58.549594: step 155170, loss = 1.79 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:59.700756: step 155180, loss = 1.87 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:57:00.877307: step 155190, loss = 1.90 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:02.039537: step 155200, loss = 1.77 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:57:03.206396: step 155210, loss = 1.79 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:57:04.379465: step 155220, loss = 1.99 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:57:05.542890: step 155230, loss = 1.78 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:57:06.696123: step 155240, loss = 1.81 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:57:07.888907: step 155250, loss = 1.88 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:09.085317: step 155260, loss = 2.19 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:57:10.263627: step 155270, loss = 1.82 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:11.451293: step 155280, loss = 1.94 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:12.636225: step 155290, loss = 1.76 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:13.826694: step 155300, loss = 1.88 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:15.011450: step 155310, loss = 1.81 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:16.194446: step 155320, loss = 1.81 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:17.363114: step 155330, loss = 1.75 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:57:18.529164: step 155340, loss = 1.73 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:57:19.715351: step 155350, loss = 1.82 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:20.893767: step 155360, loss = 1.85 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:22.053336: step 155370, loss = 1.85 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:57:23.206272: step 155380, loss = 1.82 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:57:24.392559: step 155390, loss = 1.85 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:25.546281: step 155400, loss = 1.89 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:57:26.704195: step 155410, loss = 1.82 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:57:27.888929: step 155420, loss = 1.85 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:29.173476: step 155430, loss = 1.97 (996.5 examples/sec; 0.128 sec/batch)
2017-05-05 02:57:30.255050: step 155440, loss = 1.93 (1183.5 examples/sec; 0.108 sec/batch)
2017-05-05 02:57:31.433153: step 155450, loss = 1.85 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:32.628414: step 155460, loss = 1.81 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:57:33.803078: step 155470, loss = 2.07 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:57:34.966605: step 155480, loss = 1.72 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:57:36.127813: step 155490, loss = 1.82 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:57:37.308368: step 155500, loss = 1.88 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:38.486670: step 155510, loss = 1.87 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:39.670639: step 155520, loss = 1.91 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:40.852368: step 155530, loss = 1.89 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:42.031659: step 155540, loss = 1.83 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:43.225110: step 155550, loss = 1.96 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:44.426671: step 155560, loss = 1.79 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:57:45.604959: step 155570, loss = 1.82 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:46.797291: step 155580, loss = 1.72 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:47.977606: step 155590, loss = 1.91 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:49.139839: step 155600, loss = 1.82 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:57:50.290106: step 155610, loss = 1.94 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:57:51.465598: step 155620, loss = 1.87 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:52.655714: step 155630, loss = 1.82 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:53.841068: step 155640, loss = 1.75 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:55.045371: step 155650, loss = 1.90 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:57:56.253208: step 155660, loss = 1.79 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:57:57.449870: step 155670, loss = 1.99 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:57:58.651576: step 155680, loss = 1.80 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:57:59.841975: step 155690, loss = 1.73 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:58:01.016036: step 155700, loss = 2.01 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:58:02.235611: step 155710, loss = 1.93 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:58:03.410597: step 155720, loss = 1.88 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:58:04.623553: step 155730, loss = 2.02 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:58:05.827420: step 155740, loss = 1.94 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:58:07.025931: step 155750, loss = 1.82 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:58:08.230041: step 155760, loss = 1.82 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:58:09.397814: step 155770, loss = 1.89 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:58:10.564540: step 155780, loss = 1.86 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:58:11.726339: step 155790, loss = 1.75 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:58:12.890405: step 155800, loss = 2.07 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:58:14.044351: step 155810, loss = 1.85 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:58:15.227369: step 155820, loss = 1.93 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:16.390617: step 155830, loss = 1.95 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:58:17.562137: step 155840, loss = 1.93 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:58:18.723272: step 155850, loss = 2.10 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:58:19.899701: step 155860, loss = 1.94 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:21.046609: step 155870, loss = 1.87 (1116.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:58:22.202187: step 155880, loss = 1.84 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:58:23.354577: step 155890, loss = 1.77 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:58:24.529499: step 155900, loss = 1.92 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:58:25.681943: step 155910, loss = 1.96 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:58:26.854873: step 155920, loss = 1.88 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:58:28.035902: step 155930, loss = 1.96 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:29.192801: step 155940, loss = 1.99 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:58:30.338433: step 155950, loss = 1.85 (1117.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:58:31.502901: step 155960, loss = 1.92 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:58:32.657630: step 155970, loss = 1.81 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:58:33.816546: step 155980, loss = 2.07 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:58:34.979709: step 155990, loss = 1.78 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:58:36.165505: step 156000, loss = 1.96 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:58:37.339643: step 156010, loss = 1.76 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:58:38.516457: step 156020, loss = 1.82 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:39.708855: step 156030, loss = 1.81 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:58:40.901041: step 156040, loss = 1.98 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:58:42.064199: step 156050, loss = 2.05 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:58:43.240681: step 156060, loss = 1.82 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:44.426295: step 156070, loss = 1.98 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:58:45.601472: step 156080, loss = 1.74 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:46.799120: step 156090, loss = 1.80 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:58:47.996520: step 156100, loss = 2.00 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:58:49.175784: step 156110, loss = 1.85 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:50.369687: step 156120, loss = 1.80 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:58:51.561687: step 156130, loss = 1.97 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:58:52.757218: step 156140, loss = 1.92 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:58:53.942267: step 156150, loss = 1.93 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:58:55.157537: step 156160, loss = 2.02 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:58:56.326826: step 156170, loss = 1.82 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:58:57.483431: step 156180, loss = 1.80 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:58:58.664930: step 156190, loss = 1.79 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:59.830306: step 156200, loss = 1.89 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:01.001974: step 156210, loss = 2.01 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:02.151435: step 156220, loss = 1.98 (1113.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:59:03.320526: step 156230, loss = 1.84 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:04.503360: step 156240, loss = 1.90 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:59:05.642445: step 156250, loss = 1.75 (1123.7 examples/sec; 0.114 sec/batch)
2017-05-05 02:59:06.806431: step 156260, loss = 1.74 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:07.982465: step 156270, loss = 1.93 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:59:09.134025: step 156280, loss = 1.99 (1111.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:59:10.288439: step 156290, loss = 1.79 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:59:11.458794: step 156300, loss = 1.77 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:12.622055: step 156310, loss = 1.83 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:13.778331: step 156320, loss = 1.76 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:14.938836: step 156330, loss = 1.80 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:16.099153: step 156340, loss = 1.78 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:17.269495: step 156350, loss = 1.82 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:18.423734: step 156360, loss = 1.86 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:59:19.594031: step 156370, loss = 1.82 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:20.762442: step 156380, loss = 1.90 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:21.916441: step 156390, loss = 1.94 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:59:23.086747: step 156400, loss = 1.95 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:24.275553: step 156410, loss = 1.85 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:59:25.546824: step 156420, loss = 1.92 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-05 02:59:26.635265: step 156430, loss = 1.93 (1176.0 examples/sec; 0.109 sec/batch)
2017-05-05 02:59:27.819323: step 156440, loss = 1.79 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:59:28.975113: step 156450, loss = 1.97 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:30.151480: step 156460, loss = 1.95 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:59:31.314809: step 156470, loss = 1.81 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:32.458437: step 156480, loss = 2.01 (1119.2 examples/sec; 0.114 sec/batch)
2017-05-05 02:59:33.618028: step 156490, loss = 1.74 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:34.793784: step 156500, loss = 1.93 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:59:35.975032: step 156510, loss = 1.85 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:59:37.156188: step 156520, loss = 1.95 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:59:38.315704: step 156530, loss = 1.95 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:39.489822: step 156540, loss = 1.95 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:40.645190: step 156550, loss = 1.86 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:41.793643: step 156560, loss = 1.93 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:59:42.982946: step 156570, loss = 1.96 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:59:44.155325: step 156580, loss = 1.74 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:45.318397: step 156590, loss = 1.78 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:46.485003: step 156600, loss = 1.95 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:47.641020: step 156610, loss = 1.94 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:48.840912: step 156620, loss = 1.94 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:59:49.991009: step 156630, loss = 1.89 (1112.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:59:51.151729: step 156640, loss = 1.94 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:52.319815: step 156650, loss = 1.81 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:53.482567: step 156660, loss = 1.74 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:54.640083: step 156670, loss = 1.79 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:55.821326: step 156680, loss = 1.84 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:59:56.992560: step 156690, loss = 1.80 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:58.140600: step 156700, loss = 1.73 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:59:59.319206: step 156710, loss = 1.90 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:00:00.475568: step 156720, loss = 1.76 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:00:01.633546: step 156730, loss = 1.93 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:00:02.795416: step 156740, loss = 1.95 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:00:03.956978: step 156750, loss = 1.92 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:00:05.145945: step 156760, loss = 1.83 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:06.306196: step 156770, loss = 1.92 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:00:07.489293: step 156780, loss = 1.88 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:00:08.679970: step 156790, loss = 1.85 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:09.847752: step 156800, loss = 1.81 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:00:11.046603: step 156810, loss = 1.86 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:12.225661: step 156820, loss = 1.80 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:00:13.416751: step 156830, loss = 1.99 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:14.619136: step 156840, loss = 1.84 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:15.835722: step 156850, loss = 1.77 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:00:17.052073: step 156860, loss = 1.80 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:00:18.250654: step 156870, loss = 2.02 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:19.439088: step 156880, loss = 1.87 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:20.639404: step 156890, loss = 1.93 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:21.828775: step 156900, loss = 1.89 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:23.030095: step 156910, loss = 1.97 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:24.208263: step 156920, loss = 1.83 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:00:25.403797: step 156930, loss = 1.83 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:26.589602: step 156940, loss = 1.74 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:27.776859: step 156950, loss = 2.03 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:28.987289: step 156960, loss = 1.87 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:30.194126: step 156970, loss = 1.89 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:31.414669: step 156980, loss = 1.83 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:00:32.632915: step 156990, loss = 1.96 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:00:33.816962: step 157000, loss = 1.75 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:00:35.055687: step 157010, loss = 1.94 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-05 03:00:36.273681: step 157020, loss = 1.77 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:00:37.476221: step 157030, loss = 1.87 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:38.682928: step 157040, loss = 1.80 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:39.904952: step 157050, loss = 1.92 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:00:41.116115: step 157060, loss = 2.01 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:42.317305: step 157070, loss = 1.87 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:43.533853: step 157080, loss = 1.76 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:00:44.742488: step 157090, loss = 1.78 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:45.933485: step 157100, loss = 1.86 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:47.151270: step 157110, loss = 1.87 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:00:48.379391: step 157120, loss = 1.97 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-05 03:00:49.593679: step 157130, loss = 1.99 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:50.800476: step 157140, loss = 2.03 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:51.987292: step 157150, loss = 1.83 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:53.195468: step 157160, loss = 1.89 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:54.384458: step 157170, loss = 1.92 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:55.578096: step 157180, loss = 1.93 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:56.775706: step 157190, loss = 1.76 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:57.958414: step 157200, loss = 1.87 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:00:59.141050: step 157210, loss = 1.78 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:00.328863: step 157220, loss = 1.91 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:01:01.511315: step 157230, loss = 1.94 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:02.677937: step 157240, loss = 1.90 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:03.868308: step 157250, loss = 1.77 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:01:05.028473: step 157260, loss = 1.93 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:06.186783: step 157270, loss = 1.87 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:07.355498: step 157280, loss = 1.94 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:08.503965: step 157290, loss = 1.84 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:01:09.675699: step 157300, loss = 1.88 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:10.837209: step 157310, loss = 1.91 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:12.002016: step 157320, loss = 1.96 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:13.168271: step 157330, loss = 1.86 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:14.321459: step 157340, loss = 1.84 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:01:15.506473: step 157350, loss = 1.80 (1080.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:01:16.685368: step 157360, loss = 1.76 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:17.855671: step 157370, loss = 1.89 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:19.014836: step 157380, loss = 1.89 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:20.187102: step 157390, loss = 1.96 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:21.358996: step 157400, loss = 1.79 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:22.612559: step 157410, loss = 1.91 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-05 03:01:23.679870: step 157420, loss = 1.80 (1199.3 examples/sec; 0.107 sec/batch)
2017-05-05 03:01:24.837079: step 157430, loss = 1.95 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:25.991127: step 157440, loss = 1.93 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:01:27.172005: step 157450, loss = 1.93 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:28.363725: step 157460, loss = 1.76 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:01:29.531768: step 157470, loss = 1.78 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:30.682844: step 157480, loss = 1.78 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:01:31.857449: step 157490, loss = 1.93 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:33.045588: step 157500, loss = 1.97 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:01:34.194466: step 157510, loss = 1.74 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:01:35.349177: step 157520, loss = 1.78 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:01:36.526184: step 157530, loss = 1.98 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:37.681869: step 157540, loss = 1.89 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:38.856006: step 157550, loss = 1.71 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:40.011527: step 157560, loss = 1.89 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:41.174805: step 157570, loss = 1.89 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:42.344721: step 157580, loss = 1.85 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:43.513848: step 157590, loss = 1.95 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:44.696259: step 157600, loss = 1.80 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:45.842122: step 157610, loss = 1.70 (1117.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:01:47.022964: step 157620, loss = 2.00 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:48.202333: step 157630, loss = 2.01 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:49.362827: step 157640, loss = 1.87 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:50.514012: step 157650, loss = 1.90 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:01:51.695020: step 157660, loss = 2.01 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:52.853575: step 157670, loss = 1.92 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:53.994126: step 157680, loss = 1.99 (1122.3 examples/sec; 0.114 sec/batch)
2017-05-05 03:01:55.175918: step 157690, loss = 2.09 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:56.344047: step 157700, loss = 1.91 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:57.512217: step 157710, loss = 1.81 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:58.703290: step 157720, loss = 1.86 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:01:59.888922: step 157730, loss = 2.01 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:01.075898: step 157740, loss = 1.87 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:02.263515: step 157750, loss = 1.94 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:03.447854: step 157760, loss = 1.79 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:02:04.620662: step 157770, loss = 1.85 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:02:05.812418: step 157780, loss = 1.76 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:07.024729: step 157790, loss = 1.92 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:02:08.234633: step 157800, loss = 1.86 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:02:09.429497: step 157810, loss = 1.90 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:10.626508: step 157820, loss = 1.77 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:02:11.832768: step 157830, loss = 2.03 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:02:13.043436: step 157840, loss = 1.93 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:02:14.232184: step 157850, loss = 1.92 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:15.433001: step 157860, loss = 1.87 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:02:16.655570: step 157870, loss = 1.73 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:02:17.848162: step 157880, loss = 1.90 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:19.068278: step 157890, loss = 1.86 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:02:20.271125: step 157900, loss = 1.86 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:02:21.456589: step 157910, loss = 1.92 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:22.667114: step 157920, loss = 1.88 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:02:23.873611: step 157930, loss = 1.88 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:02:25.067840: step 157940, loss = 1.94 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:26.271475: step 157950, loss = 1.94 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:02:27.446320: step 157960, loss = 2.04 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:02:28.646393: step 157970, loss = 1.88 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:02:29.814694: step 157980, loss = 1.87 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:02:30.994693: step 157990, loss = 1.74 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:02:32.170416: step 158000, loss = 1.88 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:02:33.359934: step 158010, loss = 1.90 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:34.526969: step 158020, loss = 1.84 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:02:35.690203: step 158030, loss = 1.80 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:02:36.857549: step 158040, loss = 1.84 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:02:38.024790: step 158050, loss = 1.96 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:02:39.202741: step 158060, loss = 1.97 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:02:40.356972: step 158070, loss = 1.84 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:02:41.510608: step 158080, loss = 1.77 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:02:42.676804: step 158090, loss = 1.91 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:02:43.850794: step 158100, loss = 1.82 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:02:45.011464: step 158110, loss = 1.84 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:02:46.173833: step 158120, loss = 1.90 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:02:47.360348: step 158130, loss = 1.89 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:48.519904: step 158140, loss = 1.82 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:02:49.683397: step 158150, loss = 1.80 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:02:50.832631: step 158160, loss = 1.84 (1113.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:02:52.035736: step 158170, loss = 1.84 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:02:53.182728: step 158180, loss = 1.81 (1116.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:02:54.327103: step 158190, loss = 1.89 (1118.5 examples/sec; 0.114 sec/batch)
2017-05-05 03:02:55.493420: step 158200, loss = 2.06 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:02:56.652860: step 158210, loss = 1.97 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:02:57.808547: step 158220, loss = 1.83 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:02:58.977625: step 158230, loss = 1.81 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:03:00.149733: step 158240, loss = 1.90 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:03:01.302965: step 158250, loss = 1.98 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:03:02.450619: step 158260, loss = 1.80 (1115.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:03:03.623726: step 158270, loss = 1.72 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:03:04.798829: step 158280, loss = 1.87 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:05.969256: step 158290, loss = 2.07 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:03:07.153107: step 158300, loss = 1.94 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:08.326327: step 158310, loss = 1.98 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:03:09.478847: step 158320, loss = 2.00 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:03:10.642338: step 158330, loss = 1.89 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:03:11.820192: step 158340, loss = 1.85 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:13.000462: step 158350, loss = 1.96 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:14.159125: step 158360, loss = 1.80 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:03:15.326775: step 158370, loss = 1.87 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:03:16.504881: step 158380, loss = 1.81 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:17.643404: step 158390, loss = 1.97 (1124.3 examples/sec; 0.114 sec/batch)
2017-05-05 03:03:18.913958: step 158400, loss = 1.74 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-05 03:03:20.008891: step 158410, loss = 1.77 (1169.0 examples/sec; 0.109 sec/batch)
2017-05-05 03:03:21.168009: step 158420, loss = 1.94 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:03:22.341753: step 158430, loss = 1.83 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:03:23.496327: step 158440, loss = 1.99 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:03:24.658306: step 158450, loss = 2.03 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:03:25.826537: step 158460, loss = 1.85 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:03:26.995780: step 158470, loss = 1.84 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:03:28.192855: step 158480, loss = 1.90 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:03:29.372526: step 158490, loss = 1.81 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:30.558764: step 158500, loss = 1.94 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:31.743902: step 158510, loss = 1.81 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:32.937998: step 158520, loss = 1.88 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:34.116396: step 158530, loss = 1.79 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:35.319598: step 158540, loss = 1.78 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:03:36.516170: step 158550, loss = 1.92 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:03:37.709626: step 158560, loss = 1.80 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:38.906263: step 158570, loss = 2.01 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:03:40.098362: step 158580, loss = 1.94 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:41.298181: step 158590, loss = 1.84 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:03:42.537314: step 158600, loss = 1.98 (1033.0 examples/sec; 0.124 sec/batch)
2017-05-05 03:03:43.671722: step 158610, loss = 1.89 (1128.3 examples/sec; 0.113 sec/batch)
2017-05-05 03:03:44.847796: step 158620, loss = 1.92 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:46.029356: step 158630, loss = 2.01 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:47.207400: step 158640, loss = 1.85 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:48.403444: step 158650, loss = 2.05 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:03:49.591064: step 158660, loss = 1.94 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:50.797738: step 158670, loss = 1.84 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:03:51.995446: step 158680, loss = 1.82 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:03:53.194143: step 158690, loss = 1.83 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:03:54.382808: step 158700, loss = 1.90 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:55.586562: step 158710, loss = 1.79 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:03:56.775663: step 158720, loss = 2.10 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:57.958033: step 158730, loss = 1.80 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:59.138456: step 158740, loss = 1.86 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:00.321969: step 158750, loss = 1.73 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:01.478157: step 158760, loss = 1.75 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:02.656040: step 158770, loss = 1.85 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:03.825251: step 158780, loss = 1.87 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:05.002986: step 158790, loss = 1.85 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:06.172060: step 158800, loss = 1.76 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:07.339744: step 158810, loss = 1.74 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:08.521613: step 158820, loss = 1.87 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:09.690940: step 158830, loss = 1.77 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:10.888028: step 158840, loss = 2.00 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:04:12.083423: step 158850, loss = 1.96 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:04:13.260676: step 158860, loss = 1.93 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:14.428968: step 158870, loss = 1.84 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:15.602472: step 158880, loss = 1.92 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:16.788217: step 158890, loss = 1.73 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:04:17.945879: step 158900, loss = 1.75 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:19.117129: step 158910, loss = 1.79 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:20.282751: step 158920, loss = 1.84 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:21.451107: step 158930, loss = 1.82 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:22.608670: step 158940, loss = 1.90 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:23.794650: step 158950, loss = 1.94 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:04:24.958445: step 158960, loss = 1.91 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:26.104829: step 158970, loss = 2.05 (1116.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:04:27.280303: step 158980, loss = 1.81 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:28.446361: step 158990, loss = 1.93 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:29.625210: step 159000, loss = 1.88 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:30.801022: step 159010, loss = 2.03 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:31.974489: step 159020, loss = 1.76 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:33.126026: step 159030, loss = 1.92 (1111.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:04:34.272759: step 159040, loss = 1.82 (1116.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:04:35.431608: step 159050, loss = 1.89 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:36.578425: step 159060, loss = 1.81 (1116.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:04:37.726627: step 159070, loss = 1.88 (1114.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:04:38.894459: step 159080, loss = 1.89 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:40.052790: step 159090, loss = 1.83 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:41.214428: step 159100, loss = 2.01 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:42.375873: step 159110, loss = 1.96 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:43.557381: step 159120, loss = 1.80 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:44.707649: step 159130, loss = 1.94 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:04:45.881149: step 159140, loss = 1.88 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:47.039221: step 159150, loss = 1.90 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:48.203647: step 159160, loss = 1.90 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:49.364393: step 159170, loss = 1.85 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:50.534297: step 159180, loss = 1.80 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:51.720298: step 159190, loss = 2.01 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:04:52.916929: step 159200, loss = 1.80 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:04:54.095175: step 159210, loss = 1.83 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:55.274667: step 159220, loss = 1.64 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:56.433320: step 159230, loss = 1.72 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:57.584205: step 159240, loss = 1.70 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:04:58.728305: step 159250, loss = 1.83 (1118.8 examples/sec; 0.114 sec/batch)
2017-05-05 03:04:59.896385: step 159260, loss = 1.88 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:01.045280: step 159270, loss = 1.85 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:05:02.214722: step 159280, loss = 1.85 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:03.383978: step 159290, loss = 1.91 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:04.570730: step 159300, loss = 1.88 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:05:05.720177: step 159310, loss = 1.88 (1113.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:05:06.878139: step 159320, loss = 1.88 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:08.048255: step 159330, loss = 1.79 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:09.230875: step 159340, loss = 2.02 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:05:10.402943: step 159350, loss = 1.76 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:11.591371: step 159360, loss = 1.83 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:05:12.774482: step 159370, loss = 1.77 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:05:13.940508: step 159380, loss = 2.00 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:15.242676: step 159390, loss = 1.81 (983.0 examples/sec; 0.130 sec/batch)
2017-05-05 03:05:16.327772: step 159400, loss = 1.88 (1179.6 examples/sec; 0.109 sec/batch)
2017-05-05 03:05:17.510972: step 159410, loss = 1.78 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:05:18.681706: step 159420, loss = 1.88 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:19.874306: step 159430, loss = 2.04 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:05:21.040208: step 159440, loss = 1.91 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:22.207969: step 159450, loss = 1.72 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:23.390356: step 159460, loss = 1.83 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:05:24.567615: step 159470, loss = 2.04 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:05:25.720856: step 159480, loss = 1.93 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:05:26.890754: step 159490, loss = 1.81 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:28.056072: step 159500, loss = 1.87 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:29.221211: step 159510, loss = 1.90 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:30.381010: step 159520, loss = 1.84 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:31.558319: step 159530, loss = 1.88 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:05:32.725331: step 159540, loss = 1.88 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:33.882275: step 159550, loss = 1.77 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:35.058775: step 159560, loss = 1.83 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:05:36.222313: step 159570, loss = 1.70 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:37.383967: step 159580, loss = 1.78 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:38.535718: step 159590, loss = 1.95 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:05:39.721808: step 159600, loss = 1.78 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:05:40.901559: step 159610, loss = 1.72 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:05:42.066336: step 159620, loss = 1.97 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:43.253728: step 159630, loss = 1.88 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:05:44.444547: step 159640, loss = 1.90 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:05:45.633810: step 159650, loss = 1.77 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:05:46.844409: step 159660, loss = 1.82 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:05:48.054271: step 159670, loss = 1.89 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:05:49.287586: step 159680, loss = 1.86 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:05:50.480990: step 159690, loss = 1.86 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:05:51.703738: step 159700, loss = 1.80 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:05:52.927995: step 159710, loss = 1.93 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:05:54.145854: step 159720, loss = 1.94 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:05:55.369785: step 159730, loss = 1.89 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:05:56.600569: step 159740, loss = 1.87 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:05:57.809021: step 159750, loss = 1.90 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:05:59.034563: step 159760, loss = 1.67 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-05 03:06:00.246886: step 159770, loss = 1.82 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:06:01.442029: step 159780, loss = 1.98 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:02.644530: step 159790, loss = 1.94 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:03.873592: step 159800, loss = 1.94 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-05 03:06:05.090967: step 159810, loss = 1.67 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:06:06.294321: step 159820, loss = 1.89 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:07.492508: step 159830, loss = 1.77 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:08.700159: step 159840, loss = 1.91 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:06:09.877694: step 159850, loss = 1.85 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:06:11.079386: step 159860, loss = 1.92 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:12.280018: step 159870, loss = 1.71 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:13.469913: step 159880, loss = 1.87 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:06:14.670171: step 159890, loss = 1.90 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:15.869807: step 159900, loss = 1.95 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:17.076918: step 159910, loss = 1.99 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:06:18.281345: step 159920, loss = 1.85 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:19.502052: step 159930, loss = 1.79 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:06:20.729282: step 159940, loss = 1.85 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:06:21.927898: step 159950, loss = 1.85 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:23.148696: step 159960, loss = 1.82 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:06:24.380519: step 159970, loss = 1.90 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-05 03:06:25.565621: step 159980, loss = 1.85 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:06:26.754603: step 159990, loss = 1.83 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:06:27.937694: step 160000, loss = 1.90 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:06:29.118456: step 160010, loss = 1.77 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:06:30.283594: step 160020, loss = 1.75 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:06:31.446600: step 160030, loss = 1.81 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:06:32.616163: step 160040, loss = 1.87 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:06:33.751527: step 160050, loss = 2.01 (1127.4 examples/sec; 0.114 sec/batch)
2017-05-05 03:06:34.922752: step 160060, loss = 1.97 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:06:36.081896: step 160070, loss = 1.80 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:06:37.250765: step 160080, loss = 1.86 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:06:38.400497: step 160090, loss = 1.89 (1113.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:06:39.562190: step 160100, loss = 1.76 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:06:40.736706: step 160110, loss = 1.93 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:06:41.906640: step 160120, loss = 1.90 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:06:43.081328: step 160130, loss = 1.96 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:06:44.232481: step 160140, loss = 1.89 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:06:45.405889: step 160150, loss = 1.84 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:06:46.559002: step 160160, loss = 2.04 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:06:47.739466: step 160170, loss = 1.78 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:06:48.901319: step 160180, loss = 1.78 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:06:50.087783: step 160190, loss = 1.91 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:06:51.280376: step 160200, loss = 2.00 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:06:52.466148: step 160210, loss = 1.91 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:06:53.612700: step 160220, loss = 1.95 (1116.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:06:54.788180: step 160230, loss = 1.92 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:06:55.968009: step 160240, loss = 1.87 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:06:57.150558: step 160250, loss = 1.91 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:06:58.311394: step 160260, loss = 1.95 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:06:59.484200: step 160270, loss = 1.94 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:00.648425: step 160280, loss = 1.83 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:01.809938: step 160290, loss = 1.91 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:02.966499: step 160300, loss = 1.83 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:04.134681: step 160310, loss = 1.76 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:05.303156: step 160320, loss = 2.07 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:06.453964: step 160330, loss = 1.89 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:07:07.635195: step 160340, loss = 1.77 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:08.797643: step 160350, loss = 1.82 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:09.956588: step 160360, loss = 1.90 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:11.143541: step 160370, loss = 2.10 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:07:12.425627: step 160380, loss = 2.03 (998.4 examples/sec; 0.128 sec/batch)
2017-05-05 03:07:13.486358: step 160390, loss = 1.92 (1206.7 examples/sec; 0.106 sec/batch)
2017-05-05 03:07:14.656471: step 160400, loss = 1.80 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:15.810897: step 160410, loss = 1.78 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:07:16.988641: step 160420, loss = 1.87 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:18.135721: step 160430, loss = 1.86 (1115.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:07:19.306995: step 160440, loss = 1.81 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:20.489709: step 160450, loss = 1.81 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:21.641807: step 160460, loss = 1.83 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:07:22.810521: step 160470, loss = 2.02 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:23.979702: step 160480, loss = 1.85 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:25.134437: step 160490, loss = 1.86 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:07:26.300753: step 160500, loss = 1.66 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:27.477193: step 160510, loss = 1.89 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:28.642727: step 160520, loss = 1.90 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:29.790783: step 160530, loss = 1.74 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:07:30.957297: step 160540, loss = 1.79 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:32.135501: step 160550, loss = 1.93 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:33.275740: step 160560, loss = 1.88 (1122.6 examples/sec; 0.114 sec/batch)
2017-05-05 03:07:34.423343: step 160570, loss = 1.80 (1115.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:07:35.603109: step 160580, loss = 1.91 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:36.747586: step 160590, loss = 1.99 (1118.4 examples/sec; 0.114 sec/batch)
2017-05-05 03:07:37.904418: step 160600, loss = 1.94 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:39.086104: step 160610, loss = 1.89 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:40.252362: step 160620, loss = 1.97 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:41.425305: step 160630, loss = 2.10 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:42.599042: step 160640, loss = 1.84 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:43.760439: step 160650, loss = 1.73 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:44.940336: step 160660, loss = 1.81 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:46.081563: step 160670, loss = 1.92 (1121.6 examples/sec; 0.114 sec/batch)
2017-05-05 03:07:47.247402: step 160680, loss = 1.96 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:48.415371: step 160690, loss = 1.75 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:49.568208: step 160700, loss = 1.83 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:07:50.735972: step 160710, loss = 1.91 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:51.895124: step 160720, loss = 2.07 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:53.068624: step 160730, loss = 1.76 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:54.234505: step 160740, loss = 1.83 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:55.401526: step 160750, loss = 1.80 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:56.581620: step 160760, loss = 1.90 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:57.766149: step 160770, loss = 1.97 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:58.958581: step 160780, loss = 1.83 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:08:00.141966: step 160790, loss = 1.85 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:08:01.327824: step 160800, loss = 1.99 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:08:02.504222: step 160810, loss = 1.96 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:08:03.707141: step 160820, loss = 1.87 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:08:04.891403: step 160830, loss = 2.01 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:08:06.069780: step 160840, loss = 1.83 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:08:07.271872: step 160850, loss = 1.87 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:08:08.453518: step 160860, loss = 1.90 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:08:09.649931: step 160870, loss = 1.98 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:08:10.841476: step 160880, loss = 1.77 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:08:12.034773: step 160890, loss = 1.95 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:08:13.218477: step 160900, loss = 1.75 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:08:14.442031: step 160910, loss = 1.68 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:15.648882: step 160920, loss = 1.80 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:16.853625: step 160930, loss = 1.94 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:08:18.064815: step 160940, loss = 1.87 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:19.295829: step 160950, loss = 1.82 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 03:08:20.524656: step 160960, loss = 1.83 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:08:21.748685: step 160970, loss = 1.83 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:22.926162: step 160980, loss = 1.84 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:08:24.136881: step 160990, loss = 1.93 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:25.349982: step 161000, loss = 1.92 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:26.580151: step 161010, loss = 1.91 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:08:27.795090: step 161020, loss = 1.81 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:29.009529: step 161030, loss = 1.90 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:30.221737: step 161040, loss = 2.01 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:31.439006: step 161050, loss = 1.82 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:32.649614: step 161060, loss = 1.98 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:33.869730: step 161070, loss = 1.77 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:35.085911: step 161080, loss = 1.86 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:36.305635: step 161090, loss = 1.86 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:37.535676: step 161100, loss = 2.00 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:08:38.758320: step 161110, loss = 1.77 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:39.973841: step 161120, loss = 1.91 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:41.222854: step 161130, loss = 1.89 (1024.8 examples/sec; 0.125 sec/batch)
2017-05-05 03:08:42.432104: step 161140, loss = 1.88 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:43.683628: step 161150, loss = 1.94 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-05 03:08:44.879336: step 161160, loss = 1.88 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:08:46.090022: step 161170, loss = 1.77 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:47.296294: step 161180, loss = 1.94 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:48.512981: step 161190, loss = 1.95 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:49.730364: step 161200, loss = 2.04 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:50.970572: step 161210, loss = 1.82 (1032.1 examples/sec; 0.124 sec/batch)
2017-05-05 03:08:52.166477: step 161220, loss = 1.82 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:08:53.416253: step 161230, loss = 1.88 (1024.2 examples/sec; 0.125 sec/batch)
2017-05-05 03:08:54.642494: step 161240, loss = 1.81 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 03:08:55.870425: step 161250, loss = 2.05 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-05 03:08:57.082958: step 161260, loss = 1.89 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:58.314372: step 161270, loss = 1.87 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:08:59.513499: step 161280, loss = 1.95 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:09:00.724745: step 161290, loss = 1.94 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:01.947473: step 161300, loss = 1.88 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:03.165487: step 161310, loss = 1.99 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:04.402425: step 161320, loss = 1.97 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-05 03:09:05.587834: step 161330, loss = 1.82 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:09:06.804126: step 161340, loss = 2.12 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:08.028655: step 161350, loss = 1.87 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:09.254892: step 161360, loss = 1.83 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 03:09:10.544826: step 161370, loss = 1.87 (992.3 examples/sec; 0.129 sec/batch)
2017-05-05 03:09:11.657916: step 161380, loss = 1.73 (1150.0 examples/sec; 0.111 sec/batch)
2017-05-05 03:09:12.870915: step 161390, loss = 1.75 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:14.080628: step 161400, loss = 1.91 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:15.299581: step 161410, loss = 1.89 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:16.502830: step 161420, loss = 1.77 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:09:17.707393: step 161430, loss = 1.76 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:09:18.919672: step 161440, loss = 1.92 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:20.135623: step 161450, loss = 1.80 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:21.359457: step 161460, loss = 1.77 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:22.579704: step 161470, loss = 1.85 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:23.790928: step 161480, loss = 1.91 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:24.999026: step 161490, loss = 1.74 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:26.222742: step 161500, loss = 1.86 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:27.438950: step 161510, loss = 1.91 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:28.626318: step 161520, loss = 1.96 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:09:29.802720: step 161530, loss = 1.92 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:09:30.994990: step 161540, loss = 2.02 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:09:32.190934: step 161550, loss = 1.83 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:09:33.363947: step 161560, loss = 1.91 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:09:34.606341: step 161570, loss = 2.06 (1030.3 examples/sec; 0.124 sec/batch)
2017-05-05 03:09:35.740728: step 161580, loss = 1.89 (1128.4 examples/sec; 0.113 sec/batch)
2017-05-05 03:09:36.980847: step 161590, loss = 1.99 (1032.2 examples/sec; 0.124 sec/batch)
2017-05-05 03:09:38.181839: step 161600, loss = 1.92 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:09:39.423541: step 161610, loss = 2.01 (1030.8 examples/sec; 0.124 sec/batch)
2017-05-05 03:09:40.651480: step 161620, loss = 1.92 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-05 03:09:41.882985: step 161630, loss = 1.85 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-05 03:09:43.106849: step 161640, loss = 1.88 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:44.313441: step 161650, loss = 1.95 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:45.543510: step 161660, loss = 1.86 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:09:46.771531: step 161670, loss = 1.78 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-05 03:09:47.988924: step 161680, loss = 1.76 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:49.203933: step 161690, loss = 1.78 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:50.422113: step 161700, loss = 1.92 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:51.661874: step 161710, loss = 1.73 (1032.5 examples/sec; 0.124 sec/batch)
2017-05-05 03:09:52.856956: step 161720, loss = 1.82 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:09:54.065374: step 161730, loss = 1.92 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:55.264826: step 161740, loss = 1.93 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:09:56.492515: step 161750, loss = 1.86 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:09:57.687083: step 161760, loss = 1.81 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:09:58.900134: step 161770, loss = 1.78 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:10:00.096701: step 161780, loss = 1.77 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:10:01.315303: step 161790, loss = 2.02 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:10:02.523849: step 161800, loss = 1.81 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:10:03.738188: step 161810, loss = 1.96 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:10:04.954450: step 161820, loss = 1.80 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:10:06.191160: step 161830, loss = 1.83 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-05 03:10:07.392766: step 161840, loss = 1.90 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:10:08.623285: step 161850, loss = 1.85 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-05 03:10:09.829992: step 161860, loss = 2.08 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:10:11.057170: step 161870, loss = 1.97 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:10:12.266643: step 161880, loss = 1.84 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:10:13.499449: step 161890, loss = 1.73 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-05 03:10:14.711580: step 161900, loss = 1.85 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:10:15.933507: step 161910, loss = 1.72 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:10:17.188814: step 161920, loss = 1.95 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-05 03:10:18.397331: step 161930, loss = 1.94 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:10:19.597727: step 161940, loss = 1.75 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:10:20.821694: step 161950, loss = 1.86 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:10:22.034667: step 161960, loss = 1.82 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:10:23.273152: step 161970, loss = 1.92 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-05 03:10:24.459882: step 161980, loss = 1.98 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:25.654983: step 161990, loss = 1.74 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:10:26.864460: step 162000, loss = 1.91 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:10:28.058521: step 162010, loss = 1.90 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:29.254078: step 162020, loss = 1.86 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:10:30.428441: step 162030, loss = 1.76 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:10:31.599663: step 162040, loss = 1.96 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:10:32.790120: step 162050, loss = 1.84 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:33.975493: step 162060, loss = 1.92 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:35.168206: step 162070, loss = 1.81 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:36.360399: step 162080, loss = 1.87 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:37.551959: step 162090, loss = 1.88 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:38.745488: step 162100, loss = 1.86 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:39.942530: step 162110, loss = 1.91 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:10:41.128994: step 162120, loss = 1.89 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:42.311614: step 162130, loss = 2.08 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:10:43.503204: step 162140, loss = 1.82 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:44.693685: step 162150, loss = 1.99 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:45.866412: step 162160, loss = 1.80 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:10:47.073138: step 162170, loss = 1.76 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:10:48.258426: step 162180, loss = 1.86 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:49.455573: step 162190, loss = 1.69 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:10:50.629723: step 162200, loss = 1.88 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:10:51.814526: step 162210, loss = 1.86 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:10:52.985841: step 162220, loss = 1.84 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:10:54.168165: step 162230, loss = 1.84 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:10:55.358987: step 162240, loss = 1.77 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:56.535587: step 162250, loss = 1.87 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:10:57.701723: step 162260, loss = 1.75 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:10:58.868301: step 162270, loss = 1.96 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:00.055878: step 162280, loss = 1.85 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:11:01.210744: step 162290, loss = 1.89 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:11:02.380844: step 162300, loss = 1.71 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:03.557680: step 162310, loss = 1.84 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:04.725655: step 162320, loss = 1.77 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:05.893878: step 162330, loss = 1.80 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:07.068384: step 162340, loss = 1.87 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:08.270024: step 162350, loss = 1.88 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:11:09.534127: step 162360, loss = 1.82 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-05 03:11:10.608727: step 162370, loss = 1.83 (1191.1 examples/sec; 0.107 sec/batch)
2017-05-05 03:11:11.792065: step 162380, loss = 1.81 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:12.958072: step 162390, loss = 1.74 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:14.111069: step 162400, loss = 1.78 (1110.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:11:15.294056: step 162410, loss = 1.87 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:16.466759: step 162420, loss = 1.88 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:17.626834: step 162430, loss = 2.02 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:11:18.780598: step 162440, loss = 2.00 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:11:19.950778: step 162450, loss = 1.81 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:21.112084: step 162460, loss = 1.94 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:11:22.255950: step 162470, loss = 1.80 (1119.0 examples/sec; 0.114 sec/batch)
2017-05-05 03:11:23.437891: step 162480, loss = 1.75 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:24.589160: step 162490, loss = 1.86 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:11:25.740216: step 162500, loss = 1.78 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:11:26.903023: step 162510, loss = 1.70 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:11:28.087390: step 162520, loss = 1.93 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:29.276406: step 162530, loss = 1.93 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:11:30.439021: step 162540, loss = 1.85 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:11:31.611668: step 162550, loss = 1.73 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:32.789042: step 162560, loss = 1.84 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:33.953327: step 162570, loss = 1.84 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:11:35.133431: step 162580, loss = 1.94 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:36.317901: step 162590, loss = 1.76 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:37.495478: step 162600, loss = 1.89 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:38.671654: step 162610, loss = 1.83 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:39.857511: step 162620, loss = 1.88 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:11:41.051922: step 162630, loss = 2.05 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:11:42.227726: step 162640, loss = 2.04 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:43.401447: step 162650, loss = 1.86 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:44.593522: step 162660, loss = 1.83 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:11:45.775123: step 162670, loss = 1.97 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:46.974737: step 162680, loss = 1.87 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:11:48.176514: step 162690, loss = 1.84 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:11:49.369948: step 162700, loss = 1.73 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:11:50.588715: step 162710, loss = 1.87 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:11:51.801373: step 162720, loss = 1.77 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:11:52.980118: step 162730, loss = 1.83 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:54.141575: step 162740, loss = 2.00 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:11:55.347605: step 162750, loss = 1.95 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:11:56.542535: step 162760, loss = 1.79 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:11:57.745677: step 162770, loss = 1.88 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:11:58.949801: step 162780, loss = 1.97 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:00.146826: step 162790, loss = 1.98 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:01.353793: step 162800, loss = 1.77 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:12:02.554475: step 162810, loss = 1.91 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:03.755105: step 162820, loss = 2.02 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:04.934207: step 162830, loss = 1.87 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:06.116697: step 162840, loss = 1.86 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:07.326510: step 162850, loss = 1.83 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:12:08.513605: step 162860, loss = 1.84 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:12:09.676709: step 162870, loss = 1.86 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:12:10.862945: step 162880, loss = 1.92 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:12:12.049722: step 162890, loss = 1.84 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:12:13.235431: step 162900, loss = 1.85 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:12:14.406157: step 162910, loss = 1.85 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:15.586196: step 162920, loss = 1.87 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:16.785153: step 162930, loss = 1.85 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:17.960953: step 162940, loss = 2.09 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:19.174160: step 162950, loss = 1.90 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:12:20.378588: step 162960, loss = 1.71 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:21.583543: step 162970, loss = 1.88 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:22.817758: step 162980, loss = 1.97 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-05 03:12:24.034808: step 162990, loss = 1.77 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:12:25.229832: step 163000, loss = 1.91 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:26.426012: step 163010, loss = 1.82 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:27.603034: step 163020, loss = 1.87 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:28.819088: step 163030, loss = 1.87 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:12:30.008400: step 163040, loss = 1.84 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:12:31.245127: step 163050, loss = 1.89 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-05 03:12:32.424750: step 163060, loss = 1.80 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:33.624147: step 163070, loss = 1.95 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:34.823155: step 163080, loss = 1.86 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:36.036722: step 163090, loss = 1.89 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:12:37.218883: step 163100, loss = 1.97 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:38.403169: step 163110, loss = 1.84 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:39.588790: step 163120, loss = 1.75 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:12:40.758909: step 163130, loss = 1.78 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:41.924650: step 163140, loss = 1.80 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:43.099048: step 163150, loss = 1.98 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:44.268891: step 163160, loss = 1.86 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:45.442224: step 163170, loss = 1.86 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:46.595713: step 163180, loss = 1.89 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:12:47.785754: step 163190, loss = 1.79 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:12:48.947909: step 163200, loss = 1.81 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:12:50.111793: step 163210, loss = 1.70 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:12:51.300329: step 163220, loss = 1.78 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:12:52.475345: step 163230, loss = 1.83 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:53.642405: step 163240, loss = 1.95 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:54.809522: step 163250, loss = 1.77 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:55.989927: step 163260, loss = 1.85 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:57.166848: step 163270, loss = 1.90 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:58.335856: step 163280, loss = 1.77 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:59.496875: step 163290, loss = 1.92 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:00.666044: step 163300, loss = 1.78 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:01.817188: step 163310, loss = 1.84 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:13:02.993411: step 163320, loss = 1.74 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:04.173316: step 163330, loss = 2.03 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:05.334533: step 163340, loss = 1.77 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:06.598202: step 163350, loss = 1.87 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-05 03:13:07.674749: step 163360, loss = 1.93 (1189.0 examples/sec; 0.108 sec/batch)
2017-05-05 03:13:08.846951: step 163370, loss = 1.77 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:10.011095: step 163380, loss = 1.92 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:11.174629: step 163390, loss = 1.79 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:12.352739: step 163400, loss = 1.95 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:13.535863: step 163410, loss = 1.76 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:14.691869: step 163420, loss = 2.02 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:15.868370: step 163430, loss = 1.93 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:17.048184: step 163440, loss = 1.84 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:18.192394: step 163450, loss = 1.95 (1118.7 examples/sec; 0.114 sec/batch)
2017-05-05 03:13:19.361060: step 163460, loss = 1.94 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:20.525225: step 163470, loss = 1.81 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:21.681480: step 163480, loss = 1.78 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:22.852511: step 163490, loss = 1.86 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:24.009078: step 163500, loss = 1.90 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:25.182145: step 163510, loss = 1.79 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:26.328812: step 163520, loss = 1.79 (1116.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:13:27.514514: step 163530, loss = 1.75 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:13:28.674140: step 163540, loss = 1.87 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:29.842737: step 163550, loss = 1.95 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:31.019359: step 163560, loss = 2.09 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:32.165191: step 163570, loss = 1.86 (1117.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:13:33.359032: step 163580, loss = 1.82 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:13:34.522376: step 163590, loss = 1.90 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:35.685437: step 163600, loss = 1.89 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:36.849672: step 163610, loss = 1.84 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:38.010461: step 163620, loss = 1.95 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:39.183881: step 163630, loss = 2.02 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:40.362304: step 163640, loss = 1.86 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:41.532013: step 163650, loss = 1.84 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:42.706075: step 163660, loss = 1.86 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:43.863130: step 163670, loss = 1.82 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:45.024148: step 163680, loss = 1.97 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:46.165714: step 163690, loss = 2.03 (1121.3 examples/sec; 0.114 sec/batch)
2017-05-05 03:13:47.320925: step 163700, loss = 1.89 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:48.490592: step 163710, loss = 1.77 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:49.643782: step 163720, loss = 2.06 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:13:50.822690: step 163730, loss = 1.88 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:51.979066: step 163740, loss = 2.01 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:53.167688: step 163750, loss = 1.78 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:13:54.335935: step 163760, loss = 1.76 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:55.507372: step 163770, loss = 1.82 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:56.679846: step 163780, loss = 1.99 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:57.844573: step 163790, loss = 1.85 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:58.999864: step 163800, loss = 1.82 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:00.161910: step 163810, loss = 1.98 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:01.339764: step 163820, loss = 1.75 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:02.510760: step 163830, loss = 2.09 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:03.667832: step 163840, loss = 1.73 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:04.859559: step 163850, loss = 1.75 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:14:06.022420: step 163860, loss = 1.81 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:07.209533: step 163870, loss = 1.87 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:14:08.370461: step 163880, loss = 1.77 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:09.544741: step 163890, loss = 1.66 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:10.738730: step 163900, loss = 1.92 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:14:11.906371: step 163910, loss = 1.82 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:13.064745: step 163920, loss = 1.83 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:14.234482: step 163930, loss = 1.93 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:15.403153: step 163940, loss = 2.07 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:16.578444: step 163950, loss = 1.90 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:17.726798: step 163960, loss = 1.82 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:14:18.898970: step 163970, loss = 1.83 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:20.069259: step 163980, loss = 1.94 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:21.241406: step 163990, loss = 1.98 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:22.415778: step 164000, loss = 1.94 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:23.595617: step 164010, loss = 2.07 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:24.775044: step 164020, loss = 1.82 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:25.947753: step 164030, loss = 1.81 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:27.158548: step 164040, loss = 1.77 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:14:28.367084: step 164050, loss = 1.86 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:14:29.532385: step 164060, loss = 1.82 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:30.691826: step 164070, loss = 1.79 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:31.878135: step 164080, loss = 1.86 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:14:33.048223: step 164090, loss = 1.78 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:34.215839: step 164100, loss = 1.82 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:35.393968: step 164110, loss = 2.00 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:36.557899: step 164120, loss = 1.88 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:37.716907: step 164130, loss = 1.88 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:38.885825: step 164140, loss = 1.87 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:40.046829: step 164150, loss = 1.83 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:41.210062: step 164160, loss = 1.95 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:42.374545: step 164170, loss = 1.86 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:43.533893: step 164180, loss = 1.98 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:44.698914: step 164190, loss = 1.80 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:45.858836: step 164200, loss = 1.89 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:47.013727: step 164210, loss = 2.02 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:14:48.172294: step 164220, loss = 1.81 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:49.336833: step 164230, loss = 1.89 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:50.494120: step 164240, loss = 1.79 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:51.672798: step 164250, loss = 1.94 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:52.846079: step 164260, loss = 1.82 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:54.022639: step 164270, loss = 1.96 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:55.201610: step 164280, loss = 1.82 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:56.365336: step 164290, loss = 1.82 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:57.558684: step 164300, loss = 1.74 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:14:58.744413: step 164310, loss = 1.81 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:14:59.904908: step 164320, loss = 1.99 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:01.087379: step 164330, loss = 1.80 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:15:02.345938: step 164340, loss = 2.02 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-05 03:15:03.439723: step 164350, loss = 1.93 (1170.2 examples/sec; 0.109 sec/batch)
2017-05-05 03:15:04.597266: step 164360, loss = 1.87 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:05.743404: step 164370, loss = 1.90 (1116.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:15:06.919366: step 164380, loss = 1.86 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:15:08.113212: step 164390, loss = 1.80 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:15:09.286157: step 164400, loss = 2.04 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:10.443148: step 164410, loss = 1.95 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:11.607104: step 164420, loss = 1.86 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:12.777989: step 164430, loss = 1.96 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:13.937273: step 164440, loss = 1.94 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:15.099386: step 164450, loss = 2.14 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:16.276550: step 164460, loss = 1.79 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:15:17.439800: step 164470, loss = 1.85 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:18.610563: step 164480, loss = 1.91 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:19.772559: step 164490, loss = 1.80 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:20.967751: step 164500, loss = 1.87 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:15:22.121272: step 164510, loss = 1.99 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:15:23.307022: step 164520, loss = 2.08 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:15:24.468190: step 164530, loss = 1.94 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:25.635044: step 164540, loss = 1.88 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:26.792587: step 164550, loss = 1.76 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:27.948672: step 164560, loss = 1.87 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:29.109481: step 164570, loss = 1.84 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:30.269611: step 164580, loss = 1.92 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:31.443800: step 164590, loss = 1.82 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:32.598459: step 164600, loss = 1.89 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:15:33.769552: step 164610, loss = 1.81 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:34.950938: step 164620, loss = 1.92 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:15:36.133000: step 164630, loss = 1.85 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:15:37.316445: step 164640, loss = 1.90 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:15:38.478219: step 164650, loss = 1.83 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:39.651396: step 164660, loss = 1.97 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:40.823534: step 164670, loss = 1.99 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:41.969149: step 164680, loss = 1.78 (1117.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:15:43.157454: step 164690, loss = 2.03 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:15:44.326289: step 164700, loss = 1.76 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:45.501624: step 164710, loss = 1.87 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:15:46.671152: step 164720, loss = 1.84 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:47.854978: step 164730, loss = 1.85 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:15:49.039083: step 164740, loss = 1.90 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:15:50.213991: step 164750, loss = 1.88 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:51.386539: step 164760, loss = 1.83 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:52.557593: step 164770, loss = 1.78 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:53.733413: step 164780, loss = 1.90 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:15:54.898881: step 164790, loss = 1.82 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:56.055304: step 164800, loss = 2.02 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:57.220177: step 164810, loss = 1.88 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:58.379754: step 164820, loss = 1.78 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:59.527304: step 164830, loss = 2.03 (1115.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:16:00.700169: step 164840, loss = 1.82 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:01.853545: step 164850, loss = 1.90 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:16:03.037061: step 164860, loss = 1.87 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:04.225326: step 164870, loss = 2.00 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:16:05.394706: step 164880, loss = 1.91 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:06.577503: step 164890, loss = 1.82 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:07.756661: step 164900, loss = 1.82 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:08.929641: step 164910, loss = 1.87 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:10.101908: step 164920, loss = 1.90 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:11.266775: step 164930, loss = 1.77 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:12.444248: step 164940, loss = 1.95 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:13.601663: step 164950, loss = 1.90 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:14.763120: step 164960, loss = 1.99 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:15.917892: step 164970, loss = 1.96 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:16:17.066981: step 164980, loss = 1.79 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:16:18.233071: step 164990, loss = 1.76 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:19.400673: step 165000, loss = 1.80 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:20.550674: step 165010, loss = 1.84 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:16:21.715153: step 165020, loss = 2.00 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:22.890986: step 165030, loss = 1.90 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:24.060650: step 165040, loss = 1.94 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:25.234777: step 165050, loss = 1.83 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:26.421510: step 165060, loss = 1.80 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:16:27.616757: step 165070, loss = 1.81 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:16:28.783906: step 165080, loss = 1.79 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:29.934643: step 165090, loss = 1.89 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:16:31.086988: step 165100, loss = 1.95 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:16:32.250172: step 165110, loss = 1.97 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:33.407506: step 165120, loss = 1.91 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:34.564728: step 165130, loss = 1.73 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:35.725408: step 165140, loss = 1.84 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:36.887223: step 165150, loss = 1.92 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:38.071585: step 165160, loss = 1.81 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:39.235946: step 165170, loss = 1.72 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:40.412953: step 165180, loss = 1.82 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:41.583260: step 165190, loss = 1.85 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:42.750964: step 165200, loss = 1.75 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:43.925845: step 165210, loss = 1.94 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:45.100879: step 165220, loss = 1.90 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:46.239222: step 165230, loss = 1.86 (1124.4 examples/sec; 0.114 sec/batch)
2017-05-05 03:16:47.408638: step 165240, loss = 1.82 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:48.582524: step 165250, loss = 1.79 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:49.745340: step 165260, loss = 1.79 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:50.907635: step 165270, loss = 1.86 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:52.080351: step 165280, loss = 1.83 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:53.252281: step 165290, loss = 1.93 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:54.424069: step 165300, loss = 1.89 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:55.596949: step 165310, loss = 1.82 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:56.759611: step 165320, loss = 1.93 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:58.017841: step 165330, loss = 1.79 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-05 03:16:59.120274: step 165340, loss = 1.85 (1161.1 examples/sec; 0.110 sec/batch)
2017-05-05 03:17:00.285996: step 165350, loss = 1.86 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:01.440042: step 165360, loss = 1.96 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:17:02.587804: step 165370, loss = 1.99 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:17:03.765296: step 165380, loss = 1.89 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:17:04.933214: step 165390, loss = 1.85 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:06.076982: step 165400, loss = 1.85 (1119.1 examples/sec; 0.114 sec/batch)
2017-05-05 03:17:07.230195: step 165410, loss = 1.75 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:17:08.385604: step 165420, loss = 1.77 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:17:09.529468: step 165430, loss = 1.76 (1119.0 examples/sec; 0.114 sec/batch)
2017-05-05 03:17:10.683454: step 165440, loss = 1.84 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:17:11.855626: step 165450, loss = 1.74 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:13.021614: step 165460, loss = 1.79 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:14.207138: step 165470, loss = 1.98 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:17:15.387127: step 165480, loss = 2.02 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:17:16.562284: step 165490, loss = 1.91 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:17:17.733893: step 165500, loss = 1.78 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:18.901168: step 165510, loss = 1.68 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:20.060059: step 165520, loss = 1.93 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:17:21.250460: step 165530, loss = 1.80 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:17:22.377344: step 165540, loss = 2.04 (1135.9 examples/sec; 0.113 sec/batch)
2017-05-05 03:17:23.559778: step 165550, loss = 1.73 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:17:24.750147: step 165560, loss = 1.84 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:17:25.916954: step 165570, loss = 1.87 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:27.093636: step 165580, loss = 1.77 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:17:28.257550: step 165590, loss = 1.91 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:17:29.445409: step 165600, loss = 1.92 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:17:30.604703: step 165610, loss = 1.97 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:17:31.747694: step 165620, loss = 1.73 (1119.9 examples/sec; 0.114 sec/batch)
2017-05-05 03:17:32.923775: step 165630, loss = 1.98 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:17:34.069596: step 165640, loss = 1.94 (1117.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:17:35.236102: step 165650, loss = 1.89 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:36.410556: step 165660, loss = 1.95 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:37.574439: step 165670, loss = 1.96 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:17:38.741199: step 165680, loss = 1.99 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:39.929909: step 165690, loss = 1.85 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:17:41.127464: step 165700, loss = 1.93 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:17:42.330314: step 165710, loss = 1.80 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:17:43.532021: step 165720, loss = 1.86 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:17:44.720360: step 165730, loss = 1.76 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:17:45.942167: step 165740, loss = 2.06 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:17:47.170411: step 165750, loss = 1.86 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-05 03:17:48.375547: step 165760, loss = 1.91 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:17:49.573062: step 165770, loss = 1.84 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:17:50.796023: step 165780, loss = 1.85 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:17:52.022498: step 165790, loss = 1.83 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:17:53.244080: step 165800, loss = 1.89 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:17:54.464994: step 165810, loss = 1.93 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:17:55.671205: step 165820, loss = 1.88 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:17:56.885279: step 165830, loss = 1.84 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:17:58.083475: step 165840, loss = 2.03 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:17:59.313195: step 165850, loss = 1.82 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:18:00.524008: step 165860, loss = 1.96 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:01.735721: step 165870, loss = 1.73 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:02.953994: step 165880, loss = 1.87 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:04.171070: step 165890, loss = 1.90 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:05.381681: step 165900, loss = 1.84 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:06.606060: step 165910, loss = 1.92 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:07.804407: step 165920, loss = 1.84 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:18:09.022609: step 165930, loss = 1.77 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:10.216945: step 165940, loss = 1.85 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:18:11.443026: step 165950, loss = 1.85 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:18:12.656206: step 165960, loss = 1.77 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:13.890967: step 165970, loss = 2.07 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:18:15.100075: step 165980, loss = 1.88 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:16.306863: step 165990, loss = 1.93 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:17.524458: step 166000, loss = 1.82 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:18.731114: step 166010, loss = 1.82 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:19.934746: step 166020, loss = 1.93 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:18:21.163521: step 166030, loss = 1.83 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 03:18:22.367215: step 166040, loss = 1.80 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:18:23.580404: step 166050, loss = 2.02 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:24.778178: step 166060, loss = 1.74 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:18:25.993986: step 166070, loss = 1.70 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:27.212694: step 166080, loss = 1.93 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:28.401291: step 166090, loss = 1.71 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:18:29.572222: step 166100, loss = 1.78 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:18:30.741553: step 166110, loss = 1.76 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:18:31.941555: step 166120, loss = 1.83 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:18:33.157062: step 166130, loss = 1.83 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:34.331009: step 166140, loss = 1.90 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:18:35.586948: step 166150, loss = 1.86 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-05 03:18:36.798492: step 166160, loss = 1.85 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:38.013567: step 166170, loss = 1.84 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:39.217024: step 166180, loss = 1.88 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:18:40.460993: step 166190, loss = 1.96 (1029.0 examples/sec; 0.124 sec/batch)
2017-05-05 03:18:41.686271: step 166200, loss = 1.91 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 03:18:42.904827: step 166210, loss = 1.93 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:44.134119: step 166220, loss = 1.78 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-05 03:18:45.343335: step 166230, loss = 1.84 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:46.553420: step 166240, loss = 1.86 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:47.773454: step 166250, loss = 1.77 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:48.978656: step 166260, loss = 2.04 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:50.180694: step 166270, loss = 1.87 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:18:51.380871: step 166280, loss = 2.09 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:18:52.595473: step 166290, loss = 1.95 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:53.788093: step 166300, loss = 1.86 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:18:55.141137: step 166310, loss = 2.01 (946.0 examples/sec; 0.135 sec/batch)
2017-05-05 03:18:56.315619: step 166320, loss = 1.90 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:18:57.416653: step 166330, loss = 1.78 (1162.5 examples/sec; 0.110 sec/batch)
2017-05-05 03:18:58.636085: step 166340, loss = 1.90 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:59.869387: step 166350, loss = 1.87 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:19:01.078767: step 166360, loss = 1.92 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:02.292800: step 166370, loss = 1.83 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:03.501803: step 166380, loss = 1.72 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:04.714747: step 166390, loss = 1.78 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:05.907794: step 166400, loss = 1.85 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:19:07.129504: step 166410, loss = 1.75 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:08.314112: step 166420, loss = 1.82 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:19:09.534716: step 166430, loss = 2.07 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:10.734442: step 166440, loss = 1.91 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:19:11.957368: step 166450, loss = 1.89 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:13.168097: step 166460, loss = 1.68 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:14.385976: step 166470, loss = 1.84 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:15.612052: step 166480, loss = 1.71 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:19:16.828759: step 166490, loss = 1.77 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:18.022896: step 166500, loss = 1.94 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:19:19.241984: step 166510, loss = 1.89 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:20.520723: step 166520, loss = 1.85 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-05 03:19:21.657322: step 166530, loss = 1.90 (1126.2 examples/sec; 0.114 sec/batch)
2017-05-05 03:19:22.886316: step 166540, loss = 1.88 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:19:24.103222: step 166550, loss = 1.98 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:25.313264: step 166560, loss = 2.05 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:26.539182: step 166570, loss = 1.89 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-05 03:19:27.738599: step 166580, loss = 1.81 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:19:28.967069: step 166590, loss = 1.84 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:19:30.177170: step 166600, loss = 1.89 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:31.425577: step 166610, loss = 1.74 (1025.3 examples/sec; 0.125 sec/batch)
2017-05-05 03:19:32.620295: step 166620, loss = 1.85 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:19:33.848126: step 166630, loss = 1.89 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:19:35.074914: step 166640, loss = 1.79 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 03:19:36.289162: step 166650, loss = 1.83 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:37.491809: step 166660, loss = 1.96 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:19:38.695930: step 166670, loss = 1.88 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:19:39.910728: step 166680, loss = 1.92 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:41.117336: step 166690, loss = 1.87 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:42.308680: step 166700, loss = 1.84 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:19:43.519112: step 166710, loss = 1.85 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:44.717457: step 166720, loss = 1.95 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:19:45.913056: step 166730, loss = 1.97 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:19:47.129413: step 166740, loss = 1.93 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:48.355153: step 166750, loss = 1.82 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-05 03:19:49.551078: step 166760, loss = 1.84 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:19:50.765078: step 166770, loss = 1.98 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:51.968227: step 166780, loss = 1.85 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:19:53.200563: step 166790, loss = 1.95 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-05 03:19:54.407431: step 166800, loss = 1.85 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:55.618099: step 166810, loss = 1.89 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:56.829339: step 166820, loss = 1.89 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:58.036953: step 166830, loss = 1.83 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:59.255507: step 166840, loss = 1.80 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:20:00.461448: step 166850, loss = 1.90 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:20:01.644837: step 166860, loss = 1.84 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:20:02.851365: step 166870, loss = 1.82 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:20:04.055771: step 166880, loss = 2.03 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:20:05.244476: step 166890, loss = 1.88 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:20:06.420938: step 166900, loss = 1.70 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:20:07.584301: step 166910, loss = 1.88 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:08.762014: step 166920, loss = 1.89 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:20:09.918366: step 166930, loss = 1.87 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:11.119811: step 166940, loss = 1.97 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:20:12.295310: step 166950, loss = 1.72 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:20:13.445782: step 166960, loss = 1.85 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:20:14.614118: step 166970, loss = 1.81 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:15.785486: step 166980, loss = 1.87 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:16.952771: step 166990, loss = 1.97 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:18.113711: step 167000, loss = 1.88 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:19.282456: step 167010, loss = 1.72 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:20.440988: step 167020, loss = 1.93 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:21.586133: step 167030, loss = 1.82 (1117.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:20:22.741951: step 167040, loss = 1.84 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:23.927821: step 167050, loss = 1.75 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:20:25.095028: step 167060, loss = 1.78 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:26.263374: step 167070, loss = 1.77 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:27.422188: step 167080, loss = 2.01 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:28.591914: step 167090, loss = 1.77 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:29.747198: step 167100, loss = 1.81 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:30.926615: step 167110, loss = 1.84 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:20:32.077836: step 167120, loss = 1.81 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:20:33.246735: step 167130, loss = 1.83 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:34.408315: step 167140, loss = 2.17 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:35.574614: step 167150, loss = 1.82 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:36.732339: step 167160, loss = 1.91 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:37.888634: step 167170, loss = 1.90 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:39.051506: step 167180, loss = 1.83 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:40.212279: step 167190, loss = 1.74 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:41.401072: step 167200, loss = 1.87 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:20:42.551744: step 167210, loss = 1.82 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:20:43.717990: step 167220, loss = 1.81 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:44.881005: step 167230, loss = 1.83 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:46.040133: step 167240, loss = 1.95 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:47.226660: step 167250, loss = 1.80 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:20:48.394197: step 167260, loss = 1.99 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:49.544057: step 167270, loss = 1.96 (1113.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:20:50.704970: step 167280, loss = 1.77 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:51.882448: step 167290, loss = 1.82 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:20:53.050597: step 167300, loss = 1.89 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:54.272914: step 167310, loss = 2.04 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:20:55.332371: step 167320, loss = 1.99 (1208.2 examples/sec; 0.106 sec/batch)
2017-05-05 03:20:56.511464: step 167330, loss = 1.96 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:20:57.663701: step 167340, loss = 1.81 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:20:58.832288: step 167350, loss = 1.88 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:59.986161: step 167360, loss = 1.84 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:21:01.152423: step 167370, loss = 1.91 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:02.290851: step 167380, loss = 1.86 (1124.4 examples/sec; 0.114 sec/batch)
2017-05-05 03:21:03.478904: step 167390, loss = 1.96 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:04.616943: step 167400, loss = 1.85 (1124.7 examples/sec; 0.114 sec/batch)
2017-05-05 03:21:05.769954: step 167410, loss = 1.93 (1110.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:21:06.937313: step 167420, loss = 1.93 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:08.120652: step 167430, loss = 1.98 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:09.293065: step 167440, loss = 1.90 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:10.460324: step 167450, loss = 1.91 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:11.631086: step 167460, loss = 1.73 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:12.838365: step 167470, loss = 1.84 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:21:14.026525: step 167480, loss = 1.83 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:15.194966: step 167490, loss = 1.84 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:16.378579: step 167500, loss = 1.68 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:17.611650: step 167510, loss = 1.81 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-05 03:21:18.743684: step 167520, loss = 1.87 (1130.7 examples/sec; 0.113 sec/batch)
2017-05-05 03:21:19.944835: step 167530, loss = 1.83 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:21:21.120982: step 167540, loss = 1.73 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:22.309687: step 167550, loss = 1.84 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:23.506316: step 167560, loss = 1.98 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:21:24.700083: step 167570, loss = 1.82 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:25.878204: step 167580, loss = 1.90 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:27.053519: step 167590, loss = 1.91 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:28.245320: step 167600, loss = 1.89 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:29.423963: step 167610, loss = 1.90 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:30.596090: step 167620, loss = 1.72 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:31.809031: step 167630, loss = 1.82 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:21:33.002837: step 167640, loss = 1.90 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:34.173045: step 167650, loss = 1.88 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:35.357458: step 167660, loss = 1.77 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:36.554010: step 167670, loss = 1.99 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:21:37.749829: step 167680, loss = 1.96 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:21:38.925351: step 167690, loss = 1.75 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:40.121274: step 167700, loss = 1.97 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:21:41.317836: step 167710, loss = 1.83 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:21:42.499170: step 167720, loss = 1.89 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:43.687668: step 167730, loss = 1.85 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:44.870210: step 167740, loss = 1.79 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:46.045122: step 167750, loss = 1.89 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:47.207973: step 167760, loss = 1.94 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:21:48.396679: step 167770, loss = 1.95 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:49.582594: step 167780, loss = 1.80 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:50.773720: step 167790, loss = 1.74 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:51.955961: step 167800, loss = 1.79 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:53.130907: step 167810, loss = 2.04 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:54.298517: step 167820, loss = 1.87 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:55.477132: step 167830, loss = 2.00 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:56.658575: step 167840, loss = 1.99 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:57.828797: step 167850, loss = 1.85 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:59.029579: step 167860, loss = 1.81 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:22:00.228461: step 167870, loss = 1.81 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:22:01.425723: step 167880, loss = 1.93 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:22:02.618281: step 167890, loss = 1.97 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:22:03.814850: step 167900, loss = 1.82 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:22:04.986987: step 167910, loss = 1.92 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:22:06.193577: step 167920, loss = 1.76 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:07.410160: step 167930, loss = 1.98 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:22:08.617980: step 167940, loss = 1.95 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:09.803340: step 167950, loss = 1.83 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:22:10.983215: step 167960, loss = 1.97 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:22:12.170046: step 167970, loss = 1.81 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:22:13.357713: step 167980, loss = 1.75 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:22:14.529165: step 167990, loss = 1.92 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:22:15.703913: step 168000, loss = 1.97 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:22:16.877906: step 168010, loss = 1.82 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:22:18.031145: step 168020, loss = 1.83 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:22:19.204707: step 168030, loss = 1.76 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:22:20.397487: step 168040, loss = 1.89 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:22:21.588674: step 168050, loss = 1.91 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:22:22.757467: step 168060, loss = 1.81 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:22:23.936566: step 168070, loss = 1.89 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:22:25.142978: step 168080, loss = 1.93 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:26.357133: step 168090, loss = 1.90 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:27.571650: step 168100, loss = 1.84 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:28.753308: step 168110, loss = 1.79 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:22:29.936011: step 168120, loss = 1.72 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:22:31.147759: step 168130, loss = 2.05 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:32.368025: step 168140, loss = 1.80 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:22:33.580400: step 168150, loss = 2.09 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:34.786064: step 168160, loss = 2.01 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:36.008839: step 168170, loss = 1.74 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:22:37.218661: step 168180, loss = 1.90 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:38.439878: step 168190, loss = 1.84 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:22:39.648006: step 168200, loss = 1.93 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:40.867536: step 168210, loss = 1.85 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:22:42.083033: step 168220, loss = 1.76 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:22:43.287924: step 168230, loss = 1.79 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:22:44.491401: step 168240, loss = 1.81 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:22:45.676629: step 168250, loss = 1.77 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:22:46.860064: step 168260, loss = 1.86 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:22:48.059587: step 168270, loss = 1.94 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:22:49.236784: step 168280, loss = 1.77 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:22:50.425192: step 168290, loss = 1.92 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:22:51.715316: step 168300, loss = 1.82 (992.2 examples/sec; 0.129 sec/batch)
2017-05-05 03:22:52.825300: step 168310, loss = 1.92 (1153.2 examples/sec; 0.111 sec/batch)
2017-05-05 03:22:54.028778: step 168320, loss = 1.78 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:22:55.234702: step 168330, loss = 1.83 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:56.452511: step 168340, loss = 1.70 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:22:57.678934: step 168350, loss = 1.87 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-05 03:22:58.883298: step 168360, loss = 1.93 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:00.117727: step 168370, loss = 1.89 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:23:01.339912: step 168380, loss = 2.00 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:02.559909: step 168390, loss = 1.97 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:03.781842: step 168400, loss = 1.93 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:05.029721: step 168410, loss = 1.73 (1025.7 examples/sec; 0.125 sec/batch)
2017-05-05 03:23:06.225113: step 168420, loss = 1.90 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:07.440779: step 168430, loss = 1.99 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:08.644222: step 168440, loss = 1.84 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:09.858967: step 168450, loss = 1.94 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:11.074866: step 168460, loss = 1.66 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:12.285378: step 168470, loss = 1.90 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:13.489640: step 168480, loss = 1.96 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:14.708815: step 168490, loss = 1.78 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:15.989401: step 168500, loss = 1.89 (999.5 examples/sec; 0.128 sec/batch)
2017-05-05 03:23:17.095163: step 168510, loss = 1.89 (1157.6 examples/sec; 0.111 sec/batch)
2017-05-05 03:23:18.306701: step 168520, loss = 1.79 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:19.524619: step 168530, loss = 1.71 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:20.718210: step 168540, loss = 1.90 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:23:21.927620: step 168550, loss = 1.89 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:23.125755: step 168560, loss = 1.93 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:24.330889: step 168570, loss = 1.80 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:25.540748: step 168580, loss = 1.85 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:26.751442: step 168590, loss = 1.97 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:27.972942: step 168600, loss = 2.07 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:29.196965: step 168610, loss = 1.85 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:30.405596: step 168620, loss = 1.85 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:31.607312: step 168630, loss = 1.91 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:32.824224: step 168640, loss = 1.82 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:34.041914: step 168650, loss = 1.95 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:35.241027: step 168660, loss = 1.88 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:36.450998: step 168670, loss = 1.82 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:37.652502: step 168680, loss = 1.91 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:38.870337: step 168690, loss = 1.97 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:40.097203: step 168700, loss = 2.03 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-05 03:23:41.307615: step 168710, loss = 1.93 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:42.510731: step 168720, loss = 1.83 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:43.737540: step 168730, loss = 1.86 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 03:23:44.944435: step 168740, loss = 1.79 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:46.158456: step 168750, loss = 1.90 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:47.362188: step 168760, loss = 1.94 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:48.611140: step 168770, loss = 2.00 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-05 03:23:49.813128: step 168780, loss = 1.81 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:51.005900: step 168790, loss = 1.93 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:23:52.213970: step 168800, loss = 1.97 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:53.434551: step 168810, loss = 1.78 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:54.645288: step 168820, loss = 1.88 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:55.868002: step 168830, loss = 1.84 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:57.105140: step 168840, loss = 1.82 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-05 03:23:58.303357: step 168850, loss = 1.95 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:59.540134: step 168860, loss = 1.95 (1034.9 examples/sec; 0.124 sec/batch)
2017-05-05 03:24:00.780842: step 168870, loss = 1.74 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-05 03:24:01.985086: step 168880, loss = 1.84 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:03.207033: step 168890, loss = 1.81 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:24:04.382405: step 168900, loss = 1.79 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:05.587503: step 168910, loss = 1.85 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:06.803667: step 168920, loss = 1.81 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:24:08.006292: step 168930, loss = 1.94 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:09.213874: step 168940, loss = 1.82 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:10.404963: step 168950, loss = 1.81 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:24:11.603809: step 168960, loss = 1.82 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:12.821341: step 168970, loss = 1.84 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:24:14.012008: step 168980, loss = 1.86 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:24:15.218638: step 168990, loss = 1.82 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:16.433877: step 169000, loss = 1.84 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:24:17.621446: step 169010, loss = 2.01 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:24:18.802806: step 169020, loss = 1.92 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:19.974640: step 169030, loss = 1.88 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:24:21.185550: step 169040, loss = 1.88 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:22.373932: step 169050, loss = 1.76 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:24:23.580010: step 169060, loss = 1.80 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:24.782427: step 169070, loss = 1.73 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:25.961697: step 169080, loss = 1.76 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:27.164139: step 169090, loss = 1.91 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:28.354541: step 169100, loss = 1.78 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:24:29.534312: step 169110, loss = 1.87 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:30.728689: step 169120, loss = 1.95 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:24:31.938793: step 169130, loss = 1.99 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:33.142786: step 169140, loss = 1.91 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:34.332545: step 169150, loss = 1.90 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:24:35.499421: step 169160, loss = 1.94 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:24:36.681563: step 169170, loss = 1.95 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:37.857630: step 169180, loss = 1.78 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:39.035926: step 169190, loss = 1.96 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:40.211358: step 169200, loss = 1.89 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:41.391978: step 169210, loss = 1.81 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:42.596477: step 169220, loss = 1.89 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:43.814293: step 169230, loss = 1.90 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:24:45.032682: step 169240, loss = 2.01 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:24:46.246736: step 169250, loss = 1.76 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:47.460273: step 169260, loss = 1.87 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:48.662888: step 169270, loss = 1.91 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:49.875067: step 169280, loss = 1.88 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:51.195584: step 169290, loss = 1.89 (969.6 examples/sec; 0.132 sec/batch)
2017-05-05 03:24:52.295392: step 169300, loss = 1.95 (1163.5 examples/sec; 0.110 sec/batch)
2017-05-05 03:24:53.509062: step 169310, loss = 1.73 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:54.701675: step 169320, loss = 1.84 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:24:55.882100: step 169330, loss = 1.88 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:57.060389: step 169340, loss = 1.89 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:58.235631: step 169350, loss = 1.84 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:59.427127: step 169360, loss = 1.75 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:25:00.625991: step 169370, loss = 2.00 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:25:01.822299: step 169380, loss = 1.78 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:25:03.056496: step 169390, loss = 1.72 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:04.276575: step 169400, loss = 1.86 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:05.507550: step 169410, loss = 2.09 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:06.697112: step 169420, loss = 1.87 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:25:07.931600: step 169430, loss = 1.94 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:09.142290: step 169440, loss = 1.90 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:10.374897: step 169450, loss = 1.67 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:11.580343: step 169460, loss = 1.84 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:12.818404: step 169470, loss = 1.92 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-05 03:25:14.029118: step 169480, loss = 2.04 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:15.311289: step 169490, loss = 1.72 (998.3 examples/sec; 0.128 sec/batch)
2017-05-05 03:25:16.418330: step 169500, loss = 1.90 (1156.2 examples/sec; 0.111 sec/batch)
2017-05-05 03:25:17.641762: step 169510, loss = 1.91 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:18.856808: step 169520, loss = 1.95 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:20.090407: step 169530, loss = 1.83 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:21.298828: step 169540, loss = 1.78 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:22.527812: step 169550, loss = 1.92 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:23.733312: step 169560, loss = 1.88 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:24.961175: step 169570, loss = 1.80 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:26.169349: step 169580, loss = 1.83 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:27.412792: step 169590, loss = 1.91 (1029.4 examples/sec; 0.124 sec/batch)
2017-05-05 03:25:28.605502: step 169600, loss = 1.87 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:25:29.820739: step 169610, loss = 1.79 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:31.050708: step 169620, loss = 1.95 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:32.265840: step 169630, loss = 1.77 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:33.491359: step 169640, loss = 1.94 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:34.706470: step 169650, loss = 1.82 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:35.919700: step 169660, loss = 1.85 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:37.147170: step 169670, loss = 1.76 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:38.348437: step 169680, loss = 1.77 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:25:39.556123: step 169690, loss = 1.94 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:40.776371: step 169700, loss = 1.86 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:41.994662: step 169710, loss = 1.95 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:43.217256: step 169720, loss = 2.00 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:44.441892: step 169730, loss = 2.21 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:45.648717: step 169740, loss = 1.84 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:46.872839: step 169750, loss = 1.97 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:48.098086: step 169760, loss = 1.82 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:49.329302: step 169770, loss = 1.69 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:50.531657: step 169780, loss = 1.93 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:25:51.772935: step 169790, loss = 2.11 (1031.2 examples/sec; 0.124 sec/batch)
2017-05-05 03:25:52.960818: step 169800, loss = 1.91 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:25:54.173699: step 169810, loss = 1.85 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:55.384356: step 169820, loss = 1.92 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:56.611725: step 169830, loss = 1.86 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:57.797824: step 169840, loss = 1.98 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:25:59.038170: step 169850, loss = 1.83 (1032.0 examples/sec; 0.124 sec/batch)
2017-05-05 03:26:00.259843: step 169860, loss = 1.78 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:01.485891: step 169870, loss = 1.94 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:26:02.692588: step 169880, loss = 1.97 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:03.898693: step 169890, loss = 1.90 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:05.119550: step 169900, loss = 1.70 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:06.341287: step 169910, loss = 1.93 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:07.541114: step 169920, loss = 1.94 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:26:08.756996: step 169930, loss = 1.88 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:09.949406: step 169940, loss = 1.77 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:26:11.205581: step 169950, loss = 1.83 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-05 03:26:12.429440: step 169960, loss = 1.87 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:13.644301: step 169970, loss = 1.88 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:14.858609: step 169980, loss = 1.94 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:16.082417: step 169990, loss = 1.86 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:17.301116: step 170000, loss = 1.66 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:18.536606: step 170010, loss = 1.82 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-05 03:26:19.740959: step 170020, loss = 1.90 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:26:20.959582: step 170030, loss = 1.94 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:22.162844: step 170040, loss = 1.86 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:26:23.394805: step 170050, loss = 1.86 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:26:24.604034: step 170060, loss = 1.87 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:25.825026: step 170070, loss = 1.83 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:27.020593: step 170080, loss = 1.88 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:26:28.227846: step 170090, loss = 1.94 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:29.420509: step 170100, loss = 1.82 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:26:30.632382: step 170110, loss = 2.07 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:31.854643: step 170120, loss = 1.85 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:33.078302: step 170130, loss = 1.84 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:34.285533: step 170140, loss = 1.90 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:35.507157: step 170150, loss = 1.97 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:36.708991: step 170160, loss = 1.86 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:26:37.919342: step 170170, loss = 1.94 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:39.133833: step 170180, loss = 1.88 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:40.378136: step 170190, loss = 1.96 (1028.7 examples/sec; 0.124 sec/batch)
2017-05-05 03:26:41.548404: step 170200, loss = 2.01 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:26:42.763149: step 170210, loss = 1.87 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:43.986512: step 170220, loss = 1.83 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:45.218251: step 170230, loss = 1.94 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-05 03:26:46.425401: step 170240, loss = 1.94 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:47.658038: step 170250, loss = 1.94 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-05 03:26:48.873126: step 170260, loss = 1.83 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:50.077868: step 170270, loss = 1.78 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:26:51.415340: step 170280, loss = 1.89 (957.0 examples/sec; 0.134 sec/batch)
2017-05-05 03:26:52.525175: step 170290, loss = 1.78 (1153.3 examples/sec; 0.111 sec/batch)
2017-05-05 03:26:53.708769: step 170300, loss = 1.92 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:26:54.899517: step 170310, loss = 1.93 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:26:56.085125: step 170320, loss = 1.92 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:26:57.273395: step 170330, loss = 1.77 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:26:58.459318: step 170340, loss = 1.83 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:26:59.650984: step 170350, loss = 1.84 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:00.849673: step 170360, loss = 1.91 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:02.045710: step 170370, loss = 1.84 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:03.249840: step 170380, loss = 2.02 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:04.449114: step 170390, loss = 1.80 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:05.650126: step 170400, loss = 1.86 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:06.849091: step 170410, loss = 1.88 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:08.064718: step 170420, loss = 2.00 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:27:09.258535: step 170430, loss = 1.78 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:10.469896: step 170440, loss = 1.94 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:27:11.696955: step 170450, loss = 1.97 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 03:27:12.916508: step 170460, loss = 1.75 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:27:14.088016: step 170470, loss = 1.90 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:27:15.377764: step 170480, loss = 1.79 (992.4 examples/sec; 0.129 sec/batch)
2017-05-05 03:27:16.491907: step 170490, loss = 1.82 (1148.9 examples/sec; 0.111 sec/batch)
2017-05-05 03:27:17.693731: step 170500, loss = 1.79 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:18.902794: step 170510, loss = 1.80 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:27:20.114173: step 170520, loss = 1.91 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:27:21.320848: step 170530, loss = 1.88 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:27:22.519952: step 170540, loss = 1.77 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:23.726717: step 170550, loss = 1.74 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:27:24.907397: step 170560, loss = 1.85 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:27:26.082800: step 170570, loss = 2.07 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:27:27.285201: step 170580, loss = 1.90 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:28.480953: step 170590, loss = 1.90 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:29.675702: step 170600, loss = 1.72 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:30.862392: step 170610, loss = 1.80 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:32.033210: step 170620, loss = 1.83 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:27:33.206593: step 170630, loss = 1.85 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:27:34.353865: step 170640, loss = 2.02 (1115.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:27:35.554499: step 170650, loss = 1.85 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:36.764857: step 170660, loss = 1.83 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:27:37.938135: step 170670, loss = 1.86 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:27:39.113461: step 170680, loss = 1.72 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:27:40.322502: step 170690, loss = 1.86 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:27:41.506439: step 170700, loss = 1.96 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:27:42.698427: step 170710, loss = 1.92 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:43.888545: step 170720, loss = 1.83 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:45.083327: step 170730, loss = 1.81 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:46.279693: step 170740, loss = 1.77 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:47.470139: step 170750, loss = 1.93 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:48.659037: step 170760, loss = 1.91 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:49.828829: step 170770, loss = 1.83 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:27:51.008463: step 170780, loss = 1.76 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:27:52.193640: step 170790, loss = 1.73 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:53.380476: step 170800, loss = 1.95 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:54.558749: step 170810, loss = 1.80 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:27:55.732605: step 170820, loss = 1.93 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:27:56.910818: step 170830, loss = 1.89 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:27:58.078059: step 170840, loss = 1.86 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:27:59.252001: step 170850, loss = 2.01 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:28:00.441039: step 170860, loss = 2.03 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:01.598044: step 170870, loss = 1.85 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:02.805641: step 170880, loss = 1.77 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:28:03.961906: step 170890, loss = 1.77 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:05.136616: step 170900, loss = 2.02 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:28:06.293527: step 170910, loss = 2.01 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:07.495115: step 170920, loss = 1.87 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:28:08.654080: step 170930, loss = 1.88 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:09.839079: step 170940, loss = 1.86 (1080.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:11.027841: step 170950, loss = 1.91 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:12.205988: step 170960, loss = 1.91 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:28:13.378540: step 170970, loss = 1.80 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:28:14.543272: step 170980, loss = 1.84 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:15.715487: step 170990, loss = 1.95 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:28:16.902982: step 171000, loss = 1.82 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:18.063280: step 171010, loss = 2.00 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:19.248266: step 171020, loss = 1.85 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:20.415950: step 171030, loss = 1.76 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:28:21.566413: step 171040, loss = 1.78 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:28:22.728972: step 171050, loss = 1.78 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:23.918024: step 171060, loss = 2.04 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:25.097200: step 171070, loss = 1.97 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:28:26.274861: step 171080, loss = 1.83 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:28:27.446310: step 171090, loss = 1.86 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:28:28.636728: step 171100, loss = 1.81 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:29.808587: step 171110, loss = 1.80 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:28:30.975901: step 171120, loss = 1.87 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:28:32.171905: step 171130, loss = 1.76 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:28:33.351338: step 171140, loss = 1.83 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:28:34.518455: step 171150, loss = 1.98 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:28:35.712900: step 171160, loss = 2.02 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:36.891376: step 171170, loss = 1.83 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:28:38.071694: step 171180, loss = 1.91 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:28:39.240969: step 171190, loss = 1.86 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:28:40.395189: step 171200, loss = 1.96 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:28:41.566992: step 171210, loss = 1.87 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:28:42.751874: step 171220, loss = 1.87 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:28:43.938710: step 171230, loss = 1.85 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:45.129647: step 171240, loss = 1.95 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:46.291956: step 171250, loss = 1.82 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:47.449548: step 171260, loss = 1.86 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:48.701395: step 171270, loss = 1.79 (1022.5 examples/sec; 0.125 sec/batch)
2017-05-05 03:28:49.789976: step 171280, loss = 1.92 (1175.8 examples/sec; 0.109 sec/batch)
2017-05-05 03:28:50.968149: step 171290, loss = 1.88 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:28:52.129492: step 171300, loss = 1.91 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:53.292554: step 171310, loss = 1.94 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:54.447568: step 171320, loss = 1.77 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:55.640118: step 171330, loss = 1.76 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:56.845300: step 171340, loss = 1.93 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:28:58.023733: step 171350, loss = 1.83 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:28:59.203890: step 171360, loss = 1.79 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:00.406637: step 171370, loss = 1.72 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:29:01.592468: step 171380, loss = 2.05 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:02.770883: step 171390, loss = 1.95 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:03.983476: step 171400, loss = 1.76 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:29:05.163668: step 171410, loss = 2.05 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:06.353694: step 171420, loss = 1.84 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:07.548621: step 171430, loss = 1.87 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:08.732553: step 171440, loss = 1.88 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:09.890192: step 171450, loss = 1.89 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:11.069053: step 171460, loss = 1.85 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:12.252511: step 171470, loss = 1.92 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:13.417545: step 171480, loss = 2.09 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:29:14.585148: step 171490, loss = 1.83 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:29:15.754192: step 171500, loss = 1.81 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:29:16.935018: step 171510, loss = 1.99 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:18.084603: step 171520, loss = 1.90 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:29:19.251659: step 171530, loss = 1.84 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:29:20.422977: step 171540, loss = 1.81 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:29:21.578455: step 171550, loss = 1.87 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:22.737050: step 171560, loss = 2.03 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:23.899279: step 171570, loss = 2.01 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:25.061580: step 171580, loss = 1.94 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:26.216541: step 171590, loss = 1.90 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:29:27.387919: step 171600, loss = 2.06 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:29:28.562677: step 171610, loss = 1.87 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:29:29.709682: step 171620, loss = 1.88 (1116.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:29:30.877694: step 171630, loss = 1.90 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:29:32.054457: step 171640, loss = 2.00 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:33.234861: step 171650, loss = 1.73 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:34.394936: step 171660, loss = 1.92 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:35.573598: step 171670, loss = 1.90 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:36.755472: step 171680, loss = 1.92 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:37.895389: step 171690, loss = 1.82 (1122.9 examples/sec; 0.114 sec/batch)
2017-05-05 03:29:39.049053: step 171700, loss = 1.87 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:29:40.250163: step 171710, loss = 1.73 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:29:41.409516: step 171720, loss = 1.80 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:42.573855: step 171730, loss = 1.94 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:43.764877: step 171740, loss = 1.93 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:44.946373: step 171750, loss = 1.85 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:46.137659: step 171760, loss = 1.69 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:47.331855: step 171770, loss = 1.97 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:48.532457: step 171780, loss = 1.75 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:29:49.719609: step 171790, loss = 1.80 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:50.919974: step 171800, loss = 1.93 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:29:52.134926: step 171810, loss = 1.85 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:29:53.342175: step 171820, loss = 1.88 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:29:54.527215: step 171830, loss = 1.84 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:55.731018: step 171840, loss = 2.13 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:29:56.936871: step 171850, loss = 1.86 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:29:58.130268: step 171860, loss = 1.79 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:59.311799: step 171870, loss = 1.81 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:00.489561: step 171880, loss = 1.92 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:01.674199: step 171890, loss = 1.90 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:02.871555: step 171900, loss = 1.94 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:30:04.071657: step 171910, loss = 2.04 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:30:05.270594: step 171920, loss = 2.03 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:30:06.447769: step 171930, loss = 1.82 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:07.677391: step 171940, loss = 1.94 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:30:08.890276: step 171950, loss = 1.79 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:30:10.095037: step 171960, loss = 1.79 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:30:11.312069: step 171970, loss = 1.89 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:30:12.521037: step 171980, loss = 1.79 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:30:13.727196: step 171990, loss = 1.71 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:30:14.939879: step 172000, loss = 1.92 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:30:16.153316: step 172010, loss = 1.77 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:30:17.371311: step 172020, loss = 2.08 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:30:18.568682: step 172030, loss = 1.86 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:30:19.773044: step 172040, loss = 1.82 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:30:20.978668: step 172050, loss = 2.03 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:30:22.177392: step 172060, loss = 1.90 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:30:23.372725: step 172070, loss = 1.90 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:30:24.580794: step 172080, loss = 1.99 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:30:25.774810: step 172090, loss = 1.94 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:30:26.963708: step 172100, loss = 1.94 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:30:28.144056: step 172110, loss = 1.91 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:29.351031: step 172120, loss = 1.85 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:30:30.538853: step 172130, loss = 1.89 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:30:31.731208: step 172140, loss = 1.91 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:30:32.905865: step 172150, loss = 1.82 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:34.081644: step 172160, loss = 1.85 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:35.255312: step 172170, loss = 1.92 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:36.435562: step 172180, loss = 1.77 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:37.616650: step 172190, loss = 1.89 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:38.784323: step 172200, loss = 1.84 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:39.967036: step 172210, loss = 1.96 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:41.138968: step 172220, loss = 1.95 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:42.299299: step 172230, loss = 2.14 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:30:43.455003: step 172240, loss = 1.94 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:30:44.606261: step 172250, loss = 1.86 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:30:45.857491: step 172260, loss = 1.87 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-05 03:30:46.932749: step 172270, loss = 1.73 (1190.4 examples/sec; 0.108 sec/batch)
2017-05-05 03:30:48.101481: step 172280, loss = 1.78 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:49.242279: step 172290, loss = 1.91 (1122.0 examples/sec; 0.114 sec/batch)
2017-05-05 03:30:50.403512: step 172300, loss = 1.92 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:30:51.569367: step 172310, loss = 1.92 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:52.738174: step 172320, loss = 1.87 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:53.891294: step 172330, loss = 1.78 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:30:55.075369: step 172340, loss = 1.78 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:56.240317: step 172350, loss = 1.95 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:30:57.414324: step 172360, loss = 1.80 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:58.579695: step 172370, loss = 1.97 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:59.743522: step 172380, loss = 1.81 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:31:00.923339: step 172390, loss = 1.87 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:31:02.062433: step 172400, loss = 1.80 (1123.7 examples/sec; 0.114 sec/batch)
2017-05-05 03:31:03.229874: step 172410, loss = 2.01 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:31:04.405161: step 172420, loss = 1.78 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:31:05.574013: step 172430, loss = 1.87 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:31:06.757540: step 172440, loss = 1.94 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:31:07.921423: step 172450, loss = 2.11 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:31:09.089920: step 172460, loss = 1.78 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:31:10.234275: step 172470, loss = 1.89 (1118.5 examples/sec; 0.114 sec/batch)
2017-05-05 03:31:11.393845: step 172480, loss = 1.94 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:31:12.573567: step 172490, loss = 1.93 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:31:13.730480: step 172500, loss = 1.77 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:31:14.913291: step 172510, loss = 1.77 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:31:16.110502: step 172520, loss = 1.90 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:17.296347: step 172530, loss = 1.95 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:31:18.501088: step 172540, loss = 1.80 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:19.717141: step 172550, loss = 1.92 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:31:20.911467: step 172560, loss = 1.92 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:31:22.092589: step 172570, loss = 1.84 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:31:23.327339: step 172580, loss = 1.98 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:31:24.539394: step 172590, loss = 1.85 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:31:25.746159: step 172600, loss = 1.79 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:31:26.955593: step 172610, loss = 1.82 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:31:28.144310: step 172620, loss = 1.80 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:31:29.343436: step 172630, loss = 1.80 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:30.527084: step 172640, loss = 1.75 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:31:31.706511: step 172650, loss = 1.97 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:31:32.870466: step 172660, loss = 1.96 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:31:34.046827: step 172670, loss = 1.74 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:31:35.250562: step 172680, loss = 1.94 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:36.435272: step 172690, loss = 1.75 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:31:37.636872: step 172700, loss = 1.93 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:38.845338: step 172710, loss = 1.77 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:31:40.030560: step 172720, loss = 1.87 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:31:41.224839: step 172730, loss = 2.06 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:31:42.395872: step 172740, loss = 1.75 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:31:43.589876: step 172750, loss = 1.86 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:31:44.770410: step 172760, loss = 1.84 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:31:45.964091: step 172770, loss = 1.92 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:31:47.170186: step 172780, loss = 1.85 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:31:48.405423: step 172790, loss = 1.86 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-05 03:31:49.601287: step 172800, loss = 1.77 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:50.791939: step 172810, loss = 1.87 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:31:52.008761: step 172820, loss = 1.86 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:31:53.202593: step 172830, loss = 1.81 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:31:54.401023: step 172840, loss = 1.83 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:55.597289: step 172850, loss = 1.79 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:56.818577: step 172860, loss = 1.83 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:31:58.021428: step 172870, loss = 1.95 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:59.234283: step 172880, loss = 2.04 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:32:00.455750: step 172890, loss = 2.04 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:32:01.644915: step 172900, loss = 1.80 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:02.843576: step 172910, loss = 1.89 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:32:04.033103: step 172920, loss = 2.02 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:05.232377: step 172930, loss = 2.12 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:32:06.418319: step 172940, loss = 1.85 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:07.608299: step 172950, loss = 1.84 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:08.806470: step 172960, loss = 1.89 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:32:09.995700: step 172970, loss = 1.93 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:11.197836: step 172980, loss = 1.87 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:32:12.391697: step 172990, loss = 1.84 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:13.568943: step 173000, loss = 1.74 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:32:14.740030: step 173010, loss = 2.01 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:32:15.918835: step 173020, loss = 1.99 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:32:17.107570: step 173030, loss = 1.88 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:18.292780: step 173040, loss = 1.98 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:19.457780: step 173050, loss = 1.77 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:32:20.646029: step 173060, loss = 1.87 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:21.808070: step 173070, loss = 1.77 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:32:22.983112: step 173080, loss = 1.92 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:32:24.173539: step 173090, loss = 1.90 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:25.339100: step 173100, loss = 1.84 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:32:26.525120: step 173110, loss = 1.94 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:27.696336: step 173120, loss = 1.80 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:32:28.879631: step 173130, loss = 1.92 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:32:30.024415: step 173140, loss = 1.89 (1118.1 examples/sec; 0.114 sec/batch)
2017-05-05 03:32:31.198413: step 173150, loss = 1.78 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:32:32.366754: step 173160, loss = 1.88 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:32:33.531415: step 173170, loss = 1.81 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:32:34.703602: step 173180, loss = 1.80 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:32:35.897018: step 173190, loss = 1.85 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:37.084684: step 173200, loss = 2.02 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:38.277430: step 173210, loss = 1.72 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:39.486838: step 173220, loss = 1.72 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:32:40.688539: step 173230, loss = 1.76 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:32:41.879187: step 173240, loss = 1.80 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:43.187567: step 173250, loss = 1.87 (978.3 examples/sec; 0.131 sec/batch)
2017-05-05 03:32:44.286882: step 173260, loss = 2.04 (1164.4 examples/sec; 0.110 sec/batch)
2017-05-05 03:32:45.495973: step 173270, loss = 1.75 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:32:46.699534: step 173280, loss = 1.80 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:32:47.902589: step 173290, loss = 1.81 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:32:49.116630: step 173300, loss = 1.78 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:32:50.319597: step 173310, loss = 1.73 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:32:51.511879: step 173320, loss = 1.84 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:52.720107: step 173330, loss = 1.84 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:32:53.898097: step 173340, loss = 1.95 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:32:55.090651: step 173350, loss = 1.79 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:56.270271: step 173360, loss = 1.81 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:32:57.435450: step 173370, loss = 1.91 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:32:58.601309: step 173380, loss = 1.92 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:32:59.768199: step 173390, loss = 2.04 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:00.964479: step 173400, loss = 1.79 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:33:02.110865: step 173410, loss = 1.97 (1116.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:33:03.270462: step 173420, loss = 1.96 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:04.439175: step 173430, loss = 1.71 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:05.613417: step 173440, loss = 1.83 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:06.803439: step 173450, loss = 1.80 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:33:07.959992: step 173460, loss = 1.80 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:09.122440: step 173470, loss = 1.76 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:10.301296: step 173480, loss = 1.84 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:11.475637: step 173490, loss = 1.73 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:12.631574: step 173500, loss = 1.89 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:13.786400: step 173510, loss = 1.75 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:33:14.968027: step 173520, loss = 1.72 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:16.128656: step 173530, loss = 1.76 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:17.280223: step 173540, loss = 1.81 (1111.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:33:18.445748: step 173550, loss = 1.85 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:19.626641: step 173560, loss = 1.79 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:20.805511: step 173570, loss = 1.93 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:21.942891: step 173580, loss = 1.91 (1125.4 examples/sec; 0.114 sec/batch)
2017-05-05 03:33:23.108524: step 173590, loss = 1.80 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:24.269862: step 173600, loss = 1.78 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:25.438910: step 173610, loss = 1.77 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:26.605953: step 173620, loss = 1.82 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:27.781054: step 173630, loss = 1.78 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:28.974958: step 173640, loss = 1.85 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:33:30.145797: step 173650, loss = 1.73 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:31.339662: step 173660, loss = 1.83 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:33:32.511474: step 173670, loss = 1.79 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:33.691268: step 173680, loss = 1.71 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:34.867705: step 173690, loss = 1.73 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:36.048156: step 173700, loss = 1.82 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:37.206223: step 173710, loss = 1.93 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:38.363742: step 173720, loss = 1.81 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:39.532595: step 173730, loss = 1.78 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:40.695848: step 173740, loss = 1.92 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:41.852170: step 173750, loss = 1.96 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:43.034731: step 173760, loss = 1.91 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:44.193185: step 173770, loss = 1.78 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:45.371776: step 173780, loss = 1.96 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:46.515985: step 173790, loss = 1.82 (1118.7 examples/sec; 0.114 sec/batch)
2017-05-05 03:33:47.673898: step 173800, loss = 1.87 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:48.839564: step 173810, loss = 1.70 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:49.998888: step 173820, loss = 1.95 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:51.162819: step 173830, loss = 1.92 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:52.319531: step 173840, loss = 1.76 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:53.498555: step 173850, loss = 1.95 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:54.652397: step 173860, loss = 1.98 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:33:55.809057: step 173870, loss = 2.17 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:56.986389: step 173880, loss = 1.98 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:58.151812: step 173890, loss = 2.04 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:59.313183: step 173900, loss = 1.85 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:00.483109: step 173910, loss = 1.97 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:01.647472: step 173920, loss = 1.87 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:02.822360: step 173930, loss = 2.00 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:04.004611: step 173940, loss = 1.85 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:05.184687: step 173950, loss = 2.11 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:06.354600: step 173960, loss = 1.84 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:07.538137: step 173970, loss = 1.87 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:08.727163: step 173980, loss = 1.98 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:34:09.887461: step 173990, loss = 1.96 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:11.053163: step 174000, loss = 1.78 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:12.233020: step 174010, loss = 1.95 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:13.396359: step 174020, loss = 1.80 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:14.564036: step 174030, loss = 1.87 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:15.742304: step 174040, loss = 1.77 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:16.912991: step 174050, loss = 1.99 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:18.082484: step 174060, loss = 1.97 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:19.262601: step 174070, loss = 1.91 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:20.451296: step 174080, loss = 1.85 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:34:21.613896: step 174090, loss = 2.01 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:22.781147: step 174100, loss = 1.92 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:23.943793: step 174110, loss = 1.88 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:25.113765: step 174120, loss = 1.87 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:26.283656: step 174130, loss = 1.87 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:27.442029: step 174140, loss = 2.13 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:28.602180: step 174150, loss = 1.91 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:29.743919: step 174160, loss = 1.77 (1121.1 examples/sec; 0.114 sec/batch)
2017-05-05 03:34:30.912550: step 174170, loss = 2.08 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:32.077407: step 174180, loss = 2.06 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:33.245903: step 174190, loss = 1.86 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:34.419899: step 174200, loss = 2.07 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:35.585632: step 174210, loss = 1.87 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:36.760881: step 174220, loss = 1.76 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:37.902530: step 174230, loss = 1.80 (1121.2 examples/sec; 0.114 sec/batch)
2017-05-05 03:34:39.167526: step 174240, loss = 1.80 (1011.9 examples/sec; 0.127 sec/batch)
2017-05-05 03:34:40.242157: step 174250, loss = 1.85 (1191.1 examples/sec; 0.107 sec/batch)
2017-05-05 03:34:41.428408: step 174260, loss = 1.83 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:34:42.618723: step 174270, loss = 1.79 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:34:43.781099: step 174280, loss = 1.96 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:44.968656: step 174290, loss = 2.15 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:34:46.134804: step 174300, loss = 1.92 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:47.314861: step 174310, loss = 1.81 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:48.475964: step 174320, loss = 1.86 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:49.623002: step 174330, loss = 1.87 (1115.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:34:50.795342: step 174340, loss = 1.81 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:51.972990: step 174350, loss = 1.97 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:53.126789: step 174360, loss = 1.94 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:34:54.285109: step 174370, loss = 1.93 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:55.464781: step 174380, loss = 1.83 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:56.620169: step 174390, loss = 1.90 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:57.783682: step 174400, loss = 1.92 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:58.968604: step 174410, loss = 1.83 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:00.135728: step 174420, loss = 1.74 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:01.299677: step 174430, loss = 1.82 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:02.482919: step 174440, loss = 1.86 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:03.647703: step 174450, loss = 1.77 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:04.833446: step 174460, loss = 1.98 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:35:05.985443: step 174470, loss = 1.97 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:35:07.142693: step 174480, loss = 1.71 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:08.324781: step 174490, loss = 1.96 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:09.497156: step 174500, loss = 1.99 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:10.696053: step 174510, loss = 1.89 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:35:11.883365: step 174520, loss = 1.74 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:35:13.066082: step 174530, loss = 1.88 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:14.233318: step 174540, loss = 1.77 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:15.412431: step 174550, loss = 1.81 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:16.556608: step 174560, loss = 1.92 (1118.7 examples/sec; 0.114 sec/batch)
2017-05-05 03:35:17.707504: step 174570, loss = 2.06 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:35:18.858006: step 174580, loss = 1.86 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:35:20.014605: step 174590, loss = 1.79 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:21.192619: step 174600, loss = 1.91 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:22.335275: step 174610, loss = 1.85 (1120.2 examples/sec; 0.114 sec/batch)
2017-05-05 03:35:23.503980: step 174620, loss = 1.91 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:24.694651: step 174630, loss = 1.86 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:35:25.843478: step 174640, loss = 1.78 (1114.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:35:27.029515: step 174650, loss = 1.88 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:35:28.201349: step 174660, loss = 1.78 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:29.367995: step 174670, loss = 1.88 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:30.529556: step 174680, loss = 1.94 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:31.692018: step 174690, loss = 1.83 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:32.867965: step 174700, loss = 1.81 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:34.022335: step 174710, loss = 1.85 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:35:35.193366: step 174720, loss = 2.03 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:36.367913: step 174730, loss = 1.90 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:37.540436: step 174740, loss = 1.77 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:38.687192: step 174750, loss = 1.85 (1116.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:35:39.841834: step 174760, loss = 1.83 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:35:41.012999: step 174770, loss = 1.83 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:42.170777: step 174780, loss = 1.82 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:43.337686: step 174790, loss = 1.86 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:44.508197: step 174800, loss = 1.91 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:45.659518: step 174810, loss = 1.93 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:35:46.833714: step 174820, loss = 1.77 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:48.017762: step 174830, loss = 1.82 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:49.217028: step 174840, loss = 1.88 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:35:50.384176: step 174850, loss = 2.04 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:51.546652: step 174860, loss = 2.02 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:52.724944: step 174870, loss = 1.98 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:53.883550: step 174880, loss = 1.91 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:55.057983: step 174890, loss = 1.99 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:56.237515: step 174900, loss = 1.71 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:57.396491: step 174910, loss = 1.77 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:58.559634: step 174920, loss = 1.97 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:59.744476: step 174930, loss = 1.97 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:00.931421: step 174940, loss = 1.87 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:02.102482: step 174950, loss = 1.83 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:36:03.291695: step 174960, loss = 1.86 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:04.498542: step 174970, loss = 1.84 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:36:05.692817: step 174980, loss = 1.90 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:06.881131: step 174990, loss = 1.84 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:08.064008: step 175000, loss = 2.04 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:09.256385: step 175010, loss = 1.92 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:10.431304: step 175020, loss = 1.74 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:36:11.622173: step 175030, loss = 1.99 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:12.816292: step 175040, loss = 1.80 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:13.991569: step 175050, loss = 1.82 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:15.179017: step 175060, loss = 2.05 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:16.372644: step 175070, loss = 2.04 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:17.562476: step 175080, loss = 1.82 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:18.755704: step 175090, loss = 1.78 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:19.963434: step 175100, loss = 1.79 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:36:21.176984: step 175110, loss = 1.84 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:36:22.381986: step 175120, loss = 1.79 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:36:23.590091: step 175130, loss = 1.89 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:36:24.810847: step 175140, loss = 1.94 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:36:25.984957: step 175150, loss = 1.77 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:36:27.192966: step 175160, loss = 1.89 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:36:28.397837: step 175170, loss = 1.99 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:36:29.571294: step 175180, loss = 1.76 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:36:30.741209: step 175190, loss = 1.77 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:36:31.919953: step 175200, loss = 2.04 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:33.078752: step 175210, loss = 1.79 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:36:34.240690: step 175220, loss = 1.76 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:36:35.486298: step 175230, loss = 1.84 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-05 03:36:36.552036: step 175240, loss = 1.80 (1201.0 examples/sec; 0.107 sec/batch)
2017-05-05 03:36:37.710829: step 175250, loss = 1.97 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:36:38.888836: step 175260, loss = 1.93 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:40.051507: step 175270, loss = 2.01 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:36:41.221722: step 175280, loss = 1.83 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:36:42.390142: step 175290, loss = 2.02 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:36:43.566285: step 175300, loss = 1.82 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:44.744394: step 175310, loss = 1.77 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:45.896706: step 175320, loss = 1.85 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:36:47.048285: step 175330, loss = 1.80 (1111.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:36:48.229343: step 175340, loss = 1.97 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:49.416796: step 175350, loss = 1.79 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:50.588750: step 175360, loss = 1.67 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:36:51.771848: step 175370, loss = 1.81 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:52.925515: step 175380, loss = 1.87 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:36:54.082025: step 175390, loss = 1.86 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:36:55.265431: step 175400, loss = 1.70 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:56.437422: step 175410, loss = 1.87 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:36:57.602049: step 175420, loss = 1.75 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:36:58.784618: step 175430, loss = 1.91 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:59.971861: step 175440, loss = 1.89 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:01.171051: step 175450, loss = 1.90 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:37:02.343023: step 175460, loss = 1.79 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:03.515708: step 175470, loss = 1.93 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:04.700444: step 175480, loss = 1.87 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:37:05.892987: step 175490, loss = 1.71 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:07.086582: step 175500, loss = 2.00 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:08.278669: step 175510, loss = 1.79 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:09.437047: step 175520, loss = 2.02 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:10.623202: step 175530, loss = 1.74 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:11.824721: step 175540, loss = 1.88 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:37:13.017414: step 175550, loss = 1.81 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:14.206594: step 175560, loss = 1.80 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:15.395931: step 175570, loss = 1.96 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:16.580675: step 175580, loss = 1.89 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:37:17.736524: step 175590, loss = 1.96 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:18.918961: step 175600, loss = 1.87 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:37:20.084672: step 175610, loss = 1.74 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:21.247737: step 175620, loss = 1.93 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:22.419699: step 175630, loss = 1.89 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:23.619041: step 175640, loss = 1.95 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:37:24.776923: step 175650, loss = 1.83 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:25.917880: step 175660, loss = 1.89 (1121.9 examples/sec; 0.114 sec/batch)
2017-05-05 03:37:27.091423: step 175670, loss = 1.97 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:28.257551: step 175680, loss = 1.82 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:29.408872: step 175690, loss = 1.84 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:37:30.576071: step 175700, loss = 1.86 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:31.739896: step 175710, loss = 1.93 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:32.902085: step 175720, loss = 1.82 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:34.070653: step 175730, loss = 1.71 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:35.235635: step 175740, loss = 1.78 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:36.410515: step 175750, loss = 1.80 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:37.568205: step 175760, loss = 1.77 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:38.742066: step 175770, loss = 1.93 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:39.934688: step 175780, loss = 1.76 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:41.127670: step 175790, loss = 1.72 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:42.290332: step 175800, loss = 1.90 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:43.449145: step 175810, loss = 1.72 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:44.613056: step 175820, loss = 1.71 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:45.778005: step 175830, loss = 1.89 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:46.949720: step 175840, loss = 1.83 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:48.110817: step 175850, loss = 1.88 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:49.283577: step 175860, loss = 1.76 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:50.461107: step 175870, loss = 1.91 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:37:51.631089: step 175880, loss = 1.84 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:52.817714: step 175890, loss = 1.67 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:53.989222: step 175900, loss = 2.01 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:55.154877: step 175910, loss = 1.97 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:56.333390: step 175920, loss = 1.81 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:37:57.499416: step 175930, loss = 1.83 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:58.685517: step 175940, loss = 1.88 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:59.842997: step 175950, loss = 1.87 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:38:01.015925: step 175960, loss = 1.99 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:38:02.188510: step 175970, loss = 2.05 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:38:03.351843: step 175980, loss = 1.92 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:38:04.553097: step 175990, loss = 1.86 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:38:05.728330: step 176000, loss = 1.86 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:06.924528: step 176010, loss = 1.84 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:38:08.119219: step 176020, loss = 1.79 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:09.314431: step 176030, loss = 1.87 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:38:10.500792: step 176040, loss = 1.90 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:11.673542: step 176050, loss = 1.89 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:38:12.852574: step 176060, loss = 1.90 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:14.037682: step 176070, loss = 1.92 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:15.243385: step 176080, loss = 1.92 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:38:16.424994: step 176090, loss = 1.79 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:17.603576: step 176100, loss = 1.72 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:18.803794: step 176110, loss = 1.86 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:38:20.014839: step 176120, loss = 1.96 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:38:21.224376: step 176130, loss = 1.84 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:38:22.430908: step 176140, loss = 1.77 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:38:23.606090: step 176150, loss = 1.97 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:24.811170: step 176160, loss = 1.86 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:38:25.994030: step 176170, loss = 1.90 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:27.189397: step 176180, loss = 1.81 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:38:28.372266: step 176190, loss = 1.80 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:29.547872: step 176200, loss = 1.99 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:30.735433: step 176210, loss = 1.84 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:32.026319: step 176220, loss = 1.86 (991.6 examples/sec; 0.129 sec/batch)
2017-05-05 03:38:33.125708: step 176230, loss = 1.83 (1164.3 examples/sec; 0.110 sec/batch)
2017-05-05 03:38:34.345807: step 176240, loss = 1.92 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:38:35.526631: step 176250, loss = 1.94 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:36.708321: step 176260, loss = 1.84 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:37.890277: step 176270, loss = 1.86 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:39.069367: step 176280, loss = 1.91 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:40.245840: step 176290, loss = 1.77 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:41.418909: step 176300, loss = 1.89 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:38:42.577051: step 176310, loss = 1.88 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:38:43.757731: step 176320, loss = 1.80 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:44.943523: step 176330, loss = 1.92 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:46.115372: step 176340, loss = 1.89 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:38:47.289583: step 176350, loss = 1.83 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:38:48.476902: step 176360, loss = 1.90 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:49.668617: step 176370, loss = 1.84 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:50.854981: step 176380, loss = 1.91 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:52.046389: step 176390, loss = 1.97 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:53.237771: step 176400, loss = 1.88 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:54.439921: step 176410, loss = 1.87 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:38:55.661308: step 176420, loss = 1.83 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:38:56.877902: step 176430, loss = 1.91 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:38:58.063086: step 176440, loss = 1.77 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:59.251999: step 176450, loss = 1.80 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:39:00.434116: step 176460, loss = 2.01 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:39:01.613137: step 176470, loss = 1.80 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:39:02.799054: step 176480, loss = 1.81 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:39:03.955431: step 176490, loss = 1.75 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:05.118418: step 176500, loss = 2.05 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:06.276729: step 176510, loss = 2.04 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:07.443173: step 176520, loss = 1.75 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:08.616166: step 176530, loss = 1.69 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:09.772559: step 176540, loss = 1.88 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:10.938875: step 176550, loss = 1.90 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:12.108807: step 176560, loss = 2.02 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:13.272440: step 176570, loss = 1.89 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:14.420736: step 176580, loss = 1.90 (1114.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:39:15.606955: step 176590, loss = 1.83 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:39:16.766081: step 176600, loss = 1.75 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:17.909949: step 176610, loss = 1.92 (1119.0 examples/sec; 0.114 sec/batch)
2017-05-05 03:39:19.066373: step 176620, loss = 1.75 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:20.226834: step 176630, loss = 1.85 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:21.383759: step 176640, loss = 1.82 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:22.554826: step 176650, loss = 1.88 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:23.729663: step 176660, loss = 1.80 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:24.904928: step 176670, loss = 1.79 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:39:26.062336: step 176680, loss = 1.77 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:27.231468: step 176690, loss = 1.85 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:28.396899: step 176700, loss = 1.78 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:29.546144: step 176710, loss = 1.87 (1113.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:39:30.705762: step 176720, loss = 1.92 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:31.861841: step 176730, loss = 1.86 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:33.032215: step 176740, loss = 1.81 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:34.197316: step 176750, loss = 1.88 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:35.360836: step 176760, loss = 1.95 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:36.524300: step 176770, loss = 1.84 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:37.680400: step 176780, loss = 1.98 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:38.867805: step 176790, loss = 1.98 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:39:40.052825: step 176800, loss = 2.02 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:39:41.225611: step 176810, loss = 1.97 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:42.395176: step 176820, loss = 1.99 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:43.554942: step 176830, loss = 1.85 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:44.725637: step 176840, loss = 1.89 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:45.893312: step 176850, loss = 1.79 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:47.073434: step 176860, loss = 2.11 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:39:48.244138: step 176870, loss = 1.84 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:49.399524: step 176880, loss = 1.83 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:50.553927: step 176890, loss = 1.76 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:39:51.716362: step 176900, loss = 1.68 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:52.871558: step 176910, loss = 1.68 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:54.022914: step 176920, loss = 1.87 (1111.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:39:55.172877: step 176930, loss = 1.94 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:39:56.339641: step 176940, loss = 1.86 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:57.508924: step 176950, loss = 1.83 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:58.666956: step 176960, loss = 1.81 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:59.843592: step 176970, loss = 1.78 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:01.010576: step 176980, loss = 1.85 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:02.166656: step 176990, loss = 1.91 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:03.328377: step 177000, loss = 1.92 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:04.505303: step 177010, loss = 1.98 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:05.648792: step 177020, loss = 1.89 (1119.4 examples/sec; 0.114 sec/batch)
2017-05-05 03:40:06.801320: step 177030, loss = 1.91 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:40:07.969678: step 177040, loss = 1.87 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:09.153801: step 177050, loss = 1.91 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:10.318635: step 177060, loss = 1.76 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:11.484294: step 177070, loss = 1.91 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:12.649349: step 177080, loss = 1.97 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:13.804977: step 177090, loss = 1.91 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:14.986868: step 177100, loss = 2.11 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:16.147674: step 177110, loss = 1.76 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:17.313664: step 177120, loss = 1.94 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:18.492028: step 177130, loss = 1.72 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:19.676355: step 177140, loss = 1.84 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:20.856206: step 177150, loss = 2.05 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:22.014958: step 177160, loss = 1.84 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:23.184703: step 177170, loss = 2.02 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:24.372488: step 177180, loss = 1.81 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:40:25.554623: step 177190, loss = 1.87 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:26.733381: step 177200, loss = 1.70 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:28.014773: step 177210, loss = 1.91 (998.9 examples/sec; 0.128 sec/batch)
2017-05-05 03:40:29.107463: step 177220, loss = 2.00 (1171.4 examples/sec; 0.109 sec/batch)
2017-05-05 03:40:30.282058: step 177230, loss = 1.89 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:31.443391: step 177240, loss = 1.85 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:32.640084: step 177250, loss = 1.91 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:40:33.798333: step 177260, loss = 1.94 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:34.979005: step 177270, loss = 1.93 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:36.151663: step 177280, loss = 1.92 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:37.313759: step 177290, loss = 1.70 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:38.459004: step 177300, loss = 1.77 (1117.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:40:39.629619: step 177310, loss = 2.03 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:40.785602: step 177320, loss = 1.86 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:41.933318: step 177330, loss = 1.93 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:40:43.101290: step 177340, loss = 1.89 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:44.262274: step 177350, loss = 1.89 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:45.417033: step 177360, loss = 1.76 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:40:46.580687: step 177370, loss = 1.93 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:47.761911: step 177380, loss = 2.04 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:48.935402: step 177390, loss = 1.94 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:50.091502: step 177400, loss = 1.93 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:51.263613: step 177410, loss = 1.88 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:52.466861: step 177420, loss = 1.77 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:40:53.637597: step 177430, loss = 1.87 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:54.803928: step 177440, loss = 2.02 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:55.952122: step 177450, loss = 1.96 (1114.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:40:57.125750: step 177460, loss = 1.74 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:58.271218: step 177470, loss = 1.99 (1117.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:40:59.437969: step 177480, loss = 1.85 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:41:00.583639: step 177490, loss = 1.87 (1117.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:41:01.755415: step 177500, loss = 1.69 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:41:02.931613: step 177510, loss = 2.05 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:41:04.098415: step 177520, loss = 1.71 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:41:05.252831: step 177530, loss = 1.94 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:41:06.434221: step 177540, loss = 1.90 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:41:07.588473: step 177550, loss = 1.83 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:41:08.754816: step 177560, loss = 1.77 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:41:09.919586: step 177570, loss = 1.71 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:41:11.082930: step 177580, loss = 1.97 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:41:12.245454: step 177590, loss = 1.84 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:41:13.401370: step 177600, loss = 1.76 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:41:14.571122: step 177610, loss = 1.83 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:41:15.766170: step 177620, loss = 1.89 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:41:16.962517: step 177630, loss = 1.93 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:41:18.135405: step 177640, loss = 1.84 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:41:19.315379: step 177650, loss = 1.70 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:41:20.513793: step 177660, loss = 1.93 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:41:21.700592: step 177670, loss = 1.88 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:41:22.889824: step 177680, loss = 1.97 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:41:24.101759: step 177690, loss = 1.92 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:41:25.322101: step 177700, loss = 1.73 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:41:26.528664: step 177710, loss = 1.94 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:41:27.764442: step 177720, loss = 1.72 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-05 03:41:28.976260: step 177730, loss = 1.94 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:41:30.189284: step 177740, loss = 1.88 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:41:31.414679: step 177750, loss = 2.00 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:41:32.667733: step 177760, loss = 1.89 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-05 03:41:33.897579: step 177770, loss = 1.74 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-05 03:41:35.086086: step 177780, loss = 1.83 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:41:36.316440: step 177790, loss = 1.82 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-05 03:41:37.518679: step 177800, loss = 1.88 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:41:38.692899: step 177810, loss = 1.78 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:41:39.896471: step 177820, loss = 1.91 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:41:41.118967: step 177830, loss = 1.79 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:41:42.311152: step 177840, loss = 1.72 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:41:43.535220: step 177850, loss = 1.91 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:41:44.748862: step 177860, loss = 1.89 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:41:45.974602: step 177870, loss = 1.84 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-05 03:41:47.166646: step 177880, loss = 1.97 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:41:48.418254: step 177890, loss = 2.02 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-05 03:41:49.644046: step 177900, loss = 1.86 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 03:41:50.892052: step 177910, loss = 1.91 (1025.6 examples/sec; 0.125 sec/batch)
2017-05-05 03:41:52.083599: step 177920, loss = 1.89 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:41:53.331932: step 177930, loss = 1.83 (1025.4 examples/sec; 0.125 sec/batch)
2017-05-05 03:41:54.544632: step 177940, loss = 1.75 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:41:55.778005: step 177950, loss = 1.96 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-05 03:41:57.008260: step 177960, loss = 1.87 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:41:58.251093: step 177970, loss = 1.95 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-05 03:41:59.438072: step 177980, loss = 1.82 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:00.672001: step 177990, loss = 1.86 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-05 03:42:01.860956: step 178000, loss = 1.83 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:03.081573: step 178010, loss = 1.78 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:42:04.254133: step 178020, loss = 1.81 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:42:05.479178: step 178030, loss = 1.85 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:42:06.693044: step 178040, loss = 1.86 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:42:07.911631: step 178050, loss = 1.88 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:42:09.114111: step 178060, loss = 1.85 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:42:10.342960: step 178070, loss = 1.88 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:42:11.570416: step 178080, loss = 2.04 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-05 03:42:12.783557: step 178090, loss = 1.89 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:42:13.965363: step 178100, loss = 1.90 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:15.193754: step 178110, loss = 1.88 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:42:16.405274: step 178120, loss = 1.74 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:42:17.618564: step 178130, loss = 1.92 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:42:18.832100: step 178140, loss = 1.88 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:42:20.062337: step 178150, loss = 1.79 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:42:21.266844: step 178160, loss = 1.77 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:42:22.477045: step 178170, loss = 2.03 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:42:23.651381: step 178180, loss = 1.82 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:42:24.829836: step 178190, loss = 1.83 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:26.102475: step 178200, loss = 1.84 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-05 03:42:27.185339: step 178210, loss = 1.87 (1182.1 examples/sec; 0.108 sec/batch)
2017-05-05 03:42:28.373076: step 178220, loss = 1.89 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:29.559454: step 178230, loss = 1.75 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:30.755639: step 178240, loss = 1.81 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:42:31.963847: step 178250, loss = 1.85 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:42:33.156889: step 178260, loss = 1.85 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:34.351663: step 178270, loss = 1.85 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:35.569345: step 178280, loss = 1.91 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:42:36.757575: step 178290, loss = 1.85 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:37.923756: step 178300, loss = 1.88 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:42:39.127000: step 178310, loss = 1.94 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:42:40.301521: step 178320, loss = 1.73 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:42:41.485558: step 178330, loss = 1.72 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:42.663890: step 178340, loss = 1.84 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:43.846991: step 178350, loss = 1.71 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:45.008465: step 178360, loss = 1.96 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:42:46.184131: step 178370, loss = 1.81 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:47.368021: step 178380, loss = 1.80 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:48.558803: step 178390, loss = 1.82 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:49.766887: step 178400, loss = 1.88 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:42:50.915612: step 178410, loss = 1.90 (1114.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:42:52.083558: step 178420, loss = 1.89 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:42:53.265643: step 178430, loss = 1.92 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:54.435103: step 178440, loss = 1.81 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:42:55.619699: step 178450, loss = 1.84 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:56.794728: step 178460, loss = 1.81 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:57.985060: step 178470, loss = 1.82 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:59.188684: step 178480, loss = 1.86 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:00.372821: step 178490, loss = 1.90 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:01.553417: step 178500, loss = 2.00 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:02.751793: step 178510, loss = 2.10 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:03.953483: step 178520, loss = 1.70 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:05.136670: step 178530, loss = 1.89 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:06.339494: step 178540, loss = 2.05 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:07.551775: step 178550, loss = 1.82 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:43:08.757408: step 178560, loss = 1.78 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:43:09.952593: step 178570, loss = 1.85 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:11.137680: step 178580, loss = 1.86 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:12.312054: step 178590, loss = 1.91 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:13.505727: step 178600, loss = 1.76 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:14.685679: step 178610, loss = 1.83 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:15.867217: step 178620, loss = 1.98 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:17.057255: step 178630, loss = 2.04 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:18.220692: step 178640, loss = 1.96 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:43:19.392743: step 178650, loss = 1.87 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:20.572502: step 178660, loss = 1.84 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:21.735301: step 178670, loss = 1.93 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:43:22.920063: step 178680, loss = 1.98 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:24.127581: step 178690, loss = 1.87 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:43:25.343168: step 178700, loss = 1.85 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:43:26.528183: step 178710, loss = 1.79 (1080.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:27.759941: step 178720, loss = 1.74 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-05 03:43:28.987292: step 178730, loss = 1.77 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:43:30.180408: step 178740, loss = 1.74 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:31.399082: step 178750, loss = 1.86 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:43:32.600144: step 178760, loss = 1.90 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:33.800107: step 178770, loss = 1.74 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:34.991591: step 178780, loss = 1.78 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:36.190045: step 178790, loss = 1.99 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:37.379975: step 178800, loss = 1.83 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:38.550206: step 178810, loss = 1.75 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:39.714810: step 178820, loss = 1.79 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:43:40.905641: step 178830, loss = 2.03 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:42.088437: step 178840, loss = 1.68 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:43.280459: step 178850, loss = 1.70 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:44.462871: step 178860, loss = 1.93 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:45.636428: step 178870, loss = 1.85 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:46.833477: step 178880, loss = 1.84 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:48.010214: step 178890, loss = 1.82 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:49.178308: step 178900, loss = 1.87 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:50.347743: step 178910, loss = 1.88 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:51.534833: step 178920, loss = 1.92 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:52.722393: step 178930, loss = 1.83 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:53.894163: step 178940, loss = 1.92 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:55.071948: step 178950, loss = 1.96 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:56.255367: step 178960, loss = 1.85 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:57.446751: step 178970, loss = 1.99 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:58.615473: step 178980, loss = 1.70 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:59.773810: step 178990, loss = 2.03 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:44:00.941035: step 179000, loss = 1.95 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:44:02.135036: step 179010, loss = 1.75 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:03.349834: step 179020, loss = 1.82 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:04.532581: step 179030, loss = 1.82 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:44:05.718097: step 179040, loss = 1.82 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:06.925140: step 179050, loss = 1.82 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:08.124835: step 179060, loss = 2.04 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:09.332298: step 179070, loss = 2.21 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:10.523666: step 179080, loss = 1.75 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:11.722215: step 179090, loss = 1.84 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:12.913281: step 179100, loss = 1.84 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:14.088217: step 179110, loss = 1.93 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:44:15.274963: step 179120, loss = 1.87 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:16.462714: step 179130, loss = 1.79 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:17.622286: step 179140, loss = 1.86 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:44:18.807296: step 179150, loss = 1.82 (1080.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:19.996619: step 179160, loss = 1.80 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:21.194083: step 179170, loss = 1.88 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:22.364981: step 179180, loss = 2.00 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:44:23.675913: step 179190, loss = 1.95 (976.4 examples/sec; 0.131 sec/batch)
2017-05-05 03:44:24.773747: step 179200, loss = 1.89 (1165.9 examples/sec; 0.110 sec/batch)
2017-05-05 03:44:25.957419: step 179210, loss = 1.88 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:44:27.167737: step 179220, loss = 1.92 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:28.371938: step 179230, loss = 1.93 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:29.588681: step 179240, loss = 1.91 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:44:30.809270: step 179250, loss = 1.84 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:44:32.017750: step 179260, loss = 1.86 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:33.237838: step 179270, loss = 1.94 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:44:34.475337: step 179280, loss = 1.71 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-05 03:44:35.685387: step 179290, loss = 1.81 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:36.893768: step 179300, loss = 1.88 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:38.111769: step 179310, loss = 1.78 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:44:39.317584: step 179320, loss = 1.92 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:40.519376: step 179330, loss = 1.93 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:41.740714: step 179340, loss = 1.76 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:44:42.958049: step 179350, loss = 1.83 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:44:44.159860: step 179360, loss = 1.97 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:45.397717: step 179370, loss = 1.90 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-05 03:44:46.603446: step 179380, loss = 1.95 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:47.893021: step 179390, loss = 1.72 (992.6 examples/sec; 0.129 sec/batch)
2017-05-05 03:44:49.010421: step 179400, loss = 1.81 (1145.5 examples/sec; 0.112 sec/batch)
2017-05-05 03:44:50.224130: step 179410, loss = 2.00 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:51.430679: step 179420, loss = 1.74 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:52.637544: step 179430, loss = 1.68 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:53.844979: step 179440, loss = 1.80 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:55.057903: step 179450, loss = 1.79 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:56.261114: step 179460, loss = 1.78 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:57.477153: step 179470, loss = 2.03 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:44:58.676778: step 179480, loss = 1.97 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:59.865986: step 179490, loss = 1.93 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:45:01.068153: step 179500, loss = 1.86 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:45:02.245816: step 179510, loss = 1.76 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:03.433649: step 179520, loss = 1.97 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:45:04.613306: step 179530, loss = 1.95 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:05.799997: step 179540, loss = 1.99 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:45:06.991119: step 179550, loss = 1.79 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:45:08.189353: step 179560, loss = 1.86 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:45:09.364307: step 179570, loss = 1.91 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:10.532454: step 179580, loss = 1.88 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:11.695709: step 179590, loss = 1.88 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:12.875859: step 179600, loss = 1.83 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:14.048177: step 179610, loss = 1.83 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:15.212699: step 179620, loss = 1.85 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:16.390878: step 179630, loss = 1.86 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:17.531442: step 179640, loss = 1.87 (1122.2 examples/sec; 0.114 sec/batch)
2017-05-05 03:45:18.703716: step 179650, loss = 1.79 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:19.905851: step 179660, loss = 1.86 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:45:21.077208: step 179670, loss = 1.95 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:22.228741: step 179680, loss = 1.78 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:45:23.396639: step 179690, loss = 1.84 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:24.566202: step 179700, loss = 1.96 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:25.744457: step 179710, loss = 1.85 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:26.918627: step 179720, loss = 1.77 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:28.102167: step 179730, loss = 1.94 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:29.293984: step 179740, loss = 1.87 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:45:30.459205: step 179750, loss = 1.94 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:31.622849: step 179760, loss = 1.73 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:32.821179: step 179770, loss = 1.80 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:45:34.012505: step 179780, loss = 1.85 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:45:35.197299: step 179790, loss = 1.79 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:36.390625: step 179800, loss = 1.74 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:45:37.564495: step 179810, loss = 1.88 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:38.744424: step 179820, loss = 2.03 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:39.932316: step 179830, loss = 1.80 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:45:41.099678: step 179840, loss = 1.67 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:42.258777: step 179850, loss = 1.97 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:43.425326: step 179860, loss = 1.85 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:44.624509: step 179870, loss = 1.86 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:45:45.792964: step 179880, loss = 1.84 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:46.954565: step 179890, loss = 1.94 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:48.154344: step 179900, loss = 1.74 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:45:49.312935: step 179910, loss = 1.85 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:50.495171: step 179920, loss = 1.81 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:51.668497: step 179930, loss = 1.98 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:52.829896: step 179940, loss = 1.77 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:53.990733: step 179950, loss = 1.92 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:55.144564: step 179960, loss = 1.96 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:45:56.312551: step 179970, loss = 1.86 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:57.465300: step 179980, loss = 1.95 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:45:58.633026: step 179990, loss = 1.68 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:59.769656: step 180000, loss = 1.76 (1126.1 examples/sec; 0.114 sec/batch)
2017-05-05 03:46:00.918083: step 180010, loss = 1.80 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:46:02.068770: step 180020, loss = 1.81 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:46:03.226113: step 180030, loss = 2.02 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:04.375681: step 180040, loss = 1.90 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:46:05.534160: step 180050, loss = 1.95 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:06.712004: step 180060, loss = 1.83 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:07.866826: step 180070, loss = 1.84 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:46:09.046367: step 180080, loss = 1.94 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:10.197801: step 180090, loss = 1.92 (1111.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:46:11.365409: step 180100, loss = 1.71 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:12.532784: step 180110, loss = 1.96 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:13.673969: step 180120, loss = 1.90 (1121.6 examples/sec; 0.114 sec/batch)
2017-05-05 03:46:14.826422: step 180130, loss = 1.93 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:46:15.977880: step 180140, loss = 1.92 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:46:17.146875: step 180150, loss = 1.77 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:18.309792: step 180160, loss = 1.79 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:19.473346: step 180170, loss = 1.81 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:20.752027: step 180180, loss = 1.94 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-05 03:46:21.844299: step 180190, loss = 1.90 (1171.9 examples/sec; 0.109 sec/batch)
2017-05-05 03:46:23.019333: step 180200, loss = 1.89 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:24.215212: step 180210, loss = 1.84 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:46:25.418498: step 180220, loss = 1.93 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:46:26.628080: step 180230, loss = 1.73 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:46:27.822257: step 180240, loss = 2.05 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:46:29.012799: step 180250, loss = 1.85 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:46:30.184525: step 180260, loss = 1.86 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:31.371987: step 180270, loss = 1.80 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:46:32.568522: step 180280, loss = 1.85 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:46:33.745295: step 180290, loss = 1.77 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:34.915581: step 180300, loss = 1.88 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:36.085348: step 180310, loss = 1.85 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:37.250303: step 180320, loss = 1.80 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:38.427775: step 180330, loss = 1.91 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:39.590987: step 180340, loss = 1.87 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:40.770475: step 180350, loss = 1.74 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:41.934762: step 180360, loss = 1.87 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:43.098200: step 180370, loss = 1.82 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:44.265882: step 180380, loss = 1.81 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:45.420867: step 180390, loss = 1.99 (1108.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:46:46.597605: step 180400, loss = 1.84 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:47.762548: step 180410, loss = 1.92 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:48.938066: step 180420, loss = 1.82 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:50.102023: step 180430, loss = 1.66 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:51.271528: step 180440, loss = 2.00 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:52.434014: step 180450, loss = 1.75 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:53.606138: step 180460, loss = 1.94 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:54.772545: step 180470, loss = 2.01 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:55.949413: step 180480, loss = 1.88 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:57.136833: step 180490, loss = 1.84 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:46:58.299016: step 180500, loss = 1.88 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:59.469119: step 180510, loss = 2.00 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:00.640666: step 180520, loss = 1.74 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:01.781326: step 180530, loss = 1.76 (1122.2 examples/sec; 0.114 sec/batch)
2017-05-05 03:47:02.973016: step 180540, loss = 1.81 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:47:04.135328: step 180550, loss = 1.86 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:05.288950: step 180560, loss = 1.78 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:06.455846: step 180570, loss = 1.68 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:07.644292: step 180580, loss = 1.76 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:47:08.793538: step 180590, loss = 1.73 (1113.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:09.954208: step 180600, loss = 1.82 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:11.112051: step 180610, loss = 1.66 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:12.289065: step 180620, loss = 1.92 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:47:13.442394: step 180630, loss = 2.03 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:14.602931: step 180640, loss = 1.80 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:15.758390: step 180650, loss = 1.79 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:16.918364: step 180660, loss = 1.94 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:18.080862: step 180670, loss = 1.97 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:19.244269: step 180680, loss = 1.74 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:20.382272: step 180690, loss = 1.97 (1124.8 examples/sec; 0.114 sec/batch)
2017-05-05 03:47:21.535467: step 180700, loss = 1.67 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:22.701182: step 180710, loss = 1.96 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:23.868917: step 180720, loss = 1.88 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:25.052605: step 180730, loss = 1.90 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:47:26.217589: step 180740, loss = 1.83 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:27.388722: step 180750, loss = 2.21 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:28.557106: step 180760, loss = 1.80 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:29.736915: step 180770, loss = 1.99 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:47:30.921705: step 180780, loss = 1.93 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:47:32.109783: step 180790, loss = 1.82 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:47:33.277713: step 180800, loss = 1.83 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:34.455178: step 180810, loss = 1.67 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:47:35.621632: step 180820, loss = 1.84 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:36.786680: step 180830, loss = 1.85 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:37.947757: step 180840, loss = 1.85 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:39.137020: step 180850, loss = 2.04 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:47:40.310633: step 180860, loss = 1.86 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:41.473115: step 180870, loss = 1.84 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:42.605819: step 180880, loss = 1.86 (1130.0 examples/sec; 0.113 sec/batch)
2017-05-05 03:47:43.756023: step 180890, loss = 1.86 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:44.914939: step 180900, loss = 1.81 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:46.050834: step 180910, loss = 1.90 (1126.9 examples/sec; 0.114 sec/batch)
2017-05-05 03:47:47.207840: step 180920, loss = 1.78 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:48.367559: step 180930, loss = 1.83 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:49.505505: step 180940, loss = 1.75 (1124.8 examples/sec; 0.114 sec/batch)
2017-05-05 03:47:50.656320: step 180950, loss = 1.81 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:51.820674: step 180960, loss = 1.78 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:52.972578: step 180970, loss = 1.90 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:54.145879: step 180980, loss = 1.97 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:55.318578: step 180990, loss = 1.76 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:56.482461: step 181000, loss = 1.83 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:57.638865: step 181010, loss = 1.87 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:58.793343: step 181020, loss = 1.82 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:59.948748: step 181030, loss = 1.83 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:01.102780: step 181040, loss = 1.95 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:02.268565: step 181050, loss = 1.87 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:03.433074: step 181060, loss = 2.09 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:04.592160: step 181070, loss = 1.97 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:05.749891: step 181080, loss = 1.78 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:06.925732: step 181090, loss = 1.76 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:48:08.102349: step 181100, loss = 1.93 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:48:09.278139: step 181110, loss = 1.99 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:48:10.453692: step 181120, loss = 2.04 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:48:11.628386: step 181130, loss = 1.94 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:12.786929: step 181140, loss = 1.86 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:13.946041: step 181150, loss = 1.89 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:15.114030: step 181160, loss = 1.88 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:16.397070: step 181170, loss = 1.82 (997.6 examples/sec; 0.128 sec/batch)
2017-05-05 03:48:17.475452: step 181180, loss = 1.74 (1187.0 examples/sec; 0.108 sec/batch)
2017-05-05 03:48:18.637351: step 181190, loss = 1.96 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:19.798055: step 181200, loss = 1.75 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:20.948762: step 181210, loss = 1.89 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:22.092425: step 181220, loss = 1.85 (1119.2 examples/sec; 0.114 sec/batch)
2017-05-05 03:48:23.254875: step 181230, loss = 1.87 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:24.405475: step 181240, loss = 1.85 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:25.588896: step 181250, loss = 1.98 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:48:26.764859: step 181260, loss = 1.85 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:48:27.921593: step 181270, loss = 1.74 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:29.067622: step 181280, loss = 1.72 (1116.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:30.211561: step 181290, loss = 1.81 (1118.9 examples/sec; 0.114 sec/batch)
2017-05-05 03:48:31.362331: step 181300, loss = 1.70 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:32.548426: step 181310, loss = 1.96 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:48:33.717332: step 181320, loss = 1.85 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:34.867814: step 181330, loss = 1.83 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:36.018874: step 181340, loss = 1.78 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:37.188688: step 181350, loss = 1.78 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:38.342323: step 181360, loss = 2.01 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:39.516641: step 181370, loss = 1.86 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:40.666140: step 181380, loss = 1.88 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:41.800937: step 181390, loss = 1.96 (1128.0 examples/sec; 0.113 sec/batch)
2017-05-05 03:48:42.975956: step 181400, loss = 1.88 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:48:44.148254: step 181410, loss = 1.84 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:45.297876: step 181420, loss = 1.97 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:46.441848: step 181430, loss = 1.85 (1118.9 examples/sec; 0.114 sec/batch)
2017-05-05 03:48:47.601482: step 181440, loss = 1.89 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:48.758340: step 181450, loss = 1.84 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:49.922819: step 181460, loss = 1.92 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:51.102811: step 181470, loss = 1.97 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:48:52.263034: step 181480, loss = 1.75 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:53.405816: step 181490, loss = 1.93 (1120.1 examples/sec; 0.114 sec/batch)
2017-05-05 03:48:54.555718: step 181500, loss = 1.93 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:55.749989: step 181510, loss = 1.91 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:48:56.893555: step 181520, loss = 1.82 (1119.3 examples/sec; 0.114 sec/batch)
2017-05-05 03:48:58.047148: step 181530, loss = 1.78 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:59.191065: step 181540, loss = 1.74 (1119.0 examples/sec; 0.114 sec/batch)
2017-05-05 03:49:00.351459: step 181550, loss = 1.84 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:49:01.501577: step 181560, loss = 1.91 (1112.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:49:02.659465: step 181570, loss = 1.93 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:49:03.822630: step 181580, loss = 2.02 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:49:05.000891: step 181590, loss = 1.82 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:49:06.160599: step 181600, loss = 1.86 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:49:07.325506: step 181610, loss = 1.84 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:49:08.497767: step 181620, loss = 1.90 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:09.671375: step 181630, loss = 1.90 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:10.846970: step 181640, loss = 1.84 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:49:12.022710: step 181650, loss = 1.86 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:49:13.203326: step 181660, loss = 1.95 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:49:14.367943: step 181670, loss = 1.91 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:49:15.548524: step 181680, loss = 1.96 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:49:16.714627: step 181690, loss = 1.87 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:17.897801: step 181700, loss = 1.73 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:49:19.084560: step 181710, loss = 1.80 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:49:20.241563: step 181720, loss = 1.80 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:49:21.412006: step 181730, loss = 1.84 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:22.572093: step 181740, loss = 1.91 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:49:23.746154: step 181750, loss = 1.89 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:24.909696: step 181760, loss = 1.84 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:49:26.049825: step 181770, loss = 1.77 (1122.7 examples/sec; 0.114 sec/batch)
2017-05-05 03:49:27.222676: step 181780, loss = 1.92 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:28.381029: step 181790, loss = 2.00 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:49:29.534556: step 181800, loss = 2.12 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:49:30.753646: step 181810, loss = 2.04 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:49:31.991533: step 181820, loss = 1.92 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-05 03:49:33.216323: step 181830, loss = 1.76 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:49:34.457951: step 181840, loss = 1.88 (1030.9 examples/sec; 0.124 sec/batch)
2017-05-05 03:49:35.703766: step 181850, loss = 1.73 (1027.4 examples/sec; 0.125 sec/batch)
2017-05-05 03:49:36.941759: step 181860, loss = 1.78 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-05 03:49:38.198521: step 181870, loss = 1.86 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-05 03:49:39.461762: step 181880, loss = 1.87 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-05 03:49:40.721959: step 181890, loss = 1.87 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-05 03:49:41.914622: step 181900, loss = 1.88 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:49:43.145306: step 181910, loss = 1.86 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-05 03:49:44.315195: step 181920, loss = 1.90 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:45.550002: step 181930, loss = 1.89 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:49:46.819274: step 181940, loss = 1.72 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-05 03:49:48.057548: step 181950, loss = 1.83 (1033.7 examples/sec; 0.124 sec/batch)
2017-05-05 03:49:49.333304: step 181960, loss = 1.80 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-05 03:49:50.511645: step 181970, loss = 1.86 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:49:51.727888: step 181980, loss = 1.85 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:49:52.927061: step 181990, loss = 1.76 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:49:54.128612: step 182000, loss = 1.89 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:49:55.334479: step 182010, loss = 2.00 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:49:56.547215: step 182020, loss = 1.87 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:49:57.762442: step 182030, loss = 1.90 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:49:58.962400: step 182040, loss = 1.87 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:00.172943: step 182050, loss = 1.86 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:01.374601: step 182060, loss = 1.71 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:02.566196: step 182070, loss = 1.86 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:50:03.780057: step 182080, loss = 1.98 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:04.991447: step 182090, loss = 1.92 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:06.177096: step 182100, loss = 1.87 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:50:07.369720: step 182110, loss = 1.78 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:50:08.562936: step 182120, loss = 1.77 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:50:09.762784: step 182130, loss = 1.82 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:10.951180: step 182140, loss = 1.84 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:50:12.153622: step 182150, loss = 1.75 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:13.437710: step 182160, loss = 1.86 (996.8 examples/sec; 0.128 sec/batch)
2017-05-05 03:50:14.535639: step 182170, loss = 1.94 (1165.8 examples/sec; 0.110 sec/batch)
2017-05-05 03:50:15.730613: step 182180, loss = 1.92 (1071.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:50:16.947574: step 182190, loss = 2.16 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:50:18.139162: step 182200, loss = 1.92 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:50:19.369758: step 182210, loss = 1.83 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-05 03:50:20.563779: step 182220, loss = 1.95 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:50:21.768960: step 182230, loss = 1.79 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:22.983697: step 182240, loss = 1.98 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:24.194208: step 182250, loss = 1.93 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:25.422584: step 182260, loss = 1.80 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:50:26.599006: step 182270, loss = 1.93 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:50:27.817102: step 182280, loss = 1.85 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:50:29.030198: step 182290, loss = 1.85 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:30.251092: step 182300, loss = 1.77 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:50:31.454076: step 182310, loss = 1.97 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:32.655260: step 182320, loss = 1.80 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:33.851696: step 182330, loss = 1.75 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:35.059778: step 182340, loss = 1.68 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:36.273288: step 182350, loss = 1.93 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:37.529422: step 182360, loss = 1.89 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-05 03:50:38.608128: step 182370, loss = 1.89 (1186.6 examples/sec; 0.108 sec/batch)
2017-05-05 03:50:39.812637: step 182380, loss = 1.90 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:41.039447: step 182390, loss = 1.71 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 03:50:42.194905: step 182400, loss = 1.91 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:50:43.410090: step 182410, loss = 1.94 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:50:44.642204: step 182420, loss = 1.93 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:50:45.830753: step 182430, loss = 1.99 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:50:47.028375: step 182440, loss = 1.85 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:48.227767: step 182450, loss = 1.80 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:49.403076: step 182460, loss = 2.04 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:50:50.569621: step 182470, loss = 1.91 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:50:51.736504: step 182480, loss = 1.90 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:50:52.915256: step 182490, loss = 1.98 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:50:54.075615: step 182500, loss = 2.00 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:50:55.436186: step 182510, loss = 1.92 (940.8 examples/sec; 0.136 sec/batch)
2017-05-05 03:50:56.948130: step 182520, loss = 1.93 (846.6 examples/sec; 0.151 sec/batch)
2017-05-05 03:50:58.452201: step 182530, loss = 1.75 (851.0 examples/sec; 0.150 sec/batch)
2017-05-05 03:50:59.968271: step 182540, loss = 1.86 (844.3 examples/sec; 0.152 sec/batch)
2017-05-05 03:51:01.525582: step 182550, loss = 1.96 (821.9 examples/sec; 0.156 sec/batch)
2017-05-05 03:51:03.057629: step 182560, loss = 2.05 (835.5 examples/sec; 0.153 sec/batch)
2017-05-05 03:51:04.586438: step 182570, loss = 1.91 (837.3 examples/sec; 0.153 sec/batch)
2017-05-05 03:51:06.120318: step 182580, loss = 1.79 (834.5 examples/sec; 0.153 sec/batch)
2017-05-05 03:51:07.617798: step 182590, loss = 1.92 (854.8 examples/sec; 0.150 sec/batch)
2017-05-05 03:51:09.137873: step 182600, loss = 1.83 (842.1 examples/sec; 0.152 sec/batch)
2017-05-05 03:51:10.696896: step 182610, loss = 2.15 (821.0 examples/sec; 0.156 sec/batch)
2017-05-05 03:51:12.232690: step 182620, loss = 1.93 (833.4 examples/sec; 0.154 sec/batch)
2017-05-05 03:51:13.750986: step 182630, loss = 1.93 (843.0 examples/sec; 0.152 sec/batch)
2017-05-05 03:51:15.280048: step 182640, loss = 1.78 (837.1 examples/sec; 0.153 sec/batch)
2017-05-05 03:51:16.736941: step 182650, loss = 1.92 (878.6 examples/sec; 0.146 sec/batch)
2017-05-05 03:51:18.237040: step 182660, loss = 1.97 (853.3 examples/sec; 0.150 sec/batch)
2017-05-05 03:51:19.734528: step 182670, loss = 1.66 (854.8 examples/sec; 0.150 sec/batch)
2017-05-05 03:51:21.238033: step 182680, loss = 1.65 (851.3 examples/sec; 0.150 sec/batch)
2017-05-05 03:51:22.775677: step 182690, loss = 1.96 (832.4 examples/sec; 0.154 sec/batch)
2017-05-05 03:51:24.285816: step 182700, loss = 1.80 (847.6 examples/sec; 0.151 sec/batch)
2017-05-05 03:51:25.789869: step 182710, loss = 2.02 (851.0 examples/sec; 0.150 sec/batch)
2017-05-05 03:51:27.344767: step 182720, loss = 1.78 (823.2 examples/sec; 0.155 sec/batch)
2017-05-05 03:51:28.879193: step 182730, loss = 1.94 (834.2 examples/sec; 0.153 sec/batch)
2017-05-05 03:51:30.347406: step 182740, loss = 1.86 (871.8 examples/sec; 0.147 sec/batch)
2017-05-05 03:51:31.859177: step 182750, loss = 1.88 (846.7 examples/sec; 0.151 sec/batch)
2017-05-05 03:51:33.333419: step 182760, loss = 1.79 (868.2 examples/sec; 0.147 sec/batch)
2017-05-05 03:51:34.812073: step 182770, loss = 1.96 (865.7 examples/sec; 0.148 sec/batch)
2017-05-05 03:51:36.278704: step 182780, loss = 1.97 (872.8 examples/sec; 0.147 sec/batch)
2017-05-05 03:51:37.795726: step 182790, loss = 1.96 (843.8 examples/sec; 0.152 sec/batch)
2017-05-05 03:51:39.292370: step 182800, loss = 1.95 (855.2 examples/sec; 0.150 sec/batch)
2017-05-05 03:51:40.751351: step 182810, loss = 1.83 (877.3 examples/sec; 0.146 sec/batch)
2017-05-05 03:51:42.286044: step 182820, loss = 1.78 (834.0 examples/sec; 0.153 sec/batch)
2017-05-05 03:51:43.794015: step 182830, loss = 1.73 (848.8 examples/sec; 0.151 sec/batch)
2017-05-05 03:51:45.323930: step 182840, loss = 1.98 (836.7 examples/sec; 0.153 sec/batch)
2017-05-05 03:51:46.820153: step 182850, loss = 2.05 (855.5 examples/sec; 0.150 sec/batch)
2017-05-05 03:51:48.347170: step 182860, loss = 1.87 (838.2 examples/sec; 0.153 sec/batch)
2017-05-05 03:51:49.888045: step 182870, loss = 1.87 (830.7 examples/sec; 0.154 sec/batch)
2017-05-05 03:51:51.393515: step 182880, loss = 1.77 (850.2 examples/sec; 0.151 sec/batch)
2017-05-05 03:51:52.886317: step 182890, loss = 1.87 (857.5 examples/sec; 0.149 sec/batch)
2017-05-05 03:51:54.358614: step 182900, loss = 1.71 (869.4 examples/sec; 0.147 sec/batch)
2017-05-05 03:51:55.944991: step 182910, loss = 1.84 (806.9 examples/sec; 0.159 sec/batch)
2017-05-05 03:51:57.427417: step 182920, loss = 1.89 (863.4 examples/sec; 0.148 sec/batch)
2017-05-05 03:51:59.009527: step 182930, loss = 1.84 (809.0 examples/sec; 0.158 sec/batch)
2017-05-05 03:52:00.560803: step 182940, loss = 1.78 (825.1 examples/sec; 0.155 sec/batch)
2017-05-05 03:52:02.127905: step 182950, loss = 1.82 (816.8 examples/sec; 0.157 sec/batch)
2017-05-05 03:52:03.622330: step 182960, loss = 1.89 (856.5 examples/sec; 0.149 sec/batch)
2017-05-05 03:52:05.125088: step 182970, loss = 1.81 (851.8 examples/sec; 0.150 sec/batch)
2017-05-05 03:52:06.674565: step 182980, loss = 1.80 (826.1 examples/sec; 0.155 sec/batch)
2017-05-05 03:52:08.196695: step 182990, loss = 1.94 (840.9 examples/sec; 0.152 sec/batch)
2017-05-05 03:52:09.711488: step 183000, loss = 1.96 (845.0 examples/sec; 0.151 sec/batch)
2017-05-05 03:52:11.244983: step 183010, loss = 1.89 (834.7 examples/sec; 0.153 sec/batch)
2017-05-05 03:52:12.771948: step 183020, loss = 1.66 (838.3 examples/sec; 0.153 sec/batch)
2017-05-05 03:52:14.308659: step 183030, loss = 1.96 (832.9 examples/sec; 0.154 sec/batch)
2017-05-05 03:52:15.904331: step 183040, loss = 1.97 (802.2 examples/sec; 0.160 sec/batch)
2017-05-05 03:52:17.461174: step 183050, loss = 1.76 (822.2 examples/sec; 0.156 sec/batch)
2017-05-05 03:52:19.057207: step 183060, loss = 1.83 (802.0 examples/sec; 0.160 sec/batch)
2017-05-05 03:52:20.616848: step 183070, loss = 1.83 (820.7 examples/sec; 0.156 sec/batch)
2017-05-05 03:52:22.096043: step 183080, loss = 1.92 (865.3 examples/sec; 0.148 sec/batch)
2017-05-05 03:52:23.713100: step 183090, loss = 1.92 (791.7 examples/sec; 0.162 sec/batch)
2017-05-05 03:52:25.214829: step 183100, loss = 1.82 (852.2 examples/sec; 0.150 sec/batch)
2017-05-05 03:52:26.744394: step 183110, loss = 1.97 (836.8 examples/sec; 0.153 sec/batch)
2017-05-05 03:52:28.271335: step 183120, loss = 1.76 (838.3 examples/sec; 0.153 sec/batch)
2017-05-05 03:52:29.853047: step 183130, loss = 1.93 (809.3 examples/sec; 0.158 sec/batch)
2017-05-05 03:52:31.420361: step 183140, loss = 1.92 (816.7 examples/sec; 0.157 sec/batch)
2017-05-05 03:52:33.144541: step 183150, loss = 2.03 (742.4 examples/sec; 0.172 sec/batch)
2017-05-05 03:52:34.501847: step 183160, loss = 1.86 (943.0 examples/sec; 0.136 sec/batch)
2017-05-05 03:52:36.051827: step 183170, loss = 1.81 (825.8 examples/sec; 0.155 sec/batch)
2017-05-05 03:52:37.549135: step 183180, loss = 1.82 (854.9 examples/sec; 0.150 sec/batch)
2017-05-05 03:52:39.131031: step 183190, loss = 1.78 (809.2 examples/sec; 0.158 sec/batch)
2017-05-05 03:52:40.714254: step 183200, loss = 1.80 (808.5 examples/sec; 0.158 sec/batch)
2017-05-05 03:52:42.279175: step 183210, loss = 1.99 (817.9 examples/sec; 0.156 sec/batch)
2017-05-05 03:52:43.836787: step 183220, loss = 1.84 (821.8 examples/sec; 0.156 sec/batch)
2017-05-05 03:52:45.376584: step 183230, loss = 1.89 (831.3 examples/sec; 0.154 sec/batch)
2017-05-05 03:52:46.950835: step 183240, loss = 1.94 (813.1 examples/sec; 0.157 sec/batch)
2017-05-05 03:52:48.438149: step 183250, loss = 1.88 (860.6 examples/sec; 0.149 sec/batch)
2017-05-05 03:52:50.003782: step 183260, loss = 1.92 (817.6 examples/sec; 0.157 sec/batch)
2017-05-05 03:52:51.585340: step 183270, loss = 1.80 (809.3 examples/sec; 0.158 sec/batch)
2017-05-05 03:52:53.071322: step 183280, loss = 1.88 (861.4 examples/sec; 0.149 sec/batch)
2017-05-05 03:52:54.631025: step 183290, loss = 1.77 (820.7 examples/sec; 0.156 sec/batch)
2017-05-05 03:52:56.145912: step 183300, loss = 1.77 (845.0 examples/sec; 0.151 sec/batch)
2017-05-05 03:52:57.667450: step 183310, loss = 1.83 (841.3 examples/sec; 0.152 sec/batch)
2017-05-05 03:52:59.239104: step 183320, loss = 1.90 (814.4 examples/sec; 0.157 sec/batch)
2017-05-05 03:53:00.783989: step 183330, loss = 1.91 (828.5 examples/sec; 0.154 sec/batch)
2017-05-05 03:53:02.327280: step 183340, loss = 1.83 (829.4 examples/sec; 0.154 sec/batch)
2017-05-05 03:53:03.990811: step 183350, loss = 1.84 (769.4 examples/sec; 0.166 sec/batch)
2017-05-05 03:53:05.371758: step 183360, loss = 1.87 (926.9 examples/sec; 0.138 sec/batch)
2017-05-05 03:53:06.885307: step 183370, loss = 1.71 (845.7 examples/sec; 0.151 sec/batch)
2017-05-05 03:53:08.368885: step 183380, loss = 1.89 (862.8 examples/sec; 0.148 sec/batch)
2017-05-05 03:53:09.904757: step 183390, loss = 1.89 (833.4 examples/sec; 0.154 sec/batch)
2017-05-05 03:53:11.438988: step 183400, loss = 1.88 (834.3 examples/sec; 0.153 sec/batch)
2017-05-05 03:53:12.969378: step 183410, loss = 1.76 (836.4 examples/sec; 0.153 sec/batch)
2017-05-05 03:53:14.517490: step 183420, loss = 1.99 (826.8 examples/sec; 0.155 sec/batch)
2017-05-05 03:53:16.118794: step 183430, loss = 2.03 (799.4 examples/sec; 0.160 sec/batch)
2017-05-05 03:53:17.652467: step 183440, loss = 1.88 (834.6 examples/sec; 0.153 sec/batch)
2017-05-05 03:53:19.222288: step 183450, loss = 1.68 (815.4 examples/sec; 0.157 sec/batch)
2017-05-05 03:53:20.762469: step 183460, loss = 1.98 (831.1 examples/sec; 0.154 sec/batch)
2017-05-05 03:53:22.313453: step 183470, loss = 1.78 (825.3 examples/sec; 0.155 sec/batch)
2017-05-05 03:53:23.847045: step 183480, loss = 1.84 (834.6 examples/sec; 0.153 sec/batch)
2017-05-05 03:53:25.369674: step 183490, loss = 1.97 (840.7 examples/sec; 0.152 sec/batch)
2017-05-05 03:53:26.834737: step 183500, loss = 1.82 (873.7 examples/sec; 0.147 sec/batch)
2017-05-05 03:53:28.380970: step 183510, loss = 1.77 (827.8 examples/sec; 0.155 sec/batch)
2017-05-05 03:53:29.933106: step 183520, loss = 1.83 (824.7 examples/sec; 0.155 sec/batch)
2017-05-05 03:53:31.442211: step 183530, loss = 1.91 (848.2 examples/sec; 0.151 sec/batch)
2017-05-05 03:53:32.920106: step 183540, loss = 1.73 (866.1 examples/sec; 0.148 sec/batch)
2017-05-05 03:53:34.457808: step 183550, loss = 1.81 (832.4 examples/sec; 0.154 sec/batch)
2017-05-05 03:53:36.029545: step 183560, loss = 1.77 (814.4 examples/sec; 0.157 sec/batch)
2017-05-05 03:53:37.605724: step 183570, loss = 1.81 (812.1 examples/sec; 0.158 sec/batch)
2017-05-05 03:53:39.107839: step 183580, loss = 1.85 (852.1 examples/sec; 0.150 sec/batch)
2017-05-05 03:53:40.628383: step 183590, loss = 1.81 (841.8 examples/sec; 0.152 sec/batch)
2017-05-05 03:53:42.108983: step 183600, loss = 2.01 (864.5 examples/sec; 0.148 sec/batch)
2017-05-05 03:53:43.638144: step 183610, loss = 1.75 (837.1 examples/sec; 0.153 sec/batch)
2017-05-05 03:53:45.129372: step 183620, loss = 2.01 (858.4 examples/sec; 0.149 sec/batch)
2017-05-05 03:53:46.681659: step 183630, loss = 1.83 (824.6 examples/sec; 0.155 sec/batch)
2017-05-05 03:53:48.244729: step 183640, loss = 1.85 (818.9 examples/sec; 0.156 sec/batch)
2017-05-05 03:53:49.763320: step 183650, loss = 1.87 (842.9 examples/sec; 0.152 sec/batch)
2017-05-05 03:53:51.333992: step 183660, loss = 1.88 (814.9 examples/sec; 0.157 sec/batch)
2017-05-05 03:53:52.869672: step 183670, loss = 1.84 (833.5 examples/sec; 0.154 sec/batch)
2017-05-05 03:53:54.409943: step 183680, loss = 1.81 (831.0 examples/sec; 0.154 sec/batch)
2017-05-05 03:53:55.975501: step 183690, loss = 1.78 (817.6 examples/sec; 0.157 sec/batch)
2017-05-05 03:53:57.508791: step 183700, loss = 1.83 (834.8 examples/sec; 0.153 sec/batch)
2017-05-05 03:53:59.003444: step 183710, loss = 2.21 (856.4 examples/sec; 0.149 sec/batch)
2017-05-05 03:54:00.473712: step 183720, loss = 1.80 (870.6 examples/sec; 0.147 sec/batch)
2017-05-05 03:54:02.003358: step 183730, loss = 1.79 (836.8 examples/sec; 0.153 sec/batch)
2017-05-05 03:54:03.566825: step 183740, loss = 1.85 (818.7 examples/sec; 0.156 sec/batch)
2017-05-05 03:54:05.051325: step 183750, loss = 1.84 (862.2 examples/sec; 0.148 sec/batch)
2017-05-05 03:54:06.537173: step 183760, loss = 1.87 (861.5 examples/sec; 0.149 sec/batch)
2017-05-05 03:54:08.014551: step 183770, loss = 1.87 (866.4 examples/sec; 0.148 sec/batch)
2017-05-05 03:54:09.517556: step 183780, loss = 1.88 (851.6 examples/sec; 0.150 sec/batch)
2017-05-05 03:54:11.008737: step 183790, loss = 1.84 (858.4 examples/sec; 0.149 sec/batch)
2017-05-05 03:54:12.545728: step 183800, loss = 1.80 (832.8 examples/sec; 0.154 sec/batch)
2017-05-05 03:54:14.026095: step 183810, loss = 1.98 (864.7 examples/sec; 0.148 sec/batch)
2017-05-05 03:54:15.558009: step 183820, loss = 1.68 (835.6 examples/sec; 0.153 sec/batch)
2017-05-05 03:54:17.033089: step 183830, loss = 1.78 (867.7 examples/sec; 0.148 sec/batch)
2017-05-05 03:54:18.534687: step 183840, loss = 1.89 (852.4 examples/sec; 0.150 sec/batch)
2017-05-05 03:54:20.127886: step 183850, loss = 1.95 (803.4 examples/sec; 0.159 sec/batch)
2017-05-05 03:54:21.658765: step 183860, loss = 1.75 (836.1 examples/sec; 0.153 sec/batch)
2017-05-05 03:54:23.150046: step 183870, loss = 1.86 (858.3 examples/sec; 0.149 sec/batch)
2017-05-05 03:54:24.655548: step 183880, loss = 1.75 (850.2 examples/sec; 0.151 sec/batch)
2017-05-05 03:54:26.133878: step 183890, loss = 1.90 (865.8 examples/sec; 0.148 sec/batch)
2017-05-05 03:54:27.647369: step 183900, loss = 1.90 (845.7 examples/sec; 0.151 sec/batch)
2017-05-05 03:54:29.168567: step 183910, loss = 1.90 (841.4 examples/sec; 0.152 sec/batch)
2017-05-05 03:54:30.688141: step 183920, loss = 1.82 (842.3 examples/sec; 0.152 sec/batch)
2017-05-05 03:54:32.207010: step 183930, loss = 1.66 (842.7 examples/sec; 0.152 sec/batch)
2017-05-05 03:54:33.771260: step 183940, loss = 1.84 (818.3 examples/sec; 0.156 sec/batch)
2017-05-05 03:54:35.347605: step 183950, loss = 1.84 (812.0 examples/sec; 0.158 sec/batch)
2017-05-05 03:54:36.878994: step 183960, loss = 1.96 (835.8 examples/sec; 0.153 sec/batch)
2017-05-05 03:54:38.459598: step 183970, loss = 2.08 (809.8 examples/sec; 0.158 sec/batch)
2017-05-05 03:54:39.980407: step 183980, loss = 1.91 (841.7 examples/sec; 0.152 sec/batch)
2017-05-05 03:54:41.503894: step 183990, loss = 1.82 (840.2 examples/sec; 0.152 sec/batch)
2017-05-05 03:54:43.015857: step 184000, loss = 1.82 (846.6 examples/sec; 0.151 sec/batch)
2017-05-05 03:54:44.562693: step 184010, loss = 1.95 (827.5 examples/sec; 0.155 sec/batch)
2017-05-05 03:54:46.115363: step 184020, loss = 1.85 (824.4 examples/sec; 0.155 sec/batch)
2017-05-05 03:54:47.688777: step 184030, loss = 1.95 (813.5 examples/sec; 0.157 sec/batch)
2017-05-05 03:54:49.199669: step 184040, loss = 1.75 (847.2 examples/sec; 0.151 sec/batch)
2017-05-05 03:54:50.794320: step 184050, loss = 1.87 (802.7 examples/sec; 0.159 sec/batch)
2017-05-05 03:54:52.366653: step 184060, loss = 1.82 (814.1 examples/sec; 0.157 sec/batch)
2017-05-05 03:54:53.901940: step 184070, loss = 1.98 (833.7 examples/sec; 0.154 sec/batch)
2017-05-05 03:54:55.422349: step 184080, loss = 1.81 (841.9 examples/sec; 0.152 sec/batch)
2017-05-05 03:54:56.949786: step 184090, loss = 1.92 (838.0 examples/sec; 0.153 sec/batch)
2017-05-05 03:54:58.500878: step 184100, loss = 1.81 (825.2 examples/sec; 0.155 sec/batch)
2017-05-05 03:55:00.052170: step 184110, loss = 1.96 (825.1 examples/sec; 0.155 sec/batch)
2017-05-05 03:55:01.576834: step 184120, loss = 1.90 (839.5 examples/sec; 0.152 sec/batch)
2017-05-05 03:55:03.108864: step 184130, loss = 1.82 (835.5 examples/sec; 0.153 sec/batch)
2017-05-05 03:55:04.710861: step 184140, loss = 1.84 (799.0 examples/sec; 0.160 sec/batch)
2017-05-05 03:55:06.101154: step 184150, loss = 1.76 (920.7 examples/sec; 0.139 sec/batch)
2017-05-05 03:55:07.614829: step 184160, loss = 1.73 (845.6 examples/sec; 0.151 sec/batch)
2017-05-05 03:55:09.120704: step 184170, loss = 1.84 (850.0 examples/sec; 0.151 sec/batch)
2017-05-05 03:55:10.670539: step 184180, loss = 1.92 (825.9 examples/sec; 0.155 sec/batch)
2017-05-05 03:55:12.180396: step 184190, loss = 1.99 (847.8 examples/sec; 0.151 sec/batch)
2017-05-05 03:55:13.657066: step 184200, loss = 1.81 (866.8 examples/sec; 0.148 sec/batch)
2017-05-05 03:55:15.216573: step 184210, loss = 1.81 (820.8 examples/sec; 0.156 sec/batch)
2017-05-05 03:55:16.777459: step 184220, loss = 1.86 (820.0 examples/sec; 0.156 sec/batch)
2017-05-05 03:55:18.295433: step 184230, loss = 1.93 (843.2 examples/sec; 0.152 sec/batch)
2017-05-05 03:55:19.796291: step 184240, loss = 1.88 (852.8 examples/sec; 0.150 sec/batch)
2017-05-05 03:55:21.338354: step 184250, loss = 1.99 (830.1 examples/sec; 0.154 sec/batch)
2017-05-05 03:55:22.802091: step 184260, loss = 2.00 (874.5 examples/sec; 0.146 sec/batch)
2017-05-05 03:55:24.292004: step 184270, loss = 1.81 (859.1 examples/sec; 0.149 sec/batch)
2017-05-05 03:55:25.792702: step 184280, loss = 1.81 (852.9 examples/sec; 0.150 sec/batch)
2017-05-05 03:55:27.334279: step 184290, loss = 1.85 (830.3 examples/sec; 0.154 sec/batch)
2017-05-05 03:55:28.857721: step 184300, loss = 1.91 (840.2 examples/sec; 0.152 sec/batch)
2017-05-05 03:55:30.378540: step 184310, loss = 1.83 (841.7 examples/sec; 0.152 sec/batch)
2017-05-05 03:55:31.874713: step 184320, loss = 1.85 (855.5 examples/sec; 0.150 sec/batch)
2017-05-05 03:55:33.385137: step 184330, loss = 1.84 (847.4 examples/sec; 0.151 sec/batch)
2017-05-05 03:55:34.899016: step 184340, loss = 1.88 (845.5 examples/sec; 0.151 sec/batch)
2017-05-05 03:55:36.388522: step 184350, loss = 1.82 (859.3 examples/sec; 0.149 sec/batch)
2017-05-05 03:55:37.885580: step 184360, loss = 1.89 (855.0 examples/sec; 0.150 sec/batch)
2017-05-05 03:55:39.411517: step 184370, loss = 1.92 (838.8 examples/sec; 0.153 sec/batch)
2017-05-05 03:55:40.952416: step 184380, loss = 1.93 (830.7 examples/sec; 0.154 sec/batch)
2017-05-05 03:55:42.463646: step 184390, loss = 1.85 (847.0 examples/sec; 0.151 sec/batch)
2017-05-05 03:55:44.040765: step 184400, loss = 1.94 (811.6 examples/sec; 0.158 sec/batch)
2017-05-05 03:55:45.587617: step 184410, loss = 1.72 (827.5 examples/sec; 0.155 sec/batch)
2017-05-05 03:55:47.086158: step 184420, loss = 1.80 (854.2 examples/sec; 0.150 sec/batch)
2017-05-05 03:55:48.549674: step 184430, loss = 1.86 (874.6 examples/sec; 0.146 sec/batch)
2017-05-05 03:55:50.068737: step 184440, loss = 1.93 (842.6 examples/sec; 0.152 sec/batch)
2017-05-05 03:55:51.577128: step 184450, loss = 1.72 (848.6 examples/sec; 0.151 sec/batch)
2017-05-05 03:55:53.091219: step 184460, loss = 1.76 (845.4 examples/sec; 0.151 sec/batch)
2017-05-05 03:55:54.594418: step 184470, loss = 1.97 (851.5 examples/sec; 0.150 sec/batch)
2017-05-05 03:55:56.129344: step 184480, loss = 1.82 (833.9 examples/sec; 0.153 sec/batch)
2017-05-05 03:55:57.597712: step 184490, loss = 1.75 (871.7 examples/sec; 0.147 sec/batch)
2017-05-05 03:55:59.119958: step 184500, loss = 1.89 (840.9 examples/sec; 0.152 sec/batch)
2017-05-05 03:56:00.624968: step 184510, loss = 2.02 (850.5 examples/sec; 0.150 sec/batch)
2017-05-05 03:56:02.162608: step 184520, loss = 1.86 (832.4 examples/sec; 0.154 sec/batch)
2017-05-05 03:56:03.702017: step 184530, loss = 1.83 (831.5 examples/sec; 0.154 sec/batch)
2017-05-05 03:56:05.210907: step 184540, loss = 1.87 (848.3 examples/sec; 0.151 sec/batch)
2017-05-05 03:56:06.727854: step 184550, loss = 1.84 (843.9 examples/sec; 0.152 sec/batch)
2017-05-05 03:56:08.215174: step 184560, loss = 1.76 (860.5 examples/sec; 0.149 sec/batch)
2017-05-05 03:56:09.759914: step 184570, loss = 1.90 (828.6 examples/sec; 0.154 sec/batch)
2017-05-05 03:56:11.294672: step 184580, loss = 1.83 (834.0 examples/sec; 0.153 sec/batch)
2017-05-05 03:56:12.804861: step 184590, loss = 1.87 (847.6 examples/sec; 0.151 sec/batch)
2017-05-05 03:56:14.337287: step 184600, loss = 1.88 (835.3 examples/sec; 0.153 sec/batch)
2017-05-05 03:56:15.860314: step 184610, loss = 1.79 (840.4 examples/sec; 0.152 sec/batch)
2017-05-05 03:56:17.464378: step 184620, loss = 1.90 (798.0 examples/sec; 0.160 sec/batch)
2017-05-05 03:56:18.956205: step 184630, loss = 1.88 (858.0 examples/sec; 0.149 sec/batch)
2017-05-05 03:56:20.473713: step 184640, loss = 1.77 (843.5 examples/sec; 0.152 sec/batch)
2017-05-05 03:56:22.007193: step 184650, loss = 1.79 (834.7 examples/sec; 0.153 sec/batch)
2017-05-05 03:56:23.581579: step 184660, loss = 1.77 (813.0 examples/sec; 0.157 sec/batch)
2017-05-05 03:56:25.112830: step 184670, loss = 1.85 (835.9 examples/sec; 0.153 sec/batch)
2017-05-05 03:56:26.648310: step 184680, loss = 1.82 (833.6 examples/sec; 0.154 sec/batch)
2017-05-05 03:56:28.163807: step 184690, loss = 1.95 (844.6 examples/sec; 0.152 sec/batch)
2017-05-05 03:56:29.687003: step 184700, loss = 1.79 (840.3 examples/sec; 0.152 sec/batch)
2017-05-05 03:56:31.262684: step 184710, loss = 1.83 (812.3 examples/sec; 0.158 sec/batch)
2017-05-05 03:56:32.841707: step 184720, loss = 1.79 (810.6 examples/sec; 0.158 sec/batch)
2017-05-05 03:56:34.378485: step 184730, loss = 1.72 (832.9 examples/sec; 0.154 sec/batch)
2017-05-05 03:56:35.862500: step 184740, loss = 1.96 (862.5 examples/sec; 0.148 sec/batch)
2017-05-05 03:56:37.390128: step 184750, loss = 1.90 (837.9 examples/sec; 0.153 sec/batch)
2017-05-05 03:56:38.901911: step 184760, loss = 1.97 (846.7 examples/sec; 0.151 sec/batch)
2017-05-05 03:56:40.410014: step 184770, loss = 1.88 (848.7 examples/sec; 0.151 sec/batch)
2017-05-05 03:56:41.942122: step 184780, loss = 1.81 (835.5 examples/sec; 0.153 sec/batch)
2017-05-05 03:56:43.482056: step 184790, loss = 2.01 (831.2 examples/sec; 0.154 sec/batch)
2017-05-05 03:56:45.004548: step 184800, loss = 1.75 (840.7 examples/sec; 0.152 sec/batch)
2017-05-05 03:56:46.509541: step 184810, loss = 1.75 (850.5 examples/sec; 0.151 sec/batch)
2017-05-05 03:56:48.044497: step 184820, loss = 1.88 (833.9 examples/sec; 0.153 sec/batch)
2017-05-05 03:56:49.535201: step 184830, loss = 1.81 (858.7 examples/sec; 0.149 sec/batch)
2017-05-05 03:56:51.070026: step 184840, loss = 1.93 (834.0 examples/sec; 0.153 sec/batch)
2017-05-05 03:56:52.606338: step 184850, loss = 2.00 (833.2 examples/sec; 0.154 sec/batch)
2017-05-05 03:56:54.131601: step 184860, loss = 1.75 (839.2 examples/sec; 0.153 sec/batch)
2017-05-05 03:56:55.661286: step 184870, loss = 1.92 (836.8 examples/sec; 0.153 sec/batch)
2017-05-05 03:56:57.183617: step 184880, loss = 1.80 (840.8 examples/sec; 0.152 sec/batch)
2017-05-05 03:56:58.749156: step 184890, loss = 1.76 (817.6 examples/sec; 0.157 sec/batch)
2017-05-05 03:57:00.266266: step 184900, loss = 1.85 (843.7 examples/sec; 0.152 sec/batch)
2017-05-05 03:57:01.782050: step 184910, loss = 1.87 (844.4 examples/sec; 0.152 sec/batch)
2017-05-05 03:57:03.325014: step 184920, loss = 1.89 (829.6 examples/sec; 0.154 sec/batch)
2017-05-05 03:57:04.754948: step 184930, loss = 1.81 (895.2 examples/sec; 0.143 sec/batch)
2017-05-05 03:57:06.230547: step 184940, loss = 1.83 (867.4 examples/sec; 0.148 sec/batch)
2017-05-05 03:57:07.712311: step 184950, loss = 1.89 (863.8 examples/sec; 0.148 sec/batch)
2017-05-05 03:57:09.260179: step 184960, loss = 1.84 (826.9 examples/sec; 0.155 sec/batch)
2017-05-05 03:57:10.822141: step 184970, loss = 1.96 (819.5 examples/sec; 0.156 sec/batch)
2017-05-05 03:57:12.297441: step 184980, loss = 1.93 (867.6 examples/sec; 0.148 sec/batch)
2017-05-05 03:57:13.809965: step 184990, loss = 1.82 (846.3 examples/sec; 0.151 sec/batch)
2017-05-05 03:57:15.326806: step 185000, loss = 1.90 (843.9 examples/sec; 0.152 sec/batch)
2017-05-05 03:57:16.841586: step 185010, loss = 1.71 (845.0 examples/sec; 0.151 sec/batch)
2017-05-05 03:57:18.379792: step 185020, loss = 2.01 (832.1 examples/sec; 0.154 sec/batch)
2017-05-05 03:57:19.897520: step 185030, loss = 1.80 (843.4 examples/sec; 0.152 sec/batch)
2017-05-05 03:57:21.448588: step 185040, loss = 1.93 (825.2 examples/sec; 0.155 sec/batch)
2017-05-05 03:57:23.051947: step 185050, loss = 1.85 (798.3 examples/sec; 0.160 sec/batch)
2017-05-05 03:57:24.528881: step 185060, loss = 1.86 (866.7 examples/sec; 0.148 sec/batch)
2017-05-05 03:57:26.073362: step 185070, loss = 1.98 (828.8 examples/sec; 0.154 sec/batch)
2017-05-05 03:57:27.592702: step 185080, loss = 1.87 (842.5 examples/sec; 0.152 sec/batch)
2017-05-05 03:57:29.146663: step 185090, loss = 2.00 (823.7 examples/sec; 0.155 sec/batch)
2017-05-05 03:57:30.702652: step 185100, loss = 1.79 (822.6 examples/sec; 0.156 sec/batch)
2017-05-05 03:57:32.280165: step 185110, loss = 1.93 (811.4 examples/sec; 0.158 sec/batch)
2017-05-05 03:57:33.753479: step 185120, loss = 1.96 (868.8 examples/sec; 0.147 sec/batch)
2017-05-05 03:57:35.408574: step 185130, loss = 1.83 (773.4 examples/sec; 0.166 sec/batch)
2017-05-05 03:57:36.815137: step 185140, loss = 1.77 (910.0 examples/sec; 0.141 sec/batch)
2017-05-05 03:57:38.367462: step 185150, loss = 2.11 (824.6 examples/sec; 0.155 sec/batch)
2017-05-05 03:57:39.865823: step 185160, loss = 1.80 (854.3 examples/sec; 0.150 sec/batch)
2017-05-05 03:57:41.349775: step 185170, loss = 1.83 (862.6 examples/sec; 0.148 sec/batch)
2017-05-05 03:57:42.885531: step 185180, loss = 1.68 (833.5 examples/sec; 0.154 sec/batch)
2017-05-05 03:57:44.415602: step 185190, loss = 1.77 (836.6 examples/sec; 0.153 sec/batch)
2017-05-05 03:57:45.967661: step 185200, loss = 1.73 (824.7 examples/sec; 0.155 sec/batch)
2017-05-05 03:57:47.507706: step 185210, loss = 1.90 (831.1 examples/sec; 0.154 sec/batch)
2017-05-05 03:57:49.032544: step 185220, loss = 1.82 (839.4 examples/sec; 0.152 sec/batch)
2017-05-05 03:57:50.548370: step 185230, loss = 1.84 (844.4 examples/sec; 0.152 sec/batch)
2017-05-05 03:57:52.081925: step 185240, loss = 1.69 (834.7 examples/sec; 0.153 sec/batch)
2017-05-05 03:57:53.594974: step 185250, loss = 1.73 (846.0 examples/sec; 0.151 sec/batch)
2017-05-05 03:57:55.112600: step 185260, loss = 1.83 (843.4 examples/sec; 0.152 sec/batch)
2017-05-05 03:57:56.657931: step 185270, loss = 1.76 (828.3 examples/sec; 0.155 sec/batch)
2017-05-05 03:57:58.216303: step 185280, loss = 1.88 (821.4 examples/sec; 0.156 sec/batch)
2017-05-05 03:57:59.728679: step 185290, loss = 1.76 (846.3 examples/sec; 0.151 sec/batch)
2017-05-05 03:58:01.243968: step 185300, loss = 1.77 (844.7 examples/sec; 0.152 sec/batch)
2017-05-05 03:58:02.806332: step 185310, loss = 1.93 (819.3 examples/sec; 0.156 sec/batch)
2017-05-05 03:58:04.327489: step 185320, loss = 1.79 (841.5 examples/sec; 0.152 sec/batch)
2017-05-05 03:58:05.947913: step 185330, loss = 1.81 (789.9 examples/sec; 0.162 sec/batch)
2017-05-05 03:58:07.339818: step 185340, loss = 1.87 (919.6 examples/sec; 0.139 sec/batch)
2017-05-05 03:58:08.859205: step 185350, loss = 1.91 (842.4 examples/sec; 0.152 sec/batch)
2017-05-05 03:58:10.409216: step 185360, loss = 1.90 (825.8 examples/sec; 0.155 sec/batch)
2017-05-05 03:58:11.944186: step 185370, loss = 1.79 (833.9 examples/sec; 0.153 sec/batch)
2017-05-05 03:58:13.458271: step 185380, loss = 1.82 (845.4 examples/sec; 0.151 sec/batch)
2017-05-05 03:58:15.024610: step 185390, loss = 1.76 (817.2 examples/sec; 0.157 sec/batch)
2017-05-05 03:58:16.517076: step 185400, loss = 1.90 (857.6 examples/sec; 0.149 sec/batch)
2017-05-05 03:58:18.015110: step 185410, loss = 1.80 (854.5 examples/sec; 0.150 sec/batch)
2017-05-05 03:58:19.594701: step 185420, loss = 1.69 (810.3 examples/sec; 0.158 sec/batch)
2017-05-05 03:58:21.149796: step 185430, loss = 2.01 (823.1 examples/sec; 0.156 sec/batch)
2017-05-05 03:58:22.667282: step 185440, loss = 1.86 (843.5 examples/sec; 0.152 sec/batch)
2017-05-05 03:58:24.238395: step 185450, loss = 1.76 (814.7 examples/sec; 0.157 sec/batch)
2017-05-05 03:58:25.745086: step 185460, loss = 1.87 (849.6 examples/sec; 0.151 sec/batch)
2017-05-05 03:58:27.240187: step 185470, loss = 1.82 (856.1 examples/sec; 0.150 sec/batch)
2017-05-05 03:58:28.846100: step 185480, loss = 2.02 (797.1 examples/sec; 0.161 sec/batch)
2017-05-05 03:58:30.339317: step 185490, loss = 1.69 (857.2 examples/sec; 0.149 sec/batch)
2017-05-05 03:58:31.824954: step 185500, loss = 1.79 (861.6 examples/sec; 0.149 sec/batch)
2017-05-05 03:58:33.390138: step 185510, loss = 1.72 (817.8 examples/sec; 0.157 sec/batch)
2017-05-05 03:58:34.968371: step 185520, loss = 1.92 (811.0 examples/sec; 0.158 sec/batch)
2017-05-05 03:58:36.509639: step 185530, loss = 1.87 (830.5 examples/sec; 0.154 sec/batch)
2017-05-05 03:58:38.171371: step 185540, loss = 1.82 (770.3 examples/sec; 0.166 sec/batch)
2017-05-05 03:58:39.664796: step 185550, loss = 1.93 (857.1 examples/sec; 0.149 sec/batch)
2017-05-05 03:58:41.227568: step 185560, loss = 1.83 (819.1 examples/sec; 0.156 sec/batch)
2017-05-05 03:58:42.731155: step 185570, loss = 1.80 (851.3 examples/sec; 0.150 sec/batch)
2017-05-05 03:58:44.298280: step 185580, loss = 1.85 (816.8 examples/sec; 0.157 sec/batch)
2017-05-05 03:58:45.797165: step 185590, loss = 1.86 (854.0 examples/sec; 0.150 sec/batch)
2017-05-05 03:58:47.286446: step 185600, loss = 1.72 (859.5 examples/sec; 0.149 sec/batch)
2017-05-05 03:58:48.893886: step 185610, loss = 1.85 (796.3 examples/sec; 0.161 sec/batch)
2017-05-05 03:58:50.409867: step 185620, loss = 1.94 (844.3 examples/sec; 0.152 sec/batch)
2017-05-05 03:58:52.022772: step 185630, loss = 1.79 (793.6 examples/sec; 0.161 sec/batch)
2017-05-05 03:58:53.536001: step 185640, loss = 1.85 (845.9 examples/sec; 0.151 sec/batch)
2017-05-05 03:58:55.232796: step 185650, loss = 1.87 (754.4 examples/sec; 0.170 sec/batch)
2017-05-05 03:58:56.557829: step 185660, loss = 1.77 (966.0 examples/sec; 0.133 sec/batch)
2017-05-05 03:58:58.114415: step 185670, loss = 1.90 (822.3 examples/sec; 0.156 sec/batch)
2017-05-05 03:58:59.649765: step 185680, loss = 1.78 (833.7 examples/sec; 0.154 sec/batch)
2017-05-05 03:59:01.165343: step 185690, loss = 1.86 (844.6 examples/sec; 0.152 sec/batch)
2017-05-05 03:59:02.693620: step 185700, loss = 1.97 (837.5 examples/sec; 0.153 sec/batch)
2017-05-05 03:59:04.258860: step 185710, loss = 1.87 (817.8 examples/sec; 0.157 sec/batch)
2017-05-05 03:59:05.775717: step 185720, loss = 1.77 (843.9 examples/sec; 0.152 sec/batch)
2017-05-05 03:59:07.304808: step 185730, loss = 2.03 (837.1 examples/sec; 0.153 sec/batch)
2017-05-05 03:59:08.887837: step 185740, loss = 1.80 (808.6 examples/sec; 0.158 sec/batch)
2017-05-05 03:59:10.421803: step 185750, loss = 1.81 (834.4 examples/sec; 0.153 sec/batch)
2017-05-05 03:59:11.904780: step 185760, loss = 1.79 (863.1 examples/sec; 0.148 sec/batch)
2017-05-05 03:59:13.485961: step 185770, loss = 1.92 (809.5 examples/sec; 0.158 sec/batch)
2017-05-05 03:59:14.962566: step 185780, loss = 2.03 (866.9 examples/sec; 0.148 sec/batch)
2017-05-05 03:59:16.550790: step 185790, loss = 1.82 (805.9 examples/sec; 0.159 sec/batch)
2017-05-05 03:59:18.073999: step 185800, loss = 1.73 (840.3 examples/sec; 0.152 sec/batch)
2017-05-05 03:59:19.608681: step 185810, loss = 1.85 (834.0 examples/sec; 0.153 sec/batch)
2017-05-05 03:59:21.158823: step 185820, loss = 1.77 (825.7 examples/sec; 0.155 sec/batch)
2017-05-05 03:59:22.669113: step 185830, loss = 1.85 (847.5 examples/sec; 0.151 sec/batch)
2017-05-05 03:59:24.201077: step 185840, loss = 1.75 (835.5 examples/sec; 0.153 sec/batch)
2017-05-05 03:59:25.842769: step 185850, loss = 1.94 (779.7 examples/sec; 0.164 sec/batch)
2017-05-05 03:59:27.403863: step 185860, loss = 1.80 (819.9 examples/sec; 0.156 sec/batch)
2017-05-05 03:59:28.946562: step 185870, loss = 1.87 (829.7 examples/sec; 0.154 sec/batch)
2017-05-05 03:59:30.443938: step 185880, loss = 1.90 (854.8 examples/sec; 0.150 sec/batch)
2017-05-05 03:59:31.958395: step 185890, loss = 1.86 (845.2 examples/sec; 0.151 sec/batch)
2017-05-05 03:59:33.454391: step 185900, loss = 1.74 (855.6 examples/sec; 0.150 sec/batch)
2017-05-05 03:59:34.984951: step 185910, loss = 1.98 (836.3 examples/sec; 0.153 sec/batch)
2017-05-05 03:59:36.537830: step 185920, loss = 2.02 (824.3 examples/sec; 0.155 sec/batch)
2017-05-05 03:59:38.051263: step 185930, loss = 1.71 (845.8 examples/sec; 0.151 sec/batch)
2017-05-05 03:59:39.601135: step 185940, loss = 1.93 (825.9 examples/sec; 0.155 sec/batch)
2017-05-05 03:59:41.180651: step 185950, loss = 1.90 (810.4 examples/sec; 0.158 sec/batch)
2017-05-05 03:59:42.682201: step 185960, loss = 1.92 (852.5 examples/sec; 0.150 sec/batch)
2017-05-05 03:59:44.221616: step 185970, loss = 1.78 (831.5 examples/sec; 0.154 sec/batch)
2017-05-05 03:59:45.712562: step 185980, loss = 1.86 (858.5 examples/sec; 0.149 sec/batch)
2017-05-05 03:59:47.261484: step 185990, loss = 1.91 (826.4 examples/sec; 0.155 sec/batch)
2017-05-05 03:59:48.829188: step 186000, loss = 1.95 (816.5 examples/sec; 0.157 sec/batch)
2017-05-05 03:59:50.369286: step 186010, loss = 1.70 (831.1 examples/sec; 0.154 sec/batch)
2017-05-05 03:59:51.878634: step 186020, loss = 1.81 (848.0 examples/sec; 0.151 sec/batch)
2017-05-05 03:59:53.435439: step 186030, loss = 1.99 (822.2 examples/sec; 0.156 sec/batch)
2017-05-05 03:59:54.960939: step 186040, loss = 1.92 (839.1 examples/sec; 0.153 sec/batch)
2017-05-05 03:59:56.515639: step 186050, loss = 2.00 (823.3 examples/sec; 0.155 sec/batch)
2017-05-05 03:59:57.986078: step 186060, loss = 1.89 (870.5 examples/sec; 0.147 sec/batch)
2017-05-05 03:59:59.559074: step 186070, loss = 1.89 (813.7 examples/sec; 0.157 sec/batch)
2017-05-05 04:00:01.065653: step 186080, loss = 1.89 (849.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:00:02.604224: step 186090, loss = 1.81 (831.9 examples/sec; 0.154 sec/batch)
2017-05-05 04:00:04.147186: step 186100, loss = 1.64 (829.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:00:05.670681: step 186110, loss = 2.04 (840.2 examples/sec; 0.152 sec/batch)
2017-05-05 04:00:07.346445: step 186120, loss = 1.85 (763.8 examples/sec; 0.168 sec/batch)
2017-05-05 04:00:08.782295: step 186130, loss = 1.78 (891.5 examples/sec; 0.144 sec/batch)
2017-05-05 04:00:10.312667: step 186140, loss = 1.99 (836.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:00:11.897658: step 186150, loss = 1.97 (807.6 examples/sec; 0.158 sec/batch)
2017-05-05 04:00:13.471414: step 186160, loss = 1.91 (813.3 examples/sec; 0.157 sec/batch)
2017-05-05 04:00:15.028454: step 186170, loss = 1.78 (822.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:00:16.592537: step 186180, loss = 1.87 (818.4 examples/sec; 0.156 sec/batch)
2017-05-05 04:00:18.131495: step 186190, loss = 1.76 (831.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:00:19.660144: step 186200, loss = 2.04 (837.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:00:21.218651: step 186210, loss = 1.80 (821.3 examples/sec; 0.156 sec/batch)
2017-05-05 04:00:22.747015: step 186220, loss = 1.90 (837.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:00:24.332122: step 186230, loss = 1.68 (807.5 examples/sec; 0.159 sec/batch)
2017-05-05 04:00:25.828598: step 186240, loss = 1.81 (855.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:00:27.385055: step 186250, loss = 1.87 (822.4 examples/sec; 0.156 sec/batch)
2017-05-05 04:00:28.994749: step 186260, loss = 1.83 (795.2 examples/sec; 0.161 sec/batch)
2017-05-05 04:00:30.501329: step 186270, loss = 1.92 (849.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:00:32.057420: step 186280, loss = 1.82 (822.6 examples/sec; 0.156 sec/batch)
2017-05-05 04:00:33.607257: step 186290, loss = 1.78 (825.9 examples/sec; 0.155 sec/batch)
2017-05-05 04:00:35.126300: step 186300, loss = 1.95 (842.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:00:36.768122: step 186310, loss = 1.89 (779.6 examples/sec; 0.164 sec/batch)
2017-05-05 04:00:38.422377: step 186320, loss = 1.93 (773.8 examples/sec; 0.165 sec/batch)
2017-05-05 04:00:39.857452: step 186330, loss = 1.93 (891.9 examples/sec; 0.144 sec/batch)
2017-05-05 04:00:41.438439: step 186340, loss = 1.81 (809.6 examples/sec; 0.158 sec/batch)
2017-05-05 04:00:42.971795: step 186350, loss = 1.87 (834.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:00:44.482585: step 186360, loss = 1.94 (847.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:00:46.022551: step 186370, loss = 1.88 (831.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:00:47.533368: step 186380, loss = 1.86 (847.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:00:49.122713: step 186390, loss = 1.82 (805.4 examples/sec; 0.159 sec/batch)
2017-05-05 04:00:50.682920: step 186400, loss = 1.88 (820.4 examples/sec; 0.156 sec/batch)
2017-05-05 04:00:52.250095: step 186410, loss = 1.82 (816.8 examples/sec; 0.157 sec/batch)
2017-05-05 04:00:53.842130: step 186420, loss = 1.95 (804.0 examples/sec; 0.159 sec/batch)
2017-05-05 04:00:55.380103: step 186430, loss = 1.76 (832.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:00:56.899813: step 186440, loss = 2.00 (842.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:00:58.419525: step 186450, loss = 1.80 (842.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:00:59.928911: step 186460, loss = 2.00 (847.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:01:01.491871: step 186470, loss = 1.82 (819.0 examples/sec; 0.156 sec/batch)
2017-05-05 04:01:03.100894: step 186480, loss = 1.92 (795.5 examples/sec; 0.161 sec/batch)
2017-05-05 04:01:04.670130: step 186490, loss = 1.82 (815.7 examples/sec; 0.157 sec/batch)
2017-05-05 04:01:06.167517: step 186500, loss = 1.79 (854.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:01:07.737844: step 186510, loss = 1.92 (815.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:01:09.233197: step 186520, loss = 1.85 (856.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:01:10.756187: step 186530, loss = 1.98 (840.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:01:12.290874: step 186540, loss = 1.87 (834.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:01:13.850526: step 186550, loss = 1.87 (820.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:01:15.428283: step 186560, loss = 1.97 (811.3 examples/sec; 0.158 sec/batch)
2017-05-05 04:01:16.999671: step 186570, loss = 1.88 (814.6 examples/sec; 0.157 sec/batch)
2017-05-05 04:01:18.544105: step 186580, loss = 1.86 (828.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:01:20.102703: step 186590, loss = 1.90 (821.3 examples/sec; 0.156 sec/batch)
2017-05-05 04:01:21.596809: step 186600, loss = 1.78 (856.7 examples/sec; 0.149 sec/batch)
2017-05-05 04:01:23.129666: step 186610, loss = 1.96 (835.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:01:24.636391: step 186620, loss = 1.86 (849.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:01:26.135452: step 186630, loss = 1.86 (853.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:01:27.693775: step 186640, loss = 1.80 (821.4 examples/sec; 0.156 sec/batch)
2017-05-05 04:01:29.227370: step 186650, loss = 2.00 (834.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:01:30.771027: step 186660, loss = 1.89 (829.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:01:32.358472: step 186670, loss = 1.77 (806.3 examples/sec; 0.159 sec/batch)
2017-05-05 04:01:33.921978: step 186680, loss = 1.97 (818.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:01:35.491508: step 186690, loss = 1.75 (815.5 examples/sec; 0.157 sec/batch)
2017-05-05 04:01:37.021143: step 186700, loss = 1.94 (836.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:01:38.516538: step 186710, loss = 2.03 (856.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:01:40.056194: step 186720, loss = 1.86 (831.4 examples/sec; 0.154 sec/batch)
2017-05-05 04:01:41.633180: step 186730, loss = 1.86 (811.7 examples/sec; 0.158 sec/batch)
2017-05-05 04:01:43.200711: step 186740, loss = 1.87 (816.6 examples/sec; 0.157 sec/batch)
2017-05-05 04:01:44.759027: step 186750, loss = 1.79 (821.4 examples/sec; 0.156 sec/batch)
2017-05-05 04:01:46.305185: step 186760, loss = 1.86 (827.9 examples/sec; 0.155 sec/batch)
2017-05-05 04:01:47.837374: step 186770, loss = 1.87 (835.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:01:49.355302: step 186780, loss = 1.87 (843.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:01:50.934030: step 186790, loss = 1.92 (810.8 examples/sec; 0.158 sec/batch)
2017-05-05 04:01:52.423894: step 186800, loss = 1.92 (859.1 examples/sec; 0.149 sec/batch)
2017-05-05 04:01:53.966787: step 186810, loss = 1.98 (829.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:01:55.558138: step 186820, loss = 1.97 (804.3 examples/sec; 0.159 sec/batch)
2017-05-05 04:01:57.087096: step 186830, loss = 1.88 (837.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:01:58.627436: step 186840, loss = 1.74 (831.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:02:00.167608: step 186850, loss = 1.79 (831.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:02:01.697694: step 186860, loss = 1.89 (836.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:02:03.224999: step 186870, loss = 1.86 (838.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:02:04.733784: step 186880, loss = 1.87 (848.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:02:06.265499: step 186890, loss = 1.90 (835.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:02:07.762100: step 186900, loss = 1.77 (855.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:02:09.369794: step 186910, loss = 1.81 (796.2 examples/sec; 0.161 sec/batch)
2017-05-05 04:02:10.869650: step 186920, loss = 1.96 (853.4 examples/sec; 0.150 sec/batch)
2017-05-05 04:02:12.512770: step 186930, loss = 1.90 (779.0 examples/sec; 0.164 sec/batch)
2017-05-05 04:02:13.998309: step 186940, loss = 1.84 (861.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:02:15.516685: step 186950, loss = 1.88 (843.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:02:17.069017: step 186960, loss = 1.86 (824.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:02:18.565725: step 186970, loss = 1.93 (855.2 examples/sec; 0.150 sec/batch)
2017-05-05 04:02:20.072402: step 186980, loss = 1.92 (849.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:02:21.622336: step 186990, loss = 1.89 (825.8 examples/sec; 0.155 sec/batch)
2017-05-05 04:02:23.111809: step 187000, loss = 1.89 (859.4 examples/sec; 0.149 sec/batch)
2017-05-05 04:02:24.625465: step 187010, loss = 1.84 (845.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:02:26.177874: step 187020, loss = 1.90 (824.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:02:27.798020: step 187030, loss = 1.90 (790.1 examples/sec; 0.162 sec/batch)
2017-05-05 04:02:29.300430: step 187040, loss = 1.71 (852.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:02:30.807167: step 187050, loss = 1.90 (849.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:02:32.424356: step 187060, loss = 1.82 (791.5 examples/sec; 0.162 sec/batch)
2017-05-05 04:02:33.943114: step 187070, loss = 1.83 (842.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:02:35.434419: step 187080, loss = 1.93 (858.3 examples/sec; 0.149 sec/batch)
2017-05-05 04:02:37.000315: step 187090, loss = 1.93 (817.4 examples/sec; 0.157 sec/batch)
2017-05-05 04:02:38.540531: step 187100, loss = 1.87 (831.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:02:40.205578: step 187110, loss = 1.81 (768.7 examples/sec; 0.167 sec/batch)
2017-05-05 04:02:41.606742: step 187120, loss = 1.83 (913.5 examples/sec; 0.140 sec/batch)
2017-05-05 04:02:43.182299: step 187130, loss = 1.88 (812.4 examples/sec; 0.158 sec/batch)
2017-05-05 04:02:44.668824: step 187140, loss = 1.89 (861.1 examples/sec; 0.149 sec/batch)
2017-05-05 04:02:46.190272: step 187150, loss = 1.81 (841.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:02:47.735941: step 187160, loss = 1.88 (828.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:02:49.259376: step 187170, loss = 1.95 (840.2 examples/sec; 0.152 sec/batch)
2017-05-05 04:02:50.777384: step 187180, loss = 1.96 (843.2 examples/sec; 0.152 sec/batch)
2017-05-05 04:02:52.382080: step 187190, loss = 1.89 (797.7 examples/sec; 0.160 sec/batch)
2017-05-05 04:02:53.849976: step 187200, loss = 1.92 (872.0 examples/sec; 0.147 sec/batch)
2017-05-05 04:02:55.447106: step 187210, loss = 1.84 (801.4 examples/sec; 0.160 sec/batch)
2017-05-05 04:02:56.969708: step 187220, loss = 1.86 (840.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:02:58.553067: step 187230, loss = 1.84 (808.4 examples/sec; 0.158 sec/batch)
2017-05-05 04:03:00.128748: step 187240, loss = 1.97 (812.3 examples/sec; 0.158 sec/batch)
2017-05-05 04:03:01.615828: step 187250, loss = 1.97 (860.7 examples/sec; 0.149 sec/batch)
2017-05-05 04:03:03.182412: step 187260, loss = 1.83 (817.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:03:04.767774: step 187270, loss = 1.80 (807.4 examples/sec; 0.159 sec/batch)
2017-05-05 04:03:06.282516: step 187280, loss = 1.89 (845.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:03:07.815908: step 187290, loss = 1.92 (834.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:03:09.345284: step 187300, loss = 1.91 (836.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:03:10.975765: step 187310, loss = 1.82 (785.0 examples/sec; 0.163 sec/batch)
2017-05-05 04:03:12.368269: step 187320, loss = 1.95 (919.2 examples/sec; 0.139 sec/batch)
2017-05-05 04:03:13.892062: step 187330, loss = 1.82 (840.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:03:15.411889: step 187340, loss = 1.77 (842.2 examples/sec; 0.152 sec/batch)
2017-05-05 04:03:16.916605: step 187350, loss = 1.89 (850.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:03:18.448839: step 187360, loss = 1.82 (835.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:03:20.013301: step 187370, loss = 1.88 (818.2 examples/sec; 0.156 sec/batch)
2017-05-05 04:03:21.494168: step 187380, loss = 1.86 (864.4 examples/sec; 0.148 sec/batch)
2017-05-05 04:03:22.988717: step 187390, loss = 2.11 (856.4 examples/sec; 0.149 sec/batch)
2017-05-05 04:03:24.578426: step 187400, loss = 1.83 (805.2 examples/sec; 0.159 sec/batch)
2017-05-05 04:03:26.090637: step 187410, loss = 1.80 (846.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:03:27.618060: step 187420, loss = 2.00 (838.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:03:29.202922: step 187430, loss = 1.95 (807.6 examples/sec; 0.158 sec/batch)
2017-05-05 04:03:30.729198: step 187440, loss = 1.78 (838.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:03:32.283605: step 187450, loss = 1.80 (823.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:03:33.825939: step 187460, loss = 1.71 (829.9 examples/sec; 0.154 sec/batch)
2017-05-05 04:03:35.358574: step 187470, loss = 2.03 (835.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:03:36.864588: step 187480, loss = 1.73 (849.9 examples/sec; 0.151 sec/batch)
2017-05-05 04:03:38.422103: step 187490, loss = 1.80 (821.8 examples/sec; 0.156 sec/batch)
2017-05-05 04:03:39.913665: step 187500, loss = 1.89 (858.2 examples/sec; 0.149 sec/batch)
2017-05-05 04:03:41.483283: step 187510, loss = 1.82 (815.5 examples/sec; 0.157 sec/batch)
2017-05-05 04:03:43.005710: step 187520, loss = 1.79 (840.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:03:44.536129: step 187530, loss = 1.79 (836.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:03:46.065459: step 187540, loss = 1.79 (837.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:03:47.537053: step 187550, loss = 1.77 (869.8 examples/sec; 0.147 sec/batch)
2017-05-05 04:03:49.067504: step 187560, loss = 1.82 (836.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:03:50.629986: step 187570, loss = 1.89 (819.2 examples/sec; 0.156 sec/batch)
2017-05-05 04:03:52.129181: step 187580, loss = 1.82 (853.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:03:53.669733: step 187590, loss = 1.79 (830.9 examples/sec; 0.154 sec/batch)
2017-05-05 04:03:55.215431: step 187600, loss = 1.88 (828.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:03:56.726389: step 187610, loss = 2.02 (847.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:03:58.235851: step 187620, loss = 2.12 (848.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:03:59.757991: step 187630, loss = 1.88 (840.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:04:01.274446: step 187640, loss = 1.94 (844.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:04:02.855549: step 187650, loss = 1.66 (809.6 examples/sec; 0.158 sec/batch)
2017-05-05 04:04:04.383644: step 187660, loss = 1.83 (837.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:04:05.871349: step 187670, loss = 1.81 (860.4 examples/sec; 0.149 sec/batch)
2017-05-05 04:04:07.394444: step 187680, loss = 1.90 (840.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:04:08.939836: step 187690, loss = 1.72 (828.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:04:10.447288: step 187700, loss = 1.78 (849.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:04:12.001049: step 187710, loss = 1.93 (823.8 examples/sec; 0.155 sec/batch)
2017-05-05 04:04:13.583047: step 187720, loss = 1.81 (809.1 examples/sec; 0.158 sec/batch)
2017-05-05 04:04:15.108117: step 187730, loss = 1.87 (839.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:04:16.588447: step 187740, loss = 1.91 (864.7 examples/sec; 0.148 sec/batch)
2017-05-05 04:04:18.113863: step 187750, loss = 1.97 (839.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:04:19.616450: step 187760, loss = 1.77 (851.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:04:21.063155: step 187770, loss = 1.91 (884.8 examples/sec; 0.145 sec/batch)
2017-05-05 04:04:22.548823: step 187780, loss = 1.88 (861.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:04:24.094938: step 187790, loss = 1.77 (827.9 examples/sec; 0.155 sec/batch)
2017-05-05 04:04:25.591553: step 187800, loss = 1.94 (855.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:04:27.082686: step 187810, loss = 1.91 (858.4 examples/sec; 0.149 sec/batch)
2017-05-05 04:04:28.575290: step 187820, loss = 1.85 (857.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:04:30.098458: step 187830, loss = 1.77 (840.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:04:31.594897: step 187840, loss = 1.95 (855.4 examples/sec; 0.150 sec/batch)
2017-05-05 04:04:33.093545: step 187850, loss = 1.79 (854.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:04:34.582796: step 187860, loss = 1.83 (859.5 examples/sec; 0.149 sec/batch)
2017-05-05 04:04:36.087073: step 187870, loss = 1.86 (850.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:04:37.578527: step 187880, loss = 1.78 (858.2 examples/sec; 0.149 sec/batch)
2017-05-05 04:04:39.092541: step 187890, loss = 1.87 (845.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:04:40.611893: step 187900, loss = 1.86 (842.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:04:42.122521: step 187910, loss = 1.89 (847.3 examples/sec; 0.151 sec/batch)
2017-05-05 04:04:43.639857: step 187920, loss = 1.86 (843.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:04:45.095690: step 187930, loss = 1.72 (879.2 examples/sec; 0.146 sec/batch)
2017-05-05 04:04:46.634066: step 187940, loss = 1.84 (832.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:04:48.121922: step 187950, loss = 2.18 (860.3 examples/sec; 0.149 sec/batch)
2017-05-05 04:04:49.634950: step 187960, loss = 1.74 (846.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:04:51.111323: step 187970, loss = 1.91 (867.0 examples/sec; 0.148 sec/batch)
2017-05-05 04:04:52.643189: step 187980, loss = 1.87 (835.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:04:54.184770: step 187990, loss = 1.88 (830.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:04:55.681587: step 188000, loss = 2.02 (855.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:04:57.200335: step 188010, loss = 1.75 (842.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:04:58.689333: step 188020, loss = 1.79 (859.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:05:00.174053: step 188030, loss = 1.86 (862.1 examples/sec; 0.148 sec/batch)
2017-05-05 04:05:01.713278: step 188040, loss = 1.88 (831.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:05:03.272256: step 188050, loss = 1.94 (821.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:05:04.778124: step 188060, loss = 1.84 (850.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:05:06.316000: step 188070, loss = 1.88 (832.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:05:07.811543: step 188080, loss = 1.86 (855.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:05:09.301219: step 188090, loss = 2.00 (859.2 examples/sec; 0.149 sec/batch)
2017-05-05 04:05:10.961392: step 188100, loss = 1.86 (771.0 examples/sec; 0.166 sec/batch)
2017-05-05 04:05:12.379541: step 188110, loss = 1.77 (902.6 examples/sec; 0.142 sec/batch)
2017-05-05 04:05:13.848988: step 188120, loss = 1.86 (871.1 examples/sec; 0.147 sec/batch)
2017-05-05 04:05:15.333953: step 188130, loss = 1.84 (862.0 examples/sec; 0.148 sec/batch)
2017-05-05 04:05:16.853635: step 188140, loss = 1.94 (842.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:05:18.385585: step 188150, loss = 1.91 (835.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:05:19.911713: step 188160, loss = 1.87 (838.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:05:21.445710: step 188170, loss = 1.84 (834.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:05:22.992064: step 188180, loss = 1.85 (827.7 examples/sec; 0.155 sec/batch)
2017-05-05 04:05:24.545828: step 188190, loss = 1.82 (823.8 examples/sec; 0.155 sec/batch)
2017-05-05 04:05:26.037639: step 188200, loss = 1.79 (858.0 examples/sec; 0.149 sec/batch)
2017-05-05 04:05:27.562572: step 188210, loss = 1.82 (839.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:05:29.094815: step 188220, loss = 1.79 (835.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:05:30.605289: step 188230, loss = 1.86 (847.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:05:32.144676: step 188240, loss = 1.85 (831.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:05:33.686726: step 188250, loss = 1.87 (830.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:05:35.169713: step 188260, loss = 1.85 (863.1 examples/sec; 0.148 sec/batch)
2017-05-05 04:05:36.734434: step 188270, loss = 1.87 (818.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:05:38.273609: step 188280, loss = 1.87 (831.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:05:39.800216: step 188290, loss = 2.07 (838.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:05:41.432377: step 188300, loss = 1.84 (784.2 examples/sec; 0.163 sec/batch)
2017-05-05 04:05:42.850457: step 188310, loss = 1.98 (902.6 examples/sec; 0.142 sec/batch)
2017-05-05 04:05:44.419886: step 188320, loss = 2.01 (815.6 examples/sec; 0.157 sec/batch)
2017-05-05 04:05:45.988651: step 188330, loss = 1.77 (815.9 examples/sec; 0.157 sec/batch)
2017-05-05 04:05:47.444393: step 188340, loss = 1.91 (879.3 examples/sec; 0.146 sec/batch)
2017-05-05 04:05:48.955599: step 188350, loss = 1.85 (847.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:05:50.505501: step 188360, loss = 1.81 (825.9 examples/sec; 0.155 sec/batch)
2017-05-05 04:05:52.030895: step 188370, loss = 1.71 (839.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:05:53.542381: step 188380, loss = 1.89 (846.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:05:55.084337: step 188390, loss = 1.72 (830.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:05:56.623665: step 188400, loss = 1.93 (831.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:05:58.137402: step 188410, loss = 1.87 (845.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:05:59.637422: step 188420, loss = 1.90 (853.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:06:01.160250: step 188430, loss = 1.82 (840.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:06:02.712101: step 188440, loss = 1.85 (824.8 examples/sec; 0.155 sec/batch)
2017-05-05 04:06:04.217327: step 188450, loss = 1.80 (850.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:06:05.745698: step 188460, loss = 1.92 (837.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:06:07.232129: step 188470, loss = 1.80 (861.1 examples/sec; 0.149 sec/batch)
2017-05-05 04:06:08.748578: step 188480, loss = 1.74 (844.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:06:10.320066: step 188490, loss = 1.85 (814.5 examples/sec; 0.157 sec/batch)
2017-05-05 04:06:11.844748: step 188500, loss = 1.94 (839.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:06:13.349838: step 188510, loss = 1.85 (850.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:06:14.887854: step 188520, loss = 1.73 (832.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:06:16.435035: step 188530, loss = 1.79 (827.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:06:17.956795: step 188540, loss = 1.86 (841.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:06:19.497099: step 188550, loss = 1.72 (831.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:06:21.007634: step 188560, loss = 1.87 (847.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:06:22.501335: step 188570, loss = 1.78 (856.9 examples/sec; 0.149 sec/batch)
2017-05-05 04:06:23.957898: step 188580, loss = 1.85 (878.8 examples/sec; 0.146 sec/batch)
2017-05-05 04:06:25.451659: step 188590, loss = 1.95 (856.9 examples/sec; 0.149 sec/batch)
2017-05-05 04:06:26.994049: step 188600, loss = 1.73 (829.9 examples/sec; 0.154 sec/batch)
2017-05-05 04:06:28.503368: step 188610, loss = 1.70 (848.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:06:29.981777: step 188620, loss = 1.74 (865.8 examples/sec; 0.148 sec/batch)
2017-05-05 04:06:31.499906: step 188630, loss = 1.96 (843.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:06:32.983567: step 188640, loss = 1.71 (862.7 examples/sec; 0.148 sec/batch)
2017-05-05 04:06:34.521147: step 188650, loss = 1.98 (832.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:06:35.987927: step 188660, loss = 1.89 (872.7 examples/sec; 0.147 sec/batch)
2017-05-05 04:06:37.481391: step 188670, loss = 1.95 (857.1 examples/sec; 0.149 sec/batch)
2017-05-05 04:06:38.999953: step 188680, loss = 2.07 (842.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:06:40.535858: step 188690, loss = 1.78 (833.4 examples/sec; 0.154 sec/batch)
2017-05-05 04:06:42.027191: step 188700, loss = 1.93 (858.3 examples/sec; 0.149 sec/batch)
2017-05-05 04:06:43.544025: step 188710, loss = 1.77 (843.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:06:45.036423: step 188720, loss = 2.00 (857.7 examples/sec; 0.149 sec/batch)
2017-05-05 04:06:46.519787: step 188730, loss = 1.80 (862.9 examples/sec; 0.148 sec/batch)
2017-05-05 04:06:48.081502: step 188740, loss = 1.98 (819.6 examples/sec; 0.156 sec/batch)
2017-05-05 04:06:49.641443: step 188750, loss = 1.81 (820.5 examples/sec; 0.156 sec/batch)
2017-05-05 04:06:51.168284: step 188760, loss = 1.82 (838.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:06:52.734485: step 188770, loss = 1.82 (817.3 examples/sec; 0.157 sec/batch)
2017-05-05 04:06:54.269479: step 188780, loss = 1.94 (833.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:06:55.843739: step 188790, loss = 1.82 (813.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:06:57.315422: step 188800, loss = 1.98 (869.8 examples/sec; 0.147 sec/batch)
2017-05-05 04:06:58.776523: step 188810, loss = 1.70 (876.0 examples/sec; 0.146 sec/batch)
2017-05-05 04:07:00.270842: step 188820, loss = 1.75 (856.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:07:01.767834: step 188830, loss = 1.83 (855.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:07:03.261886: step 188840, loss = 1.94 (856.7 examples/sec; 0.149 sec/batch)
2017-05-05 04:07:04.788099: step 188850, loss = 1.92 (838.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:07:06.304039: step 188860, loss = 1.86 (844.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:07:07.822454: step 188870, loss = 2.04 (843.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:07:09.358709: step 188880, loss = 1.82 (833.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:07:10.896114: step 188890, loss = 1.86 (832.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:07:12.444627: step 188900, loss = 1.98 (826.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:07:14.009425: step 188910, loss = 1.85 (818.0 examples/sec; 0.156 sec/batch)
2017-05-05 04:07:15.486300: step 188920, loss = 1.79 (866.7 examples/sec; 0.148 sec/batch)
2017-05-05 04:07:17.099721: step 188930, loss = 1.77 (793.4 examples/sec; 0.161 sec/batch)
2017-05-05 04:07:18.660885: step 188940, loss = 1.85 (819.9 examples/sec; 0.156 sec/batch)
2017-05-05 04:07:20.159607: step 188950, loss = 1.88 (854.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:07:21.676356: step 188960, loss = 1.96 (843.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:07:23.166493: step 188970, loss = 1.90 (859.0 examples/sec; 0.149 sec/batch)
2017-05-05 04:07:24.722834: step 188980, loss = 1.86 (822.4 examples/sec; 0.156 sec/batch)
2017-05-05 04:07:26.280117: step 188990, loss = 1.92 (821.9 examples/sec; 0.156 sec/batch)
2017-05-05 04:07:27.801299: step 189000, loss = 1.79 (841.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:07:29.363661: step 189010, loss = 1.75 (819.3 examples/sec; 0.156 sec/batch)
2017-05-05 04:07:30.947222: step 189020, loss = 2.08 (808.3 examples/sec; 0.158 sec/batch)
2017-05-05 04:07:32.485645: step 189030, loss = 1.91 (832.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:07:34.006585: step 189040, loss = 1.83 (841.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:07:35.510461: step 189050, loss = 1.80 (851.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:07:37.010820: step 189060, loss = 1.99 (853.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:07:38.543524: step 189070, loss = 1.81 (835.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:07:40.042514: step 189080, loss = 1.84 (853.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:07:41.731165: step 189090, loss = 1.89 (758.0 examples/sec; 0.169 sec/batch)
2017-05-05 04:07:43.095080: step 189100, loss = 1.93 (938.5 examples/sec; 0.136 sec/batch)
2017-05-05 04:07:44.624002: step 189110, loss = 1.83 (837.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:07:46.147867: step 189120, loss = 1.76 (840.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:07:47.649002: step 189130, loss = 1.86 (852.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:07:49.154932: step 189140, loss = 1.95 (850.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:07:50.715068: step 189150, loss = 1.79 (820.4 examples/sec; 0.156 sec/batch)
2017-05-05 04:07:52.252732: step 189160, loss = 1.95 (832.4 examples/sec; 0.154 sec/batch)
2017-05-05 04:07:53.825771: step 189170, loss = 1.81 (813.7 examples/sec; 0.157 sec/batch)
2017-05-05 04:07:55.325367: step 189180, loss = 1.75 (853.6 examples/sec; 0.150 sec/batch)
2017-05-05 04:07:56.914851: step 189190, loss = 1.85 (805.3 examples/sec; 0.159 sec/batch)
2017-05-05 04:07:58.375709: step 189200, loss = 1.80 (876.2 examples/sec; 0.146 sec/batch)
2017-05-05 04:07:59.969354: step 189210, loss = 1.81 (803.2 examples/sec; 0.159 sec/batch)
2017-05-05 04:08:01.527136: step 189220, loss = 1.93 (821.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:08:03.060732: step 189230, loss = 1.91 (834.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:08:04.582662: step 189240, loss = 1.84 (841.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:08:06.126703: step 189250, loss = 1.78 (829.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:08:07.610197: step 189260, loss = 1.89 (862.8 examples/sec; 0.148 sec/batch)
2017-05-05 04:08:09.213150: step 189270, loss = 1.81 (798.5 examples/sec; 0.160 sec/batch)
2017-05-05 04:08:10.757497: step 189280, loss = 1.89 (828.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:08:12.463355: step 189290, loss = 1.96 (750.4 examples/sec; 0.171 sec/batch)
2017-05-05 04:08:13.855619: step 189300, loss = 1.92 (919.4 examples/sec; 0.139 sec/batch)
2017-05-05 04:08:15.371390: step 189310, loss = 1.82 (844.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:08:16.970187: step 189320, loss = 1.92 (800.6 examples/sec; 0.160 sec/batch)
2017-05-05 04:08:18.485894: step 189330, loss = 1.96 (844.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:08:19.999859: step 189340, loss = 1.94 (845.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:08:21.528837: step 189350, loss = 1.96 (837.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:08:23.113588: step 189360, loss = 1.84 (807.7 examples/sec; 0.158 sec/batch)
2017-05-05 04:08:24.708107: step 189370, loss = 1.79 (802.7 examples/sec; 0.159 sec/batch)
2017-05-05 04:08:26.248630: step 189380, loss = 1.80 (830.9 examples/sec; 0.154 sec/batch)
2017-05-05 04:08:27.800343: step 189390, loss = 1.79 (824.9 examples/sec; 0.155 sec/batch)
2017-05-05 04:08:29.312489: step 189400, loss = 1.88 (846.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:08:30.838962: step 189410, loss = 1.88 (838.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:08:32.391855: step 189420, loss = 1.91 (824.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:08:33.916323: step 189430, loss = 1.79 (839.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:08:35.456284: step 189440, loss = 1.76 (831.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:08:37.011970: step 189450, loss = 1.85 (822.8 examples/sec; 0.156 sec/batch)
2017-05-05 04:08:38.495844: step 189460, loss = 1.94 (862.6 examples/sec; 0.148 sec/batch)
2017-05-05 04:08:39.965752: step 189470, loss = 1.83 (870.8 examples/sec; 0.147 sec/batch)
2017-05-05 04:08:41.522138: step 189480, loss = 1.81 (822.4 examples/sec; 0.156 sec/batch)
2017-05-05 04:08:43.016542: step 189490, loss = 1.88 (856.5 examples/sec; 0.149 sec/batch)
2017-05-05 04:08:44.567984: step 189500, loss = 2.14 (825.0 examples/sec; 0.155 sec/batch)
2017-05-05 04:08:46.093480: step 189510, loss = 1.82 (839.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:08:47.636615: step 189520, loss = 1.77 (829.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:08:49.157139: step 189530, loss = 1.97 (841.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:08:50.662789: step 189540, loss = 1.96 (850.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:08:52.162801: step 189550, loss = 1.73 (853.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:08:53.651763: step 189560, loss = 1.97 (859.7 examples/sec; 0.149 sec/batch)
2017-05-05 04:08:55.355368: step 189570, loss = 1.70 (751.3 examples/sec; 0.170 sec/batch)
2017-05-05 04:08:56.706967: step 189580, loss = 1.85 (947.0 examples/sec; 0.135 sec/batch)
2017-05-05 04:08:58.264583: step 189590, loss = 1.85 (821.8 examples/sec; 0.156 sec/batch)
2017-05-05 04:08:59.781365: step 189600, loss = 1.91 (843.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:09:01.308009: step 189610, loss = 1.71 (838.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:09:02.813015: step 189620, loss = 1.88 (850.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:09:04.369280: step 189630, loss = 1.85 (822.5 examples/sec; 0.156 sec/batch)
2017-05-05 04:09:05.860951: step 189640, loss = 1.86 (858.1 examples/sec; 0.149 sec/batch)
2017-05-05 04:09:07.445363: step 189650, loss = 1.82 (808.0 examples/sec; 0.158 sec/batch)
2017-05-05 04:09:08.958107: step 189660, loss = 1.92 (846.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:09:10.525957: step 189670, loss = 1.75 (816.4 examples/sec; 0.157 sec/batch)
2017-05-05 04:09:12.003607: step 189680, loss = 1.82 (866.2 examples/sec; 0.148 sec/batch)
2017-05-05 04:09:13.558650: step 189690, loss = 2.03 (823.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:09:15.108219: step 189700, loss = 1.83 (826.0 examples/sec; 0.155 sec/batch)
2017-05-05 04:09:16.611462: step 189710, loss = 1.87 (851.5 examples/sec; 0.150 sec/batch)
2017-05-05 04:09:18.188925: step 189720, loss = 1.80 (811.4 examples/sec; 0.158 sec/batch)
2017-05-05 04:09:19.724536: step 189730, loss = 1.89 (833.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:09:21.236886: step 189740, loss = 1.95 (846.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:09:22.784408: step 189750, loss = 1.84 (827.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:09:24.295824: step 189760, loss = 1.85 (846.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:09:25.853089: step 189770, loss = 1.85 (822.0 examples/sec; 0.156 sec/batch)
2017-05-05 04:09:27.375525: step 189780, loss = 1.96 (840.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:09:28.970942: step 189790, loss = 1.91 (802.3 examples/sec; 0.160 sec/batch)
2017-05-05 04:09:30.446454: step 189800, loss = 1.85 (867.5 examples/sec; 0.148 sec/batch)
2017-05-05 04:09:32.006655: step 189810, loss = 1.98 (820.4 examples/sec; 0.156 sec/batch)
2017-05-05 04:09:33.515039: step 189820, loss = 1.96 (848.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:09:35.040021: step 189830, loss = 2.03 (839.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:09:36.563309: step 189840, loss = 1.99 (840.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:09:38.145470: step 189850, loss = 1.86 (809.0 examples/sec; 0.158 sec/batch)
2017-05-05 04:09:39.642940: step 189860, loss = 1.74 (854.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:09:41.205270: step 189870, loss = 1.91 (819.3 examples/sec; 0.156 sec/batch)
2017-05-05 04:09:42.718946: step 189880, loss = 1.85 (845.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:09:44.187154: step 189890, loss = 1.74 (871.8 examples/sec; 0.147 sec/batch)
2017-05-05 04:09:45.690685: step 189900, loss = 1.80 (851.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:09:47.293866: step 189910, loss = 1.86 (798.4 examples/sec; 0.160 sec/batch)
2017-05-05 04:09:48.843066: step 189920, loss = 1.97 (826.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:09:50.368561: step 189930, loss = 1.83 (839.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:09:51.940151: step 189940, loss = 1.84 (814.5 examples/sec; 0.157 sec/batch)
2017-05-05 04:09:53.446989: step 189950, loss = 1.94 (849.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:09:54.954667: step 189960, loss = 1.70 (849.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:09:56.504181: step 189970, loss = 1.81 (826.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:09:57.989198: step 189980, loss = 1.87 (861.9 examples/sec; 0.149 sec/batch)
2017-05-05 04:09:59.528667: step 189990, loss = 1.79 (831.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:10:01.041462: step 190000, loss = 1.76 (846.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:10:02.586414: step 190010, loss = 1.75 (828.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:10:04.070834: step 190020, loss = 2.05 (862.3 examples/sec; 0.148 sec/batch)
2017-05-05 04:10:05.596922: step 190030, loss = 1.83 (838.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:10:07.117509: step 190040, loss = 1.85 (841.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:10:08.719271: step 190050, loss = 1.95 (799.1 examples/sec; 0.160 sec/batch)
2017-05-05 04:10:10.303454: step 190060, loss = 1.87 (808.0 examples/sec; 0.158 sec/batch)
2017-05-05 04:10:11.800391: step 190070, loss = 1.91 (855.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:10:13.483044: step 190080, loss = 1.95 (760.7 examples/sec; 0.168 sec/batch)
2017-05-05 04:10:14.944275: step 190090, loss = 1.79 (876.0 examples/sec; 0.146 sec/batch)
2017-05-05 04:10:16.503936: step 190100, loss = 1.85 (820.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:10:17.987209: step 190110, loss = 1.95 (863.0 examples/sec; 0.148 sec/batch)
2017-05-05 04:10:19.523732: step 190120, loss = 1.80 (833.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:10:21.035724: step 190130, loss = 2.21 (846.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:10:22.546926: step 190140, loss = 1.86 (847.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:10:24.018083: step 190150, loss = 1.88 (870.1 examples/sec; 0.147 sec/batch)
2017-05-05 04:10:25.526180: step 190160, loss = 1.91 (848.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:10:27.112087: step 190170, loss = 1.77 (807.1 examples/sec; 0.159 sec/batch)
2017-05-05 04:10:28.651956: step 190180, loss = 1.70 (831.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:10:30.208031: step 190190, loss = 1.88 (822.6 examples/sec; 0.156 sec/batch)
2017-05-05 04:10:31.765750: step 190200, loss = 1.99 (821.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:10:33.297447: step 190210, loss = 1.90 (835.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:10:34.836761: step 190220, loss = 1.78 (831.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:10:36.399178: step 190230, loss = 1.80 (819.2 examples/sec; 0.156 sec/batch)
2017-05-05 04:10:37.996186: step 190240, loss = 1.93 (801.5 examples/sec; 0.160 sec/batch)
2017-05-05 04:10:39.516975: step 190250, loss = 1.82 (841.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:10:41.033511: step 190260, loss = 1.90 (844.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:10:42.571903: step 190270, loss = 2.05 (832.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:10:44.170404: step 190280, loss = 1.93 (800.8 examples/sec; 0.160 sec/batch)
2017-05-05 04:10:45.612750: step 190290, loss = 1.89 (887.5 examples/sec; 0.144 sec/batch)
2017-05-05 04:10:47.160651: step 190300, loss = 1.94 (826.9 examples/sec; 0.155 sec/batch)
2017-05-05 04:10:48.703645: step 190310, loss = 1.84 (829.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:10:50.238531: step 190320, loss = 1.79 (833.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:10:51.798918: step 190330, loss = 1.87 (820.3 examples/sec; 0.156 sec/batch)
2017-05-05 04:10:53.334038: step 190340, loss = 1.99 (833.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:10:54.809367: step 190350, loss = 1.90 (867.6 examples/sec; 0.148 sec/batch)
2017-05-05 04:10:56.379049: step 190360, loss = 1.84 (815.5 examples/sec; 0.157 sec/batch)
2017-05-05 04:10:57.934649: step 190370, loss = 1.77 (822.8 examples/sec; 0.156 sec/batch)
2017-05-05 04:10:59.530658: step 190380, loss = 1.78 (802.0 examples/sec; 0.160 sec/batch)
2017-05-05 04:11:01.067935: step 190390, loss = 1.86 (832.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:11:02.605852: step 190400, loss = 2.01 (832.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:11:04.124739: step 190410, loss = 1.95 (842.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:11:05.633647: step 190420, loss = 1.84 (848.3 examples/sec; 0.151 sec/batch)
2017-05-05 04:11:07.139987: step 190430, loss = 1.86 (849.7 examples/sec; 0.151 sec/batch)
2017-05-05 04:11:08.691783: step 190440, loss = 1.85 (824.9 examples/sec; 0.155 sec/batch)
2017-05-05 04:11:10.257466: step 190450, loss = 1.93 (817.5 examples/sec; 0.157 sec/batch)
2017-05-05 04:11:11.828398: step 190460, loss = 1.76 (814.8 examples/sec; 0.157 sec/batch)
2017-05-05 04:11:13.388852: step 190470, loss = 1.69 (820.3 examples/sec; 0.156 sec/batch)
2017-05-05 04:11:14.876497: step 190480, loss = 1.89 (860.4 examples/sec; 0.149 sec/batch)
2017-05-05 04:11:16.399197: step 190490, loss = 1.81 (840.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:11:17.926841: step 190500, loss = 1.78 (837.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:11:19.472819: step 190510, loss = 1.84 (828.0 examples/sec; 0.155 sec/batch)
2017-05-05 04:11:21.006061: step 190520, loss = 1.79 (834.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:11:22.550481: step 190530, loss = 2.07 (828.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:11:24.106952: step 190540, loss = 1.86 (822.4 examples/sec; 0.156 sec/batch)
2017-05-05 04:11:25.641587: step 190550, loss = 1.84 (834.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:11:27.207092: step 190560, loss = 1.73 (817.6 examples/sec; 0.157 sec/batch)
2017-05-05 04:11:28.706032: step 190570, loss = 1.87 (853.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:11:30.201564: step 190580, loss = 1.84 (855.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:11:31.763595: step 190590, loss = 2.08 (819.4 examples/sec; 0.156 sec/batch)
2017-05-05 04:11:33.356141: step 190600, loss = 1.73 (803.7 examples/sec; 0.159 sec/batch)
2017-05-05 04:11:34.898727: step 190610, loss = 1.83 (829.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:11:36.443817: step 190620, loss = 1.94 (828.4 examples/sec; 0.155 sec/batch)
2017-05-05 04:11:38.009137: step 190630, loss = 1.83 (817.7 examples/sec; 0.157 sec/batch)
2017-05-05 04:11:39.497334: step 190640, loss = 1.80 (860.1 examples/sec; 0.149 sec/batch)
2017-05-05 04:11:41.059023: step 190650, loss = 1.88 (819.6 examples/sec; 0.156 sec/batch)
2017-05-05 04:11:42.556710: step 190660, loss = 1.84 (854.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:11:44.115017: step 190670, loss = 1.90 (821.4 examples/sec; 0.156 sec/batch)
2017-05-05 04:11:45.630768: step 190680, loss = 1.92 (844.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:11:47.163036: step 190690, loss = 1.75 (835.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:11:48.686056: step 190700, loss = 1.84 (840.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:11:50.211340: step 190710, loss = 1.93 (839.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:11:51.687579: step 190720, loss = 1.73 (867.1 examples/sec; 0.148 sec/batch)
2017-05-05 04:11:53.223610: step 190730, loss = 1.97 (833.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:11:54.750597: step 190740, loss = 1.94 (838.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:11:56.299356: step 190750, loss = 1.96 (826.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:11:57.819524: step 190760, loss = 1.85 (842.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:11:59.388112: step 190770, loss = 1.98 (816.0 examples/sec; 0.157 sec/batch)
2017-05-05 04:12:00.918737: step 190780, loss = 1.89 (836.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:12:02.430793: step 190790, loss = 1.92 (846.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:12:03.967490: step 190800, loss = 1.74 (833.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:12:05.503824: step 190810, loss = 2.08 (833.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:12:07.011363: step 190820, loss = 1.82 (849.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:12:08.562663: step 190830, loss = 1.82 (825.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:12:10.076892: step 190840, loss = 1.98 (845.3 examples/sec; 0.151 sec/batch)
2017-05-05 04:12:11.668455: step 190850, loss = 1.73 (804.2 examples/sec; 0.159 sec/batch)
2017-05-05 04:12:13.187516: step 190860, loss = 1.89 (842.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:12:14.694935: step 190870, loss = 1.89 (849.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:12:16.240195: step 190880, loss = 1.90 (828.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:12:17.740596: step 190890, loss = 1.92 (853.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:12:19.255872: step 190900, loss = 1.90 (844.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:12:20.780740: step 190910, loss = 1.75 (839.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:12:22.282366: step 190920, loss = 1.84 (852.4 examples/sec; 0.150 sec/batch)
2017-05-05 04:12:23.837996: step 190930, loss = 1.85 (822.8 examples/sec; 0.156 sec/batch)
2017-05-05 04:12:25.354049: step 190940, loss = 1.89 (844.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:12:26.951432: step 190950, loss = 2.17 (801.3 examples/sec; 0.160 sec/batch)
2017-05-05 04:12:28.450356: step 190960, loss = 1.73 (854.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:12:29.903830: step 190970, loss = 1.95 (880.6 examples/sec; 0.145 sec/batch)
2017-05-05 04:12:31.416314: step 190980, loss = 1.89 (846.3 examples/sec; 0.151 sec/batch)
2017-05-05 04:12:32.937035: step 190990, loss = 1.84 (841.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:12:34.468269: step 191000, loss = 1.73 (836.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:12:35.990997: step 191010, loss = 1.90 (840.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:12:37.485268: step 191020, loss = 1.72 (856.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:12:39.019849: step 191030, loss = 1.91 (834.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:12:40.507031: step 191040, loss = 1.97 (860.7 examples/sec; 0.149 sec/batch)
2017-05-05 04:12:41.977379: step 191050, loss = 1.78 (870.5 examples/sec; 0.147 sec/batch)
2017-05-05 04:12:43.464337: step 191060, loss = 1.88 (860.8 examples/sec; 0.149 sec/batch)
2017-05-05 04:12:45.122302: step 191070, loss = 1.87 (772.0 examples/sec; 0.166 sec/batch)
2017-05-05 04:12:46.510876: step 191080, loss = 1.79 (921.8 examples/sec; 0.139 sec/batch)
2017-05-05 04:12:48.044424: step 191090, loss = 1.82 (834.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:12:49.610963: step 191100, loss = 1.87 (817.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:12:51.147010: step 191110, loss = 1.94 (833.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:12:52.670768: step 191120, loss = 1.70 (840.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:12:54.190096: step 191130, loss = 1.74 (842.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:12:55.701945: step 191140, loss = 1.97 (846.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:12:57.263114: step 191150, loss = 1.89 (819.9 examples/sec; 0.156 sec/batch)
2017-05-05 04:12:58.778511: step 191160, loss = 1.85 (844.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:13:00.300851: step 191170, loss = 1.91 (840.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:13:01.851991: step 191180, loss = 1.91 (825.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:13:03.355655: step 191190, loss = 1.93 (851.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:13:04.869747: step 191200, loss = 1.87 (845.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:13:06.390521: step 191210, loss = 1.79 (841.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:13:07.932398: step 191220, loss = 2.01 (830.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:13:09.451662: step 191230, loss = 1.96 (842.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:13:10.992485: step 191240, loss = 1.94 (830.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:13:12.591776: step 191250, loss = 1.93 (800.4 examples/sec; 0.160 sec/batch)
2017-05-05 04:13:14.131885: step 191260, loss = 1.92 (831.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:13:15.748735: step 191270, loss = 1.90 (791.7 examples/sec; 0.162 sec/batch)
2017-05-05 04:13:17.154077: step 191280, loss = 1.88 (910.8 examples/sec; 0.141 sec/batch)
2017-05-05 04:13:18.693157: step 191290, loss = 1.86 (831.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:13:20.250193: step 191300, loss = 1.78 (822.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:13:21.783549: step 191310, loss = 1.87 (834.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:13:23.301807: step 191320, loss = 1.83 (843.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:13:24.905325: step 191330, loss = 1.81 (798.2 examples/sec; 0.160 sec/batch)
2017-05-05 04:13:26.420423: step 191340, loss = 1.94 (844.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:13:28.020479: step 191350, loss = 1.91 (800.0 examples/sec; 0.160 sec/batch)
2017-05-05 04:13:29.551538: step 191360, loss = 1.90 (836.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:13:31.088750: step 191370, loss = 1.81 (832.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:13:32.607896: step 191380, loss = 1.84 (842.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:13:34.163043: step 191390, loss = 1.74 (823.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:13:35.653359: step 191400, loss = 1.81 (858.9 examples/sec; 0.149 sec/batch)
2017-05-05 04:13:37.129035: step 191410, loss = 1.74 (867.4 examples/sec; 0.148 sec/batch)
2017-05-05 04:13:38.611188: step 191420, loss = 2.03 (863.6 examples/sec; 0.148 sec/batch)
2017-05-05 04:13:40.174722: step 191430, loss = 2.01 (818.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:13:41.679541: step 191440, loss = 1.82 (850.6 examples/sec; 0.150 sec/batch)
2017-05-05 04:13:43.216898: step 191450, loss = 2.09 (832.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:13:44.733612: step 191460, loss = 1.76 (843.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:13:46.227890: step 191470, loss = 1.80 (856.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:13:47.745276: step 191480, loss = 1.98 (843.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:13:49.284146: step 191490, loss = 1.99 (831.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:13:50.829939: step 191500, loss = 1.92 (828.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:13:52.377378: step 191510, loss = 1.74 (827.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:13:53.930711: step 191520, loss = 1.73 (824.0 examples/sec; 0.155 sec/batch)
2017-05-05 04:13:55.419808: step 191530, loss = 1.81 (859.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:13:56.906849: step 191540, loss = 1.97 (860.8 examples/sec; 0.149 sec/batch)
2017-05-05 04:13:58.410448: step 191550, loss = 1.78 (851.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:13:59.900685: step 191560, loss = 1.85 (858.9 examples/sec; 0.149 sec/batch)
2017-05-05 04:14:01.387955: step 191570, loss = 1.91 (860.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:14:02.927392: step 191580, loss = 1.75 (831.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:14:04.434918: step 191590, loss = 1.94 (849.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:14:05.937036: step 191600, loss = 1.76 (852.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:14:07.457707: step 191610, loss = 1.84 (841.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:14:08.949879: step 191620, loss = 1.89 (857.8 examples/sec; 0.149 sec/batch)
2017-05-05 04:14:10.462514: step 191630, loss = 1.87 (846.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:14:11.975768: step 191640, loss = 1.91 (845.9 examples/sec; 0.151 sec/batch)
2017-05-05 04:14:13.490202: step 191650, loss = 1.87 (845.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:14:15.030049: step 191660, loss = 1.83 (831.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:14:16.625718: step 191670, loss = 1.89 (802.2 examples/sec; 0.160 sec/batch)
2017-05-05 04:14:18.176209: step 191680, loss = 1.98 (825.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:14:19.716033: step 191690, loss = 1.82 (831.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:14:21.246555: step 191700, loss = 1.98 (836.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:14:22.769468: step 191710, loss = 1.87 (840.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:14:24.299424: step 191720, loss = 1.76 (836.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:14:25.826631: step 191730, loss = 1.85 (838.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:14:27.353425: step 191740, loss = 1.80 (838.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:14:28.887144: step 191750, loss = 1.77 (834.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:14:30.374897: step 191760, loss = 1.74 (860.4 examples/sec; 0.149 sec/batch)
2017-05-05 04:14:31.920175: step 191770, loss = 1.88 (828.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:14:33.483311: step 191780, loss = 1.76 (818.9 examples/sec; 0.156 sec/batch)
2017-05-05 04:14:35.027220: step 191790, loss = 2.00 (829.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:14:36.525279: step 191800, loss = 1.77 (854.5 examples/sec; 0.150 sec/batch)
2017-05-05 04:14:38.074161: step 191810, loss = 1.87 (826.4 examples/sec; 0.155 sec/batch)
2017-05-05 04:14:39.565168: step 191820, loss = 1.77 (858.5 examples/sec; 0.149 sec/batch)
2017-05-05 04:14:41.154717: step 191830, loss = 1.75 (805.3 examples/sec; 0.159 sec/batch)
2017-05-05 04:14:42.655745: step 191840, loss = 1.88 (852.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:14:44.204047: step 191850, loss = 1.88 (826.7 examples/sec; 0.155 sec/batch)
2017-05-05 04:14:45.702377: step 191860, loss = 1.77 (854.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:14:47.286197: step 191870, loss = 1.71 (808.2 examples/sec; 0.158 sec/batch)
2017-05-05 04:14:48.819735: step 191880, loss = 1.86 (834.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:14:50.336624: step 191890, loss = 1.91 (843.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:14:51.908805: step 191900, loss = 1.93 (814.2 examples/sec; 0.157 sec/batch)
2017-05-05 04:14:53.464079: step 191910, loss = 1.92 (823.0 examples/sec; 0.156 sec/batch)
2017-05-05 04:14:54.977212: step 191920, loss = 1.89 (845.9 examples/sec; 0.151 sec/batch)
2017-05-05 04:14:56.523940: step 191930, loss = 1.84 (827.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:14:58.040101: step 191940, loss = 1.81 (844.2 examples/sec; 0.152 sec/batch)
2017-05-05 04:14:59.567517: step 191950, loss = 2.01 (838.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:15:01.064260: step 191960, loss = 1.79 (855.2 examples/sec; 0.150 sec/batch)
2017-05-05 04:15:02.661805: step 191970, loss = 2.00 (801.2 examples/sec; 0.160 sec/batch)
2017-05-05 04:15:04.175005: step 191980, loss = 1.86 (845.9 examples/sec; 0.151 sec/batch)
2017-05-05 04:15:05.722752: step 191990, loss = 1.81 (827.0 examples/sec; 0.155 sec/batch)
2017-05-05 04:15:07.248160: step 192000, loss = 1.88 (839.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:15:08.778776: step 192010, loss = 1.87 (836.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:15:10.292589: step 192020, loss = 1.83 (845.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:15:11.855023: step 192030, loss = 1.91 (819.2 examples/sec; 0.156 sec/batch)
2017-05-05 04:15:13.376089: step 192040, loss = 1.84 (841.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:15:14.894759: step 192050, loss = 1.81 (842.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:15:16.554382: step 192060, loss = 1.77 (771.3 examples/sec; 0.166 sec/batch)
2017-05-05 04:15:17.995858: step 192070, loss = 1.96 (888.0 examples/sec; 0.144 sec/batch)
2017-05-05 04:15:19.537902: step 192080, loss = 1.85 (830.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:15:21.013782: step 192090, loss = 1.89 (867.3 examples/sec; 0.148 sec/batch)
2017-05-05 04:15:22.516745: step 192100, loss = 1.92 (851.6 examples/sec; 0.150 sec/batch)
2017-05-05 04:15:24.089811: step 192110, loss = 1.81 (813.7 examples/sec; 0.157 sec/batch)
2017-05-05 04:15:25.592327: step 192120, loss = 1.84 (851.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:15:27.123040: step 192130, loss = 1.83 (836.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:15:28.618479: step 192140, loss = 1.85 (855.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:15:30.209042: step 192150, loss = 1.72 (804.7 examples/sec; 0.159 sec/batch)
2017-05-05 04:15:31.749178: step 192160, loss = 1.70 (831.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:15:33.258656: step 192170, loss = 1.84 (848.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:15:34.797127: step 192180, loss = 1.89 (832.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:15:36.413052: step 192190, loss = 1.84 (792.1 examples/sec; 0.162 sec/batch)
2017-05-05 04:15:37.904050: step 192200, loss = 1.94 (858.5 examples/sec; 0.149 sec/batch)
2017-05-05 04:15:39.452015: step 192210, loss = 1.70 (826.9 examples/sec; 0.155 sec/batch)
2017-05-05 04:15:40.982037: step 192220, loss = 1.81 (836.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:15:42.589831: step 192230, loss = 1.82 (796.1 examples/sec; 0.161 sec/batch)
2017-05-05 04:15:44.091961: step 192240, loss = 1.99 (852.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:15:45.653398: step 192250, loss = 1.84 (819.8 examples/sec; 0.156 sec/batch)
2017-05-05 04:15:47.284764: step 192260, loss = 1.93 (784.6 examples/sec; 0.163 sec/batch)
2017-05-05 04:15:48.778917: step 192270, loss = 1.87 (856.7 examples/sec; 0.149 sec/batch)
2017-05-05 04:15:50.342291: step 192280, loss = 1.72 (818.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:15:51.852419: step 192290, loss = 1.91 (847.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:15:53.420025: step 192300, loss = 1.89 (816.5 examples/sec; 0.157 sec/batch)
2017-05-05 04:15:54.876542: step 192310, loss = 1.88 (878.8 examples/sec; 0.146 sec/batch)
2017-05-05 04:15:56.398709: step 192320, loss = 1.93 (840.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:15:57.937855: step 192330, loss = 1.91 (831.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:15:59.461776: step 192340, loss = 1.88 (839.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:16:00.975096: step 192350, loss = 1.71 (845.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:16:02.521083: step 192360, loss = 1.83 (827.9 examples/sec; 0.155 sec/batch)
2017-05-05 04:16:04.043618: step 192370, loss = 1.87 (840.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:16:05.564865: step 192380, loss = 1.91 (841.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:16:07.128425: step 192390, loss = 1.72 (818.6 examples/sec; 0.156 sec/batch)
2017-05-05 04:16:08.671875: step 192400, loss = 1.74 (829.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:16:10.178594: step 192410, loss = 1.82 (849.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:16:11.677017: step 192420, loss = 1.88 (854.2 examples/sec; 0.150 sec/batch)
2017-05-05 04:16:13.198891: step 192430, loss = 1.89 (841.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:16:14.744664: step 192440, loss = 1.81 (828.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:16:16.326917: step 192450, loss = 1.82 (809.0 examples/sec; 0.158 sec/batch)
2017-05-05 04:16:17.829571: step 192460, loss = 1.86 (851.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:16:19.412494: step 192470, loss = 1.94 (808.6 examples/sec; 0.158 sec/batch)
2017-05-05 04:16:20.900670: step 192480, loss = 1.84 (860.1 examples/sec; 0.149 sec/batch)
2017-05-05 04:16:22.434811: step 192490, loss = 1.97 (834.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:16:23.995703: step 192500, loss = 1.83 (820.0 examples/sec; 0.156 sec/batch)
2017-05-05 04:16:25.536914: step 192510, loss = 1.91 (830.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:16:27.077022: step 192520, loss = 1.89 (831.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:16:28.613457: step 192530, loss = 1.87 (833.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:16:30.166191: step 192540, loss = 1.93 (824.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:16:31.689751: step 192550, loss = 1.88 (840.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:16:33.217286: step 192560, loss = 1.89 (838.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:16:34.730320: step 192570, loss = 1.94 (846.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:16:36.273638: step 192580, loss = 1.85 (829.4 examples/sec; 0.154 sec/batch)
2017-05-05 04:16:37.807693: step 192590, loss = 1.99 (834.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:16:39.370848: step 192600, loss = 1.78 (818.9 examples/sec; 0.156 sec/batch)
2017-05-05 04:16:40.914875: step 192610, loss = 1.74 (829.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:16:42.381056: step 192620, loss = 1.92 (873.0 examples/sec; 0.147 sec/batch)
2017-05-05 04:16:43.937202: step 192630, loss = 1.82 (822.5 examples/sec; 0.156 sec/batch)
2017-05-05 04:16:45.452749: step 192640, loss = 1.97 (844.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:16:47.021256: step 192650, loss = 2.01 (816.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:16:48.557338: step 192660, loss = 2.19 (833.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:16:50.077663: step 192670, loss = 1.76 (841.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:16:51.598338: step 192680, loss = 1.85 (841.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:16:53.148948: step 192690, loss = 1.94 (825.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:16:54.640635: step 192700, loss = 1.88 (858.1 examples/sec; 0.149 sec/batch)
2017-05-05 04:16:56.214833: step 192710, loss = 1.89 (813.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:16:57.817564: step 192720, loss = 1.75 (798.6 examples/sec; 0.160 sec/batch)
2017-05-05 04:16:59.313422: step 192730, loss = 1.77 (855.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:17:00.788190: step 192740, loss = 1.88 (867.9 examples/sec; 0.147 sec/batch)
2017-05-05 04:17:02.307652: step 192750, loss = 1.79 (842.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:17:03.884129: step 192760, loss = 1.94 (811.9 examples/sec; 0.158 sec/batch)
2017-05-05 04:17:05.413462: step 192770, loss = 1.81 (837.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:17:06.960770: step 192780, loss = 1.89 (827.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:17:08.529640: step 192790, loss = 1.88 (815.9 examples/sec; 0.157 sec/batch)
2017-05-05 04:17:10.033679: step 192800, loss = 1.81 (851.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:17:11.583649: step 192810, loss = 1.91 (825.8 examples/sec; 0.155 sec/batch)
2017-05-05 04:17:13.068669: step 192820, loss = 1.83 (861.9 examples/sec; 0.149 sec/batch)
2017-05-05 04:17:14.594754: step 192830, loss = 1.77 (838.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:17:16.083070: step 192840, loss = 1.89 (860.0 examples/sec; 0.149 sec/batch)
2017-05-05 04:17:17.565796: step 192850, loss = 2.02 (863.3 examples/sec; 0.148 sec/batch)
2017-05-05 04:17:19.114413: step 192860, loss = 2.00 (826.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:17:20.710554: step 192870, loss = 1.94 (801.9 examples/sec; 0.160 sec/batch)
2017-05-05 04:17:22.245839: step 192880, loss = 1.90 (833.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:17:23.756712: step 192890, loss = 1.75 (847.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:17:25.285663: step 192900, loss = 1.81 (837.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:17:26.824063: step 192910, loss = 1.69 (832.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:17:28.334140: step 192920, loss = 1.90 (847.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:17:29.840478: step 192930, loss = 1.94 (849.7 examples/sec; 0.151 sec/batch)
2017-05-05 04:17:31.403213: step 192940, loss = 1.96 (819.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:17:32.969011: step 192950, loss = 1.75 (817.5 examples/sec; 0.157 sec/batch)
2017-05-05 04:17:34.540390: step 192960, loss = 1.86 (814.6 examples/sec; 0.157 sec/batch)
2017-05-05 04:17:36.100026: step 192970, loss = 1.81 (820.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:17:37.611114: step 192980, loss = 1.77 (847.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:17:39.200963: step 192990, loss = 1.73 (805.1 examples/sec; 0.159 sec/batch)
2017-05-05 04:17:40.740766: step 193000, loss = 1.76 (831.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:17:42.271592: step 193010, loss = 1.80 (836.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:17:43.812333: step 193020, loss = 1.88 (830.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:17:45.364605: step 193030, loss = 1.91 (824.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:17:46.859804: step 193040, loss = 1.90 (856.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:17:48.555295: step 193050, loss = 1.74 (754.9 examples/sec; 0.170 sec/batch)
2017-05-05 04:17:49.918734: step 193060, loss = 1.89 (938.8 examples/sec; 0.136 sec/batch)
2017-05-05 04:17:51.474524: step 193070, loss = 1.79 (822.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:17:53.016509: step 193080, loss = 1.72 (830.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:17:54.537650: step 193090, loss = 1.77 (841.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:17:56.078958: step 193100, loss = 1.77 (830.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:17:57.621684: step 193110, loss = 1.95 (829.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:17:59.143551: step 193120, loss = 2.04 (841.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:18:00.675643: step 193130, loss = 1.92 (835.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:18:02.238218: step 193140, loss = 1.90 (819.2 examples/sec; 0.156 sec/batch)
2017-05-05 04:18:03.773445: step 193150, loss = 1.86 (833.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:18:05.269942: step 193160, loss = 1.82 (855.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:18:06.816317: step 193170, loss = 1.82 (827.7 examples/sec; 0.155 sec/batch)
2017-05-05 04:18:08.451405: step 193180, loss = 1.89 (782.8 examples/sec; 0.164 sec/batch)
2017-05-05 04:18:09.960299: step 193190, loss = 2.00 (848.3 examples/sec; 0.151 sec/batch)
2017-05-05 04:18:11.501823: step 193200, loss = 1.84 (830.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:18:13.024773: step 193210, loss = 1.98 (840.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:18:14.592922: step 193220, loss = 1.85 (816.3 examples/sec; 0.157 sec/batch)
2017-05-05 04:18:16.110795: step 193230, loss = 1.76 (843.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:18:17.611591: step 193240, loss = 1.77 (852.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:18:19.261311: step 193250, loss = 1.78 (775.9 examples/sec; 0.165 sec/batch)
2017-05-05 04:18:20.701906: step 193260, loss = 1.90 (888.5 examples/sec; 0.144 sec/batch)
2017-05-05 04:18:22.242882: step 193270, loss = 2.02 (830.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:18:23.783305: step 193280, loss = 1.88 (830.9 examples/sec; 0.154 sec/batch)
2017-05-05 04:18:25.305922: step 193290, loss = 1.75 (840.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:18:26.816901: step 193300, loss = 1.91 (847.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:18:28.311907: step 193310, loss = 2.00 (856.2 examples/sec; 0.150 sec/batch)
2017-05-05 04:18:29.843997: step 193320, loss = 2.01 (835.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:18:31.338277: step 193330, loss = 1.89 (856.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:18:32.854894: step 193340, loss = 1.80 (844.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:18:34.328551: step 193350, loss = 1.86 (868.6 examples/sec; 0.147 sec/batch)
2017-05-05 04:18:35.864081: step 193360, loss = 1.97 (833.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:18:37.382919: step 193370, loss = 1.78 (842.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:18:38.887068: step 193380, loss = 1.82 (851.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:18:40.445871: step 193390, loss = 1.82 (821.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:18:41.930762: step 193400, loss = 1.85 (862.0 examples/sec; 0.148 sec/batch)
2017-05-05 04:18:43.468907: step 193410, loss = 1.97 (832.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:18:44.972947: step 193420, loss = 1.95 (851.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:18:46.466522: step 193430, loss = 1.94 (857.0 examples/sec; 0.149 sec/batch)
2017-05-05 04:18:48.026678: step 193440, loss = 1.81 (820.4 examples/sec; 0.156 sec/batch)
2017-05-05 04:18:49.514721: step 193450, loss = 1.81 (860.2 examples/sec; 0.149 sec/batch)
2017-05-05 04:18:50.991841: step 193460, loss = 1.85 (866.5 examples/sec; 0.148 sec/batch)
2017-05-05 04:18:52.492866: step 193470, loss = 1.80 (852.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:18:54.032730: step 193480, loss = 1.94 (831.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:18:55.631979: step 193490, loss = 1.81 (800.4 examples/sec; 0.160 sec/batch)
2017-05-05 04:18:57.138460: step 193500, loss = 1.91 (849.7 examples/sec; 0.151 sec/batch)
2017-05-05 04:18:58.730135: step 193510, loss = 1.78 (804.2 examples/sec; 0.159 sec/batch)
2017-05-05 04:19:00.240164: step 193520, loss = 1.77 (847.7 examples/sec; 0.151 sec/batch)
2017-05-05 04:19:01.751400: step 193530, loss = 1.97 (847.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:19:03.297733: step 193540, loss = 1.94 (827.8 examples/sec; 0.155 sec/batch)
2017-05-05 04:19:04.848773: step 193550, loss = 2.03 (825.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:19:06.311633: step 193560, loss = 1.99 (875.0 examples/sec; 0.146 sec/batch)
2017-05-05 04:19:07.853760: step 193570, loss = 1.89 (830.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:19:09.394898: step 193580, loss = 1.99 (830.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:19:10.924288: step 193590, loss = 1.79 (836.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:19:12.480282: step 193600, loss = 1.85 (822.6 examples/sec; 0.156 sec/batch)
2017-05-05 04:19:14.090671: step 193610, loss = 1.92 (794.8 examples/sec; 0.161 sec/batch)
2017-05-05 04:19:15.621500: step 193620, loss = 1.80 (836.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:19:17.153965: step 193630, loss = 1.81 (835.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:19:18.657046: step 193640, loss = 1.88 (851.6 examples/sec; 0.150 sec/batch)
2017-05-05 04:19:20.198266: step 193650, loss = 1.78 (830.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:19:21.718891: step 193660, loss = 1.82 (841.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:19:23.269895: step 193670, loss = 2.07 (825.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:19:24.803597: step 193680, loss = 1.79 (834.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:19:26.341398: step 193690, loss = 1.78 (832.4 examples/sec; 0.154 sec/batch)
2017-05-05 04:19:27.849998: step 193700, loss = 1.84 (848.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:19:29.442765: step 193710, loss = 1.91 (803.6 examples/sec; 0.159 sec/batch)
2017-05-05 04:19:31.009752: step 193720, loss = 1.83 (816.9 examples/sec; 0.157 sec/batch)
2017-05-05 04:19:32.566611: step 193730, loss = 1.77 (822.2 examples/sec; 0.156 sec/batch)
2017-05-05 04:19:34.112178: step 193740, loss = 1.98 (828.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:19:35.593746: step 193750, loss = 1.73 (863.9 examples/sec; 0.148 sec/batch)
2017-05-05 04:19:37.153560: step 193760, loss = 1.69 (820.6 examples/sec; 0.156 sec/batch)
2017-05-05 04:19:38.704554: step 193770, loss = 1.92 (825.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:19:40.206753: step 193780, loss = 1.77 (852.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:19:41.767302: step 193790, loss = 1.96 (820.2 examples/sec; 0.156 sec/batch)
2017-05-05 04:19:43.253692: step 193800, loss = 1.86 (861.1 examples/sec; 0.149 sec/batch)
2017-05-05 04:19:44.822921: step 193810, loss = 1.92 (815.8 examples/sec; 0.157 sec/batch)
2017-05-05 04:19:46.339607: step 193820, loss = 1.94 (843.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:19:47.943671: step 193830, loss = 1.89 (798.0 examples/sec; 0.160 sec/batch)
2017-05-05 04:19:49.462748: step 193840, loss = 1.63 (842.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:19:50.997778: step 193850, loss = 1.81 (833.9 examples/sec; 0.154 sec/batch)
2017-05-05 04:19:52.545162: step 193860, loss = 1.85 (827.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:19:54.030498: step 193870, loss = 1.83 (861.8 examples/sec; 0.149 sec/batch)
2017-05-05 04:19:55.560614: step 193880, loss = 2.01 (836.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:19:57.092667: step 193890, loss = 1.91 (835.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:19:58.600679: step 193900, loss = 1.87 (848.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:20:00.142648: step 193910, loss = 1.77 (830.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:20:01.660531: step 193920, loss = 1.75 (843.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:20:03.197698: step 193930, loss = 1.83 (832.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:20:04.681064: step 193940, loss = 1.75 (862.9 examples/sec; 0.148 sec/batch)
2017-05-05 04:20:06.188100: step 193950, loss = 1.68 (849.3 examples/sec; 0.151 sec/batch)
2017-05-05 04:20:07.684830: step 193960, loss = 1.84 (855.2 examples/sec; 0.150 sec/batch)
2017-05-05 04:20:09.278156: step 193970, loss = 1.87 (803.4 examples/sec; 0.159 sec/batch)
2017-05-05 04:20:10.835966: step 193980, loss = 1.75 (821.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:20:12.388548: step 193990, loss = 1.68 (824.4 examples/sec; 0.155 sec/batch)
2017-05-05 04:20:13.906019: step 194000, loss = 1.95 (843.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:20:15.447735: step 194010, loss = 1.93 (830.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:20:17.012003: step 194020, loss = 2.00 (818.3 examples/sec; 0.156 sec/batch)
2017-05-05 04:20:18.532471: step 194030, loss = 1.85 (841.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:20:20.199331: step 194040, loss = 1.82 (767.9 examples/sec; 0.167 sec/batch)
2017-05-05 04:20:21.593390: step 194050, loss = 1.89 (918.2 examples/sec; 0.139 sec/batch)
2017-05-05 04:20:23.055887: step 194060, loss = 1.78 (875.2 examples/sec; 0.146 sec/batch)
2017-05-05 04:20:24.592904: step 194070, loss = 1.75 (832.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:20:26.141930: step 194080, loss = 1.71 (826.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:20:27.684663: step 194090, loss = 1.73 (829.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:20:29.254940: step 194100, loss = 1.65 (815.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:20:30.763954: step 194110, loss = 1.73 (848.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:20:32.213051: step 194120, loss = 1.87 (883.3 examples/sec; 0.145 sec/batch)
2017-05-05 04:20:33.718077: step 194130, loss = 1.92 (850.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:20:35.283839: step 194140, loss = 1.80 (817.5 examples/sec; 0.157 sec/batch)
2017-05-05 04:20:36.830702: step 194150, loss = 2.03 (827.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:20:38.276886: step 194160, loss = 2.08 (885.1 examples/sec; 0.145 sec/batch)
2017-05-05 04:20:39.783076: step 194170, loss = 1.99 (849.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:20:41.314511: step 194180, loss = 1.97 (835.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:20:42.846376: step 194190, loss = 1.80 (835.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:20:44.371327: step 194200, loss = 1.87 (839.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:20:45.871114: step 194210, loss = 1.99 (853.5 examples/sec; 0.150 sec/batch)
2017-05-05 04:20:47.394686: step 194220, loss = 1.82 (840.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:20:48.926610: step 194230, loss = 1.86 (835.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:20:50.485703: step 194240, loss = 1.98 (821.0 examples/sec; 0.156 sec/batch)
2017-05-05 04:20:51.992324: step 194250, loss = 1.83 (849.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:20:53.517645: step 194260, loss = 1.89 (839.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:20:55.074506: step 194270, loss = 1.85 (822.2 examples/sec; 0.156 sec/batch)
2017-05-05 04:20:56.569079: step 194280, loss = 1.82 (856.4 examples/sec; 0.149 sec/batch)
2017-05-05 04:20:58.044087: step 194290, loss = 1.74 (867.8 examples/sec; 0.148 sec/batch)
2017-05-05 04:20:59.598152: step 194300, loss = 1.87 (823.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:21:01.052494: step 194310, loss = 2.01 (880.1 examples/sec; 0.145 sec/batch)
2017-05-05 04:21:02.580620: step 194320, loss = 1.80 (837.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:21:04.097986: step 194330, loss = 1.79 (843.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:21:05.582786: step 194340, loss = 1.79 (862.1 examples/sec; 0.148 sec/batch)
2017-05-05 04:21:07.043374: step 194350, loss = 1.83 (876.4 examples/sec; 0.146 sec/batch)
2017-05-05 04:21:08.532784: step 194360, loss = 1.87 (859.4 examples/sec; 0.149 sec/batch)
2017-05-05 04:21:10.088444: step 194370, loss = 1.82 (822.8 examples/sec; 0.156 sec/batch)
2017-05-05 04:21:11.626772: step 194380, loss = 1.82 (832.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:21:13.099805: step 194390, loss = 1.86 (869.0 examples/sec; 0.147 sec/batch)
2017-05-05 04:21:14.620030: step 194400, loss = 1.90 (842.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:21:16.211273: step 194410, loss = 1.86 (804.4 examples/sec; 0.159 sec/batch)
2017-05-05 04:21:17.733002: step 194420, loss = 1.80 (841.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:21:19.288614: step 194430, loss = 1.86 (822.8 examples/sec; 0.156 sec/batch)
2017-05-05 04:21:20.814397: step 194440, loss = 1.86 (838.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:21:22.405055: step 194450, loss = 1.93 (804.7 examples/sec; 0.159 sec/batch)
2017-05-05 04:21:23.871490: step 194460, loss = 1.85 (872.9 examples/sec; 0.147 sec/batch)
2017-05-05 04:21:25.391768: step 194470, loss = 1.90 (841.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:21:26.900821: step 194480, loss = 1.83 (848.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:21:28.429701: step 194490, loss = 1.80 (837.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:21:29.985691: step 194500, loss = 1.94 (822.6 examples/sec; 0.156 sec/batch)
2017-05-05 04:21:31.525070: step 194510, loss = 1.70 (831.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:21:33.049476: step 194520, loss = 1.87 (839.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:21:34.543722: step 194530, loss = 1.85 (856.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:21:36.127858: step 194540, loss = 1.97 (808.0 examples/sec; 0.158 sec/batch)
2017-05-05 04:21:37.675653: step 194550, loss = 2.02 (827.0 examples/sec; 0.155 sec/batch)
2017-05-05 04:21:39.163703: step 194560, loss = 1.78 (860.2 examples/sec; 0.149 sec/batch)
2017-05-05 04:21:40.695310: step 194570, loss = 1.75 (835.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:21:42.217065: step 194580, loss = 1.81 (841.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:21:43.780861: step 194590, loss = 1.74 (818.5 examples/sec; 0.156 sec/batch)
2017-05-05 04:21:45.294555: step 194600, loss = 2.02 (845.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:21:46.803831: step 194610, loss = 1.91 (848.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:21:48.320527: step 194620, loss = 1.88 (843.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:21:49.890401: step 194630, loss = 1.94 (815.4 examples/sec; 0.157 sec/batch)
2017-05-05 04:21:51.344925: step 194640, loss = 1.80 (880.0 examples/sec; 0.145 sec/batch)
2017-05-05 04:21:52.883325: step 194650, loss = 1.72 (832.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:21:54.474382: step 194660, loss = 1.86 (804.5 examples/sec; 0.159 sec/batch)
2017-05-05 04:21:55.951718: step 194670, loss = 1.93 (866.4 examples/sec; 0.148 sec/batch)
2017-05-05 04:21:57.447728: step 194680, loss = 1.83 (855.6 examples/sec; 0.150 sec/batch)
2017-05-05 04:21:58.987153: step 194690, loss = 1.79 (831.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:22:00.493970: step 194700, loss = 2.02 (849.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:22:02.076935: step 194710, loss = 1.85 (808.6 examples/sec; 0.158 sec/batch)
2017-05-05 04:22:03.575652: step 194720, loss = 1.78 (854.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:22:05.198795: step 194730, loss = 2.00 (788.6 examples/sec; 0.162 sec/batch)
2017-05-05 04:22:06.740585: step 194740, loss = 1.88 (830.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:22:08.252811: step 194750, loss = 1.84 (846.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:22:09.749346: step 194760, loss = 1.99 (855.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:22:11.319304: step 194770, loss = 2.01 (815.3 examples/sec; 0.157 sec/batch)
2017-05-05 04:22:12.865482: step 194780, loss = 1.91 (827.9 examples/sec; 0.155 sec/batch)
2017-05-05 04:22:14.440806: step 194790, loss = 1.84 (812.5 examples/sec; 0.158 sec/batch)
2017-05-05 04:22:15.965670: step 194800, loss = 1.83 (839.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:22:17.481099: step 194810, loss = 1.85 (844.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:22:19.020022: step 194820, loss = 1.87 (831.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:22:20.483506: step 194830, loss = 1.86 (874.6 examples/sec; 0.146 sec/batch)
2017-05-05 04:22:22.021723: step 194840, loss = 1.79 (832.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:22:23.586668: step 194850, loss = 1.90 (817.9 examples/sec; 0.156 sec/batch)
2017-05-05 04:22:25.115479: step 194860, loss = 1.88 (837.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:22:26.643795: step 194870, loss = 1.81 (837.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:22:28.158731: step 194880, loss = 1.91 (844.9 examples/sec; 0.151 sec/batch)
2017-05-05 04:22:29.641430: step 194890, loss = 1.84 (863.3 examples/sec; 0.148 sec/batch)
2017-05-05 04:22:31.183966: step 194900, loss = 1.73 (829.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:22:32.752208: step 194910, loss = 1.87 (816.2 examples/sec; 0.157 sec/batch)
2017-05-05 04:22:34.245366: step 194920, loss = 1.79 (857.2 examples/sec; 0.149 sec/batch)
2017-05-05 04:22:35.777272: step 194930, loss = 1.92 (835.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:22:37.302311: step 194940, loss = 1.92 (839.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:22:38.886646: step 194950, loss = 1.95 (807.9 examples/sec; 0.158 sec/batch)
2017-05-05 04:22:40.381438: step 194960, loss = 1.87 (856.3 examples/sec; 0.149 sec/batch)
2017-05-05 04:22:41.956741: step 194970, loss = 1.83 (812.5 examples/sec; 0.158 sec/batch)
2017-05-05 04:22:43.487148: step 194980, loss = 1.90 (836.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:22:45.040390: step 194990, loss = 1.77 (824.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:22:46.540107: step 195000, loss = 1.92 (853.5 examples/sec; 0.150 sec/batch)
2017-05-05 04:22:48.069852: step 195010, loss = 1.90 (836.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:22:49.563480: step 195020, loss = 1.78 (857.0 examples/sec; 0.149 sec/batch)
2017-05-05 04:22:51.257557: step 195030, loss = 1.77 (755.6 examples/sec; 0.169 sec/batch)
2017-05-05 04:22:52.669005: step 195040, loss = 1.96 (906.9 examples/sec; 0.141 sec/batch)
2017-05-05 04:22:54.223427: step 195050, loss = 1.85 (823.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:22:55.743900: step 195060, loss = 1.78 (841.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:22:57.323131: step 195070, loss = 1.87 (810.5 examples/sec; 0.158 sec/batch)
2017-05-05 04:22:58.828483: step 195080, loss = 1.74 (850.3 examples/sec; 0.151 sec/batch)
2017-05-05 04:23:00.365624: step 195090, loss = 1.87 (832.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:23:01.896747: step 195100, loss = 1.80 (836.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:23:03.405910: step 195110, loss = 1.94 (848.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:23:04.929282: step 195120, loss = 1.90 (840.2 examples/sec; 0.152 sec/batch)
2017-05-05 04:23:06.439336: step 195130, loss = 1.96 (847.7 examples/sec; 0.151 sec/batch)
2017-05-05 04:23:07.920585: step 195140, loss = 1.87 (864.1 examples/sec; 0.148 sec/batch)
2017-05-05 04:23:09.472809: step 195150, loss = 1.86 (824.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:23:10.937929: step 195160, loss = 1.81 (873.6 examples/sec; 0.147 sec/batch)
2017-05-05 04:23:12.456727: step 195170, loss = 1.75 (842.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:23:13.943274: step 195180, loss = 1.77 (861.1 examples/sec; 0.149 sec/batch)
2017-05-05 04:23:15.490566: step 195190, loss = 1.90 (827.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:23:16.998787: step 195200, loss = 1.80 (848.7 examples/sec; 0.151 sec/batch)
2017-05-05 04:23:18.506690: step 195210, loss = 1.74 (848.9 examples/sec; 0.151 sec/batch)
2017-05-05 04:23:20.036865: step 195220, loss = 1.97 (836.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:23:21.587658: step 195230, loss = 1.71 (825.4 examples/sec; 0.155 sec/batch)
2017-05-05 04:23:23.079566: step 195240, loss = 1.87 (858.0 examples/sec; 0.149 sec/batch)
2017-05-05 04:23:24.556548: step 195250, loss = 1.85 (866.6 examples/sec; 0.148 sec/batch)
2017-05-05 04:23:26.129663: step 195260, loss = 1.86 (813.7 examples/sec; 0.157 sec/batch)
2017-05-05 04:23:27.625137: step 195270, loss = 1.84 (855.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:23:29.132565: step 195280, loss = 1.74 (849.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:23:30.599474: step 195290, loss = 1.84 (872.6 examples/sec; 0.147 sec/batch)
2017-05-05 04:23:32.109073: step 195300, loss = 1.86 (847.9 examples/sec; 0.151 sec/batch)
2017-05-05 04:23:33.676594: step 195310, loss = 1.86 (816.6 examples/sec; 0.157 sec/batch)
2017-05-05 04:23:35.239510: step 195320, loss = 1.74 (819.0 examples/sec; 0.156 sec/batch)
2017-05-05 04:23:36.705784: step 195330, loss = 2.01 (873.0 examples/sec; 0.147 sec/batch)
2017-05-05 04:23:38.231734: step 195340, loss = 1.93 (838.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:23:39.727307: step 195350, loss = 1.93 (855.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:23:41.277928: step 195360, loss = 1.85 (825.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:23:42.783070: step 195370, loss = 1.78 (850.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:23:44.321539: step 195380, loss = 1.79 (832.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:23:45.864671: step 195390, loss = 1.88 (829.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:23:47.467386: step 195400, loss = 2.00 (798.6 examples/sec; 0.160 sec/batch)
2017-05-05 04:23:48.979348: step 195410, loss = 1.87 (846.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:23:50.498682: step 195420, loss = 1.98 (842.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:23:52.022732: step 195430, loss = 1.92 (839.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:23:53.518910: step 195440, loss = 1.70 (855.5 examples/sec; 0.150 sec/batch)
2017-05-05 04:23:55.087509: step 195450, loss = 1.68 (816.0 examples/sec; 0.157 sec/batch)
2017-05-05 04:23:56.596492: step 195460, loss = 1.84 (848.3 examples/sec; 0.151 sec/batch)
2017-05-05 04:23:58.166695: step 195470, loss = 1.95 (815.2 examples/sec; 0.157 sec/batch)
2017-05-05 04:23:59.707289: step 195480, loss = 1.72 (830.9 examples/sec; 0.154 sec/batch)
2017-05-05 04:24:01.245605: step 195490, loss = 1.99 (832.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:24:02.824617: step 195500, loss = 1.75 (810.6 examples/sec; 0.158 sec/batch)
2017-05-05 04:24:04.327954: step 195510, loss = 1.89 (851.4 examples/sec; 0.150 sec/batch)
2017-05-05 04:24:05.904875: step 195520, loss = 1.81 (811.7 examples/sec; 0.158 sec/batch)
2017-05-05 04:24:07.475831: step 195530, loss = 1.75 (814.8 examples/sec; 0.157 sec/batch)
2017-05-05 04:24:08.961427: step 195540, loss = 1.94 (861.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:24:10.576726: step 195550, loss = 1.92 (792.4 examples/sec; 0.162 sec/batch)
2017-05-05 04:24:12.126355: step 195560, loss = 1.76 (826.0 examples/sec; 0.155 sec/batch)
2017-05-05 04:24:13.693991: step 195570, loss = 2.00 (816.5 examples/sec; 0.157 sec/batch)
2017-05-05 04:24:15.268695: step 195580, loss = 1.77 (812.9 examples/sec; 0.157 sec/batch)
2017-05-05 04:24:16.849304: step 195590, loss = 1.89 (809.8 examples/sec; 0.158 sec/batch)
2017-05-05 04:24:18.339823: step 195600, loss = 1.79 (858.8 examples/sec; 0.149 sec/batch)
2017-05-05 04:24:19.919630: step 195610, loss = 2.13 (810.2 examples/sec; 0.158 sec/batch)
2017-05-05 04:24:21.401928: step 195620, loss = 1.79 (863.5 examples/sec; 0.148 sec/batch)
2017-05-05 04:24:22.924706: step 195630, loss = 1.80 (840.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:24:24.448661: step 195640, loss = 2.11 (839.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:24:25.995497: step 195650, loss = 1.95 (827.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:24:27.498934: step 195660, loss = 1.83 (851.4 examples/sec; 0.150 sec/batch)
2017-05-05 04:24:29.029359: step 195670, loss = 1.68 (836.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:24:30.551707: step 195680, loss = 1.68 (840.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:24:32.119191: step 195690, loss = 1.78 (816.6 examples/sec; 0.157 sec/batch)
2017-05-05 04:24:33.650561: step 195700, loss = 1.77 (835.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:24:35.174045: step 195710, loss = 1.77 (840.2 examples/sec; 0.152 sec/batch)
2017-05-05 04:24:36.746740: step 195720, loss = 1.74 (813.9 examples/sec; 0.157 sec/batch)
2017-05-05 04:24:38.280744: step 195730, loss = 1.79 (834.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:24:39.834690: step 195740, loss = 1.80 (823.7 examples/sec; 0.155 sec/batch)
2017-05-05 04:24:41.380408: step 195750, loss = 2.00 (828.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:24:42.857435: step 195760, loss = 1.89 (866.6 examples/sec; 0.148 sec/batch)
2017-05-05 04:24:44.377439: step 195770, loss = 1.83 (842.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:24:45.911741: step 195780, loss = 1.86 (834.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:24:47.461599: step 195790, loss = 1.89 (825.9 examples/sec; 0.155 sec/batch)
2017-05-05 04:24:48.951035: step 195800, loss = 1.92 (859.4 examples/sec; 0.149 sec/batch)
2017-05-05 04:24:50.488339: step 195810, loss = 1.75 (832.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:24:52.009729: step 195820, loss = 1.93 (841.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:24:53.575512: step 195830, loss = 1.99 (817.5 examples/sec; 0.157 sec/batch)
2017-05-05 04:24:55.091767: step 195840, loss = 1.85 (844.2 examples/sec; 0.152 sec/batch)
2017-05-05 04:24:56.682308: step 195850, loss = 1.86 (804.8 examples/sec; 0.159 sec/batch)
2017-05-05 04:24:58.238513: step 195860, loss = 1.73 (822.5 examples/sec; 0.156 sec/batch)
2017-05-05 04:24:59.803116: step 195870, loss = 1.87 (818.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:25:01.292442: step 195880, loss = 2.01 (859.4 examples/sec; 0.149 sec/batch)
2017-05-05 04:25:02.878415: step 195890, loss = 1.94 (807.1 examples/sec; 0.159 sec/batch)
2017-05-05 04:25:04.432873: step 195900, loss = 1.77 (823.4 examples/sec; 0.155 sec/batch)
2017-05-05 04:25:06.011689: step 195910, loss = 2.05 (810.7 examples/sec; 0.158 sec/batch)
2017-05-05 04:25:07.580734: step 195920, loss = 1.75 (815.8 examples/sec; 0.157 sec/batch)
2017-05-05 04:25:09.151918: step 195930, loss = 1.84 (814.7 examples/sec; 0.157 sec/batch)
2017-05-05 04:25:10.677654: step 195940, loss = 1.81 (838.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:25:12.259951: step 195950, loss = 1.86 (809.0 examples/sec; 0.158 sec/batch)
2017-05-05 04:25:13.761946: step 195960, loss = 1.88 (852.2 examples/sec; 0.150 sec/batch)
2017-05-05 04:25:15.361283: step 195970, loss = 1.88 (800.3 examples/sec; 0.160 sec/batch)
2017-05-05 04:25:16.892031: step 195980, loss = 1.84 (836.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:25:18.425312: step 195990, loss = 1.98 (834.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:25:19.927689: step 196000, loss = 1.89 (852.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:25:21.471924: step 196010, loss = 1.82 (828.9 examples/sec; 0.154 sec/batch)
2017-05-05 04:25:23.113894: step 196020, loss = 1.76 (779.5 examples/sec; 0.164 sec/batch)
2017-05-05 04:25:24.509698: step 196030, loss = 1.74 (917.0 examples/sec; 0.140 sec/batch)
2017-05-05 04:25:26.041784: step 196040, loss = 1.79 (835.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:25:27.552052: step 196050, loss = 1.77 (847.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:25:29.129800: step 196060, loss = 1.83 (811.3 examples/sec; 0.158 sec/batch)
2017-05-05 04:25:30.686648: step 196070, loss = 1.74 (822.2 examples/sec; 0.156 sec/batch)
2017-05-05 04:25:32.209189: step 196080, loss = 1.74 (840.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:25:33.692308: step 196090, loss = 1.82 (863.0 examples/sec; 0.148 sec/batch)
2017-05-05 04:25:35.152212: step 196100, loss = 1.82 (876.8 examples/sec; 0.146 sec/batch)
2017-05-05 04:25:36.699278: step 196110, loss = 1.90 (827.4 examples/sec; 0.155 sec/batch)
2017-05-05 04:25:38.162675: step 196120, loss = 1.78 (874.7 examples/sec; 0.146 sec/batch)
2017-05-05 04:25:39.697522: step 196130, loss = 1.81 (834.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:25:41.179493: step 196140, loss = 1.97 (863.7 examples/sec; 0.148 sec/batch)
2017-05-05 04:25:42.699880: step 196150, loss = 1.91 (841.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:25:44.229085: step 196160, loss = 1.79 (837.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:25:45.728316: step 196170, loss = 1.81 (853.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:25:47.267828: step 196180, loss = 1.79 (831.4 examples/sec; 0.154 sec/batch)
2017-05-05 04:25:48.830541: step 196190, loss = 1.92 (819.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:25:50.339729: step 196200, loss = 1.85 (848.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:25:51.913847: step 196210, loss = 1.94 (813.2 examples/sec; 0.157 sec/batch)
2017-05-05 04:25:53.263562: step 196220, loss = 1.88 (948.3 examples/sec; 0.135 sec/batch)
2017-05-05 04:25:54.449017: step 196230, loss = 1.85 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 04:25:55.634917: step 196240, loss = 1.94 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 04:25:56.835288: step 196250, loss = 1.97 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:25:58.015204: step 196260, loss = 1.86 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 04:25:59.213498: step 196270, loss = 1.95 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:26:00.399547: step 196280, loss = 1.77 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 04:26:01.577445: step 196290, loss = 1.85 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 04:26:02.753495: step 196300, loss = 1.74 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 04:26:03.936567: step 196310, loss = 1.83 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 04:26:05.107458: step 196320, loss = 1.93 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 04:26:06.276853: step 196330, loss = 1.78 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 04:26:07.457489: step 196340, loss = 1.81 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 04:26:08.628115: step 196350, loss = 1.94 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 04:26:09.797284: step 196360, loss = 1.76 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 04:26:10.971205: step 196370, loss = 1.95 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 04:26:12.133963: step 196380, loss = 1.99 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:13.289570: step 196390, loss = 1.89 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:14.444488: step 196400, loss = 1.94 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:15.616257: step 196410, loss = 1.95 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 04:26:16.765385: step 196420, loss = 1.82 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:17.889180: step 196430, loss = 1.87 (1139.0 examples/sec; 0.112 sec/batch)
2017-05-05 04:26:19.040740: step 196440, loss = 1.89 (1111.5 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:20.197456: step 196450, loss = 1.67 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:21.362395: step 196460, loss = 1.91 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:22.520327: step 196470, loss = 1.87 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:23.670336: step 196480, loss = 1.77 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:24.828105: step 196490, loss = 1.89 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:25.989920: step 196500, loss = 1.91 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:27.143284: step 196510, loss = 1.84 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:28.307844: step 196520, loss = 1.84 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:29.466596: step 196530, loss = 1.95 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:30.612663: step 196540, loss = 1.85 (1116.9 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:31.767780: step 196550, loss = 1.96 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:32.922628: step 196560, loss = 1.98 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:34.081839: step 196570, loss = 1.88 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:35.227965: step 196580, loss = 1.81 (1116.8 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:36.379114: step 196590, loss = 1.87 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:37.515620: step 196600, loss = 1.92 (1126.2 examples/sec; 0.114 sec/batch)
2017-05-05 04:26:38.676487: step 196610, loss = 1.86 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:39.831798: step 196620, loss = 1.73 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:40.981853: step 196630, loss = 1.75 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:42.120451: step 196640, loss = 1.75 (1124.2 examples/sec; 0.114 sec/batch)
2017-05-05 04:26:43.282282: step 196650, loss = 1.87 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:44.440770: step 196660, loss = 1.78 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:45.575024: step 196670, loss = 1.78 (1128.5 examples/sec; 0.113 sec/batch)
2017-05-05 04:26:46.724936: step 196680, loss = 1.91 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:47.887339: step 196690, loss = 2.00 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:49.046011: step 196700, loss = 1.79 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:50.215397: step 196710, loss = 1.85 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 04:26:51.377380: step 196720, loss = 1.82 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:52.544775: step 196730, loss = 1.77 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 04:26:53.712683: step 196740, loss = 1.86 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 04:26:54.873492: step 196750, loss = 1.79 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:56.017788: step 196760, loss = 1.79 (1118.6 examples/sec; 0.114 sec/batch)
2017-05-05 04:26:57.179810: step 196770, loss = 1.85 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:58.342524: step 196780, loss = 1.83 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:59.509193: step 196790, loss = 1.86 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 04:27:00.654546: step 196800, loss = 1.94 (1117.6 examples/sec; 0.115 sec/batch)
2017-05-05 04:27:01.817647: step 196810, loss = 1.90 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 04:27:02.982920: step 196820, loss = 1.88 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 04:27:04.128695: step 196830, loss = 1.70 (1117.1 examples/sec; 0.115 sec/batch)
2017-05-05 04:27:05.273864: step 196840, loss = 1.84 (1117.7 examples/sec; 0.115 sec/batch)
2017-05-05 04:27:06.435108: step 196850, loss = 1.97 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 04:27:07.606587: step 196860, loss = 1.85 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 04:27:08.785382: step 196870, loss = 1.81 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 04:27:09.941268: step 196880, loss = 1.97 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 04:27:11.119665: step 196890, loss = 1.99 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 04:27:12.270600: step 196900, loss = 1.92 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-05 04:27:13.422555: step 196910, loss = 1.78 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-05 04:27:14.587414: step 196920, loss = 1.80 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 04:27:15.767088: step 196930, loss = 1.82 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 04:27:16.949996: step 196940, loss = 1.85 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 04:27:18.149870: step 196950, loss = 1.78 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:19.350152: step 196960, loss = 2.05 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:20.553536: step 196970, loss = 1.98 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:21.763401: step 196980, loss = 1.84 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 04:27:22.965251: step 196990, loss = 1.79 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:24.146861: step 197000, loss = 1.79 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 04:27:25.432721: step 197010, loss = 2.04 (995.4 examples/sec; 0.129 sec/batch)
2017-05-05 04:27:26.522695: step 197020, loss = 1.83 (1174.3 examples/sec; 0.109 sec/batch)
2017-05-05 04:27:27.726384: step 197030, loss = 1.80 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:28.927616: step 197040, loss = 1.95 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:30.122284: step 197050, loss = 1.95 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 04:27:31.325163: step 197060, loss = 1.86 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:32.546670: step 197070, loss = 1.92 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-05 04:27:33.722752: step 197080, loss = 1.84 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 04:27:34.947826: step 197090, loss = 1.79 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 04:27:36.132032: step 197100, loss = 1.85 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 04:27:37.375097: step 197110, loss = 1.91 (1029.7 examples/sec; 0.124 sec/batch)
2017-05-05 04:27:38.577220: step 197120, loss = 1.85 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:39.772216: step 197130, loss = 1.92 (1071.1 examples/sec; 0.119 sec/batch)
2017-05-05 04:27:40.978193: step 197140, loss = 1.93 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:27:42.154479: step 197150, loss = 1.90 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 04:27:43.350342: step 197160, loss = 1.71 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:44.550514: step 197170, loss = 1.75 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:45.736982: step 197180, loss = 1.83 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 04:27:46.936952: step 197190, loss = 1.85 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:48.147200: step 197200, loss = 1.90 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 04:27:49.443165: step 197210, loss = 1.76 (987.7 examples/sec; 0.130 sec/batch)
2017-05-05 04:27:50.552958: step 197220, loss = 1.95 (1153.4 examples/sec; 0.111 sec/batch)
2017-05-05 04:27:51.773205: step 197230, loss = 1.89 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 04:27:52.994797: step 197240, loss = 2.04 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 04:27:54.210394: step 197250, loss = 1.94 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 04:27:55.390687: step 197260, loss = 1.88 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 04:27:56.620868: step 197270, loss = 1.67 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-05 04:27:57.807155: step 197280, loss = 1.95 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 04:27:59.053833: step 197290, loss = 1.76 (1026.7 examples/sec; 0.125 sec/batch)
2017-05-05 04:28:00.233071: step 197300, loss = 1.85 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 04:28:01.461471: step 197310, loss = 1.88 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-05 04:28:02.661877: step 197320, loss = 1.83 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:03.895537: step 197330, loss = 1.81 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-05 04:28:05.096870: step 197340, loss = 1.83 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:06.287877: step 197350, loss = 1.77 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:07.474311: step 197360, loss = 2.00 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:08.672744: step 197370, loss = 1.95 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:09.868847: step 197380, loss = 1.86 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:11.094607: step 197390, loss = 1.88 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 04:28:12.300191: step 197400, loss = 2.02 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:13.485846: step 197410, loss = 1.84 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:14.677203: step 197420, loss = 2.07 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:15.890887: step 197430, loss = 1.90 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:17.078224: step 197440, loss = 1.97 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:18.285917: step 197450, loss = 1.91 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:19.473961: step 197460, loss = 1.96 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:20.670640: step 197470, loss = 1.75 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:21.874982: step 197480, loss = 1.84 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:23.071129: step 197490, loss = 1.84 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:24.272515: step 197500, loss = 1.82 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:25.467213: step 197510, loss = 1.79 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:26.664688: step 197520, loss = 1.82 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:27.877563: step 197530, loss = 1.80 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:29.064956: step 197540, loss = 1.74 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:30.266544: step 197550, loss = 1.85 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:31.472600: step 197560, loss = 1.95 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:32.690922: step 197570, loss = 1.89 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 04:28:33.881353: step 197580, loss = 1.84 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:35.094983: step 197590, loss = 1.82 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:36.310101: step 197600, loss = 1.79 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 04:28:37.499514: step 197610, loss = 1.88 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:38.694800: step 197620, loss = 1.90 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:39.917398: step 197630, loss = 1.83 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 04:28:41.135197: step 197640, loss = 1.92 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 04:28:42.352108: step 197650, loss = 1.77 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 04:28:43.555427: step 197660, loss = 1.88 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:44.736679: step 197670, loss = 1.84 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 04:28:45.928807: step 197680, loss = 1.90 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:47.160214: step 197690, loss = 1.88 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 04:28:48.352785: step 197700, loss = 1.75 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:49.568774: step 197710, loss = 1.77 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 04:28:50.759347: step 197720, loss = 1.88 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:51.965531: step 197730, loss = 2.03 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:53.171992: step 197740, loss = 1.90 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:54.401694: step 197750, loss = 1.84 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-05 04:28:55.592469: step 197760, loss = 1.83 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:56.792387: step 197770, loss = 1.75 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:58.002028: step 197780, loss = 1.81 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:59.225302: step 197790, loss = 1.75 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 04:29:00.413358: step 197800, loss = 1.75 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 04:29:01.585006: step 197810, loss = 1.94 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 04:29:02.797955: step 197820, loss = 1.81 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:03.986747: step 197830, loss = 1.71 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 04:29:05.181322: step 197840, loss = 1.97 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 04:29:06.387102: step 197850, loss = 1.93 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:07.569612: step 197860, loss = 1.73 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 04:29:08.775248: step 197870, loss = 1.89 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:09.982822: step 197880, loss = 1.79 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:11.187789: step 197890, loss = 1.79 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:12.382220: step 197900, loss = 1.86 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 04:29:13.591820: step 197910, loss = 1.77 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:14.804024: step 197920, loss = 1.94 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:15.998384: step 197930, loss = 1.90 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 04:29:17.221645: step 197940, loss = 1.82 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 04:29:18.436986: step 197950, loss = 1.96 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 04:29:19.624520: step 197960, loss = 1.77 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 04:29:20.841038: step 197970, loss = 1.84 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 04:29:22.049605: step 197980, loss = 2.07 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:23.275414: step 197990, loss = 1.87 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 04:29:24.587709: step 198000, loss = 1.97 (975.4 examples/sec; 0.131 sec/batch)
2017-05-05 04:29:25.673977: step 198010, loss = 1.94 (1178.4 examples/sec; 0.109 sec/batch)
2017-05-05 04:29:26.866739: step 198020, loss = 1.84 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 04:29:28.084382: step 198030, loss = 1.85 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 04:29:29.284897: step 198040, loss = 2.15 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:30.493932: step 198050, loss = 1.80 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:31.695747: step 198060, loss = 1.91 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:32.904363: step 198070, loss = 1.93 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:34.102859: step 198080, loss = 1.89 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:35.303326: step 198090, loss = 1.82 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:36.514166: step 198100, loss = 1.78 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:37.734969: step 198110, loss = 1.77 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 04:29:38.941380: step 198120, loss = 1.80 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:40.160959: step 198130, loss = 1.71 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 04:29:41.370142: step 198140, loss = 1.86 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:42.556373: step 198150, loss = 1.83 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 04:29:43.766647: step 198160, loss = 1.90 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:44.998132: step 198170, loss = 1.87 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-05 04:29:46.185509: step 198180, loss = 1.77 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 04:29:47.394613: step 198190, loss = 1.92 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:48.675736: step 198200, loss = 1.89 (999.1 examples/sec; 0.128 sec/batch)
2017-05-05 04:29:49.787335: step 198210, loss = 1.88 (1151.5 examples/sec; 0.111 sec/batch)
2017-05-05 04:29:51.000769: step 198220, loss = 1.86 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:52.204127: step 198230, loss = 1.71 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:53.400025: step 198240, loss = 1.89 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:54.601415: step 198250, loss = 1.81 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:55.793286: step 198260, loss = 1.91 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-05 04:29:57.012114: step 198270, loss = 1.75 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 04:29:58.221307: step 198280, loss = 1.85 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:59.430407: step 198290, loss = 1.88 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:00.630196: step 198300, loss = 1.77 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:01.837517: step 198310, loss = 1.83 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:03.035419: step 198320, loss = 1.79 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:04.241594: step 198330, loss = 1.92 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:05.449845: step 198340, loss = 1.86 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:06.671415: step 198350, loss = 1.83 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 04:30:07.855013: step 198360, loss = 1.75 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 04:30:09.069928: step 198370, loss = 1.94 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:10.263031: step 198380, loss = 1.77 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 04:30:11.474539: step 198390, loss = 1.75 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:12.662005: step 198400, loss = 1.85 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 04:30:13.848711: step 198410, loss = 1.82 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 04:30:15.033368: step 198420, loss = 1.84 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 04:30:16.230984: step 198430, loss = 1.80 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:17.417483: step 198440, loss = 1.93 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 04:30:18.597750: step 198450, loss = 1.78 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 04:30:19.789586: step 198460, loss = 1.77 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 04:30:20.958031: step 198470, loss = 1.86 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 04:30:22.124881: step 198480, loss = 1.83 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 04:30:23.329484: step 198490, loss = 1.80 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:24.512824: step 198500, loss = 1.92 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 04:30:25.697149: step 198510, loss = 1.85 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 04:30:26.904207: step 198520, loss = 2.02 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:28.120794: step 198530, loss = 1.77 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 04:30:29.331303: step 198540, loss = 1.71 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:30.529066: step 198550, loss = 1.77 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:31.727781: step 198560, loss = 1.87 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:32.946093: step 198570, loss = 1.90 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 04:30:34.136993: step 198580, loss = 1.88 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 04:30:35.364939: step 198590, loss = 1.88 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-05 04:30:36.546865: step 198600, loss = 1.73 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 04:30:37.747020: step 198610, loss = 2.08 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:38.951813: step 198620, loss = 1.88 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:40.185695: step 198630, loss = 1.83 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-05 04:30:41.396862: step 198640, loss = 1.79 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:42.603275: step 198650, loss = 1.79 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:43.791056: step 198660, loss = 1.76 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 04:30:45.018788: step 198670, loss = 1.91 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 04:30:46.213118: step 198680, loss = 1.79 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 04:30:47.444477: step 198690, loss = 1.81 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 04:30:48.640321: step 198700, loss = 1.91 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:49.871294: step 198710, loss = 1.91 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 04:30:51.072571: step 198720, loss = 1.95 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:52.286589: step 198730, loss = 1.71 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:53.495071: step 198740, loss = 1.73 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:54.681189: step 198750, loss = 1.82 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 04:30:55.888152: step 198760, loss = 1.89 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:57.093096: step 198770, loss = 1.94 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:58.302227: step 198780, loss = 1.92 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:59.506104: step 198790, loss = 1.78 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:00.681761: step 198800, loss = 1.86 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 04:31:01.876701: step 198810, loss = 1.76 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 04:31:03.090292: step 198820, loss = 1.98 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:04.304480: step 198830, loss = 1.82 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:05.487949: step 198840, loss = 1.94 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 04:31:06.704294: step 198850, loss = 2.04 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 04:31:07.899919: step 198860, loss = 1.91 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:09.095657: step 198870, loss = 1.82 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:10.295885: step 198880, loss = 1.93 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:11.496695: step 198890, loss = 1.82 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:12.697584: step 198900, loss = 1.88 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:13.905191: step 198910, loss = 1.75 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:15.106008: step 198920, loss = 1.84 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:16.326767: step 198930, loss = 1.92 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 04:31:17.509500: step 198940, loss = 1.85 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 04:31:18.706880: step 198950, loss = 1.86 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:19.937436: step 198960, loss = 1.82 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-05 04:31:21.142384: step 198970, loss = 1.85 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:22.336745: step 198980, loss = 1.83 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 04:31:23.652839: step 198990, loss = 1.98 (972.6 examples/sec; 0.132 sec/batch)
2017-05-05 04:31:24.735419: step 199000, loss = 1.93 (1182.4 examples/sec; 0.108 sec/batch)
2017-05-05 04:31:25.925742: step 199010, loss = 1.86 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 04:31:27.138436: step 199020, loss = 1.90 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:28.347859: step 199030, loss = 1.97 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:29.547628: step 199040, loss = 1.90 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:30.730697: step 199050, loss = 2.00 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 04:31:31.941607: step 199060, loss = 1.78 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:33.151663: step 199070, loss = 1.87 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:34.357778: step 199080, loss = 1.81 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:35.572389: step 199090, loss = 1.84 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:36.773343: step 199100, loss = 1.93 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:37.989468: step 199110, loss = 1.87 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 04:31:39.192619: step 199120, loss = 1.79 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:40.413846: step 199130, loss = 1.89 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 04:31:41.603003: step 199140, loss = 1.70 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 04:31:42.815820: step 199150, loss = 1.86 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:44.014376: step 199160, loss = 1.91 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:45.234349: step 199170, loss = 2.04 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 04:31:46.425520: step 199180, loss = 2.06 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 04:31:47.711422: step 199190, loss = 1.74 (995.4 examples/sec; 0.129 sec/batch)
2017-05-05 04:31:48.827646: step 199200, loss = 1.76 (1146.7 examples/sec; 0.112 sec/batch)
2017-05-05 04:31:50.012551: step 199210, loss = 1.72 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 04:31:51.215344: step 199220, loss = 1.81 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:52.418139: step 199230, loss = 1.90 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:53.612276: step 199240, loss = 2.05 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 04:31:54.831966: step 199250, loss = 1.68 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 04:31:56.011960: step 199260, loss = 1.91 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 04:31:57.218607: step 199270, loss = 1.85 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:58.433060: step 199280, loss = 1.88 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:59.649434: step 199290, loss = 1.79 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 04:32:00.870476: step 199300, loss = 1.92 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-05 04:32:02.082154: step 199310, loss = 1.84 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:03.263384: step 199320, loss = 2.03 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 04:32:04.477735: step 199330, loss = 1.82 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:05.663593: step 199340, loss = 1.78 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:06.872328: step 199350, loss = 1.74 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:08.079013: step 199360, loss = 1.83 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:09.300026: step 199370, loss = 1.81 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-05 04:32:10.501565: step 199380, loss = 1.85 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:11.675346: step 199390, loss = 1.89 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 04:32:12.891823: step 199400, loss = 1.78 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 04:32:14.071201: step 199410, loss = 1.86 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 04:32:15.291895: step 199420, loss = 1.72 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 04:32:16.517658: step 199430, loss = 1.65 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 04:32:17.708285: step 199440, loss = 1.91 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:18.911703: step 199450, loss = 1.81 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:20.102080: step 199460, loss = 1.81 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:21.338708: step 199470, loss = 1.87 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-05 04:32:22.513689: step 199480, loss = 1.85 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 04:32:23.732925: step 199490, loss = 1.92 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 04:32:24.935750: step 199500, loss = 1.75 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:26.133697: step 199510, loss = 1.75 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:27.335152: step 199520, loss = 1.68 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:28.553395: step 199530, loss = 1.93 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 04:32:29.744357: step 199540, loss = 1.80 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:30.961608: step 199550, loss = 2.02 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 04:32:32.153044: step 199560, loss = 1.89 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:33.379180: step 199570, loss = 1.86 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 04:32:34.569797: step 199580, loss = 1.93 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:35.759571: step 199590, loss = 1.79 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:36.965514: step 199600, loss = 1.78 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:38.152142: step 199610, loss = 1.97 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:39.353465: step 199620, loss = 1.85 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:40.561619: step 199630, loss = 1.84 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:41.764408: step 199640, loss = 1.76 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:42.988417: step 199650, loss = 1.87 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 04:32:44.193629: step 199660, loss = 1.81 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:45.385840: step 199670, loss = 1.91 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:46.574390: step 199680, loss = 1.89 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:47.800647: step 199690, loss = 1.69 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 04:32:49.004557: step 199700, loss = 1.78 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:50.208759: step 199710, loss = 2.03 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:51.409879: step 199720, loss = 1.77 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:52.631395: step 199730, loss = 1.89 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-05 04:32:53.808973: step 199740, loss = 1.83 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 04:32:55.019003: step 199750, loss = 1.87 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:56.220430: step 199760, loss = 1.80 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:57.430320: step 199770, loss = 1.94 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:58.628814: step 199780, loss = 1.81 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:59.817612: step 199790, loss = 1.95 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 04:33:01.016774: step 199800, loss = 1.76 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 04:33:02.225212: step 199810, loss = 1.88 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:33:03.435843: step 199820, loss = 1.91 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 04:33:04.657944: step 199830, loss = 1.94 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-05 04:33:05.863294: step 199840, loss = 1.72 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 04:33:07.058986: step 199850, loss = 1.93 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 04:33:08.273030: step 199860, loss = 1.86 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 04:33:09.466970: step 199870, loss = 1.95 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 04:33:10.660876: step 199880, loss = 2.00 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 04:33:11.872225: step 199890, loss = 1.86 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:33:13.076495: step 199900, loss = 1.76 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:33:14.240528: step 199910, loss = 1.90 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 04:33:15.413925: step 199920, loss = 1.83 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 04:33:16.581945: step 199930, loss = 1.78 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 04:33:17.756586: step 199940, loss = 1.72 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 04:33:18.925671: step 199950, loss = 1.96 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 04:33:20.098370: step 199960, loss = 1.88 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 04:33:21.270452: step 199970, loss = 1.92 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 04:33:22.524898: step 199980, loss = 1.80 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-05 04:33:23.585475: step 199990, loss = 1.76 (1206.9 examples/sec; 0.106 sec/batch)
--- 24286.000705 seconds ---
