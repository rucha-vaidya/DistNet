I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:04:00.0
Total memory: 7.92GiB
Free memory: 2.77GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3f04430
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Quadro K620
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:03:00.0
Total memory: 1.95GiB
Free memory: 1.73GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Ignoring gpu device (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0) with Cuda multiprocessor count: 3. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
Connecting to port  19002
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Received variable values from ps
2017-05-08 18:24:35.168071: step 0, loss = 4.68 (76.0 examples/sec; 1.683 sec/batch)
2017-05-08 18:24:36.275979: step 10, loss = 4.52 (1155.3 examples/sec; 0.111 sec/batch)
2017-05-08 18:24:37.505735: step 20, loss = 4.45 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-08 18:24:38.768086: step 30, loss = 4.46 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:24:40.046993: step 40, loss = 4.43 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:24:41.299786: step 50, loss = 4.12 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-08 18:24:42.592018: step 60, loss = 4.06 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:24:43.852236: step 70, loss = 3.98 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:24:45.102418: step 80, loss = 4.23 (1023.9 examples/sec; 0.125 sec/batch)
2017-05-08 18:24:46.390046: step 90, loss = 3.89 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:24:47.789006: step 100, loss = 4.01 (915.0 examples/sec; 0.140 sec/batch)
2017-05-08 18:24:49.001503: step 110, loss = 4.09 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-08 18:24:50.277589: step 120, loss = 3.93 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:24:51.574548: step 130, loss = 3.82 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:24:52.941884: step 140, loss = 3.58 (936.1 examples/sec; 0.137 sec/batch)
2017-05-08 18:24:54.196654: step 150, loss = 3.91 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-08 18:24:55.456354: step 160, loss = 3.57 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 18:24:56.771157: step 170, loss = 3.57 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:24:58.072358: step 180, loss = 3.50 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:24:59.385173: step 190, loss = 3.57 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:25:00.789555: step 200, loss = 3.45 (911.4 examples/sec; 0.140 sec/batch)
2017-05-08 18:25:02.025060: step 210, loss = 3.38 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-08 18:25:03.318741: step 220, loss = 3.22 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:25:04.610665: step 230, loss = 3.25 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:25:05.898532: step 240, loss = 3.29 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:25:07.194713: step 250, loss = 3.30 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:25:08.469978: step 260, loss = 3.15 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:25:09.747523: step 270, loss = 3.34 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:25:11.030732: step 280, loss = 3.34 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:25:12.361747: step 290, loss = 3.18 (961.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:25:13.774865: step 300, loss = 3.12 (905.8 examples/sec; 0.141 sec/batch)
2017-05-08 18:25:14.954128: step 310, loss = 2.99 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-08 18:25:16.229279: step 320, loss = 2.93 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:25:17.487816: step 330, loss = 2.99 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-08 18:25:18.758847: step 340, loss = 2.96 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:20.054140: step 350, loss = 3.17 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:25:21.376511: step 360, loss = 2.85 (968.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:25:22.665072: step 370, loss = 3.01 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:25:23.932661: step 380, loss = 2.78 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:25.252925: step 390, loss = 2.88 (969.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:25:26.659418: step 400, loss = 2.84 (910.1 examples/sec; 0.141 sec/batch)
2017-05-08 18:25:27.863057: step 410, loss = 2.79 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-08 18:25:29.159484: step 420, loss = 2.77 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:25:30.469827: step 430, loss = 2.76 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:25:31.757126: step 440, loss = 2.71 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:25:33.036905: step 450, loss = 2.63 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:25:34.312235: step 460, loss = 2.61 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:25:35.604777: step 470, loss = 2.87 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:25:36.907073: step 480, loss = 2.67 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:25:38.216128: step 490, loss = 2.50 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:25:39.627439: step 500, loss = 2.68 (907.0 examples/sec; 0.141 sec/batch)
2017-05-08 18:25:40.834729: step 510, loss = 2.45 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-08 18:25:42.124627: step 520, loss = 2.43 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:25:43.412167: step 530, loss = 2.43 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:25:44.692332: step 540, loss = 2.34 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:25:45.970021: step 550, loss = 2.72 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:25:47.263241: step 560, loss = 2.37 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:25:48.581948: step 570, loss = 2.45 (970.6 examples/sec; 0.132 sec/batch)
2017-05-08 18:25:49.845998: step 580, loss = 2.44 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:25:51.172713: step 590, loss = 2.35 (964.8 examples/sec; 0.133 sec/batch)
2017-05-08 18:25:52.562919: step 600, loss = 2.24 (920.7 examples/sec; 0.139 sec/batch)
2017-05-08 18:25:53.761710: step 610, loss = 2.26 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-08 18:25:55.037306: step 620, loss = 2.22 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:25:56.308889: step 630, loss = 2.03 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:57.603299: step 640, loss = 2.16 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:25:58.939872: step 650, loss = 2.16 (957.7 examples/sec; 0.134 sec/batch)
2017-05-08 18:26:00.217279: step 660, loss = 2.12 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:26:01.491620: step 670, loss = 2.31 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:26:02.780995: step 680, loss = 2.18 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:26:04.065815: step 690, loss = 2.29 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:26:05.451026: step 700, loss = 2.17 (924.0 examples/sec; 0.139 sec/batch)
2017-05-08 18:26:06.647951: step 710, loss = 2.32 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-08 18:26:07.953033: step 720, loss = 2.06 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:26:09.275137: step 730, loss = 1.88 (968.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:26:10.566361: step 740, loss = 2.15 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:26:11.872136: step 750, loss = 2.17 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:26:13.178071: step 760, loss = 2.16 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:26:14.493301: step 770, loss = 2.13 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:26:15.769608: step 780, loss = 1.86 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:26:17.047746: step 790, loss = 1.88 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:26:18.426404: step 800, loss = 1.86 (928.4 examples/sec; 0.138 sec/batch)
2017-05-08 18:26:19.628731: step 810, loss = 1.92 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-08 18:26:20.905028: step 820, loss = 2.05 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:26:22.217794: step 830, loss = 1.86 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:26:23.522661: step 840, loss = 1.75 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:26:24.808159: step 850, loss = 1.88 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:26:26.101072: step 860, loss = 1.94 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:26:27.383672: step 870, loss = 1.98 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:26:28.678686: step 880, loss = 1.96 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:26:29.949150: step 890, loss = 1.96 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:26:31.316907: step 900, loss = 2.07 (935.8 examples/sec; 0.137 sec/batch)
2017-05-08 18:26:32.530142: step 910, loss = 2.13 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-08 18:26:33.828912: step 920, loss = 1.72 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:26:35.133033: step 930, loss = 1.85 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:26:36.444947: step 940, loss = 1.73 (975.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:26:37.741990: step 950, loss = 1.67 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:26:39.048092: step 960, loss = 1.83 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:26:40.328842: step 970, loss = 1.68 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:26:41.633186: step 980, loss = 1.89 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:26:42.950310: step 990, loss = 1.98 (971.8 examples/sec; 0.132 sec/batch)
2017-05-08 18:26:44.343271: step 1000, loss = 1.95 (918.9 examples/sec; 0.139 sec/batch)
2017-05-08 18:26:45.554304: step 1010, loss = 1.75 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-08 18:26:46.846884: step 1020, loss = 1.54 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:26:48.144847: step 1030, loss = 1.73 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:26:49.418443: step 1040, loss = 1.65 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:26:50.729816: step 1050, loss = 1.57 (976.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:26:52.038337: step 1060, loss = 1.49 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:26:53.335830: step 1070, loss = 1.63 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:26:54.644836: step 1080, loss = 1.53 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:26:55.962721: step 1090, loss = 1.56 (971.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:26:57.365662: step 1100, loss = 1.65 (912.4 examples/sec; 0.140 sec/batch)
2017-05-08 18:26:58.570724: step 1110, loss = 1.56 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-08 18:26:59.860557: step 1120, loss = 1.58 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:27:01.151643: step 1130, loss = 1.76 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:27:02.454028: step 1140, loss = 1.55 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:27:03.778303: step 1150, loss = 1.51 (966.6 examples/sec; 0.132 sec/batch)
2017-05-08 18:27:05.059608: step 1160, loss = 1.45 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:27:06.325868: step 1170, loss = 1.70 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:27:07.647533: step 1180, loss = 1.60 (968.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:27:08.930528: step 1190, loss = 1.79 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:27:10.350718: step 1200, loss = 1.62 (901.3 examples/sec; 0.142 sec/batch)
2017-05-08 18:27:11.553517: step 1210, loss = 1.47 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-08 18:27:12.848616: step 1220, loss = 1.67 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:27:14.139578: step 1230, loss = 1.55 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:27:15.404481: step 1240, loss = 1.42 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:27:16.737216: step 1250, loss = 1.53 (960.4 examples/sec; 0.133 sec/batch)
2017-05-08 18:27:18.030566: step 1260, loss = 1.75 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:27:19.329784: step 1270, loss = 1.52 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:27:20.650843: step 1280, loss = 1.45 (968.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:27:21.978039: step 1290, loss = 1.43 (964.4 examples/sec; 0.133 sec/batch)
2017-05-08 18:27:23.384305: step 1300, loss = 1.54 (910.2 examples/sec; 0.141 sec/batch)
2017-05-08 18:27:24.595508: step 1310, loss = 1.82 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-08 18:27:25.902713: step 1320, loss = 1.60 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:27:27.190889: step 1330, loss = 1.34 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:27:28.483826: step 1340, loss = 1.38 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:27:29.812023: step 1350, loss = 1.38 (963.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:27:31.141623: step 1360, loss = 1.60 (962.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:27:32.456429: step 1370, loss = 1.43 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:27:33.762264: step 1380, loss = 1.27 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:27:35.074597: step 1390, loss = 1.52 (975.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:27:36.475752: step 1400, loss = 1.41 (913.5 examples/sec; 0.140 sec/batch)
2017-05-08 18:27:37.686659: step 1410, loss = 1.61 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-08 18:27:38.993713: step 1420, loss = 1.43 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:27:40.260601: step 1430, loss = 1.50 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:27:41.559058: step 1440, loss = 1.48 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:27:42.889233: step 1450, loss = 1.33 (962.3 examples/sec; 0.133 sec/batch)
2017-05-08 18:27:44.200028: step 1460, loss = 1.48 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:27:45.484059: step 1470, loss = 1.30 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:27:46.802733: step 1480, loss = 1.42 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:27:48.091693: step 1490, loss = 1.35 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:27:49.504646: step 1500, loss = 1.31 (905.9 examples/sec; 0.141 sec/batch)
2017-05-08 18:27:50.702901: step 1510, loss = 1.19 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-08 18:27:51.990682: step 1520, loss = 1.59 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:27:53.270809: step 1530, loss = 1.29 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:27:54.575868: step 1540, loss = 1.30 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:27:55.890708: step 1550, loss = 1.56 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:27:57.207061: step 1560, loss = 1.37 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:27:58.539997: step 1570, loss = 1.40 (960.3 examples/sec; 0.133 sec/batch)
2017-05-08 18:27:59.849776: step 1580, loss = 1.20 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:28:01.169347: step 1590, loss = 1.48 (970.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:28:02.582431: step 1600, loss = 1.21 (905.8 examples/sec; 0.141 sec/batch)
2017-05-08 18:28:03.767946: step 1610, loss = 1.38 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-08 18:28:05.046760: step 1620, loss = 1.31 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:06.369110: step 1630, loss = 1.35 (968.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:28:07.652752: step 1640, loss = 1.29 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:08.937240: step 1650, loss = 1.33 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:10.258662: step 1660, loss = 1.10 (968.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:28:11.561728: step 1670, loss = 1.15 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:28:12.864024: step 1680, loss = 1.45 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:28:14.133615: step 1690, loss = 1.30 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:28:15.521010: step 1700, loss = 1.17 (922.6 examples/sec; 0.139 sec/batch)
2017-05-08 18:28:16.759044: step 1710, loss = 1.37 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-08 18:28:18.040413: step 1720, loss = 1.33 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:19.361409: step 1730, loss = 1.33 (969.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:28:20.660198: step 1740, loss = 1.45 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:28:21.944979: step 1750, loss = 1.27 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:23.251186: step 1760, loss = 1.44 (979.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:28:24.580756: step 1770, loss = 1.44 (962.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:28:25.870905: step 1780, loss = 1.29 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:28:27.174207: step 1790, loss = 1.46 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:28:28.545992: step 1800, loss = 1.30 (933.1 examples/sec; 0.137 sec/batch)
2017-05-08 18:28:29.752108: step 1810, loss = 1.27 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-08 18:28:31.036551: step 1820, loss = 1.26 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:32.375787: step 1830, loss = 1.40 (955.8 examples/sec; 0.134 sec/batch)
2017-05-08 18:28:33.667689: step 1840, loss = 1.36 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:28:34.962427: step 1850, loss = 1.29 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:28:36.269733: step 1860, loss = 1.05 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:28:37.575241: step 1870, loss = 1.22 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:28:38.857329: step 1880, loss = 1.45 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:40.147439: step 1890, loss = 1.07 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:28:41.551308: step 1900, loss = 1.09 (911.8 examples/sec; 0.140 sec/batch)
2017-05-08 18:28:42.777573: step 1910, loss = 1.26 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-08 18:28:44.076470: step 1920, loss = 1.36 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:28:45.372775: step 1930, loss = 1.00 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:28:46.660065: step 1940, loss = 1.26 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:28:47.959989: step 1950, loss = 1.21 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:28:49.245300: step 1960, loss = 1.20 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:28:50.525375: step 1970, loss = 1.01 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:51.853826: step 1980, loss = 1.03 (963.5 examples/sec; 0.133 sec/batch)
2017-05-08 18:28:53.163814: step 1990, loss = 1.12 (977.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:28:54.542100: step 2000, loss = 1.14 (928.7 examples/sec; 0.138 sec/batch)
2017-05-08 18:28:55.740986: step 2010, loss = 1.09 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-08 18:28:57.037106: step 2020, loss = 1.19 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:28:58.322880: step 2030, loss = 1.31 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:28:59.610348: step 2040, loss = 1.08 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:29:00.898881: step 2050, loss = 1.06 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:29:02.187744: step 2060, loss = 1.46 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:29:03.531523: step 2070, loss = 1.29 (952.5 examples/sec; 0.134 sec/batch)
2017-05-08 18:29:04.830373: step 2080, loss = 1.16 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:29:06.138085: step 2090, loss = 1.14 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:29:07.529157: step 2100, loss = 1.16 (920.2 examples/sec; 0.139 sec/batch)
2017-05-08 18:29:08.773849: step 2110, loss = 1.31 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-08 18:29:10.074087: step 2120, loss = 1.20 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:29:11.351399: step 2130, loss = 1.12 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:29:12.654121: step 2140, loss = 0.99 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:29:13.976059: step 2150, loss = 1.20 (968.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:29:15.280492: step 2160, loss = 1.16 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:29:16.573968: step 2170, loss = 1.12 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:29:17.892753: step 2180, loss = 1.13 (970.6 examples/sec; 0.132 sec/batch)
2017-05-08 18:29:19.193778: step 2190, loss = 1.06 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:29:20.571999: step 2200, loss = 1.08 (928.7 examples/sec; 0.138 sec/batch)
2017-05-08 18:29:21.762481: step 2210, loss = 1.00 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-08 18:29:23.045544: step 2220, loss = 1.09 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:29:24.319187: step 2230, loss = 1.07 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:29:25.595196: step 2240, loss = 1.20 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:29:26.897617: step 2250, loss = 1.14 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:29:28.221378: step 2260, loss = 1.24 (966.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:29:29.539543: step 2270, loss = 1.09 (971.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:29:30.843945: step 2280, loss = 1.06 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:29:32.154468: step 2290, loss = 1.09 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:29:33.546182: step 2300, loss = 1.01 (919.7 examples/sec; 0.139 sec/batch)
2017-05-08 18:29:34.738735: step 2310, loss = 1.19 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-08 18:29:36.040562: step 2320, loss = 1.27 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:29:37.327210: step 2330, loss = 1.01 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:29:38.656556: step 2340, loss = 1.26 (962.9 examples/sec; 0.133 sec/batch)
2017-05-08 18:29:39.970124: step 2350, loss = 1.11 (974.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:29:41.294648: step 2360, loss = 1.17 (966.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:29:42.594809: step 2370, loss = 1.40 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:29:43.879037: step 2380, loss = 1.27 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:29:45.194922: step 2390, loss = 1.10 (972.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:29:46.586529: step 2400, loss = 1.04 (919.8 examples/sec; 0.139 sec/batch)
2017-05-08 18:29:47.763035: step 2410, loss = 1.22 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-08 18:29:49.055179: step 2420, loss = 1.02 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:29:50.339075: step 2430, loss = 1.18 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:29:51.600995: step 2440, loss = 1.19 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:29:52.919213: step 2450, loss = 1.08 (970.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:29:54.223894: step 2460, loss = 1.34 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:29:55.522934: step 2470, loss = 1.07 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:29:56.785036: step 2480, loss = 1.12 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:29:58.080534: step 2490, loss = 0.93 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:29:59.482015: step 2500, loss = 1.12 (913.3 examples/sec; 0.140 sec/batch)
2017-05-08 18:30:00.670962: step 2510, loss = 1.24 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-08 18:30:01.966825: step 2520, loss = 1.16 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:30:03.294102: step 2530, loss = 1.32 (964.4 examples/sec; 0.133 sec/batch)
2017-05-08 18:30:04.621822: step 2540, loss = 0.95 (964.1 examples/sec; 0.133 sec/batch)
2017-05-08 18:30:05.952758: step 2550, loss = 1.02 (961.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:30:07.282355: step 2560, loss = 1.05 (962.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:30:08.613155: step 2570, loss = 1.00 (961.8 examples/sec; 0.133 sec/batch)
2017-05-08 18:30:09.895218: step 2580, loss = 0.91 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:30:11.180248: step 2590, loss = 1.03 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:12.567905: step 2600, loss = 1.18 (922.4 examples/sec; 0.139 sec/batch)
2017-05-08 18:30:13.764525: step 2610, loss = 1.17 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-08 18:30:15.058774: step 2620, loss = 1.15 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:16.328427: step 2630, loss = 1.01 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:30:17.627311: step 2640, loss = 0.96 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:30:18.912524: step 2650, loss = 0.95 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:20.224643: step 2660, loss = 1.13 (975.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:30:21.525148: step 2670, loss = 1.18 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:30:22.805130: step 2680, loss = 0.87 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:30:24.085187: step 2690, loss = 1.17 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:30:25.474986: step 2700, loss = 1.10 (921.0 examples/sec; 0.139 sec/batch)
2017-05-08 18:30:26.671879: step 2710, loss = 1.15 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-08 18:30:27.937565: step 2720, loss = 1.02 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:30:29.244635: step 2730, loss = 1.05 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:30:30.535719: step 2740, loss = 0.97 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:31.828701: step 2750, loss = 1.02 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:33.159385: step 2760, loss = 1.05 (961.9 examples/sec; 0.133 sec/batch)
2017-05-08 18:30:34.467732: step 2770, loss = 1.05 (978.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:30:35.759014: step 2780, loss = 1.23 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:37.058663: step 2790, loss = 1.03 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:30:38.446213: step 2800, loss = 1.16 (922.5 examples/sec; 0.139 sec/batch)
2017-05-08 18:30:39.649523: step 2810, loss = 1.09 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-08 18:30:40.963980: step 2820, loss = 1.04 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:30:42.257878: step 2830, loss = 0.90 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:43.546578: step 2840, loss = 0.92 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:44.853645: step 2850, loss = 1.10 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:30:46.170518: step 2860, loss = 1.34 (971.8 examples/sec; 0.132 sec/batch)
2017-05-08 18:30:47.458129: step 2870, loss = 1.09 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:48.773122: step 2880, loss = 1.14 (973.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:30:50.098705: step 2890, loss = 1.17 (965.6 examples/sec; 0.133 sec/batch)
2017-05-08 18:30:51.494749: step 2900, loss = 1.01 (916.9 examples/sec; 0.140 sec/batch)
2017-05-08 18:30:52.776905: step 2910, loss = 0.95 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:30:54.037130: step 2920, loss = 1.13 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:30:55.340597: step 2930, loss = 1.13 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:30:56.631699: step 2940, loss = 0.84 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:57.925131: step 2950, loss = 1.15 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:59.248914: step 2960, loss = 1.15 (966.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:31:00.555025: step 2970, loss = 0.90 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:31:01.837726: step 2980, loss = 0.91 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:03.100147: step 2990, loss = 1.05 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:31:04.499374: step 3000, loss = 1.11 (914.8 examples/sec; 0.140 sec/batch)
2017-05-08 18:31:05.697397: step 3010, loss = 1.19 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-08 18:31:06.978626: step 3020, loss = 0.90 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:08.245324: step 3030, loss = 1.00 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:31:09.541003: step 3040, loss = 1.20 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:31:10.834693: step 3050, loss = 0.94 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:31:12.113907: step 3060, loss = 1.14 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:13.404755: step 3070, loss = 1.04 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:31:14.693414: step 3080, loss = 0.97 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:31:15.987514: step 3090, loss = 1.04 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:31:17.403043: step 3100, loss = 0.87 (904.3 examples/sec; 0.142 sec/batch)
2017-05-08 18:31:18.638559: step 3110, loss = 1.26 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-08 18:31:19.911234: step 3120, loss = 1.08 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:31:21.205063: step 3130, loss = 0.87 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:31:22.512763: step 3140, loss = 0.89 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:31:23.802418: step 3150, loss = 1.14 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:31:25.083410: step 3160, loss = 1.11 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:26.375473: step 3170, loss = 1.16 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:31:27.649159: step 3180, loss = 0.99 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:31:28.989669: step 3190, loss = 1.12 (954.9 examples/sec; 0.134 sec/batch)
2017-05-08 18:31:30.390686: step 3200, loss = 0.93 (913.6 examples/sec; 0.140 sec/batch)
2017-05-08 18:31:31.629914: step 3210, loss = 0.87 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-08 18:31:32.945533: step 3220, loss = 1.01 (972.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:31:34.271120: step 3230, loss = 1.16 (965.6 examples/sec; 0.133 sec/batch)
2017-05-08 18:31:35.577257: step 3240, loss = 0.96 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:31:36.904265: step 3250, loss = 1.00 (964.6 examples/sec; 0.133 sec/batch)
2017-05-08 18:31:38.248741: step 3260, loss = 0.98 (952.0 examples/sec; 0.134 sec/batch)
2017-05-08 18:31:39.549635: step 3270, loss = 1.02 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:31:40.823254: step 3280, loss = 0.89 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:31:42.115159: step 3290, loss = 0.90 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:31:43.493570: step 3300, loss = 1.18 (928.6 examples/sec; 0.138 sec/batch)
2017-05-08 18:31:44.699282: step 3310, loss = 0.99 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-08 18:31:45.990585: step 3320, loss = 1.23 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:31:47.299917: step 3330, loss = 0.88 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:31:48.610569: step 3340, loss = 1.10 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:31:49.910885: step 3350, loss = 1.03 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:31:51.233101: step 3360, loss = 1.06 (968.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:31:52.538568: step 3370, loss = 0.96 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:31:53.841864: step 3380, loss = 0.93 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:31:55.122624: step 3390, loss = 1.00 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:56.510055: step 3400, loss = 1.07 (922.6 examples/sec; 0.139 sec/batch)
2017-05-08 18:31:57.747175: step 3410, loss = 0.98 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-08 18:31:59.092663: step 3420, loss = 0.99 (951.3 examples/sec; 0.135 sec/batch)
2017-05-08 18:32:00.360610: step 3430, loss = 1.03 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:32:01.659676: step 3440, loss = 1.02 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:32:02.953910: step 3450, loss = 1.12 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:32:04.232001: step 3460, loss = 1.05 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:32:05.528324: step 3470, loss = 1.04 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:32:06.834051: step 3480, loss = 1.03 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:32:08.129591: step 3490, loss = 1.17 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:32:09.551550: step 3500, loss = 0.95 (900.2 examples/sec; 0.142 sec/batch)
2017-05-08 18:32:10.745136: step 3510, loss = 0.88 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-08 18:32:12.019972: step 3520, loss = 1.12 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:32:13.317828: step 3530, loss = 1.04 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:32:14.593854: step 3540, loss = 0.97 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:32:15.908941: step 3550, loss = 1.07 (973.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:32:17.221279: step 3560, loss = 0.92 (975.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:32:18.545128: step 3570, loss = 1.23 (966.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:32:19.847604: step 3580, loss = 0.96 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:32:21.127162: step 3590, loss = 0.98 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:32:22.551302: step 3600, loss = 1.38 (898.8 examples/sec; 0.142 sec/batch)
2017-05-08 18:32:23.777384: step 3610, loss = 0.97 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-08 18:32:25.064845: step 3620, loss = 0.90 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:32:26.353392: step 3630, loss = 0.84 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:32:27.667254: step 3640, loss = 0.89 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:32:28.985922: step 3650, loss = 1.06 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:32:30.287116: step 3660, loss = 0.91 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:32:31.579114: step 3670, loss = 1.11 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:32:32.876089: step 3680, loss = 1.07 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:32:34.159382: step 3690, loss = 0.80 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:32:35.536532: step 3700, loss = 1.09 (929.5 examples/sec; 0.138 sec/batch)
2017-05-08 18:32:36.758501: step 3710, loss = 1.03 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-08 18:32:38.075517: step 3720, loss = 1.06 (971.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:32:39.371051: step 3730, loss = 1.13 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:32:40.647437: step 3740, loss = 0.97 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:32:41.952781: step 3750, loss = 1.04 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:32:43.274145: step 3760, loss = 1.22 (968.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:32:44.588036: step 3770, loss = 1.09 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:32:45.895971: step 3780, loss = 0.80 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:32:47.189027: step 3790, loss = 1.31 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:32:48.589962: step 3800, loss = 0.98 (913.7 examples/sec; 0.140 sec/batch)
2017-05-08 18:32:49.801038: step 3810, loss = 0.94 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-08 18:32:51.109879: step 3820, loss = 0.88 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:32:52.430657: step 3830, loss = 1.22 (969.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:32:53.714346: step 3840, loss = 0.89 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:32:55.007584: step 3850, loss = 1.10 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:32:56.293649: step 3860, loss = 1.08 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:32:57.601738: step 3870, loss = 1.29 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:32:58.901770: step 3880, loss = 0.86 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:33:00.201401: step 3890, loss = 1.00 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:33:01.615483: step 3900, loss = 1.05 (905.2 examples/sec; 0.141 sec/batch)
2017-05-08 18:33:02.822608: step 3910, loss = 1.05 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-08 18:33:04.128505: step 3920, loss = 0.94 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:33:05.405874: step 3930, loss = 0.98 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:33:06.720819: step 3940, loss = 1.03 (973.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:33:08.046599: step 3950, loss = 1.15 (965.5 examples/sec; 0.133 sec/batch)
2017-05-08 18:33:09.377129: step 3960, loss = 0.98 (962.0 examples/sec; 0.133 sec/batch)
2017-05-08 18:33:10.671364: step 3970, loss = 0.93 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:33:12.005017: step 3980, loss = 1.04 (959.8 examples/sec; 0.133 sec/batch)
2017-05-08 18:33:13.294369: step 3990, loss = 0.90 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:33:14.701061: step 4000, loss = 1.05 (909.9 examples/sec; 0.141 sec/batch)
2017-05-08 18:33:15.918000: step 4010, loss = 0.94 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-08 18:33:17.230434: step 4020, loss = 0.98 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:33:18.556365: step 4030, loss = 0.85 (965.4 examples/sec; 0.133 sec/batch)
2017-05-08 18:33:19.835809: step 4040, loss = 1.08 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:33:21.177176: step 4050, loss = 0.98 (954.2 examples/sec; 0.134 sec/batch)
2017-05-08 18:33:22.445538: step 4060, loss = 1.03 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:33:23.728381: step 4070, loss = 1.21 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:33:25.056045: step 4080, loss = 0.95 (964.1 examples/sec; 0.133 sec/batch)
2017-05-08 18:33:26.376520: step 4090, loss = 1.14 (969.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:33:27.760175: step 4100, loss = 0.80 (925.1 examples/sec; 0.138 sec/batch)
2017-05-08 18:33:28.934555: step 4110, loss = 0.76 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-08 18:33:30.218334: step 4120, loss = 0.93 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:33:31.518936: step 4130, loss = 0.99 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:33:32.847282: step 4140, loss = 1.03 (963.6 examples/sec; 0.133 sec/batch)
2017-05-08 18:33:34.167290: step 4150, loss = 0.84 (969.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:33:35.447065: step 4160, loss = 0.94 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:33:36.724944: step 4170, loss = 1.00 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:33:38.007188: step 4180, loss = 0.95 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:33:39.294401: step 4190, loss = 1.01 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:33:40.689344: step 4200, loss = 1.19 (917.6 examples/sec; 0.139 sec/batch)
2017-05-08 18:33:41.922382: step 4210, loss = 1.03 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-08 18:33:43.239917: step 4220, loss = 0.91 (971.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:33:44.513639: step 4230, loss = 0.87 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:33:45.828224: step 4240, loss = 1.00 (973.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:33:47.145321: step 4250, loss = 1.20 (971.8 examples/sec; 0.132 sec/batch)
2017-05-08 18:33:48.468725: step 4260, loss = 0.86 (967.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:33:49.756988: step 4270, loss = 1.15 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:33:51.041512: step 4280, loss = 1.03 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:33:52.328554: step 4290, loss = 0.92 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:33:53.727724: step 4300, loss = 0.89 (914.8 examples/sec; 0.140 sec/batch)
2017-05-08 18:33:54.940921: step 4310, loss = 1.14 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-08 18:33:56.316641: step 4320, loss = 0.95 (930.4 examples/sec; 0.138 sec/batch)
2017-05-08 18:33:57.566291: step 4330, loss = 0.94 (1024.3 examples/sec; 0.125 sec/batch)
2017-05-08 18:33:58.875543: step 4340, loss = 1.05 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:34:00.142856: step 4350, loss = 1.02 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:34:01.463203: step 4360, loss = 0.97 (969.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:34:02.768550: step 4370, loss = 0.94 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:34:04.093284: step 4380, loss = 0.88 (966.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:34:05.398332: step 4390, loss = 1.04 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:34:06.789543: step 4400, loss = 0.85 (920.1 examples/sec; 0.139 sec/batch)
2017-05-08 18:34:08.004845: step 4410, loss = 0.99 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-08 18:34:09.312045: step 4420, loss = 1.09 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:34:10.645696: step 4430, loss = 0.92 (959.8 examples/sec; 0.133 sec/batch)
2017-05-08 18:34:11.938787: step 4440, loss = 1.03 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:34:13.225971: step 4450, loss = 0.94 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:34:14.569617: step 4460, loss = 1.07 (952.6 examples/sec; 0.134 sec/batch)
2017-05-08 18:34:15.868303: step 4470, loss = 1.00 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:34:17.156667: step 4480, loss = 1.01 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:34:18.460069: step 4490, loss = 0.84 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:34:19.849561: step 4500, loss = 0.99 (921.2 examples/sec; 0.139 sec/batch)
2017-05-08 18:34:21.122775: step 4510, loss = 1.00 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:34:22.355690: step 4520, loss = 0.79 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-08 18:34:23.631257: step 4530, loss = 0.89 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:34:24.950495: step 4540, loss = 0.80 (970.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:34:26.251731: step 4550, loss = 1.05 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:34:27.588939: step 4560, loss = 0.87 (957.2 examples/sec; 0.134 sec/batch)
2017-05-08 18:34:28.921570: step 4570, loss = 0.91 (960.5 examples/sec; 0.133 sec/batch)
2017-05-08 18:34:30.218392: step 4580, loss = 1.04 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:34:31.517205: step 4590, loss = 0.91 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:34:32.891475: step 4600, loss = 0.96 (931.4 examples/sec; 0.137 sec/batch)
2017-05-08 18:34:34.071902: step 4610, loss = 0.93 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-08 18:34:35.371432: step 4620, loss = 0.89 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:34:36.650874: step 4630, loss = 0.96 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:34:37.951998: step 4640, loss = 0.89 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:34:39.227169: step 4650, loss = 0.94 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:34:40.499936: step 4660, loss = 1.01 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:34:41.820854: step 4670, loss = 0.99 (969.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:34:43.101972: step 4680, loss = 0.89 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:34:44.398643: step 4690, loss = 0.85 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:34:45.792854: step 4700, loss = 1.03 (918.1 examples/sec; 0.139 sec/batch)
2017-05-08 18:34:47.005716: step 4710, loss = 1.01 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-08 18:34:48.330812: step 4720, loss = 0.89 (966.0 examples/sec; 0.133 sec/batch)
2017-05-08 18:34:49.629540: step 4730, loss = 0.71 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:34:50.913070: step 4740, loss = 1.09 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:34:52.187353: step 4750, loss = 1.04 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:34:53.501595: step 4760, loss = 1.09 (973.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:34:54.825077: step 4770, loss = 0.99 (967.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:34:56.108607: step 4780, loss = 0.94 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:34:57.391221: step 4790, loss = 0.76 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:34:58.787650: step 4800, loss = 0.92 (916.6 examples/sec; 0.140 sec/batch)
2017-05-08 18:34:59.969984: step 4810, loss = 0.89 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-08 18:35:01.282911: step 4820, loss = 1.04 (974.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:35:02.643365: step 4830, loss = 0.94 (940.9 examples/sec; 0.136 sec/batch)
2017-05-08 18:35:03.954361: step 4840, loss = 0.99 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:35:05.248943: step 4850, loss = 0.79 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:35:06.511411: step 4860, loss = 0.88 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:35:07.816021: step 4870, loss = 0.97 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:35:09.139516: step 4880, loss = 0.93 (967.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:35:10.440871: step 4890, loss = 1.09 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:35:11.836251: step 4900, loss = 0.92 (917.3 examples/sec; 0.140 sec/batch)
2017-05-08 18:35:13.058047: step 4910, loss = 0.96 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-08 18:35:14.386513: step 4920, loss = 0.93 (963.5 examples/sec; 0.133 sec/batch)
2017-05-08 18:35:15.681862: step 4930, loss = 1.03 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:35:17.009358: step 4940, loss = 0.98 (964.2 examples/sec; 0.133 sec/batch)
2017-05-08 18:35:18.324818: step 4950, loss = 1.06 (973.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:35:19.647023: step 4960, loss = 0.99 (968.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:35:20.907845: step 4970, loss = 1.19 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:35:22.194232: step 4980, loss = 0.80 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:35:23.461760: step 4990, loss = 0.92 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:35:24.837571: step 5000, loss = 1.16 (930.4 examples/sec; 0.138 sec/batch)
2017-05-08 18:35:26.046857: step 5010, loss = 0.88 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-08 18:35:27.346294: step 5020, loss = 1.17 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:35:28.667348: step 5030, loss = 0.96 (968.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:35:29.972975: step 5040, loss = 1.04 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:35:31.279271: step 5050, loss = 0.99 (979.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:35:32.606528: step 5060, loss = 0.97 (964.4 examples/sec; 0.133 sec/batch)
2017-05-08 18:35:33.895703: step 5070, loss = 0.90 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:35:35.189664: step 5080, loss = 0.94 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:35:36.488668: step 5090, loss = 1.05 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:35:37.903564: step 5100, loss = 0.81 (904.7 examples/sec; 0.141 sec/batch)
2017-05-08 18:35:39.131333: step 5110, loss = 0.90 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-08 18:35:40.439383: step 5120, loss = 1.01 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:35:41.731338: step 5130, loss = 0.92 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:35:42.998400: step 5140, loss = 1.11 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:35:44.311332: step 5150, loss = 1.02 (974.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:35:45.582911: step 5160, loss = 0.88 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:35:46.903397: step 5170, loss = 1.01 (969.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:35:48.203138: step 5180, loss = 0.86 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:35:49.515739: step 5190, loss = 0.92 (975.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:35:50.918394: step 5200, loss = 1.09 (912.6 examples/sec; 0.140 sec/batch)
2017-05-08 18:35:52.132106: step 5210, loss = 0.91 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-08 18:35:53.436329: step 5220, loss = 1.04 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:35:54.721137: step 5230, loss = 1.04 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:35:56.028378: step 5240, loss = 0.83 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:35:57.327065: step 5250, loss = 1.10 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:35:58.614339: step 5260, loss = 0.89 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:35:59.931532: step 5270, loss = 0.98 (971.8 examples/sec; 0.132 sec/batch)
2017-05-08 18:36:01.205561: step 5280, loss = 1.17 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:36:02.508310: step 5290, loss = 1.05 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:36:03.917868: step 5300, loss = 1.02 (908.1 examples/sec; 0.141 sec/batch)
2017-05-08 18:36:05.126889: step 5310, loss = 0.99 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-08 18:36:06.434807: step 5320, loss = 1.17 (978.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:36:07.729427: step 5330, loss = 1.04 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:36:09.016827: step 5340, loss = 1.17 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:36:10.299069: step 5350, loss = 0.86 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:36:11.601186: step 5360, loss = 1.01 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:36:12.937159: step 5370, loss = 0.83 (958.1 examples/sec; 0.134 sec/batch)
2017-05-08 18:36:14.254262: step 5380, loss = 1.00 (971.8 examples/sec; 0.132 sec/batch)
2017-05-08 18:36:15.536574: step 5390, loss = 1.00 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:36:16.935063: step 5400, loss = 1.15 (915.3 examples/sec; 0.140 sec/batch)
2017-05-08 18:36:18.119826: step 5410, loss = 0.79 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-08 18:36:19.411871: step 5420, loss = 0.86 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:36:20.689734: step 5430, loss = 0.93 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:36:21.970193: step 5440, loss = 0.88 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:36:23.271876: step 5450, loss = 0.90 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:36:24.593285: step 5460, loss = 1.11 (968.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:36:25.872718: step 5470, loss = 0.99 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:36:27.163592: step 5480, loss = 0.90 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:36:28.448941: step 5490, loss = 0.90 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:36:29.868235: step 5500, loss = 1.17 (901.9 examples/sec; 0.142 sec/batch)
2017-05-08 18:36:31.063632: step 5510, loss = 0.88 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-08 18:36:32.395349: step 5520, loss = 1.03 (961.2 examples/sec; 0.133 sec/batch)
2017-05-08 18:36:33.672991: step 5530, loss = 0.90 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:36:34.945180: step 5540, loss = 1.10 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:36:36.243965: step 5550, loss = 0.90 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:36:37.563420: step 5560, loss = 0.86 (970.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:36:38.890167: step 5570, loss = 0.77 (964.8 examples/sec; 0.133 sec/batch)
2017-05-08 18:36:40.194340: step 5580, loss = 0.85 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:36:41.491617: step 5590, loss = 0.87 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:36:42.900391: step 5600, loss = 1.02 (908.6 examples/sec; 0.141 sec/batch)
2017-05-08 18:36:44.098894: step 5610, loss = 0.86 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-08 18:36:45.425376: step 5620, loss = 0.82 (965.0 examples/sec; 0.133 sec/batch)
2017-05-08 18:36:46.730891: step 5630, loss = 1.00 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:36:48.020713: step 5640, loss = 0.77 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:36:49.316865: step 5650, loss = 0.98 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:36:50.666482: step 5660, loss = 0.74 (948.4 examples/sec; 0.135 sec/batch)
2017-05-08 18:36:51.960697: step 5670, loss = 0.90 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:36:53.247206: step 5680, loss = 0.90 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:36:54.552533: step 5690, loss = 1.16 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:36:55.929434: step 5700, loss = 0.99 (929.6 examples/sec; 0.138 sec/batch)
2017-05-08 18:36:57.148783: step 5710, loss = 0.90 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-08 18:36:58.484246: step 5720, loss = 0.68 (958.5 examples/sec; 0.134 sec/batch)
2017-05-08 18:36:59.794021: step 5730, loss = 0.87 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:37:01.085588: step 5740, loss = 0.88 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:37:02.371915: step 5750, loss = 1.04 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:37:03.648622: step 5760, loss = 1.05 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:04.915255: step 5770, loss = 0.78 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:37:06.207318: step 5780, loss = 0.99 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:37:07.499048: step 5790, loss = 0.91 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:37:08.874322: step 5800, loss = 0.91 (930.7 examples/sec; 0.138 sec/batch)
2017-05-08 18:37:10.086567: step 5810, loss = 0.95 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-08 18:37:11.403331: step 5820, loss = 0.86 (972.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:37:12.728226: step 5830, loss = 0.90 (966.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:37:14.018299: step 5840, loss = 1.01 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:37:15.310528: step 5850, loss = 1.06 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:37:16.589962: step 5860, loss = 1.03 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:17.884951: step 5870, loss = 1.04 (988.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:37:19.154438: step 5880, loss = 0.91 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:37:20.430846: step 5890, loss = 0.87 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:21.809961: step 5900, loss = 1.02 (928.1 examples/sec; 0.138 sec/batch)
2017-05-08 18:37:23.016611: step 5910, loss = 0.87 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-08 18:37:24.325373: step 5920, loss = 1.06 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:37:25.639010: step 5930, loss = 0.76 (974.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:37:26.984619: step 5940, loss = 0.94 (951.2 examples/sec; 0.135 sec/batch)
2017-05-08 18:37:28.311179: step 5950, loss = 0.84 (964.9 examples/sec; 0.133 sec/batch)
2017-05-08 18:37:29.615016: step 5960, loss = 0.93 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:37:30.897231: step 5970, loss = 0.98 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:32.157613: step 5980, loss = 0.97 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:37:33.438083: step 5990, loss = 0.80 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:34.828929: step 6000, loss = 0.95 (920.3 examples/sec; 0.139 sec/batch)
2017-05-08 18:37:36.031915: step 6010, loss = 1.07 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-08 18:37:37.324818: step 6020, loss = 0.94 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:37:38.631256: step 6030, loss = 1.04 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:37:39.984704: step 6040, loss = 0.98 (945.7 examples/sec; 0.135 sec/batch)
2017-05-08 18:37:41.257423: step 6050, loss = 0.92 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:37:42.560991: step 6060, loss = 0.88 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:37:43.845744: step 6070, loss = 0.77 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:45.114759: step 6080, loss = 0.95 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:37:46.426436: step 6090, loss = 0.98 (975.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:37:47.817544: step 6100, loss = 0.95 (920.1 examples/sec; 0.139 sec/batch)
2017-05-08 18:37:49.062695: step 6110, loss = 0.96 (1028.0 examples/sec; 0.125 sec/batch)
2017-05-08 18:37:50.370487: step 6120, loss = 0.79 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:37:51.658700: step 6130, loss = 0.93 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:37:52.984161: step 6140, loss = 0.89 (965.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:37:54.277228: step 6150, loss = 0.98 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:37:55.616786: step 6160, loss = 0.88 (955.5 examples/sec; 0.134 sec/batch)
2017-05-08 18:37:56.901885: step 6170, loss = 0.94 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:37:58.199107: step 6180, loss = 0.90 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:37:59.484386: step 6190, loss = 0.96 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:38:00.887204: step 6200, loss = 0.87 (912.4 examples/sec; 0.140 sec/batch)
2017-05-08 18:38:02.085428: step 6210, loss = 0.91 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-08 18:38:03.405553: step 6220, loss = 0.78 (969.6 examples/sec; 0.132 sec/batch)
2017-05-08 18:38:04.718942: step 6230, loss = 0.81 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:38:06.012796: step 6240, loss = 0.94 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:38:07.291550: step 6250, loss = 0.77 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:38:08.567598: step 6260, loss = 0.99 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:38:09.842188: step 6270, loss = 0.99 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:38:11.134605: step 6280, loss = 1.00 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:38:12.437592: step 6290, loss = 0.81 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:38:13.851826: step 6300, loss = 0.83 (905.1 examples/sec; 0.141 sec/batch)
2017-05-08 18:38:15.052414: step 6310, loss = 1.06 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-08 18:38:16.325495: step 6320, loss = 1.00 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:38:17.636045: step 6330, loss = 0.75 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:38:18.942096: step 6340, loss = 0.81 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:38:20.257040: step 6350, loss = 1.24 (973.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:38:21.541426: step 6360, loss = 0.92 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:38:22.842994: step 6370, loss = 0.96 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:38:24.156083: step 6380, loss = 0.94 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:38:25.486787: step 6390, loss = 0.76 (961.9 examples/sec; 0.133 sec/batch)
2017-05-08 18:38:26.884017: step 6400, loss = 0.92 (916.1 examples/sec; 0.140 sec/batch)
2017-05-08 18:38:28.107278: step 6410, loss = 0.97 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-08 18:38:29.406391: step 6420, loss = 0.92 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:38:30.696103: step 6430, loss = 0.81 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:38:31.999177: step 6440, loss = 1.05 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:38:33.318607: step 6450, loss = 0.89 (970.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:38:34.654167: step 6460, loss = 0.89 (958.4 examples/sec; 0.134 sec/batch)
2017-05-08 18:38:35.947854: step 6470, loss = 0.99 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:38:37.255811: step 6480, loss = 1.02 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:38:38.542912: step 6490, loss = 0.95 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:38:39.935487: step 6500, loss = 0.90 (919.2 examples/sec; 0.139 sec/batch)
2017-05-08 18:38:41.142543: step 6510, loss = 0.97 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-08 18:38:42.445714: step 6520, loss = 0.92 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:38:43.742312: step 6530, loss = 0.85 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:38:45.040782: step 6540, loss = 0.80 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:38:46.319671: step 6550, loss = 1.08 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:38:47.604836: step 6560, loss = 0.86 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:38:48.906879: step 6570, loss = 1.06 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:38:50.233774: step 6580, loss = 0.91 (964.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:38:51.545945: step 6590, loss = 0.99 (975.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:38:52.924775: step 6600, loss = 1.00 (928.3 examples/sec; 0.138 sec/batch)
2017-05-08 18:38:54.125370: step 6610, loss = 0.86 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-08 18:38:55.404983: step 6620, loss = 0.99 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:38:56.702807: step 6630, loss = 0.97 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:38:57.964692: step 6640, loss = 0.99 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:38:59.253483: step 6650, loss = 0.72 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:39:00.553615: step 6660, loss = 0.95 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:39:01.840008: step 6670, loss = 1.00 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:39:03.140090: step 6680, loss = 0.80 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:39:04.419531: step 6690, loss = 0.84 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:39:05.826384: step 6700, loss = 1.03 (909.8 examples/sec; 0.141 sec/batch)
2017-05-08 18:39:07.049350: step 6710, loss = 1.02 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-08 18:39:08.373647: step 6720, loss = 0.80 (966.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:39:09.686977: step 6730, loss = 1.04 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:39:10.976273: step 6740, loss = 0.75 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:39:12.280150: step 6750, loss = 1.24 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:39:13.573425: step 6760, loss = 0.96 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:39:14.869935: step 6770, loss = 1.02 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:39:16.204027: step 6780, loss = 0.89 (959.5 examples/sec; 0.133 sec/batch)
2017-05-08 18:39:17.500207: step 6790, loss = 0.88 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:39:18.916834: step 6800, loss = 0.92 (903.6 examples/sec; 0.142 sec/batch)
2017-05-08 18:39:20.111485: step 6810, loss = 0.89 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-08 18:39:21.443091: step 6820, loss = 0.87 (961.3 examples/sec; 0.133 sec/batch)
2017-05-08 18:39:22.754496: step 6830, loss = 1.09 (976.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:39:24.084713: step 6840, loss = 0.95 (962.2 examples/sec; 0.133 sec/batch)
2017-05-08 18:39:25.416056: step 6850, loss = 0.88 (961.4 examples/sec; 0.133 sec/batch)
2017-05-08 18:39:26.703397: step 6860, loss = 0.88 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:39:28.007393: step 6870, loss = 1.00 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:39:29.356773: step 6880, loss = 1.05 (948.6 examples/sec; 0.135 sec/batch)
2017-05-08 18:39:30.672855: step 6890, loss = 1.10 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 18:39:32.076045: step 6900, loss = 0.90 (912.2 examples/sec; 0.140 sec/batch)
2017-05-08 18:39:33.292829: step 6910, loss = 0.78 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-08 18:39:34.630801: step 6920, loss = 0.83 (956.7 examples/sec; 0.134 sec/batch)
2017-05-08 18:39:35.928212: step 6930, loss = 0.84 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:39:37.218198: step 6940, loss = 0.83 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:39:38.555654: step 6950, loss = 0.93 (957.0 examples/sec; 0.134 sec/batch)
2017-05-08 18:39:39.844849: step 6960, loss = 0.95 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:39:41.146650: step 6970, loss = 1.07 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:39:42.424371: step 6980, loss = 0.81 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:39:43.706715: step 6990, loss = 0.90 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:39:45.121426: step 7000, loss = 0.84 (904.8 examples/sec; 0.141 sec/batch)
2017-05-08 18:39:46.320806: step 7010, loss = 1.00 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-08 18:39:47.606926: step 7020, loss = 0.85 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:39:48.915202: step 7030, loss = 0.88 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:39:50.202267: step 7040, loss = 1.11 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:39:51.464037: step 7050, loss = 0.96 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:39:52.718097: step 7060, loss = 0.92 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-08 18:39:54.031084: step 7070, loss = 1.12 (974.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:39:55.347114: step 7080, loss = 0.84 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 18:39:56.642031: step 7090, loss = 0.87 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:39:58.033165: step 7100, loss = 1.03 (920.1 examples/sec; 0.139 sec/batch)
2017-05-08 18:39:59.242768: step 7110, loss = 0.88 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-08 18:40:00.535701: step 7120, loss = 0.97 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:40:01.871488: step 7130, loss = 1.01 (958.2 examples/sec; 0.134 sec/batch)
2017-05-08 18:40:03.203391: step 7140, loss = 0.90 (961.0 examples/sec; 0.133 sec/batch)
2017-05-08 18:40:04.469026: step 7150, loss = 0.83 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:40:05.780481: step 7160, loss = 0.72 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:40:07.068562: step 7170, loss = 0.84 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:40:08.384401: step 7180, loss = 0.86 (972.8 examples/sec; 0.132 sec/batch)
2017-05-08 18:40:09.710426: step 7190, loss = 0.87 (965.3 examples/sec; 0.133 sec/batch)
2017-05-08 18:40:11.102996: step 7200, loss = 0.95 (919.2 examples/sec; 0.139 sec/batch)
2017-05-08 18:40:12.304391: step 7210, loss = 0.90 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-08 18:40:13.616334: step 7220, loss = 0.98 (975.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:40:14.950921: step 7230, loss = 0.94 (959.1 examples/sec; 0.133 sec/batch)
2017-05-08 18:40:16.240812: step 7240, loss = 0.97 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:40:17.519495: step 7250, loss = 1.03 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:40:18.797994: step 7260, loss = 0.98 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:40:20.080493: step 7270, loss = 0.73 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:40:21.414185: step 7280, loss = 0.98 (959.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:40:22.727156: step 7290, loss = 0.85 (974.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:40:24.141183: step 7300, loss = 0.93 (905.2 examples/sec; 0.141 sec/batch)
2017-05-08 18:40:25.322632: step 7310, loss = 0.91 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-08 18:40:26.629805: step 7320, loss = 0.96 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:40:27.899080: step 7330, loss = 0.94 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:40:29.201796: step 7340, loss = 0.87 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:40:30.531058: step 7350, loss = 0.85 (962.9 examples/sec; 0.133 sec/batch)
2017-05-08 18:40:31.842310: step 7360, loss = 0.82 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:40:33.144614: step 7370, loss = 1.20 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:40:34.472153: step 7380, loss = 0.92 (964.2 examples/sec; 0.133 sec/batch)
2017-05-08 18:40:35.770981: step 7390, loss = 0.77 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:40:37.183741: step 7400, loss = 1.00 (906.0 examples/sec; 0.141 sec/batch)
2017-05-08 18:40:38.414649: step 7410, loss = 0.83 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-08 18:40:39.695093: step 7420, loss = 0.71 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:40:40.977231: step 7430, loss = 0.82 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:40:42.278533: step 7440, loss = 0.86 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:40:43.592039: step 7450, loss = 0.91 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:40:44.889384: step 7460, loss = 0.66 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:40:46.183837: step 7470, loss = 0.98 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:40:47.489234: step 7480, loss = 0.72 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:40:48.807624: step 7490, loss = 0.89 (970.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:40:50.198195: step 7500, loss = 0.83 (920.5 examples/sec; 0.139 sec/batch)
2017-05-08 18:40:51.417604: step 7510, loss = 0.92 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-08 18:40:52.697820: step 7520, loss = 1.05 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:40:54.022747: step 7530, loss = 0.97 (966.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:40:55.341755: step 7540, loss = 0.71 (970.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:40:56.645841: step 7550, loss = 0.94 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:40:57.957428: step 7560, loss = 0.85 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:40:59.251811: step 7570, loss = 0.93 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:41:00.544254: step 7580, loss = 0.79 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:41:01.838744: step 7590, loss = 0.88 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:41:03.232129: step 7600, loss = 0.84 (918.6 examples/sec; 0.139 sec/batch)
2017-05-08 18:41:04.413179: step 7610, loss = 1.06 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-08 18:41:05.720601: step 7620, loss = 0.90 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:41:06.999917: step 7630, loss = 0.80 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:41:08.287914: step 7640, loss = 1.05 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:41:09.601145: step 7650, loss = 0.88 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:41:10.912545: step 7660, loss = 0.76 (976.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:41:12.214701: step 7670, loss = 1.03 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:41:13.511935: step 7680, loss = 0.86 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:41:14.844877: step 7690, loss = 0.98 (960.3 examples/sec; 0.133 sec/batch)
2017-05-08 18:41:16.265629: step 7700, loss = 0.91 (900.9 examples/sec; 0.142 sec/batch)
2017-05-08 18:41:17.474744: step 7710, loss = 0.87 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-08 18:41:18.751647: step 7720, loss = 1.01 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:41:20.027110: step 7730, loss = 1.17 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:41:21.343568: step 7740, loss = 1.11 (972.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:41:22.653717: step 7750, loss = 1.00 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:41:23.933683: step 7760, loss = 0.88 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:41:25.225632: step 7770, loss = 0.84 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:41:26.487059: step 7780, loss = 0.84 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:41:27.788247: step 7790, loss = 0.90 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:41:29.161240: step 7800, loss = 0.86 (932.3 examples/sec; 0.137 sec/batch)
2017-05-08 18:41:30.495829: step 7810, loss = 1.08 (959.1 examples/sec; 0.133 sec/batch)
2017-05-08 18:41:31.688120: step 7820, loss = 0.77 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-08 18:41:32.975517: step 7830, loss = 0.80 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:41:34.281324: step 7840, loss = 0.93 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:41:35.600374: step 7850, loss = 0.93 (970.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:41:36.932719: step 7860, loss = 0.78 (960.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:41:38.237947: step 7870, loss = 0.91 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:41:39.536907: step 7880, loss = 0.89 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:41:40.870692: step 7890, loss = 0.94 (959.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:41:42.292361: step 7900, loss = 0.98 (900.4 examples/sec; 0.142 sec/batch)
2017-05-08 18:41:43.504680: step 7910, loss = 0.86 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-08 18:41:44.811564: step 7920, loss = 0.94 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:41:46.088908: step 7930, loss = 0.85 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:41:47.362203: step 7940, loss = 0.97 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:41:48.686676: step 7950, loss = 0.84 (966.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:41:50.030254: step 7960, loss = 0.81 (952.7 examples/sec; 0.134 sec/batch)
2017-05-08 18:41:51.323951: step 7970, loss = 0.91 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:41:52.624264: step 7980, loss = 0.89 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:41:53.922978: step 7990, loss = 0.89 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:41:55.318787: step 8000, loss = 0.80 (917.0 examples/sec; 0.140 sec/batch)
2017-05-08 18:41:56.555245: step 8010, loss = 0.86 (1035.2 examples/sec; 0.124 sec/batch)
2017-05-08 18:41:57.856925: step 8020, loss = 0.83 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:41:59.143257: step 8030, loss = 0.91 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:42:00.425929: step 8040, loss = 0.93 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:42:01.710155: step 8050, loss = 0.77 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:42:02.993797: step 8060, loss = 0.94 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:42:04.316327: step 8070, loss = 0.86 (967.8 examples/sec; 0.132 sec/batch)
2017-05-08 18:42:05.635055: step 8080, loss = 0.90 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:42:06.916261: step 8090, loss = 0.99 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:42:08.306251: step 8100, loss = 0.95 (920.9 examples/sec; 0.139 sec/batch)
2017-05-08 18:42:09.504899: step 8110, loss = 0.80 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-08 18:42:10.793946: step 8120, loss = 0.99 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:42:12.125674: step 8130, loss = 0.87 (961.2 examples/sec; 0.133 sec/batch)
2017-05-08 18:42:13.426169: step 8140, loss = 0.84 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:42:14.706348: step 8150, loss = 0.88 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:42:15.976655: step 8160, loss = 0.89 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:42:17.269755: step 8170, loss = 0.98 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:42:18.566340: step 8180, loss = 0.77 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:42:19.888298: step 8190, loss = 0.88 (968.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:42:21.281708: step 8200, loss = 0.89 (918.6 examples/sec; 0.139 sec/batch)
2017-05-08 18:42:22.477987: step 8210, loss = 0.75 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-08 18:42:23.738150: step 8220, loss = 0.85 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:42:25.009472: step 8230, loss = 0.89 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:42:26.310944: step 8240, loss = 0.98 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:42:27.643095: step 8250, loss = 0.81 (960.9 examples/sec; 0.133 sec/batch)
2017-05-08 18:42:29.011524: step 8260, loss = 0.99 (935.4 examples/sec; 0.137 sec/batch)
2017-05-08 18:42:30.316714: step 8270, loss = 0.97 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:42:31.636884: step 8280, loss = 0.89 (969.6 examples/sec; 0.132 sec/batch)
2017-05-08 18:42:32.935740: step 8290, loss = 0.83 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:42:34.315755: step 8300, loss = 0.99 (927.5 examples/sec; 0.138 sec/batch)
2017-05-08 18:42:35.498294: step 8310, loss = 0.74 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-08 18:42:36.786534: step 8320, loss = 0.84 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:42:38.056393: step 8330, loss = 0.82 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:42:39.350257: step 8340, loss = 0.84 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:42:40.654596: step 8350, loss = 0.85 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:42:41.953393: step 8360, loss = 1.05 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:42:43.265638: step 8370, loss = 0.94 (975.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:42:44.583238: step 8380, loss = 0.87 (971.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:42:45.893156: step 8390, loss = 1.06 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:42:47.336319: step 8400, loss = 0.94 (886.9 examples/sec; 0.144 sec/batch)
2017-05-08 18:42:48.535005: step 8410, loss = 0.83 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-08 18:42:49.833239: step 8420, loss = 1.07 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:42:51.130845: step 8430, loss = 0.86 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:42:52.455407: step 8440, loss = 0.82 (966.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:42:53.762193: step 8450, loss = 0.90 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:42:55.050635: step 8460, loss = 0.98 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:42:56.352822: step 8470, loss = 0.90 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:42:57.662573: step 8480, loss = 1.11 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:42:58.943483: step 8490, loss = 0.82 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:43:00.323316: step 8500, loss = 0.90 (927.6 examples/sec; 0.138 sec/batch)
2017-05-08 18:43:01.574219: step 8510, loss = 0.83 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-08 18:43:02.868841: step 8520, loss = 0.90 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:43:04.153932: step 8530, loss = 0.88 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:43:05.450991: step 8540, loss = 0.82 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:43:06.731925: step 8550, loss = 0.93 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:43:08.033368: step 8560, loss = 0.98 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:43:09.317897: step 8570, loss = 0.78 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:43:10.628835: step 8580, loss = 0.80 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:43:11.926618: step 8590, loss = 0.85 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:43:13.364424: step 8600, loss = 0.90 (890.2 examples/sec; 0.144 sec/batch)
2017-05-08 18:43:14.573502: step 8610, loss = 0.84 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-08 18:43:15.847213: step 8620, loss = 0.92 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:43:17.147083: step 8630, loss = 0.95 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:43:18.447337: step 8640, loss = 0.82 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:43:19.759171: step 8650, loss = 0.79 (975.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:43:21.062928: step 8660, loss = 0.88 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:43:22.345626: step 8670, loss = 0.94 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:43:23.632965: step 8680, loss = 0.89 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:43:24.909350: step 8690, loss = 0.95 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:43:26.319702: step 8700, loss = 0.82 (907.6 examples/sec; 0.141 sec/batch)
2017-05-08 18:43:27.500058: step 8710, loss = 0.95 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-08 18:43:28.796760: step 8720, loss = 0.84 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:43:30.095241: step 8730, loss = 0.93 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:43:31.405877: step 8740, loss = 0.86 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:43:32.710776: step 8750, loss = 1.08 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:43:33.991186: step 8760, loss = 1.09 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:43:35.306156: step 8770, loss = 1.09 (973.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:43:36.603150: step 8780, loss = 0.75 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:43:37.902010: step 8790, loss = 0.98 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:43:39.330605: step 8800, loss = 0.72 (896.0 examples/sec; 0.143 sec/batch)
2017-05-08 18:43:40.529826: step 8810, loss = 0.90 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-08 18:43:41.812211: step 8820, loss = 0.77 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:43:43.105103: step 8830, loss = 0.76 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:43:44.373135: step 8840, loss = 0.89 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:43:45.704846: step 8850, loss = 0.82 (961.2 examples/sec; 0.133 sec/batch)
2017-05-08 18:43:47.029245: step 8860, loss = 0.96 (966.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:43:48.321781: step 8870, loss = 1.14 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:43:49.647549: step 8880, loss = 0.86 (965.5 examples/sec; 0.133 sec/batch)
2017-05-08 18:43:50.952853: step 8890, loss = 0.81 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:43:52.322795: step 8900, loss = 0.87 (934.3 examples/sec; 0.137 sec/batch)
2017-05-08 18:43:53.511371: step 8910, loss = 0.95 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-08 18:43:54.800628: step 8920, loss = 0.85 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:43:56.092051: step 8930, loss = 0.75 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:43:57.400238: step 8940, loss = 1.02 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:43:58.666509: step 8950, loss = 0.71 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:43:59.980931: step 8960, loss = 0.83 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:44:01.295140: step 8970, loss = 0.81 (974.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:44:02.601320: step 8980, loss = 1.02 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:44:03.903919: step 8990, loss = 0.90 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:44:05.330428: step 9000, loss = 0.80 (897.3 examples/sec; 0.143 sec/batch)
2017-05-08 18:44:06.563766: step 9010, loss = 0.91 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-08 18:44:07.851334: step 9020, loss = 0.91 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:44:09.129383: step 9030, loss = 0.84 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:44:10.384124: step 9040, loss = 0.87 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-08 18:44:11.688779: step 9050, loss = 0.91 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:44:12.967900: step 9060, loss = 0.87 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:44:14.305046: step 9070, loss = 0.94 (957.3 examples/sec; 0.134 sec/batch)
2017-05-08 18:44:15.568101: step 9080, loss = 0.95 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:44:16.855853: step 9090, loss = 0.89 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:44:18.249945: step 9100, loss = 0.90 (918.2 examples/sec; 0.139 sec/batch)
2017-05-08 18:44:19.461957: step 9110, loss = 0.90 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-08 18:44:20.761089: step 9120, loss = 1.13 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:44:22.084986: step 9130, loss = 0.91 (966.8 examples/sec; 0.132 sec/batch)
2017-05-08 18:44:23.394805: step 9140, loss = 0.80 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:44:24.670884: step 9150, loss = 0.92 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:44:25.919940: step 9160, loss = 0.78 (1024.8 examples/sec; 0.125 sec/batch)
2017-05-08 18:44:27.254882: step 9170, loss = 0.99 (958.8 examples/sec; 0.133 sec/batch)
2017-05-08 18:44:28.530949: step 9180, loss = 0.81 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:44:29.857845: step 9190, loss = 0.78 (964.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:44:31.265102: step 9200, loss = 0.90 (909.6 examples/sec; 0.141 sec/batch)
2017-05-08 18:44:32.472610: step 9210, loss = 0.73 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-08 18:44:33.751545: step 9220, loss = 0.90 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:44:35.058247: step 9230, loss = 0.92 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:44:36.371111: step 9240, loss = 0.94 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:44:37.662822: step 9250, loss = 0.95 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:44:38.962517: step 9260, loss = 0.91 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:44:40.293949: step 9270, loss = 0.90 (961.4 examples/sec; 0.133 sec/batch)
2017-05-08 18:44:41.595606: step 9280, loss = 0.76 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:44:42.898577: step 9290, loss = 0.95 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:44:44.288353: step 9300, loss = 0.95 (921.0 examples/sec; 0.139 sec/batch)
2017-05-08 18:44:45.501433: step 9310, loss = 0.83 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-08 18:44:46.777863: step 9320, loss = 0.85 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:44:48.062619: step 9330, loss = 0.78 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:44:49.375428: step 9340, loss = 0.94 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:44:50.671957: step 9350, loss = 1.00 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:44:51.990369: step 9360, loss = 1.08 (970.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:44:53.316995: step 9370, loss = 0.85 (964.9 examples/sec; 0.133 sec/batch)
2017-05-08 18:44:54.605098: step 9380, loss = 0.70 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:44:55.874966: step 9390, loss = 0.92 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:44:57.256661: step 9400, loss = 0.98 (926.4 examples/sec; 0.138 sec/batch)
2017-05-08 18:44:58.465929: step 9410, loss = 0.97 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-08 18:44:59.795538: step 9420, loss = 0.89 (962.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:45:01.116281: step 9430, loss = 0.97 (969.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:45:02.442688: step 9440, loss = 0.78 (965.0 examples/sec; 0.133 sec/batch)
2017-05-08 18:45:03.735581: step 9450, loss = 0.94 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:45:05.004882: step 9460, loss = 0.83 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:45:06.313361: step 9470, loss = 0.92 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:45:07.635054: step 9480, loss = 0.94 (968.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:45:08.937902: step 9490, loss = 0.97 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:45:10.328100: step 9500, loss = 0.88 (920.7 examples/sec; 0.139 sec/batch)
2017-05-08 18:45:11.538266: step 9510, loss = 0.91 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-08 18:45:12.834417: step 9520, loss = 0.77 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:45:14.140157: step 9530, loss = 0.81 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:45:15.445523: step 9540, loss = 0.83 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:45:16.769270: step 9550, loss = 0.81 (967.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:45:18.076009: step 9560, loss = 1.01 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:45:19.375583: step 9570, loss = 0.93 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:45:20.722312: step 9580, loss = 0.79 (950.5 examples/sec; 0.135 sec/batch)
2017-05-08 18:45:22.026150: step 9590, loss = 0.70 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:45:23.423476: step 9600, loss = 0.90 (916.1 examples/sec; 0.140 sec/batch)
2017-05-08 18:45:24.607826: step 9610, loss = 0.78 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-08 18:45:25.902675: step 9620, loss = 0.83 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:45:27.210016: step 9630, loss = 0.81 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:45:28.478675: step 9640, loss = 0.61 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:45:29.775249: step 9650, loss = 0.75 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:45:31.090752: step 9660, loss = 0.88 (973.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:45:32.418965: step 9670, loss = 0.95 (963.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:45:33.707764: step 9680, loss = 1.10 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:45:35.004436: step 9690, loss = 0.98 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:45:36.381416: step 9700, loss = 0.86 (929.6 examples/sec; 0.138 sec/batch)
2017-05-08 18:45:37.580374: step 9710, loss = 0.80 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-08 18:45:38.891754: step 9720, loss = 0.94 (976.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:45:40.204522: step 9730, loss = 0.82 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:45:41.506003: step 9740, loss = 0.83 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:45:42.800627: step 9750, loss = 0.82 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:45:44.119112: step 9760, loss = 0.88 (970.8 examples/sec; 0.132 sec/batch)
2017-05-08 18:45:45.409670: step 9770, loss = 1.02 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:45:46.703116: step 9780, loss = 0.76 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:45:47.973205: step 9790, loss = 0.91 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:45:49.353948: step 9800, loss = 0.75 (927.0 examples/sec; 0.138 sec/batch)
2017-05-08 18:45:50.563667: step 9810, loss = 0.96 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-08 18:45:51.899596: step 9820, loss = 0.99 (958.1 examples/sec; 0.134 sec/batch)
2017-05-08 18:45:53.202566: step 9830, loss = 1.01 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:45:54.475834: step 9840, loss = 0.97 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:45:55.751630: step 9850, loss = 0.76 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:45:57.059340: step 9860, loss = 0.74 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:45:58.358095: step 9870, loss = 0.87 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:45:59.638794: step 9880, loss = 0.88 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:46:00.931588: step 9890, loss = 0.87 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:46:02.325951: step 9900, loss = 0.64 (918.0 examples/sec; 0.139 sec/batch)
2017-05-08 18:46:03.561473: step 9910, loss = 0.94 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-08 18:46:04.825811: step 9920, loss = 1.13 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:46:06.107014: step 9930, loss = 0.94 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:46:07.392934: step 9940, loss = 0.79 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:46:08.675635: step 9950, loss = 0.86 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:46:09.989820: step 9960, loss = 0.85 (974.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:46:11.303736: step 9970, loss = 0.79 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:46:12.586070: step 9980, loss = 0.81 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:46:13.907435: step 9990, loss = 0.77 (968.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:46:15.304409: step 10000, loss = 0.65 (916.3 examples/sec; 0.140 sec/batch)
2017-05-08 18:46:16.506949: step 10010, loss = 0.87 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-08 18:46:17.786836: step 10020, loss = 1.03 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:46:19.095416: step 10030, loss = 0.80 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:46:20.375113: step 10040, loss = 0.79 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:46:21.647700: step 10050, loss = 0.77 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:46:22.968009: step 10060, loss = 0.84 (969.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:46:24.255124: step 10070, loss = 0.90 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:46:25.590978: step 10080, loss = 0.90 (958.2 examples/sec; 0.134 sec/batch)
2017-05-08 18:46:26.885755: step 10090, loss = 0.75 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:46:28.274175: step 10100, loss = 0.83 (921.9 examples/sec; 0.139 sec/batch)
2017-05-08 18:46:29.469217: step 10110, loss = 0.86 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-08 18:46:30.819362: step 10120, loss = 1.07 (948.1 examples/sec; 0.135 sec/batch)
2017-05-08 18:46:32.151074: step 10130, loss = 0.86 (961.2 examples/sec; 0.133 sec/batch)
2017-05-08 18:46:33.440603: step 10140, loss = 0.86 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:46:34.731140: step 10150, loss = 0.82 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:46:36.049367: step 10160, loss = 0.80 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:46:37.337028: step 10170, loss = 0.85 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:46:38.671778: step 10180, loss = 0.90 (959.0 examples/sec; 0.133 sec/batch)
2017-05-08 18:46:39.949256: step 10190, loss = 0.87 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:46:41.363316: step 10200, loss = 0.77 (905.2 examples/sec; 0.141 sec/batch)
2017-05-08 18:46:42.542269: step 10210, loss = 0.86 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-08 18:46:43.847044: step 10220, loss = 0.71 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:46:45.154652: step 10230, loss = 0.87 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:46:46.473405: step 10240, loss = 0.91 (970.6 examples/sec; 0.132 sec/batch)
2017-05-08 18:46:47.785513: step 10250, loss = 0.93 (975.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:46:49.098340: step 10260, loss = 0.89 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:46:50.450598: step 10270, loss = 0.82 (946.6 examples/sec; 0.135 sec/batch)
2017-05-08 18:46:51.726737: step 10280, loss = 0.81 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:46:53.018500: step 10290, loss = 0.75 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:46:54.425827: step 10300, loss = 0.71 (909.5 examples/sec; 0.141 sec/batch)
2017-05-08 18:46:55.676334: step 10310, loss = 0.85 (1023.6 examples/sec; 0.125 sec/batch)
2017-05-08 18:46:56.974981: step 10320, loss = 0.68 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:46:58.270048: step 10330, loss = 0.78 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:46:59.533354: step 10340, loss = 0.73 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:47:00.822504: step 10350, loss = 0.93 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:47:02.086960: step 10360, loss = 0.86 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:47:03.394034: step 10370, loss = 0.92 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:47:04.669420: step 10380, loss = 0.79 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:47:05.949066: step 10390, loss = 0.77 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:47:07.336245: step 10400, loss = 0.80 (922.7 examples/sec; 0.139 sec/batch)
2017-05-08 18:47:08.551885: step 10410, loss = 0.83 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-08 18:47:09.890842: step 10420, loss = 0.73 (956.0 examples/sec; 0.134 sec/batch)
2017-05-08 18:47:11.213158: step 10430, loss = 0.79 (968.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:47:12.491845: step 10440, loss = 0.79 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:47:13.800381: step 10450, loss = 0.91 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:47:15.104598: step 10460, loss = 0.90 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:47:16.424556: step 10470, loss = 0.87 (969.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:47:17.719865: step 10480, loss = 0.86 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:47:19.007385: step 10490, loss = 0.85 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:47:20.405938: step 10500, loss = 0.90 (915.2 examples/sec; 0.140 sec/batch)
2017-05-08 18:47:21.639278: step 10510, loss = 0.79 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-08 18:47:22.992438: step 10520, loss = 0.95 (945.9 examples/sec; 0.135 sec/batch)
2017-05-08 18:47:24.310861: step 10530, loss = 0.89 (970.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:47:25.659957: step 10540, loss = 0.92 (948.8 examples/sec; 0.135 sec/batch)
2017-05-08 18:47:26.999185: step 10550, loss = 0.76 (955.8 examples/sec; 0.134 sec/batch)
2017-05-08 18:47:28.333527: step 10560, loss = 0.90 (959.3 examples/sec; 0.133 sec/batch)
2017-05-08 18:47:29.639311: step 10570, loss = 0.88 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:47:30.957199: step 10580, loss = 0.87 (971.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:47:32.274887: step 10590, loss = 0.94 (971.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:47:33.675796: step 10600, loss = 0.82 (913.7 examples/sec; 0.140 sec/batch)
2017-05-08 18:47:34.884606: step 10610, loss = 0.74 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-08 18:47:36.179500: step 10620, loss = 0.87 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:47:37.486261: step 10630, loss = 0.88 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:47:38.816089: step 10640, loss = 0.81 (962.5 examples/sec; 0.133 sec/batch)
2017-05-08 18:47:40.169724: step 10650, loss = 0.85 (945.6 examples/sec; 0.135 sec/batch)
2017-05-08 18:47:41.530713: step 10660, loss = 0.81 (940.5 examples/sec; 0.136 sec/batch)
2017-05-08 18:47:42.794191: step 10670, loss = 0.78 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 18:47:44.086336: step 10680, loss = 0.84 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:47:45.419934: step 10690, loss = 0.76 (959.8 examples/sec; 0.133 sec/batch)
2017-05-08 18:47:46.815594: step 10700, loss = 0.97 (917.1 examples/sec; 0.140 sec/batch)
2017-05-08 18:47:48.009547: step 10710, loss = 0.83 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-08 18:47:49.304853: step 10720, loss = 0.67 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:47:50.602628: step 10730, loss = 0.83 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:47:51.870430: step 10740, loss = 0.94 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:47:53.171082: step 10750, loss = 0.82 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:47:54.498149: step 10760, loss = 1.06 (964.5 examples/sec; 0.133 sec/batch)
2017-05-08 18:47:55.809320: step 10770, loss = 1.04 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:47:57.130855: step 10780, loss = 0.86 (968.6 examples/sec; 0.132 sec/batch)
2017-05-08 18:47:58.451949: step 10790, loss = 0.87 (968.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:47:59.837032: step 10800, loss = 0.76 (924.1 examples/sec; 0.139 sec/batch)
2017-05-08 18:48:01.069357: step 10810, loss = 0.92 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-08 18:48:02.392308: step 10820, loss = 1.02 (967.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:48:03.672717: step 10830, loss = 1.11 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:48:04.984987: step 10840, loss = 1.05 (975.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:48:06.289556: step 10850, loss = 0.85 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:48:07.619922: step 10860, loss = 0.90 (962.1 examples/sec; 0.133 sec/batch)
2017-05-08 18:48:08.913534: step 10870, loss = 0.76 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:48:10.191034: step 10880, loss = 0.81 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:48:11.484143: step 10890, loss = 0.85 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:48:12.890006: step 10900, loss = 0.89 (910.5 examples/sec; 0.141 sec/batch)
2017-05-08 18:48:14.103499: step 10910, loss = 1.00 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-08 18:48:15.395405: step 10920, loss = 0.88 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:48:16.684308: step 10930, loss = 0.83 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:48:18.022339: step 10940, loss = 0.82 (956.6 examples/sec; 0.134 sec/batch)
2017-05-08 18:48:19.314657: step 10950, loss = 0.85 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:48:20.620549: step 10960, loss = 0.76 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:48:21.919094: step 10970, loss = 0.89 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:48:23.241026: step 10980, loss = 0.88 (968.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:48:24.552199: step 10990, loss = 0.97 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:48:25.939255: step 11000, loss = 0.81 (922.8 examples/sec; 0.139 sec/batch)
2017-05-08 18:48:27.174689: step 11010, loss = 0.87 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-08 18:48:28.477732: step 11020, loss = 0.81 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:48:29.753802: step 11030, loss = 0.89 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:48:31.070193: step 11040, loss = 0.65 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:48:32.388705: step 11050, loss = 0.77 (970.8 examples/sec; 0.132 sec/batch)
2017-05-08 18:48:33.700708: step 11060, loss = 0.86 (975.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:48:35.000049: step 11070, loss = 0.92 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:48:36.299513: step 11080, loss = 0.86 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:48:37.622028: step 11090, loss = 0.83 (967.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:48:39.047729: step 11100, loss = 0.87 (897.8 examples/sec; 0.143 sec/batch)
2017-05-08 18:48:40.329991: step 11110, loss = 1.04 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:48:41.565252: step 11120, loss = 0.87 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-08 18:48:42.864000: step 11130, loss = 0.84 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:48:44.186515: step 11140, loss = 0.75 (967.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:48:45.512753: step 11150, loss = 0.87 (965.1 examples/sec; 0.133 sec/batch)
2017-05-08 18:48:46.792653: step 11160, loss = 0.94 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:48:48.117831: step 11170, loss = 1.04 (965.9 examples/sec; 0.133 sec/batch)
2017-05-08 18:48:49.403591: step 11180, loss = 0.88 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:48:50.725845: step 11190, loss = 0.84 (968.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:48:52.101361: step 11200, loss = 1.07 (930.6 examples/sec; 0.138 sec/batch)
2017-05-08 18:48:53.329792: step 11210, loss = 0.92 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-08 18:48:54.634738: step 11220, loss = 0.88 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:48:55.925698: step 11230, loss = 0.96 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:48:57.216157: step 11240, loss = 1.03 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:48:58.529513: step 11250, loss = 0.92 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:48:59.851092: step 11260, loss = 0.90 (968.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:49:01.141618: step 11270, loss = 0.81 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:49:02.470308: step 11280, loss = 0.93 (963.4 examples/sec; 0.133 sec/batch)
2017-05-08 18:49:03.763859: step 11290, loss = 0.83 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:49:05.176815: step 11300, loss = 0.91 (905.9 examples/sec; 0.141 sec/batch)
2017-05-08 18:49:06.396464: step 11310, loss = 0.86 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-08 18:49:07.719770: step 11320, loss = 0.80 (967.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:49:09.030507: step 11330, loss = 0.96 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:49:10.327922: step 11340, loss = 0.71 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:49:11.616859: step 11350, loss = 0.82 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:49:12.898007: step 11360, loss = 0.86 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:49:14.218121: step 11370, loss = 0.80 (969.6 examples/sec; 0.132 sec/batch)
2017-05-08 18:49:15.527513: step 11380, loss = 0.81 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:49:16.834647: step 11390, loss = 0.89 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:49:18.232598: step 11400, loss = 1.01 (915.6 examples/sec; 0.140 sec/batch)
2017-05-08 18:49:19.427435: step 11410, loss = 0.86 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-08 18:49:20.719948: step 11420, loss = 0.86 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:49:22.019190: step 11430, loss = 1.07 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:49:23.358019: step 11440, loss = 0.97 (956.1 examples/sec; 0.134 sec/batch)
2017-05-08 18:49:24.632230: step 11450, loss = 0.87 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:49:25.923720: step 11460, loss = 0.93 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:49:27.223137: step 11470, loss = 0.77 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:49:28.542329: step 11480, loss = 0.86 (970.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:49:29.852391: step 11490, loss = 0.71 (977.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:49:31.269791: step 11500, loss = 1.01 (903.1 examples/sec; 0.142 sec/batch)
2017-05-08 18:49:32.486650: step 11510, loss = 0.82 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-08 18:49:33.789587: step 11520, loss = 0.79 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:49:35.098237: step 11530, loss = 0.72 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:49:36.405867: step 11540, loss = 0.92 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:49:37.734104: step 11550, loss = 0.84 (963.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:49:39.045650: step 11560, loss = 0.80 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:49:40.323126: step 11570, loss = 0.73 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:49:41.642546: step 11580, loss = 0.91 (970.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:49:42.997273: step 11590, loss = 1.02 (944.8 examples/sec; 0.135 sec/batch)
2017-05-08 18:49:44.415561: step 11600, loss = 0.67 (902.5 examples/sec; 0.142 sec/batch)
2017-05-08 18:49:45.607985: step 11610, loss = 0.81 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-08 18:49:46.906017: step 11620, loss = 0.82 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:49:48.192185: step 11630, loss = 0.94 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:49:49.507347: step 11640, loss = 0.68 (973.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:49:50.809069: step 11650, loss = 0.98 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:49:52.110074: step 11660, loss = 1.05 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:49:53.404781: step 11670, loss = 0.88 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:49:54.700618: step 11680, loss = 0.72 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:49:56.024383: step 11690, loss = 0.71 (966.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:49:57.405988: step 11700, loss = 0.78 (926.5 examples/sec; 0.138 sec/batch)
2017-05-08 18:49:58.606285: step 11710, loss = 0.85 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-08 18:49:59.884139: step 11720, loss = 1.01 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:50:01.199480: step 11730, loss = 0.70 (973.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:50:02.543364: step 11740, loss = 0.84 (952.5 examples/sec; 0.134 sec/batch)
2017-05-08 18:50:03.842508: step 11750, loss = 0.82 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:50:05.147755: step 11760, loss = 0.88 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:50:06.471676: step 11770, loss = 1.01 (966.8 examples/sec; 0.132 sec/batch)
2017-05-08 18:50:07.770387: step 11780, loss = 1.02 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:50:09.073562: step 11790, loss = 0.66 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:50:10.479710: step 11800, loss = 0.74 (910.3 examples/sec; 0.141 sec/batch)
2017-05-08 18:50:11.689052: step 11810, loss = 0.88 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-08 18:50:13.005549: step 11820, loss = 0.67 (972.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:50:14.321814: step 11830, loss = 0.88 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:50:15.632963: step 11840, loss = 0.89 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:50:16.913163: step 11850, loss = 0.87 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:50:18.209694: step 11860, loss = 0.83 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:50:19.483061: step 11870, loss = 0.82 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:50:20.778580: step 11880, loss = 0.85 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:50:22.096501: step 11890, loss = 0.78 (971.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:50:23.463710: step 11900, loss = 0.91 (936.2 examples/sec; 0.137 sec/batch)
2017-05-08 18:50:24.672406: step 11910, loss = 0.75 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-08 18:50:25.947976: step 11920, loss = 0.80 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:50:27.264745: step 11930, loss = 0.79 (972.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:50:28.556998: step 11940, loss = 0.90 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:50:29.867440: step 11950, loss = 0.69 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:50:31.161331: step 11960, loss = 0.78 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:50:32.484626: step 11970, loss = 1.09 (967.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:50:33.799420: step 11980, loss = 0.96 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:50:35.090101: step 11990, loss = 0.88 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:50:36.473885: step 12000, loss = 0.79 (925.0 examples/sec; 0.138 sec/batch)
2017-05-08 18:50:37.669859: step 12010, loss = 0.73 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-08 18:50:38.947545: step 12020, loss = 0.80 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:50:40.288613: step 12030, loss = 0.87 (954.5 examples/sec; 0.134 sec/batch)
2017-05-08 18:50:41.595166: step 12040, loss = 0.84 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:50:42.885868: step 12050, loss = 0.84 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:50:44.178725: step 12060, loss = 0.75 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:50:45.508798: step 12070, loss = 0.94 (962.3 examples/sec; 0.133 sec/batch)
2017-05-08 18:50:46.801755: step 12080, loss = 0.83 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:50:48.079630: step 12090, loss = 0.90 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:50:49.464838: step 12100, loss = 0.83 (924.0 examples/sec; 0.139 sec/batch)
2017-05-08 18:50:50.678020: step 12110, loss = 0.84 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-08 18:50:51.949588: step 12120, loss = 0.76 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:50:53.234989: step 12130, loss = 0.76 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:50:54.558629: step 12140, loss = 0.91 (967.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:50:55.836196: step 12150, loss = 0.81 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:50:57.124752: step 12160, loss = 0.73 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:50:58.436197: step 12170, loss = 0.88 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:50:59.759913: step 12180, loss = 0.72 (967.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:51:01.050790: step 12190, loss = 0.73 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:51:02.462959: step 12200, loss = 0.72 (906.4 examples/sec; 0.141 sec/batch)
2017-05-08 18:51:03.626537: step 12210, loss = 1.13 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-08 18:51:04.960742: step 12220, loss = 0.94 (959.4 examples/sec; 0.133 sec/batch)
2017-05-08 18:51:06.266633: step 12230, loss = 0.80 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:51:07.560562: step 12240, loss = 1.03 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:51:08.882388: step 12250, loss = 0.84 (968.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:51:10.162394: step 12260, loss = 0.70 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:51:11.426080: step 12270, loss = 0.79 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:51:12.728010: step 12280, loss = 0.87 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:51:14.019060: step 12290, loss = 0.80 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:51:15.412608: step 12300, loss = 0.79 (918.5 examples/sec; 0.139 sec/batch)
2017-05-08 18:51:16.628363: step 12310, loss = 0.75 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-08 18:51:17.946373: step 12320, loss = 0.99 (971.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:51:19.261988: step 12330, loss = 0.88 (972.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:51:20.594662: step 12340, loss = 0.75 (960.5 examples/sec; 0.133 sec/batch)
2017-05-08 18:51:21.890221: step 12350, loss = 0.90 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:51:23.171967: step 12360, loss = 0.82 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:51:24.498345: step 12370, loss = 0.83 (965.0 examples/sec; 0.133 sec/batch)
2017-05-08 18:51:25.820358: step 12380, loss = 0.97 (968.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:51:27.124003: step 12390, loss = 0.92 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:51:28.535438: step 12400, loss = 0.71 (906.9 examples/sec; 0.141 sec/batch)
2017-05-08 18:51:29.716538: step 12410, loss = 0.86 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-08 18:51:31.031382: step 12420, loss = 0.74 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:51:32.315088: step 12430, loss = 0.96 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:51:33.609919: step 12440, loss = 0.93 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:51:34.913061: step 12450, loss = 0.93 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:51:36.245503: step 12460, loss = 0.90 (960.6 examples/sec; 0.133 sec/batch)
2017-05-08 18:51:37.543737: step 12470, loss = 0.84 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:51:38.822372: step 12480, loss = 0.88 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:51:40.111808: step 12490, loss = 0.90 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:51:41.529486: step 12500, loss = 0.93 (902.9 examples/sec; 0.142 sec/batch)
2017-05-08 18:51:42.751570: step 12510, loss = 0.88 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-08 18:51:44.055779: step 12520, loss = 1.06 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:51:45.384988: step 12530, loss = 0.84 (963.0 examples/sec; 0.133 sec/batch)
2017-05-08 18:51:46.664630: step 12540, loss = 0.89 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:51:47.971744: step 12550, loss = 0.90 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:51:49.281573: step 12560, loss = 0.92 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:51:50.578412: step 12570, loss = 0.76 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:51:51.900388: step 12580, loss = 1.00 (968.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:51:53.197015: step 12590, loss = 0.95 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:51:54.594653: step 12600, loss = 0.89 (915.8 examples/sec; 0.140 sec/batch)
2017-05-08 18:51:55.790549: step 12610, loss = 0.78 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-08 18:51:57.099875: step 12620, loss = 0.81 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:51:58.420173: step 12630, loss = 0.86 (969.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:51:59.706270: step 12640, loss = 0.79 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:52:01.029011: step 12650, loss = 0.89 (967.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:52:02.362134: step 12660, loss = 0.89 (960.1 examples/sec; 0.133 sec/batch)
2017-05-08 18:52:03.678640: step 12670, loss = 0.72 (972.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:52:04.990032: step 12680, loss = 0.79 (976.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:52:06.302828: step 12690, loss = 0.85 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:52:07.721172: step 12700, loss = 0.94 (902.5 examples/sec; 0.142 sec/batch)
2017-05-08 18:52:08.981426: step 12710, loss = 0.97 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:52:10.255940: step 12720, loss = 0.92 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:52:11.567544: step 12730, loss = 0.89 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:52:12.875715: step 12740, loss = 0.85 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:52:14.200179: step 12750, loss = 0.89 (966.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:52:15.512441: step 12760, loss = 0.94 (975.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:52:16.835679: step 12770, loss = 0.91 (967.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:52:18.116978: step 12780, loss = 0.88 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:52:19.411256: step 12790, loss = 0.90 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:52:20.814425: step 12800, loss = 0.77 (912.2 examples/sec; 0.140 sec/batch)
2017-05-08 18:52:22.031754: step 12810, loss = 0.95 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-08 18:52:23.331289: step 12820, loss = 0.86 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:52:24.635889: step 12830, loss = 0.86 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:52:25.966235: step 12840, loss = 0.80 (962.2 examples/sec; 0.133 sec/batch)
2017-05-08 18:52:27.245954: step 12850, loss = 0.84 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:52:28.536548: step 12860, loss = 0.75 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:52:29.874411: step 12870, loss = 0.68 (956.7 examples/sec; 0.134 sec/batch)
2017-05-08 18:52:31.168720: step 12880, loss = 0.87 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:52:32.477681: step 12890, loss = 0.73 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:52:33.897769: step 12900, loss = 0.91 (901.4 examples/sec; 0.142 sec/batch)
2017-05-08 18:52:35.118306: step 12910, loss = 0.97 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-08 18:52:36.381363: step 12920, loss = 0.86 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:52:37.680218: step 12930, loss = 0.83 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:52:38.975654: step 12940, loss = 0.73 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:52:40.301967: step 12950, loss = 0.75 (965.1 examples/sec; 0.133 sec/batch)
2017-05-08 18:52:41.578966: step 12960, loss = 0.77 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:52:42.890531: step 12970, loss = 0.82 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:52:44.180004: step 12980, loss = 0.89 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:52:45.477245: step 12990, loss = 1.00 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:52:46.900574: step 13000, loss = 1.04 (899.3 examples/sec; 0.142 sec/batch)
2017-05-08 18:52:48.112770: step 13010, loss = 0.97 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-08 18:52:49.393548: step 13020, loss = 0.78 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:52:50.677889: step 13030, loss = 0.80 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:52:51.968832: step 13040, loss = 0.89 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:52:53.270229: step 13050, loss = 0.72 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:52:54.589071: step 13060, loss = 0.86 (970.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:52:55.897086: step 13070, loss = 0.73 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:52:57.192808: step 13080, loss = 0.83 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:52:58.520741: step 13090, loss = 0.85 (963.9 examples/sec; 0.133 sec/batch)
2017-05-08 18:52:59.882084: step 13100, loss = 0.82 (940.3 examples/sec; 0.136 sec/batch)
2017-05-08 18:53:01.122636: step 13110, loss = 0.88 (1031.8 examples/sec; 0.124 sec/batch)
2017-05-08 18:53:02.414282: step 13120, loss = 0.97 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:53:03.708410: step 13130, loss = 0.72 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:53:05.015394: step 13140, loss = 0.84 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:53:06.343784: step 13150, loss = 1.02 (963.6 examples/sec; 0.133 sec/batch)
2017-05-08 18:53:07.635315: step 13160, loss = 0.98 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:53:08.923315: step 13170, loss = 0.96 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:53:10.199106: step 13180, loss = 0.88 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:53:11.509581: step 13190, loss = 0.88 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:53:12.943157: step 13200, loss = 0.79 (892.9 examples/sec; 0.143 sec/batch)
2017-05-08 18:53:14.166932: step 13210, loss = 0.93 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-08 18:53:15.451289: step 13220, loss = 0.87 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:53:16.752615: step 13230, loss = 0.79 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:53:18.082501: step 13240, loss = 0.90 (962.5 examples/sec; 0.133 sec/batch)
2017-05-08 18:53:19.366109: step 13250, loss = 1.10 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:53:20.667248: step 13260, loss = 0.84 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:53:21.986982: step 13270, loss = 0.83 (969.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:53:23.307515: step 13280, loss = 0.81 (969.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:53:24.583528: step 13290, loss = 0.75 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:53:25.963517: step 13300, loss = 0.69 (927.5 examples/sec; 0.138 sec/batch)
2017-05-08 18:53:27.155380: step 13310, loss = 0.83 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-08 18:53:28.474232: step 13320, loss = 0.71 (970.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:53:29.790266: step 13330, loss = 0.77 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 18:53:31.101742: step 13340, loss = 0.90 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:53:32.448612: step 13350, loss = 1.06 (950.3 examples/sec; 0.135 sec/batch)
2017-05-08 18:53:33.717850: step 13360, loss = 0.92 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:53:35.008338: step 13370, loss = 0.90 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:53:36.330847: step 13380, loss = 0.83 (967.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:53:37.656989: step 13390, loss = 0.84 (965.2 examples/sec; 0.133 sec/batch)
2017-05-08 18:53:39.040541: step 13400, loss = 0.87 (925.2 examples/sec; 0.138 sec/batch)
2017-05-08 18:53:40.240750: step 13410, loss = 0.85 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-08 18:53:41.525867: step 13420, loss = 0.71 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:53:42.827616: step 13430, loss = 0.95 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:53:44.129479: step 13440, loss = 1.02 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:53:45.446233: step 13450, loss = 0.77 (972.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:53:46.736229: step 13460, loss = 0.77 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:53:48.023682: step 13470, loss = 0.92 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:53:49.344084: step 13480, loss = 1.00 (969.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:53:50.638356: step 13490, loss = 0.85 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:53:52.031034: step 13500, loss = 0.75 (919.1 examples/sec; 0.139 sec/batch)
2017-05-08 18:53:53.223826: step 13510, loss = 0.76 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-08 18:53:54.519973: step 13520, loss = 0.68 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:53:55.816549: step 13530, loss = 0.95 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:53:57.151751: step 13540, loss = 0.94 (958.7 examples/sec; 0.134 sec/batch)
2017-05-08 18:53:58.432368: step 13550, loss = 0.89 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:53:59.759837: step 13560, loss = 0.72 (964.2 examples/sec; 0.133 sec/batch)
2017-05-08 18:54:01.050980: step 13570, loss = 0.94 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:54:02.355747: step 13580, loss = 0.80 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:54:03.663930: step 13590, loss = 0.84 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:54:05.075626: step 13600, loss = 0.87 (906.7 examples/sec; 0.141 sec/batch)
2017-05-08 18:54:06.345082: step 13610, loss = 0.98 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:54:07.649516: step 13620, loss = 0.98 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:54:08.945049: step 13630, loss = 0.90 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:54:10.261532: step 13640, loss = 0.80 (972.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:54:11.564903: step 13650, loss = 0.68 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:54:12.876484: step 13660, loss = 0.98 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:54:14.192159: step 13670, loss = 0.87 (972.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:54:15.502290: step 13680, loss = 1.00 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:54:16.830642: step 13690, loss = 0.84 (963.6 examples/sec; 0.133 sec/batch)
2017-05-08 18:54:18.252151: step 13700, loss = 0.80 (900.5 examples/sec; 0.142 sec/batch)
2017-05-08 18:54:19.486321: step 13710, loss = 0.88 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-08 18:54:20.785128: step 13720, loss = 0.89 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:54:22.090109: step 13730, loss = 1.26 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:54:23.406774: step 13740, loss = 0.75 (972.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:54:24.705937: step 13750, loss = 0.93 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:54:25.989771: step 13760, loss = 0.91 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:54:27.299862: step 13770, loss = 0.63 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:54:28.622975: step 13780, loss = 0.80 (967.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:54:29.918832: step 13790, loss = 0.97 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:54:31.347299: step 13800, loss = 0.88 (896.1 examples/sec; 0.143 sec/batch)
2017-05-08 18:54:32.558792: step 13810, loss = 0.80 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-08 18:54:33.842049: step 13820, loss = 0.75 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:54:35.162396: step 13830, loss = 0.92 (969.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:54:36.531489: step 13840, loss = 0.79 (934.9 examples/sec; 0.137 sec/batch)
2017-05-08 18:54:37.837539: step 13850, loss = 1.03 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:54:39.137421: step 13860, loss = 0.68 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:54:40.429375: step 13870, loss = 1.06 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:54:41.740951: step 13880, loss = 0.93 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:54:43.039598: step 13890, loss = 0.90 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:54:44.428246: step 13900, loss = 0.91 (921.8 examples/sec; 0.139 sec/batch)
2017-05-08 18:54:45.633233: step 13910, loss = 0.89 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-08 18:54:46.936966: step 13920, loss = 0.82 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:54:48.256278: step 13930, loss = 0.79 (970.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:54:49.538880: step 13940, loss = 0.84 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:54:50.836481: step 13950, loss = 0.83 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:54:52.148669: step 13960, loss = 0.76 (975.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:54:53.480548: step 13970, loss = 1.00 (961.0 examples/sec; 0.133 sec/batch)
2017-05-08 18:54:54.777218: step 13980, loss = 0.88 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:54:56.090408: step 13990, loss = 0.84 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:54:57.506258: step 14000, loss = 0.76 (904.1 examples/sec; 0.142 sec/batch)
2017-05-08 18:54:58.706411: step 14010, loss = 0.75 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-08 18:54:59.974930: step 14020, loss = 0.82 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:55:01.297100: step 14030, loss = 0.71 (968.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:55:02.641934: step 14040, loss = 0.82 (951.8 examples/sec; 0.134 sec/batch)
2017-05-08 18:55:03.903860: step 14050, loss = 1.00 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:55:05.203730: step 14060, loss = 0.78 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:55:06.482714: step 14070, loss = 0.81 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:55:07.761488: step 14080, loss = 0.82 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:55:09.068236: step 14090, loss = 0.82 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:55:10.476752: step 14100, loss = 0.86 (908.8 examples/sec; 0.141 sec/batch)
2017-05-08 18:55:11.668760: step 14110, loss = 0.66 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-08 18:55:12.948304: step 14120, loss = 0.79 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:55:14.241256: step 14130, loss = 0.72 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:55:15.553978: step 14140, loss = 0.79 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:55:16.866485: step 14150, loss = 0.74 (975.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:55:18.190525: step 14160, loss = 0.75 (966.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:55:19.462534: step 14170, loss = 0.93 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:55:20.728046: step 14180, loss = 0.84 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:55:22.006821: step 14190, loss = 0.85 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:55:23.405439: step 14200, loss = 0.85 (915.2 examples/sec; 0.140 sec/batch)
2017-05-08 18:55:24.607298: step 14210, loss = 0.77 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-08 18:55:25.894540: step 14220, loss = 0.96 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:55:27.210410: step 14230, loss = 0.86 (972.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:55:28.472874: step 14240, loss = 1.00 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:55:29.751920: step 14250, loss = 0.81 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:55:31.074902: step 14260, loss = 1.07 (967.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:55:32.368971: step 14270, loss = 0.99 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:55:33.665730: step 14280, loss = 0.84 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:55:34.954105: step 14290, loss = 0.81 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:55:36.350899: step 14300, loss = 0.92 (916.4 examples/sec; 0.140 sec/batch)
2017-05-08 18:55:37.587821: step 14310, loss = 0.84 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-08 18:55:38.876199: step 14320, loss = 1.07 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:55:40.173435: step 14330, loss = 0.59 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:55:41.496663: step 14340, loss = 0.68 (967.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:55:42.819584: step 14350, loss = 0.85 (967.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:55:44.106280: step 14360, loss = 0.82 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:55:45.436289: step 14370, loss = 0.87 (962.4 examples/sec; 0.133 sec/batch)
2017-05-08 18:55:46.731788: step 14380, loss = 0.97 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:55:48.056705: step 14390, loss = 0.88 (966.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:55:49.463672: step 14400, loss = 0.73 (909.8 examples/sec; 0.141 sec/batch)
2017-05-08 18:55:50.673218: step 14410, loss = 0.64 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-08 18:55:51.979027: step 14420, loss = 0.75 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:55:53.308475: step 14430, loss = 0.86 (962.8 examples/sec; 0.133 sec/batch)
2017-05-08 18:55:54.598888: step 14440, loss = 0.78 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:55:55.884074: step 14450, loss = 0.91 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:55:57.163906: step 14460, loss = 0.85 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:55:58.434327: step 14470, loss = 0.69 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:55:59.726840: step 14480, loss = 0.85 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:56:01.048559: step 14490, loss = 0.73 (968.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:56:02.423226: step 14500, loss = 0.82 (931.1 examples/sec; 0.137 sec/batch)
2017-05-08 18:56:03.624918: step 14510, loss = 0.87 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-08 18:56:04.903540: step 14520, loss = 1.04 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:56:06.192679: step 14530, loss = 1.05 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:56:07.471600: step 14540, loss = 0.96 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:56:08.774546: step 14550, loss = 0.95 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:56:10.092255: step 14560, loss = 0.82 (971.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:56:11.426526: step 14570, loss = 0.73 (959.3 examples/sec; 0.133 sec/batch)
2017-05-08 18:56:12.715024: step 14580, loss = 1.06 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:56:13.983426: step 14590, loss = 0.95 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:56:15.365894: step 14600, loss = 1.03 (925.9 examples/sec; 0.138 sec/batch)
2017-05-08 18:56:16.592162: step 14610, loss = 0.83 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-08 18:56:17.903204: step 14620, loss = 0.99 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:56:19.227751: step 14630, loss = 0.72 (966.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:56:20.522873: step 14640, loss = 0.67 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:56:21.840783: step 14650, loss = 0.62 (971.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:56:23.164665: step 14660, loss = 0.89 (966.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:56:24.511927: step 14670, loss = 0.74 (950.1 examples/sec; 0.135 sec/batch)
2017-05-08 18:56:25.822523: step 14680, loss = 0.75 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:56:27.113353: step 14690, loss = 0.76 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:56:28.535639: step 14700, loss = 0.74 (900.0 examples/sec; 0.142 sec/batch)
2017-05-08 18:56:29.739122: step 14710, loss = 0.73 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-08 18:56:31.065370: step 14720, loss = 0.77 (965.1 examples/sec; 0.133 sec/batch)
2017-05-08 18:56:32.368563: step 14730, loss = 0.79 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:56:33.661433: step 14740, loss = 0.72 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:56:34.953538: step 14750, loss = 0.77 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:56:36.229847: step 14760, loss = 0.80 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:56:37.519520: step 14770, loss = 0.87 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:56:38.822069: step 14780, loss = 0.75 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:56:40.138851: step 14790, loss = 0.80 (972.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:56:41.512119: step 14800, loss = 0.90 (932.1 examples/sec; 0.137 sec/batch)
2017-05-08 18:56:42.756742: step 14810, loss = 0.91 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-08 18:56:44.072326: step 14820, loss = 0.78 (973.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:56:45.367064: step 14830, loss = 0.94 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:56:46.688754: step 14840, loss = 0.94 (968.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:56:47.969392: step 14850, loss = 0.77 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:56:49.278351: step 14860, loss = 1.02 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:56:50.616395: step 14870, loss = 0.87 (956.6 examples/sec; 0.134 sec/batch)
2017-05-08 18:56:51.907393: step 14880, loss = 1.04 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:56:53.245702: step 14890, loss = 0.95 (956.4 examples/sec; 0.134 sec/batch)
2017-05-08 18:56:54.654989: step 14900, loss = 0.75 (908.3 examples/sec; 0.141 sec/batch)
2017-05-08 18:56:55.862978: step 14910, loss = 0.86 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-08 18:56:57.202657: step 14920, loss = 0.66 (955.5 examples/sec; 0.134 sec/batch)
2017-05-08 18:56:58.488285: step 14930, loss = 0.86 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:56:59.789789: step 14940, loss = 0.88 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:57:01.063678: step 14950, loss = 0.81 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:57:02.394729: step 14960, loss = 0.93 (961.6 examples/sec; 0.133 sec/batch)
2017-05-08 18:57:03.688369: step 14970, loss = 0.75 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:57:04.992987: step 14980, loss = 0.69 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:57:06.297116: step 14990, loss = 0.79 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:57:07.709017: step 15000, loss = 0.93 (906.6 examples/sec; 0.141 sec/batch)
2017-05-08 18:57:08.932440: step 15010, loss = 0.91 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-08 18:57:10.195365: step 15020, loss = 0.78 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:57:11.490516: step 15030, loss = 0.87 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:57:12.786038: step 15040, loss = 0.80 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:57:14.131016: step 15050, loss = 0.77 (951.7 examples/sec; 0.134 sec/batch)
2017-05-08 18:57:15.442234: step 15060, loss = 1.10 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:57:16.760402: step 15070, loss = 0.61 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:57:18.097707: step 15080, loss = 0.87 (957.1 examples/sec; 0.134 sec/batch)
2017-05-08 18:57:19.379889: step 15090, loss = 0.87 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:57:20.744039: step 15100, loss = 0.96 (938.3 examples/sec; 0.136 sec/batch)
2017-05-08 18:57:21.969206: step 15110, loss = 0.79 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-08 18:57:23.290737: step 15120, loss = 0.73 (968.6 examples/sec; 0.132 sec/batch)
2017-05-08 18:57:24.616378: step 15130, loss = 0.93 (965.6 examples/sec; 0.133 sec/batch)
2017-05-08 18:57:25.892337: step 15140, loss = 0.87 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:57:27.184318: step 15150, loss = 0.97 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:57:28.464818: step 15160, loss = 0.88 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:57:29.794727: step 15170, loss = 0.77 (962.5 examples/sec; 0.133 sec/batch)
2017-05-08 18:57:31.097964: step 15180, loss = 0.96 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:57:32.412339: step 15190, loss = 0.84 (973.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:57:33.815935: step 15200, loss = 0.84 (911.9 examples/sec; 0.140 sec/batch)
2017-05-08 18:57:35.085036: step 15210, loss = 0.76 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:57:36.381429: step 15220, loss = 0.82 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:57:37.737271: step 15230, loss = 0.96 (944.1 examples/sec; 0.136 sec/batch)
2017-05-08 18:57:39.049431: step 15240, loss = 0.78 (975.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:57:40.330532: step 15250, loss = 0.95 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:57:41.666567: step 15260, loss = 0.89 (958.1 examples/sec; 0.134 sec/batch)
2017-05-08 18:57:42.989027: step 15270, loss = 0.75 (967.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:57:44.257578: step 15280, loss = 0.92 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:57:45.559524: step 15290, loss = 0.83 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:57:46.960000: step 15300, loss = 0.83 (914.0 examples/sec; 0.140 sec/batch)
2017-05-08 18:57:48.192365: step 15310, loss = 0.75 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-08 18:57:49.471790: step 15320, loss = 0.82 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:57:50.788606: step 15330, loss = 0.67 (972.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:57:52.107772: step 15340, loss = 0.60 (970.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:57:53.408423: step 15350, loss = 0.76 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:57:54.724801: step 15360, loss = 0.78 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:57:56.051703: step 15370, loss = 0.84 (964.6 examples/sec; 0.133 sec/batch)
2017-05-08 18:57:57.345200: step 15380, loss = 0.72 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:57:58.637932: step 15390, loss = 0.86 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:58:00.083326: step 15400, loss = 0.85 (885.6 examples/sec; 0.145 sec/batch)
2017-05-08 18:58:01.263559: step 15410, loss = 0.75 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-08 18:58:02.576874: step 15420, loss = 0.80 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:58:03.878968: step 15430, loss = 0.90 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:58:05.149785: step 15440, loss = 0.95 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:58:06.438772: step 15450, loss = 0.78 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:58:07.765770: step 15460, loss = 0.76 (964.6 examples/sec; 0.133 sec/batch)
2017-05-08 18:58:09.072988: step 15470, loss = 0.76 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:58:10.354785: step 15480, loss = 0.86 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:58:11.660545: step 15490, loss = 0.88 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:58:13.068692: step 15500, loss = 0.76 (909.0 examples/sec; 0.141 sec/batch)
2017-05-08 18:58:14.262997: step 15510, loss = 0.71 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-08 18:58:15.549804: step 15520, loss = 0.83 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:58:16.879277: step 15530, loss = 0.84 (962.8 examples/sec; 0.133 sec/batch)
2017-05-08 18:58:18.202615: step 15540, loss = 0.80 (967.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:58:19.489553: step 15550, loss = 0.83 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:58:20.784582: step 15560, loss = 0.90 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:58:22.118486: step 15570, loss = 0.96 (959.6 examples/sec; 0.133 sec/batch)
2017-05-08 18:58:23.426919: step 15580, loss = 0.71 (978.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:58:24.720662: step 15590, loss = 0.87 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:58:26.143085: step 15600, loss = 0.83 (899.9 examples/sec; 0.142 sec/batch)
2017-05-08 18:58:27.343887: step 15610, loss = 0.78 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-08 18:58:28.656080: step 15620, loss = 0.81 (975.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:58:29.955894: step 15630, loss = 1.00 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:58:31.290790: step 15640, loss = 0.76 (958.9 examples/sec; 0.133 sec/batch)
2017-05-08 18:58:32.578332: step 15650, loss = 0.75 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:58:33.866214: step 15660, loss = 0.94 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:58:35.166432: step 15670, loss = 0.83 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:58:36.501296: step 15680, loss = 0.75 (958.9 examples/sec; 0.133 sec/batch)
2017-05-08 18:58:37.814268: step 15690, loss = 0.81 (974.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:58:39.196156: step 15700, loss = 0.72 (926.3 examples/sec; 0.138 sec/batch)
2017-05-08 18:58:40.396971: step 15710, loss = 0.82 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-08 18:58:41.721105: step 15720, loss = 0.74 (966.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:58:43.015509: step 15730, loss = 0.74 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:58:44.304522: step 15740, loss = 0.81 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:58:45.639706: step 15750, loss = 0.92 (958.7 examples/sec; 0.134 sec/batch)
2017-05-08 18:58:46.960042: step 15760, loss = 0.79 (969.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:58:48.238356: step 15770, loss = 0.73 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:58:49.512533: step 15780, loss = 0.77 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:58:50.813001: step 15790, loss = 0.71 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:58:52.242021: step 15800, loss = 0.68 (895.7 examples/sec; 0.143 sec/batch)
2017-05-08 18:58:53.497304: step 15810, loss = 0.81 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:58:54.804096: step 15820, loss = 0.91 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:58:56.114531: step 15830, loss = 0.73 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:58:57.457945: step 15840, loss = 0.90 (952.8 examples/sec; 0.134 sec/batch)
2017-05-08 18:58:58.726727: step 15850, loss = 0.79 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:59:00.029079: step 15860, loss = 0.70 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:59:01.304921: step 15870, loss = 1.00 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:59:02.635589: step 15880, loss = 0.99 (961.9 examples/sec; 0.133 sec/batch)
2017-05-08 18:59:03.914227: step 15890, loss = 0.72 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:59:05.297089: step 15900, loss = 0.92 (925.6 examples/sec; 0.138 sec/batch)
2017-05-08 18:59:06.496661: step 15910, loss = 0.95 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-08 18:59:07.785484: step 15920, loss = 0.78 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:59:09.069732: step 15930, loss = 0.71 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:59:10.365363: step 15940, loss = 0.67 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:59:11.712821: step 15950, loss = 0.97 (949.9 examples/sec; 0.135 sec/batch)
2017-05-08 18:59:13.000664: step 15960, loss = 0.89 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:59:14.280554: step 15970, loss = 0.69 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:59:15.539803: step 15980, loss = 0.77 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:59:16.823350: step 15990, loss = 0.80 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:59:18.226908: step 16000, loss = 0.94 (912.0 examples/sec; 0.140 sec/batch)
2017-05-08 18:59:19.440209: step 16010, loss = 0.73 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-08 18:59:20.718884: step 16020, loss = 0.80 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:59:21.999203: step 16030, loss = 0.65 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:59:23.293955: step 16040, loss = 0.76 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:59:24.558317: step 16050, loss = 1.07 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:59:25.901645: step 16060, loss = 0.84 (952.9 examples/sec; 0.134 sec/batch)
2017-05-08 18:59:27.195485: step 16070, loss = 0.79 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:59:28.482732: step 16080, loss = 0.78 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:59:29.785004: step 16090, loss = 0.66 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:59:31.185431: step 16100, loss = 0.79 (914.0 examples/sec; 0.140 sec/batch)
2017-05-08 18:59:32.376861: step 16110, loss = 0.92 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-08 18:59:33.687305: step 16120, loss = 0.75 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:59:35.021953: step 16130, loss = 0.77 (959.1 examples/sec; 0.133 sec/batch)
2017-05-08 18:59:36.302195: step 16140, loss = 0.82 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:59:37.589750: step 16150, loss = 0.55 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:59:38.865401: step 16160, loss = 0.91 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:59:40.170534: step 16170, loss = 0.71 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:59:41.476040: step 16180, loss = 0.86 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:59:42.768156: step 16190, loss = 0.86 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:59:44.189366: step 16200, loss = 0.80 (900.6 examples/sec; 0.142 sec/batch)
2017-05-08 18:59:45.392230: step 16210, loss = 1.00 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-08 18:59:46.684525: step 16220, loss = 0.86 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:59:47.961934: step 16230, loss = 0.87 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:59:49.260312: step 16240, loss = 0.82 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:59:50.587821: step 16250, loss = 0.76 (964.2 examples/sec; 0.133 sec/batch)
2017-05-08 18:59:51.909886: step 16260, loss = 0.90 (968.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:59:53.201139: step 16270, loss = 0.64 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:59:54.485845: step 16280, loss = 0.86 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:59:55.796642: step 16290, loss = 0.78 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:59:57.227077: step 16300, loss = 1.09 (894.8 examples/sec; 0.143 sec/batch)
2017-05-08 18:59:58.407701: step 16310, loss = 0.73 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-08 18:59:59.686936: step 16320, loss = 0.71 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:00:01.016109: step 16330, loss = 0.80 (963.0 examples/sec; 0.133 sec/batch)
2017-05-08 19:00:02.342885: step 16340, loss = 0.80 (964.7 examples/sec; 0.133 sec/batch)
2017-05-08 19:00:03.641518: step 16350, loss = 0.93 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:00:04.979419: step 16360, loss = 0.97 (956.7 examples/sec; 0.134 sec/batch)
2017-05-08 19:00:06.290329: step 16370, loss = 0.74 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:00:07.585787: step 16380, loss = 0.91 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:00:08.875225: step 16390, loss = 0.74 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:00:10.270756: step 16400, loss = 0.84 (917.2 examples/sec; 0.140 sec/batch)
2017-05-08 19:00:11.482290: step 16410, loss = 1.26 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-08 19:00:12.772239: step 16420, loss = 0.77 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:00:14.070288: step 16430, loss = 0.83 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:00:15.373719: step 16440, loss = 1.01 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:00:16.682770: step 16450, loss = 0.83 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:00:17.992892: step 16460, loss = 0.79 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:00:19.262532: step 16470, loss = 0.91 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:00:20.551384: step 16480, loss = 0.81 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:00:21.853772: step 16490, loss = 0.88 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:00:23.293881: step 16500, loss = 0.80 (888.8 examples/sec; 0.144 sec/batch)
2017-05-08 19:00:24.490467: step 16510, loss = 0.69 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-08 19:00:25.804593: step 16520, loss = 0.76 (974.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:00:27.122341: step 16530, loss = 0.89 (971.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:00:28.394825: step 16540, loss = 0.82 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:00:29.715354: step 16550, loss = 0.90 (969.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:00:31.052621: step 16560, loss = 1.01 (957.2 examples/sec; 0.134 sec/batch)
2017-05-08 19:00:32.343649: step 16570, loss = 0.83 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:00:33.636496: step 16580, loss = 0.84 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:00:34.970879: step 16590, loss = 0.94 (959.3 examples/sec; 0.133 sec/batch)
2017-05-08 19:00:36.381584: step 16600, loss = 0.87 (907.3 examples/sec; 0.141 sec/batch)
2017-05-08 19:00:37.607572: step 16610, loss = 0.83 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-08 19:00:38.908046: step 16620, loss = 0.86 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:00:40.223650: step 16630, loss = 0.76 (972.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:00:41.524387: step 16640, loss = 0.92 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:00:42.811692: step 16650, loss = 0.85 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:00:44.082158: step 16660, loss = 0.77 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:00:45.372136: step 16670, loss = 0.86 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:00:46.681170: step 16680, loss = 0.91 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:00:47.986739: step 16690, loss = 0.81 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:00:49.405325: step 16700, loss = 0.86 (902.3 examples/sec; 0.142 sec/batch)
2017-05-08 19:00:50.617523: step 16710, loss = 0.72 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-08 19:00:51.934866: step 16720, loss = 0.74 (971.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:00:53.268885: step 16730, loss = 0.69 (959.5 examples/sec; 0.133 sec/batch)
2017-05-08 19:00:54.565880: step 16740, loss = 0.75 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:00:55.825032: step 16750, loss = 0.88 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:00:57.114887: step 16760, loss = 0.78 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:00:58.444533: step 16770, loss = 0.82 (962.7 examples/sec; 0.133 sec/batch)
2017-05-08 19:00:59.718620: step 16780, loss = 0.84 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:01:01.037672: step 16790, loss = 0.77 (970.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:01:02.487171: step 16800, loss = 0.70 (883.1 examples/sec; 0.145 sec/batch)
2017-05-08 19:01:03.697974: step 16810, loss = 0.83 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-08 19:01:05.003680: step 16820, loss = 0.95 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:01:06.304950: step 16830, loss = 0.74 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:01:07.621384: step 16840, loss = 0.80 (972.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:01:08.936914: step 16850, loss = 0.91 (973.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:01:10.227640: step 16860, loss = 0.82 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:01:11.537931: step 16870, loss = 0.70 (976.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:01:12.853344: step 16880, loss = 0.68 (973.1 examples/sec; 0.132 sec/batch)
2017-05-08 19:01:14.151442: step 16890, loss = 0.91 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:01:15.537190: step 16900, loss = 0.86 (923.7 examples/sec; 0.139 sec/batch)
2017-05-08 19:01:16.742956: step 16910, loss = 0.75 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-08 19:01:18.073964: step 16920, loss = 0.76 (961.7 examples/sec; 0.133 sec/batch)
2017-05-08 19:01:19.381941: step 16930, loss = 0.89 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:01:20.667570: step 16940, loss = 0.86 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:01:21.989143: step 16950, loss = 0.78 (968.5 examples/sec; 0.132 sec/batch)
2017-05-08 19:01:23.311572: step 16960, loss = 0.79 (967.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:01:24.595557: step 16970, loss = 0.64 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:01:25.892255: step 16980, loss = 0.88 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:01:27.181224: step 16990, loss = 0.75 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:01:28.587004: step 17000, loss = 0.96 (910.5 examples/sec; 0.141 sec/batch)
2017-05-08 19:01:29.808349: step 17010, loss = 0.67 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-08 19:01:31.108191: step 17020, loss = 0.88 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:01:32.383682: step 17030, loss = 0.81 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:01:33.660234: step 17040, loss = 0.83 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:01:34.958098: step 17050, loss = 0.81 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:01:36.274678: step 17060, loss = 0.90 (972.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:01:37.560575: step 17070, loss = 0.94 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:01:38.859772: step 17080, loss = 0.90 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:01:40.155519: step 17090, loss = 0.86 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:01:41.529203: step 17100, loss = 0.80 (931.8 examples/sec; 0.137 sec/batch)
2017-05-08 19:01:42.745424: step 17110, loss = 0.69 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-08 19:01:44.069977: step 17120, loss = 0.80 (966.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:01:45.372685: step 17130, loss = 0.73 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:01:46.670412: step 17140, loss = 0.74 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:01:47.980866: step 17150, loss = 0.89 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:01:49.333395: step 17160, loss = 0.88 (946.4 examples/sec; 0.135 sec/batch)
2017-05-08 19:01:50.595190: step 17170, loss = 0.76 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:01:51.883041: step 17180, loss = 0.76 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:01:53.179945: step 17190, loss = 0.70 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:01:54.589725: step 17200, loss = 0.76 (907.9 examples/sec; 0.141 sec/batch)
2017-05-08 19:01:55.798271: step 17210, loss = 0.75 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-08 19:01:57.076178: step 17220, loss = 0.83 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:01:58.346940: step 17230, loss = 0.85 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:01:59.641070: step 17240, loss = 0.75 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:02:00.916556: step 17250, loss = 0.87 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:02.197713: step 17260, loss = 0.94 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:03.502604: step 17270, loss = 0.82 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:02:04.805665: step 17280, loss = 0.88 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:02:06.106120: step 17290, loss = 0.84 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:02:07.512242: step 17300, loss = 0.70 (910.3 examples/sec; 0.141 sec/batch)
2017-05-08 19:02:08.709570: step 17310, loss = 0.65 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-08 19:02:09.992803: step 17320, loss = 0.81 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:11.270105: step 17330, loss = 1.05 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:12.530779: step 17340, loss = 0.81 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:02:13.819852: step 17350, loss = 0.90 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:02:15.100077: step 17360, loss = 0.76 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:16.365969: step 17370, loss = 0.81 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:02:17.696395: step 17380, loss = 0.94 (962.1 examples/sec; 0.133 sec/batch)
2017-05-08 19:02:18.991061: step 17390, loss = 0.85 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:02:20.366283: step 17400, loss = 0.81 (930.8 examples/sec; 0.138 sec/batch)
2017-05-08 19:02:21.597808: step 17410, loss = 0.72 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-08 19:02:22.887745: step 17420, loss = 0.92 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:02:24.184770: step 17430, loss = 0.78 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:02:25.496048: step 17440, loss = 0.70 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:02:26.795930: step 17450, loss = 0.74 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:02:28.102981: step 17460, loss = 0.92 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:02:29.400013: step 17470, loss = 0.61 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:02:30.707059: step 17480, loss = 0.79 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:02:32.017917: step 17490, loss = 1.03 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:02:33.412617: step 17500, loss = 0.83 (917.8 examples/sec; 0.139 sec/batch)
2017-05-08 19:02:34.596960: step 17510, loss = 0.79 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-08 19:02:35.858903: step 17520, loss = 0.65 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:02:37.131749: step 17530, loss = 0.89 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:02:38.476233: step 17540, loss = 0.83 (952.0 examples/sec; 0.134 sec/batch)
2017-05-08 19:02:39.758308: step 17550, loss = 0.82 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:41.064466: step 17560, loss = 0.73 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:02:42.344395: step 17570, loss = 0.83 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:43.631727: step 17580, loss = 0.76 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:02:44.893258: step 17590, loss = 0.91 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:02:46.266624: step 17600, loss = 0.73 (932.0 examples/sec; 0.137 sec/batch)
2017-05-08 19:02:47.455861: step 17610, loss = 0.81 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-08 19:02:48.778067: step 17620, loss = 0.83 (968.1 examples/sec; 0.132 sec/batch)
2017-05-08 19:02:50.083710: step 17630, loss = 0.77 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:02:51.373962: step 17640, loss = 0.89 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:02:52.651747: step 17650, loss = 0.67 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:53.943540: step 17660, loss = 0.87 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:02:55.248643: step 17670, loss = 0.82 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:02:56.551503: step 17680, loss = 0.77 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:02:57.832880: step 17690, loss = 0.74 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:59.201213: step 17700, loss = 1.14 (935.4 examples/sec; 0.137 sec/batch)
2017-05-08 19:03:00.386826: step 17710, loss = 0.98 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-08 19:03:01.689057: step 17720, loss = 0.96 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:03:02.994697: step 17730, loss = 0.70 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:03:04.268962: step 17740, loss = 0.93 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:03:05.575430: step 17750, loss = 0.77 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:03:06.864561: step 17760, loss = 0.86 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:03:08.172429: step 17770, loss = 0.67 (978.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:03:09.458545: step 17780, loss = 0.81 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:03:10.735535: step 17790, loss = 0.69 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:03:12.124522: step 17800, loss = 0.84 (921.5 examples/sec; 0.139 sec/batch)
2017-05-08 19:03:13.307537: step 17810, loss = 0.82 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-08 19:03:14.625349: step 17820, loss = 0.76 (971.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:03:15.931906: step 17830, loss = 0.92 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:03:17.228982: step 17840, loss = 0.93 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:03:18.506935: step 17850, loss = 0.81 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:03:19.752514: step 17860, loss = 0.92 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:03:21.044671: step 17870, loss = 0.76 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:03:22.313311: step 17880, loss = 1.02 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:03:23.622030: step 17890, loss = 0.82 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:03:25.037314: step 17900, loss = 0.73 (904.4 examples/sec; 0.142 sec/batch)
2017-05-08 19:03:26.254342: step 17910, loss = 0.79 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-08 19:03:27.537108: step 17920, loss = 0.75 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:03:28.822053: step 17930, loss = 0.79 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:03:30.116574: step 17940, loss = 0.67 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:03:31.430185: step 17950, loss = 0.82 (974.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:03:32.710641: step 17960, loss = 0.89 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:03:34.021865: step 17970, loss = 0.68 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:03:35.316378: step 17980, loss = 1.00 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:03:36.589106: step 17990, loss = 0.82 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:03:37.974060: step 18000, loss = 0.82 (924.2 examples/sec; 0.138 sec/batch)
2017-05-08 19:03:39.177215: step 18010, loss = 0.73 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-08 19:03:40.491246: step 18020, loss = 0.79 (974.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:03:41.782137: step 18030, loss = 0.76 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:03:43.081955: step 18040, loss = 0.72 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:03:44.371421: step 18050, loss = 0.83 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:03:45.663545: step 18060, loss = 0.88 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:03:46.974848: step 18070, loss = 0.67 (976.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:03:48.281905: step 18080, loss = 0.79 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:03:49.555177: step 18090, loss = 0.73 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:03:50.925808: step 18100, loss = 0.72 (933.9 examples/sec; 0.137 sec/batch)
2017-05-08 19:03:52.146542: step 18110, loss = 0.67 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-08 19:03:53.458021: step 18120, loss = 0.80 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:03:54.750140: step 18130, loss = 0.75 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:03:56.061761: step 18140, loss = 0.92 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:03:57.371783: step 18150, loss = 0.79 (977.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:03:58.657657: step 18160, loss = 0.71 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:03:59.962834: step 18170, loss = 0.91 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:04:01.291593: step 18180, loss = 0.63 (963.3 examples/sec; 0.133 sec/batch)
2017-05-08 19:04:02.557183: step 18190, loss = 0.74 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:04:03.938904: step 18200, loss = 0.81 (926.4 examples/sec; 0.138 sec/batch)
2017-05-08 19:04:05.156517: step 18210, loss = 0.77 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-08 19:04:06.461335: step 18220, loss = 0.86 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:04:07.751178: step 18230, loss = 1.05 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:04:09.112795: step 18240, loss = 0.77 (940.1 examples/sec; 0.136 sec/batch)
2017-05-08 19:04:10.370706: step 18250, loss = 0.73 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:04:11.645275: step 18260, loss = 0.81 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:04:12.948536: step 18270, loss = 0.71 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:04:14.215968: step 18280, loss = 0.80 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:04:15.506940: step 18290, loss = 0.91 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:04:16.891718: step 18300, loss = 0.81 (924.3 examples/sec; 0.138 sec/batch)
2017-05-08 19:04:18.117163: step 18310, loss = 0.81 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-08 19:04:19.417346: step 18320, loss = 0.90 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:04:20.694106: step 18330, loss = 0.76 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:04:21.957786: step 18340, loss = 0.93 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:04:23.231381: step 18350, loss = 0.89 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:04:24.516971: step 18360, loss = 0.87 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:04:25.807763: step 18370, loss = 0.87 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:04:27.114349: step 18380, loss = 0.97 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:04:28.409502: step 18390, loss = 0.77 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:04:29.791587: step 18400, loss = 0.73 (926.1 examples/sec; 0.138 sec/batch)
2017-05-08 19:04:30.971038: step 18410, loss = 0.90 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-08 19:04:32.282186: step 18420, loss = 0.82 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:04:33.534676: step 18430, loss = 0.90 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:04:34.854164: step 18440, loss = 0.77 (970.1 examples/sec; 0.132 sec/batch)
2017-05-08 19:04:36.192904: step 18450, loss = 0.78 (956.1 examples/sec; 0.134 sec/batch)
2017-05-08 19:04:37.494650: step 18460, loss = 0.76 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:04:38.776398: step 18470, loss = 0.86 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:04:40.060784: step 18480, loss = 0.81 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:04:41.336180: step 18490, loss = 0.82 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:04:42.723490: step 18500, loss = 0.80 (922.7 examples/sec; 0.139 sec/batch)
2017-05-08 19:04:43.891410: step 18510, loss = 0.91 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-08 19:04:45.179672: step 18520, loss = 0.90 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:04:46.462979: step 18530, loss = 0.83 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:04:47.764594: step 18540, loss = 0.96 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:04:49.059742: step 18550, loss = 0.77 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:04:50.357975: step 18560, loss = 0.98 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:04:51.668751: step 18570, loss = 0.73 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:04:52.959786: step 18580, loss = 0.62 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:04:54.226708: step 18590, loss = 0.84 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:04:55.615120: step 18600, loss = 0.65 (921.9 examples/sec; 0.139 sec/batch)
2017-05-08 19:04:56.835576: step 18610, loss = 0.75 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-08 19:04:58.167840: step 18620, loss = 0.70 (960.8 examples/sec; 0.133 sec/batch)
2017-05-08 19:04:59.446607: step 18630, loss = 0.71 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:05:00.743703: step 18640, loss = 0.68 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:05:02.075298: step 18650, loss = 0.73 (961.2 examples/sec; 0.133 sec/batch)
2017-05-08 19:05:03.342553: step 18660, loss = 0.72 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:05:04.665359: step 18670, loss = 0.77 (967.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:05:05.925981: step 18680, loss = 0.73 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:05:07.230646: step 18690, loss = 0.94 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:05:08.629731: step 18700, loss = 0.75 (914.9 examples/sec; 0.140 sec/batch)
2017-05-08 19:05:09.840037: step 18710, loss = 0.86 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-08 19:05:11.116170: step 18720, loss = 0.82 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:05:12.390055: step 18730, loss = 0.83 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:05:13.679018: step 18740, loss = 0.83 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:05:14.990248: step 18750, loss = 0.86 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:05:16.272648: step 18760, loss = 0.78 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:05:17.578417: step 18770, loss = 0.77 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:05:18.902715: step 18780, loss = 0.72 (966.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:05:20.139439: step 18790, loss = 0.65 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-08 19:05:21.511927: step 18800, loss = 0.88 (932.6 examples/sec; 0.137 sec/batch)
2017-05-08 19:05:22.699495: step 18810, loss = 0.86 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-08 19:05:23.981807: step 18820, loss = 0.86 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:05:25.297955: step 18830, loss = 0.85 (972.5 examples/sec; 0.132 sec/batch)
2017-05-08 19:05:26.603981: step 18840, loss = 0.93 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:05:27.858925: step 18850, loss = 0.76 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:05:29.182724: step 18860, loss = 0.77 (966.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:05:30.477383: step 18870, loss = 0.97 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:05:31.760595: step 18880, loss = 0.75 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:05:33.045860: step 18890, loss = 0.92 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:05:34.439415: step 18900, loss = 0.74 (918.5 examples/sec; 0.139 sec/batch)
2017-05-08 19:05:35.632050: step 18910, loss = 0.98 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-08 19:05:36.948204: step 18920, loss = 0.81 (972.5 examples/sec; 0.132 sec/batch)
2017-05-08 19:05:38.225814: step 18930, loss = 0.89 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:05:39.533869: step 18940, loss = 0.97 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:05:40.830925: step 18950, loss = 0.75 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:05:42.124012: step 18960, loss = 0.95 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:05:43.449174: step 18970, loss = 0.84 (965.9 examples/sec; 0.133 sec/batch)
2017-05-08 19:05:44.753154: step 18980, loss = 0.74 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:05:46.048136: step 18990, loss = 0.76 (988.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:05:47.453326: step 19000, loss = 0.71 (910.9 examples/sec; 0.141 sec/batch)
2017-05-08 19:05:48.670724: step 19010, loss = 0.67 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-08 19:05:49.983732: step 19020, loss = 0.90 (974.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:05:51.308736: step 19030, loss = 0.77 (966.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:05:52.562123: step 19040, loss = 0.70 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:05:53.869342: step 19050, loss = 0.81 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:05:55.157621: step 19060, loss = 1.08 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:05:56.422615: step 19070, loss = 0.70 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:05:57.727365: step 19080, loss = 0.76 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:05:59.042305: step 19090, loss = 0.89 (973.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:06:00.425336: step 19100, loss = 0.72 (925.5 examples/sec; 0.138 sec/batch)
2017-05-08 19:06:01.608583: step 19110, loss = 0.74 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-08 19:06:02.907679: step 19120, loss = 0.71 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:06:04.208825: step 19130, loss = 0.76 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:06:05.516492: step 19140, loss = 0.77 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:06:06.828132: step 19150, loss = 0.96 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:06:08.124398: step 19160, loss = 0.80 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:06:09.400411: step 19170, loss = 0.83 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:06:10.658091: step 19180, loss = 0.64 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:06:11.958489: step 19190, loss = 0.79 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:06:13.350968: step 19200, loss = 0.68 (919.2 examples/sec; 0.139 sec/batch)
2017-05-08 19:06:14.541844: step 19210, loss = 0.94 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-08 19:06:15.803752: step 19220, loss = 0.77 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:06:17.076333: step 19230, loss = 0.75 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:06:18.371998: step 19240, loss = 0.78 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:06:19.646405: step 19250, loss = 0.86 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:06:20.912696: step 19260, loss = 0.79 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:06:22.200357: step 19270, loss = 0.82 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:06:23.515881: step 19280, loss = 0.83 (973.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:06:24.826523: step 19290, loss = 0.73 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:06:26.216443: step 19300, loss = 0.96 (920.9 examples/sec; 0.139 sec/batch)
2017-05-08 19:06:27.450709: step 19310, loss = 0.87 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-08 19:06:28.768567: step 19320, loss = 0.98 (971.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:06:30.023428: step 19330, loss = 0.80 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:06:31.310663: step 19340, loss = 0.73 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:06:32.603832: step 19350, loss = 0.87 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:06:33.902143: step 19360, loss = 0.83 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:06:35.231411: step 19370, loss = 0.74 (962.9 examples/sec; 0.133 sec/batch)
2017-05-08 19:06:36.504358: step 19380, loss = 0.79 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:06:37.813801: step 19390, loss = 0.91 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:06:39.211895: step 19400, loss = 0.95 (915.5 examples/sec; 0.140 sec/batch)
2017-05-08 19:06:40.425839: step 19410, loss = 0.83 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-08 19:06:41.722630: step 19420, loss = 0.84 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:06:43.053684: step 19430, loss = 0.75 (961.6 examples/sec; 0.133 sec/batch)
2017-05-08 19:06:44.350970: step 19440, loss = 0.86 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:06:45.665716: step 19450, loss = 0.81 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:06:46.954891: step 19460, loss = 0.72 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:06:48.258399: step 19470, loss = 0.73 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:06:49.553054: step 19480, loss = 0.78 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:06:50.872743: step 19490, loss = 0.96 (969.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:06:52.271682: step 19500, loss = 0.94 (915.0 examples/sec; 0.140 sec/batch)
2017-05-08 19:06:53.467863: step 19510, loss = 0.85 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-08 19:06:54.753965: step 19520, loss = 0.93 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:06:56.015786: step 19530, loss = 0.79 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:06:57.311099: step 19540, loss = 0.84 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:06:58.613080: step 19550, loss = 0.73 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:06:59.913300: step 19560, loss = 0.77 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:07:01.205245: step 19570, loss = 0.90 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:07:02.491094: step 19580, loss = 0.85 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:07:03.772596: step 19590, loss = 0.72 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:07:05.143810: step 19600, loss = 0.83 (933.5 examples/sec; 0.137 sec/batch)
2017-05-08 19:07:06.405719: step 19610, loss = 0.81 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:07:07.619412: step 19620, loss = 0.74 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-08 19:07:08.887625: step 19630, loss = 1.06 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:07:10.155417: step 19640, loss = 0.87 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:07:11.432947: step 19650, loss = 0.86 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:07:12.705853: step 19660, loss = 0.94 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:07:13.983441: step 19670, loss = 0.81 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:07:15.266330: step 19680, loss = 0.78 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:07:16.526698: step 19690, loss = 0.86 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:07:17.947686: step 19700, loss = 0.91 (900.8 examples/sec; 0.142 sec/batch)
2017-05-08 19:07:19.151247: step 19710, loss = 0.63 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-08 19:07:20.438397: step 19720, loss = 0.69 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:07:21.714176: step 19730, loss = 0.86 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:07:23.009637: step 19740, loss = 0.83 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:07:24.276673: step 19750, loss = 0.74 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:07:25.559745: step 19760, loss = 0.73 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:07:26.842711: step 19770, loss = 0.84 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:07:28.167137: step 19780, loss = 0.73 (966.5 examples/sec; 0.132 sec/batch)
2017-05-08 19:07:29.459492: step 19790, loss = 0.77 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:07:30.850873: step 19800, loss = 0.76 (919.9 examples/sec; 0.139 sec/batch)
2017-05-08 19:07:32.113099: step 19810, loss = 0.89 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:07:33.391791: step 19820, loss = 0.85 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:07:34.689736: step 19830, loss = 0.76 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:07:35.965838: step 19840, loss = 0.82 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:07:37.260104: step 19850, loss = 0.82 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:07:38.567638: step 19860, loss = 0.81 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:07:39.876149: step 19870, loss = 0.91 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:07:41.186220: step 19880, loss = 0.74 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:07:42.473604: step 19890, loss = 0.86 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:07:43.867869: step 19900, loss = 0.83 (918.1 examples/sec; 0.139 sec/batch)
2017-05-08 19:07:45.079477: step 19910, loss = 0.74 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-08 19:07:46.383607: step 19920, loss = 0.91 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:07:47.718252: step 19930, loss = 0.79 (959.1 examples/sec; 0.133 sec/batch)
2017-05-08 19:07:49.014175: step 19940, loss = 0.72 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:07:50.323530: step 19950, loss = 0.90 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:07:51.612283: step 19960, loss = 0.81 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:07:52.887760: step 19970, loss = 0.80 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:07:54.210749: step 19980, loss = 0.85 (967.5 examples/sec; 0.132 sec/batch)
2017-05-08 19:07:55.536924: step 19990, loss = 0.73 (965.2 examples/sec; 0.133 sec/batch)
2017-05-08 19:07:56.909600: step 20000, loss = 0.84 (932.5 examples/sec; 0.137 sec/batch)
2017-05-08 19:07:58.148559: step 20010, loss = 1.00 (1033.1 examples/sec; 0.124 sec/batch)
2017-05-08 19:07:59.429606: step 20020, loss = 1.00 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:00.718642: step 20030, loss = 0.66 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:08:02.005778: step 20040, loss = 0.83 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:08:03.288284: step 20050, loss = 0.75 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:04.567628: step 20060, loss = 0.76 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:05.829367: step 20070, loss = 0.61 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:08:07.112085: step 20080, loss = 0.88 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:08.437808: step 20090, loss = 0.80 (965.5 examples/sec; 0.133 sec/batch)
2017-05-08 19:08:09.812127: step 20100, loss = 1.11 (931.4 examples/sec; 0.137 sec/batch)
2017-05-08 19:08:11.017011: step 20110, loss = 0.73 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-08 19:08:12.288197: step 20120, loss = 0.80 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:13.615366: step 20130, loss = 0.69 (964.5 examples/sec; 0.133 sec/batch)
2017-05-08 19:08:14.897103: step 20140, loss = 0.91 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:16.205532: step 20150, loss = 0.96 (978.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:08:17.476542: step 20160, loss = 0.88 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:18.779972: step 20170, loss = 0.87 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:08:20.086425: step 20180, loss = 1.06 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:08:21.408978: step 20190, loss = 0.86 (967.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:08:22.802294: step 20200, loss = 0.91 (918.7 examples/sec; 0.139 sec/batch)
2017-05-08 19:08:23.999421: step 20210, loss = 0.90 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-08 19:08:25.295300: step 20220, loss = 0.74 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:08:26.576560: step 20230, loss = 0.86 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:27.845220: step 20240, loss = 0.83 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:29.138842: step 20250, loss = 0.78 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:08:30.403859: step 20260, loss = 0.81 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:31.662440: step 20270, loss = 0.61 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:08:32.952095: step 20280, loss = 0.84 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:08:34.233112: step 20290, loss = 1.05 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:35.598889: step 20300, loss = 0.70 (937.2 examples/sec; 0.137 sec/batch)
2017-05-08 19:08:36.850151: step 20310, loss = 0.69 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:08:38.144129: step 20320, loss = 0.75 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:08:39.448128: step 20330, loss = 0.81 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:08:40.743201: step 20340, loss = 0.71 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:08:42.024686: step 20350, loss = 0.82 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:43.302589: step 20360, loss = 0.86 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:44.566652: step 20370, loss = 0.78 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:08:45.850000: step 20380, loss = 0.86 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:47.137806: step 20390, loss = 0.73 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:08:48.522378: step 20400, loss = 0.88 (924.5 examples/sec; 0.138 sec/batch)
2017-05-08 19:08:49.728149: step 20410, loss = 0.67 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-08 19:08:51.002770: step 20420, loss = 0.76 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:52.293811: step 20430, loss = 0.81 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:08:53.603303: step 20440, loss = 0.73 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:08:54.891164: step 20450, loss = 1.00 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:08:56.169247: step 20460, loss = 0.71 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:57.437236: step 20470, loss = 0.75 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:58.745013: step 20480, loss = 0.68 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:09:00.045563: step 20490, loss = 0.88 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:09:01.441972: step 20500, loss = 0.78 (916.6 examples/sec; 0.140 sec/batch)
2017-05-08 19:09:02.637510: step 20510, loss = 0.82 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-08 19:09:03.939137: step 20520, loss = 0.76 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:09:05.251994: step 20530, loss = 0.89 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:09:06.561485: step 20540, loss = 0.59 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:09:07.885857: step 20550, loss = 0.71 (966.5 examples/sec; 0.132 sec/batch)
2017-05-08 19:09:09.166800: step 20560, loss = 0.88 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:09:10.439597: step 20570, loss = 0.78 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:09:11.742988: step 20580, loss = 0.80 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:09:13.053143: step 20590, loss = 1.05 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:09:14.447568: step 20600, loss = 0.79 (918.0 examples/sec; 0.139 sec/batch)
2017-05-08 19:09:15.630847: step 20610, loss = 0.88 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-08 19:09:16.916200: step 20620, loss = 0.87 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:09:18.192648: step 20630, loss = 0.76 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:09:19.464941: step 20640, loss = 0.66 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:09:20.746615: step 20650, loss = 0.75 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:09:22.060867: step 20660, loss = 0.64 (973.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:09:23.363076: step 20670, loss = 0.83 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:09:24.688287: step 20680, loss = 0.86 (965.9 examples/sec; 0.133 sec/batch)
2017-05-08 19:09:25.962977: step 20690, loss = 0.76 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:09:27.369821: step 20700, loss = 0.81 (909.8 examples/sec; 0.141 sec/batch)
2017-05-08 19:09:28.562645: step 20710, loss = 0.76 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-08 19:09:29.884892: step 20720, loss = 1.01 (968.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:09:31.177392: step 20730, loss = 0.89 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:09:32.482373: step 20740, loss = 0.79 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:09:33.768588: step 20750, loss = 0.80 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:09:35.081402: step 20760, loss = 0.80 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:09:36.426696: step 20770, loss = 0.69 (951.5 examples/sec; 0.135 sec/batch)
2017-05-08 19:09:37.730167: step 20780, loss = 0.84 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:09:39.028370: step 20790, loss = 0.72 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:09:40.417530: step 20800, loss = 0.76 (921.4 examples/sec; 0.139 sec/batch)
2017-05-08 19:09:41.609189: step 20810, loss = 0.81 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-08 19:09:42.897838: step 20820, loss = 0.88 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:09:44.191772: step 20830, loss = 0.76 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:09:45.511642: step 20840, loss = 0.79 (969.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:09:46.839973: step 20850, loss = 0.77 (963.6 examples/sec; 0.133 sec/batch)
2017-05-08 19:09:48.118143: step 20860, loss = 0.88 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:09:49.380834: step 20870, loss = 0.87 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:09:50.648590: step 20880, loss = 0.90 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:09:51.908344: step 20890, loss = 0.78 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:09:53.318950: step 20900, loss = 0.79 (907.4 examples/sec; 0.141 sec/batch)
2017-05-08 19:09:54.512233: step 20910, loss = 0.69 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-08 19:09:55.810609: step 20920, loss = 0.69 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:09:57.123573: step 20930, loss = 0.84 (974.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:09:58.421293: step 20940, loss = 0.76 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:09:59.695014: step 20950, loss = 0.72 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:10:00.969845: step 20960, loss = 0.84 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:10:02.262767: step 20970, loss = 0.71 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:10:03.585202: step 20980, loss = 0.76 (967.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:10:04.888712: step 20990, loss = 0.84 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:10:06.270726: step 21000, loss = 0.70 (926.2 examples/sec; 0.138 sec/batch)
2017-05-08 19:10:07.451847: step 21010, loss = 0.82 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-08 19:10:08.733006: step 21020, loss = 0.92 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:10:10.012561: step 21030, loss = 0.84 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:10:11.300881: step 21040, loss = 0.77 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:10:12.608492: step 21050, loss = 0.66 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:10:13.890166: step 21060, loss = 0.87 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:10:15.189106: step 21070, loss = 0.96 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:10:16.491842: step 21080, loss = 0.81 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:10:17.796726: step 21090, loss = 0.76 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:10:19.211846: step 21100, loss = 0.73 (904.5 examples/sec; 0.142 sec/batch)
2017-05-08 19:10:20.404402: step 21110, loss = 0.88 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-08 19:10:21.668472: step 21120, loss = 0.81 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:10:22.936138: step 21130, loss = 0.99 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:10:24.259837: step 21140, loss = 0.87 (967.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:10:25.570875: step 21150, loss = 0.76 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:10:26.863035: step 21160, loss = 0.80 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:10:28.165098: step 21170, loss = 0.62 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:10:29.471607: step 21180, loss = 0.95 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:10:30.750002: step 21190, loss = 0.81 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:10:32.126224: step 21200, loss = 0.79 (930.1 examples/sec; 0.138 sec/batch)
2017-05-08 19:10:33.321965: step 21210, loss = 0.82 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-08 19:10:34.626888: step 21220, loss = 0.78 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:10:35.933370: step 21230, loss = 0.84 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:10:37.241064: step 21240, loss = 0.71 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:10:38.550128: step 21250, loss = 0.62 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:10:39.856748: step 21260, loss = 0.81 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:10:41.161334: step 21270, loss = 0.86 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:10:42.436723: step 21280, loss = 0.73 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:10:43.717256: step 21290, loss = 0.99 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:10:45.105134: step 21300, loss = 0.91 (922.3 examples/sec; 0.139 sec/batch)
2017-05-08 19:10:46.306615: step 21310, loss = 0.74 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-08 19:10:47.595954: step 21320, loss = 0.80 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:10:48.906796: step 21330, loss = 0.81 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:10:50.193795: step 21340, loss = 0.83 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:10:51.491092: step 21350, loss = 0.77 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:10:52.778331: step 21360, loss = 0.90 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:10:54.097297: step 21370, loss = 0.78 (970.5 examples/sec; 0.132 sec/batch)
2017-05-08 19:10:55.385372: step 21380, loss = 0.68 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:10:56.708660: step 21390, loss = 0.76 (967.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:10:58.086725: step 21400, loss = 0.86 (928.8 examples/sec; 0.138 sec/batch)
2017-05-08 19:10:59.282656: step 21410, loss = 0.72 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-08 19:11:00.543584: step 21420, loss = 0.74 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:11:01.840970: step 21430, loss = 0.69 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:11:03.118123: step 21440, loss = 0.77 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:04.449513: step 21450, loss = 0.76 (961.4 examples/sec; 0.133 sec/batch)
2017-05-08 19:11:05.728176: step 21460, loss = 0.93 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:07.026524: step 21470, loss = 1.01 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:11:08.314978: step 21480, loss = 0.76 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:11:09.608182: step 21490, loss = 0.85 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:11:10.979555: step 21500, loss = 0.75 (933.4 examples/sec; 0.137 sec/batch)
2017-05-08 19:11:12.198907: step 21510, loss = 0.74 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-08 19:11:13.469826: step 21520, loss = 0.81 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:11:14.764890: step 21530, loss = 0.83 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:11:16.061006: step 21540, loss = 0.90 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:11:17.353765: step 21550, loss = 0.84 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:11:18.640137: step 21560, loss = 0.76 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:11:19.917554: step 21570, loss = 0.84 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:21.188219: step 21580, loss = 0.82 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:11:22.453996: step 21590, loss = 0.81 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:11:23.831694: step 21600, loss = 0.76 (929.1 examples/sec; 0.138 sec/batch)
2017-05-08 19:11:25.015269: step 21610, loss = 0.88 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-08 19:11:26.294163: step 21620, loss = 0.82 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:27.555071: step 21630, loss = 0.79 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:11:28.838292: step 21640, loss = 0.76 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:30.104762: step 21650, loss = 0.83 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:11:31.396108: step 21660, loss = 0.70 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:11:32.689261: step 21670, loss = 0.89 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:11:34.027722: step 21680, loss = 1.00 (956.3 examples/sec; 0.134 sec/batch)
2017-05-08 19:11:35.331198: step 21690, loss = 0.96 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:11:36.702887: step 21700, loss = 0.90 (933.2 examples/sec; 0.137 sec/batch)
2017-05-08 19:11:37.914896: step 21710, loss = 0.88 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-08 19:11:39.190339: step 21720, loss = 0.77 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:40.488043: step 21730, loss = 0.81 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:11:41.767948: step 21740, loss = 0.82 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:43.059249: step 21750, loss = 0.74 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:11:44.339917: step 21760, loss = 0.86 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:45.621601: step 21770, loss = 0.73 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:46.930414: step 21780, loss = 0.85 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:11:48.228963: step 21790, loss = 0.84 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:11:49.599746: step 21800, loss = 0.78 (933.8 examples/sec; 0.137 sec/batch)
2017-05-08 19:11:50.793399: step 21810, loss = 0.76 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-08 19:11:52.054756: step 21820, loss = 0.78 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:11:53.351518: step 21830, loss = 1.02 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:11:54.648073: step 21840, loss = 0.93 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:11:55.965210: step 21850, loss = 0.82 (971.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:11:57.227891: step 21860, loss = 0.80 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:11:58.499935: step 21870, loss = 0.76 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:11:59.829788: step 21880, loss = 0.75 (962.5 examples/sec; 0.133 sec/batch)
2017-05-08 19:12:01.111964: step 21890, loss = 0.93 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:12:02.491423: step 21900, loss = 0.88 (927.9 examples/sec; 0.138 sec/batch)
2017-05-08 19:12:03.691838: step 21910, loss = 0.81 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-08 19:12:04.954113: step 21920, loss = 0.67 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:12:06.259103: step 21930, loss = 1.01 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:12:07.534930: step 21940, loss = 0.83 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:12:08.849086: step 21950, loss = 0.70 (974.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:12:10.153300: step 21960, loss = 0.93 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:12:11.451347: step 21970, loss = 0.73 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:12:12.737806: step 21980, loss = 0.84 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:12:14.027993: step 21990, loss = 0.77 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:12:15.447485: step 22000, loss = 0.78 (901.8 examples/sec; 0.142 sec/batch)
2017-05-08 19:12:16.662186: step 22010, loss = 0.93 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-08 19:12:17.961891: step 22020, loss = 0.68 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:12:19.241927: step 22030, loss = 0.76 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:12:20.497186: step 22040, loss = 0.76 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:12:21.793918: step 22050, loss = 0.86 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:12:23.116503: step 22060, loss = 0.83 (967.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:12:24.422853: step 22070, loss = 0.88 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:12:25.700645: step 22080, loss = 0.88 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:12:26.987935: step 22090, loss = 0.69 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:12:28.428138: step 22100, loss = 0.63 (888.8 examples/sec; 0.144 sec/batch)
2017-05-08 19:12:29.595558: step 22110, loss = 0.92 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-08 19:12:30.871580: step 22120, loss = 0.81 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:12:32.146712: step 22130, loss = 0.68 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:12:33.439309: step 22140, loss = 0.78 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:12:34.770570: step 22150, loss = 0.76 (961.5 examples/sec; 0.133 sec/batch)
2017-05-08 19:12:36.027745: step 22160, loss = 0.78 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:12:37.296019: step 22170, loss = 0.79 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:12:38.552130: step 22180, loss = 0.83 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:12:39.803599: step 22190, loss = 0.83 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:12:41.210897: step 22200, loss = 0.80 (909.5 examples/sec; 0.141 sec/batch)
2017-05-08 19:12:42.404107: step 22210, loss = 0.79 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-08 19:12:43.687430: step 22220, loss = 0.64 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:12:44.990180: step 22230, loss = 0.71 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:12:46.297805: step 22240, loss = 0.83 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:12:47.599608: step 22250, loss = 0.76 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:12:48.920570: step 22260, loss = 0.82 (969.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:12:50.242615: step 22270, loss = 0.83 (968.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:12:51.511253: step 22280, loss = 0.86 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:12:52.822482: step 22290, loss = 0.84 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:12:54.227634: step 22300, loss = 0.76 (910.9 examples/sec; 0.141 sec/batch)
2017-05-08 19:12:55.426507: step 22310, loss = 1.05 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-08 19:12:56.707877: step 22320, loss = 0.76 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:12:57.992862: step 22330, loss = 0.79 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:12:59.271542: step 22340, loss = 0.77 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:13:00.565552: step 22350, loss = 0.88 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:13:01.865167: step 22360, loss = 0.70 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:13:03.135877: step 22370, loss = 0.82 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:13:04.460033: step 22380, loss = 0.95 (966.7 examples/sec; 0.132 sec/batch)
2017-05-08 19:13:05.727396: step 22390, loss = 0.82 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:13:07.151526: step 22400, loss = 0.86 (898.8 examples/sec; 0.142 sec/batch)
2017-05-08 19:13:08.356203: step 22410, loss = 0.71 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-08 19:13:09.661697: step 22420, loss = 0.77 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:13:10.967713: step 22430, loss = 0.88 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:13:12.238158: step 22440, loss = 0.82 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:13:13.517898: step 22450, loss = 0.73 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:13:14.789817: step 22460, loss = 0.89 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:13:16.062667: step 22470, loss = 0.79 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:13:17.357057: step 22480, loss = 0.89 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:13:18.631359: step 22490, loss = 0.83 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:13:20.025967: step 22500, loss = 0.79 (917.8 examples/sec; 0.139 sec/batch)
2017-05-08 19:13:21.237921: step 22510, loss = 0.80 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-08 19:13:22.540102: step 22520, loss = 0.84 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:13:23.861400: step 22530, loss = 0.65 (968.7 examples/sec; 0.132 sec/batch)
2017-05-08 19:13:25.151886: step 22540, loss = 0.81 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:13:26.458487: step 22550, loss = 0.82 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:13:27.761235: step 22560, loss = 0.78 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:13:29.041517: step 22570, loss = 0.67 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:13:30.321170: step 22580, loss = 0.69 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:13:31.625041: step 22590, loss = 0.87 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:13:33.010119: step 22600, loss = 0.75 (924.1 examples/sec; 0.139 sec/batch)
2017-05-08 19:13:34.226407: step 22610, loss = 0.71 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-08 19:13:35.476978: step 22620, loss = 1.01 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:13:36.759650: step 22630, loss = 0.70 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:13:38.060799: step 22640, loss = 0.87 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:13:39.347630: step 22650, loss = 0.76 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:13:40.646121: step 22660, loss = 0.68 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:13:41.940126: step 22670, loss = 0.87 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:13:43.226585: step 22680, loss = 0.75 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:13:44.501977: step 22690, loss = 0.84 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:13:45.899647: step 22700, loss = 0.93 (915.8 examples/sec; 0.140 sec/batch)
2017-05-08 19:13:47.157467: step 22710, loss = 0.82 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:13:48.412297: step 22720, loss = 0.82 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:13:49.735396: step 22730, loss = 0.84 (967.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:13:51.035679: step 22740, loss = 0.79 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:13:52.326738: step 22750, loss = 0.69 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:13:53.607961: step 22760, loss = 0.75 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:13:54.875705: step 22770, loss = 0.86 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:13:56.141796: step 22780, loss = 1.04 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:13:57.432149: step 22790, loss = 0.82 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:13:58.810873: step 22800, loss = 0.81 (928.4 examples/sec; 0.138 sec/batch)
2017-05-08 19:14:00.009366: step 22810, loss = 0.82 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-08 19:14:01.258834: step 22820, loss = 0.78 (1024.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:14:02.532123: step 22830, loss = 0.70 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:03.792796: step 22840, loss = 0.85 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:14:05.072726: step 22850, loss = 0.89 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:14:06.373626: step 22860, loss = 0.88 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:14:07.631037: step 22870, loss = 0.93 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:14:08.946183: step 22880, loss = 0.91 (973.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:14:10.251844: step 22890, loss = 0.80 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:14:11.607495: step 22900, loss = 0.77 (944.2 examples/sec; 0.136 sec/batch)
2017-05-08 19:14:12.803528: step 22910, loss = 0.85 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-08 19:14:14.093997: step 22920, loss = 0.71 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:14:15.377306: step 22930, loss = 0.88 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:14:16.651840: step 22940, loss = 0.81 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:17.983532: step 22950, loss = 0.74 (961.2 examples/sec; 0.133 sec/batch)
2017-05-08 19:14:19.228708: step 22960, loss = 0.74 (1028.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:14:20.503579: step 22970, loss = 0.72 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:21.772994: step 22980, loss = 0.80 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:23.094863: step 22990, loss = 0.73 (968.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:14:24.466951: step 23000, loss = 0.88 (932.9 examples/sec; 0.137 sec/batch)
2017-05-08 19:14:25.671120: step 23010, loss = 0.81 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-08 19:14:26.940828: step 23020, loss = 0.63 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:28.206354: step 23030, loss = 0.88 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:29.483269: step 23040, loss = 0.77 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:14:30.747429: step 23050, loss = 0.86 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:14:32.035236: step 23060, loss = 0.66 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:14:33.330787: step 23070, loss = 0.66 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:14:34.604648: step 23080, loss = 0.66 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:35.921637: step 23090, loss = 0.77 (971.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:14:37.288482: step 23100, loss = 0.79 (936.5 examples/sec; 0.137 sec/batch)
2017-05-08 19:14:38.562489: step 23110, loss = 0.89 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:39.765070: step 23120, loss = 0.72 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-08 19:14:41.011714: step 23130, loss = 0.73 (1026.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:14:42.311481: step 23140, loss = 0.79 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:14:43.586617: step 23150, loss = 0.85 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:14:44.848804: step 23160, loss = 0.86 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:14:46.141554: step 23170, loss = 0.75 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:14:47.410407: step 23180, loss = 0.91 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:48.691859: step 23190, loss = 0.74 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:14:50.099017: step 23200, loss = 0.56 (909.6 examples/sec; 0.141 sec/batch)
2017-05-08 19:14:51.297576: step 23210, loss = 0.73 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-08 19:14:52.563879: step 23220, loss = 0.64 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:53.846507: step 23230, loss = 0.91 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:14:55.139721: step 23240, loss = 0.83 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:14:56.412123: step 23250, loss = 0.81 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:57.718139: step 23260, loss = 0.81 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:14:59.048013: step 23270, loss = 0.89 (962.5 examples/sec; 0.133 sec/batch)
2017-05-08 19:15:00.323325: step 23280, loss = 0.82 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:15:01.658178: step 23290, loss = 0.92 (958.9 examples/sec; 0.133 sec/batch)
2017-05-08 19:15:03.033335: step 23300, loss = 0.72 (930.8 examples/sec; 0.138 sec/batch)
2017-05-08 19:15:04.201752: step 23310, loss = 0.63 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-08 19:15:05.487834: step 23320, loss = 0.83 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:15:06.785782: step 23330, loss = 0.79 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:15:08.060908: step 23340, loss = 0.79 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:15:09.356502: step 23350, loss = 0.75 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:15:10.657493: step 23360, loss = 0.77 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:15:11.979806: step 23370, loss = 0.90 (968.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:15:13.254694: step 23380, loss = 0.74 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:15:14.555926: step 23390, loss = 0.61 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:15:15.949229: step 23400, loss = 0.71 (918.7 examples/sec; 0.139 sec/batch)
2017-05-08 19:15:17.133408: step 23410, loss = 0.91 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-08 19:15:18.445851: step 23420, loss = 0.79 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:15:19.727402: step 23430, loss = 0.79 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:15:20.988420: step 23440, loss = 0.94 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:15:22.266713: step 23450, loss = 0.70 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:15:23.539342: step 23460, loss = 0.86 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:15:24.796584: step 23470, loss = 0.70 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:15:26.091936: step 23480, loss = 0.83 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:15:27.348734: step 23490, loss = 0.74 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:15:28.733662: step 23500, loss = 0.84 (924.2 examples/sec; 0.138 sec/batch)
2017-05-08 19:15:29.931257: step 23510, loss = 0.94 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-08 19:15:31.222086: step 23520, loss = 0.88 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:15:32.516239: step 23530, loss = 0.79 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:15:33.804691: step 23540, loss = 0.97 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:15:35.117855: step 23550, loss = 0.73 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:15:36.384266: step 23560, loss = 0.69 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:15:37.671195: step 23570, loss = 0.93 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:15:38.981792: step 23580, loss = 0.88 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:15:40.281168: step 23590, loss = 0.82 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:15:41.706968: step 23600, loss = 0.79 (897.7 examples/sec; 0.143 sec/batch)
2017-05-08 19:15:42.889729: step 23610, loss = 0.91 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-08 19:15:44.172510: step 23620, loss = 0.78 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:15:45.447278: step 23630, loss = 0.65 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:15:46.767556: step 23640, loss = 0.79 (969.5 examples/sec; 0.132 sec/batch)
2017-05-08 19:15:48.057637: step 23650, loss = 0.71 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:15:49.335587: step 23660, loss = 0.87 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:15:50.625510: step 23670, loss = 0.91 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:15:51.936672: step 23680, loss = 0.71 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:15:53.208731: step 23690, loss = 0.87 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:15:54.583801: step 23700, loss = 0.78 (930.9 examples/sec; 0.138 sec/batch)
2017-05-08 19:15:55.755907: step 23710, loss = 0.82 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-08 19:15:57.032086: step 23720, loss = 0.72 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:15:58.313583: step 23730, loss = 0.75 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:15:59.643873: step 23740, loss = 0.74 (962.2 examples/sec; 0.133 sec/batch)
2017-05-08 19:16:00.940856: step 23750, loss = 0.88 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:16:02.235910: step 23760, loss = 0.61 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:16:03.503641: step 23770, loss = 0.75 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:16:04.795789: step 23780, loss = 0.69 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:16:06.097924: step 23790, loss = 0.80 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:16:07.464902: step 23800, loss = 0.72 (936.4 examples/sec; 0.137 sec/batch)
2017-05-08 19:16:08.668725: step 23810, loss = 0.96 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-08 19:16:10.002550: step 23820, loss = 0.96 (959.6 examples/sec; 0.133 sec/batch)
2017-05-08 19:16:11.279436: step 23830, loss = 0.94 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:12.556728: step 23840, loss = 0.66 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:13.848542: step 23850, loss = 0.92 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:16:15.139751: step 23860, loss = 0.68 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:16:16.408432: step 23870, loss = 0.73 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:16:17.701337: step 23880, loss = 0.89 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:16:18.970990: step 23890, loss = 0.73 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:16:20.344883: step 23900, loss = 0.95 (931.7 examples/sec; 0.137 sec/batch)
2017-05-08 19:16:21.522363: step 23910, loss = 0.95 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-08 19:16:22.825110: step 23920, loss = 0.74 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:16:24.137798: step 23930, loss = 0.69 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:16:25.412768: step 23940, loss = 0.72 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:16:26.707370: step 23950, loss = 0.92 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:16:27.965745: step 23960, loss = 0.65 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:16:29.245500: step 23970, loss = 0.59 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:30.551496: step 23980, loss = 0.76 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:16:31.834091: step 23990, loss = 0.71 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:33.210738: step 24000, loss = 0.87 (929.8 examples/sec; 0.138 sec/batch)
2017-05-08 19:16:34.405527: step 24010, loss = 0.82 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-08 19:16:35.693110: step 24020, loss = 0.73 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:16:37.001853: step 24030, loss = 0.83 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:16:38.293046: step 24040, loss = 0.74 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:16:39.582264: step 24050, loss = 0.83 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:16:40.900045: step 24060, loss = 0.75 (971.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:16:42.192117: step 24070, loss = 0.86 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:16:43.510132: step 24080, loss = 0.96 (971.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:16:44.770960: step 24090, loss = 0.81 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:16:46.147007: step 24100, loss = 0.81 (930.2 examples/sec; 0.138 sec/batch)
2017-05-08 19:16:47.337896: step 24110, loss = 0.88 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-08 19:16:48.605602: step 24120, loss = 0.86 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:16:49.885697: step 24130, loss = 0.84 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:51.154083: step 24140, loss = 0.67 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:16:52.479135: step 24150, loss = 0.77 (966.0 examples/sec; 0.133 sec/batch)
2017-05-08 19:16:53.742958: step 24160, loss = 0.71 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:16:55.026388: step 24170, loss = 0.78 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:56.295084: step 24180, loss = 0.85 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:16:57.581531: step 24190, loss = 0.73 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:16:58.958671: step 24200, loss = 0.81 (929.5 examples/sec; 0.138 sec/batch)
2017-05-08 19:17:00.146666: step 24210, loss = 0.81 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-08 19:17:01.443473: step 24220, loss = 0.93 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:17:02.775998: step 24230, loss = 0.83 (960.6 examples/sec; 0.133 sec/batch)
2017-05-08 19:17:04.045224: step 24240, loss = 0.99 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:17:05.319962: step 24250, loss = 0.69 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:17:06.611416: step 24260, loss = 0.83 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:17:07.878677: step 24270, loss = 0.81 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:17:09.167632: step 24280, loss = 0.86 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:17:10.433150: step 24290, loss = 0.68 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:17:11.782378: step 24300, loss = 0.81 (948.7 examples/sec; 0.135 sec/batch)
2017-05-08 19:17:12.978584: step 24310, loss = 0.75 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-08 19:17:14.253647: step 24320, loss = 0.73 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:15.543227: step 24330, loss = 0.72 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:17:16.852790: step 24340, loss = 0.73 (977.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:17:18.131785: step 24350, loss = 0.82 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:19.424921: step 24360, loss = 0.71 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:17:20.716203: step 24370, loss = 0.97 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:17:22.021892: step 24380, loss = 0.72 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:17:23.282855: step 24390, loss = 0.84 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:17:24.664661: step 24400, loss = 0.84 (926.3 examples/sec; 0.138 sec/batch)
2017-05-08 19:17:25.844843: step 24410, loss = 0.93 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-08 19:17:27.115065: step 24420, loss = 0.68 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:17:28.381602: step 24430, loss = 0.76 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:17:29.659596: step 24440, loss = 0.80 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:30.938584: step 24450, loss = 0.66 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:32.235595: step 24460, loss = 0.74 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:17:33.559084: step 24470, loss = 0.66 (967.1 examples/sec; 0.132 sec/batch)
2017-05-08 19:17:34.844762: step 24480, loss = 0.73 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:17:36.121052: step 24490, loss = 0.80 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:37.475875: step 24500, loss = 0.87 (944.8 examples/sec; 0.135 sec/batch)
2017-05-08 19:17:38.773331: step 24510, loss = 0.91 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:17:39.968364: step 24520, loss = 0.99 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-08 19:17:41.233936: step 24530, loss = 0.77 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:17:42.490176: step 24540, loss = 0.62 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:17:43.758493: step 24550, loss = 0.82 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:17:45.058903: step 24560, loss = 0.86 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:17:46.339430: step 24570, loss = 0.70 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:47.585837: step 24580, loss = 0.77 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:17:48.869075: step 24590, loss = 0.82 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:50.282593: step 24600, loss = 0.81 (905.5 examples/sec; 0.141 sec/batch)
2017-05-08 19:17:51.458206: step 24610, loss = 0.82 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-08 19:17:52.749804: step 24620, loss = 0.72 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:17:54.019108: step 24630, loss = 0.70 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:17:55.309845: step 24640, loss = 0.57 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:17:56.576805: step 24650, loss = 0.71 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:17:57.860960: step 24660, loss = 0.98 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:59.133588: step 24670, loss = 0.83 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:18:00.387131: step 24680, loss = 0.78 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:18:01.649605: step 24690, loss = 0.70 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:18:03.026867: step 24700, loss = 0.85 (929.4 examples/sec; 0.138 sec/batch)
2017-05-08 19:18:04.234177: step 24710, loss = 0.80 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-08 19:18:05.506859: step 24720, loss = 0.77 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:18:06.784587: step 24730, loss = 0.90 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:18:08.057820: step 24740, loss = 0.85 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:18:09.370538: step 24750, loss = 0.82 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:18:10.712521: step 24760, loss = 0.93 (953.8 examples/sec; 0.134 sec/batch)
2017-05-08 19:18:11.992534: step 24770, loss = 0.86 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:18:13.268066: step 24780, loss = 0.65 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:18:14.531220: step 24790, loss = 0.79 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:18:15.896685: step 24800, loss = 0.78 (937.4 examples/sec; 0.137 sec/batch)
2017-05-08 19:18:17.076986: step 24810, loss = 0.74 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-08 19:18:18.391788: step 24820, loss = 0.73 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:18:19.712665: step 24830, loss = 0.81 (969.1 examples/sec; 0.132 sec/batch)
2017-05-08 19:18:21.058229: step 24840, loss = 0.67 (951.3 examples/sec; 0.135 sec/batch)
2017-05-08 19:18:22.354722: step 24850, loss = 0.81 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:18:23.646231: step 24860, loss = 0.70 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:18:24.998989: step 24870, loss = 0.76 (946.2 examples/sec; 0.135 sec/batch)
2017-05-08 19:18:26.261619: step 24880, loss = 0.66 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:18:27.622203: step 24890, loss = 0.76 (940.8 examples/sec; 0.136 sec/batch)
2017-05-08 19:18:29.012035: step 24900, loss = 0.89 (921.0 examples/sec; 0.139 sec/batch)
2017-05-08 19:18:30.214726: step 24910, loss = 0.87 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-08 19:18:31.536334: step 24920, loss = 0.82 (968.5 examples/sec; 0.132 sec/batch)
2017-05-08 19:18:32.866773: step 24930, loss = 0.71 (962.1 examples/sec; 0.133 sec/batch)
2017-05-08 19:18:34.138744: step 24940, loss = 0.86 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:18:35.400684: step 24950, loss = 0.84 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:18:36.698720: step 24960, loss = 0.78 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:18:38.057730: step 24970, loss = 0.81 (941.9 examples/sec; 0.136 sec/batch)
2017-05-08 19:18:39.365660: step 24980, loss = 0.79 (978.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:18:40.689846: step 24990, loss = 0.70 (966.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:18:42.063939: step 25000, loss = 0.86 (931.5 examples/sec; 0.137 sec/batch)
2017-05-08 19:18:43.258870: step 25010, loss = 0.81 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-08 19:18:44.630548: step 25020, loss = 0.94 (933.2 examples/sec; 0.137 sec/batch)
2017-05-08 19:18:45.956820: step 25030, loss = 0.68 (965.1 examples/sec; 0.133 sec/batch)
2017-05-08 19:18:47.205794: step 25040, loss = 0.81 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:18:48.487739: step 25050, loss = 0.77 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:18:49.781101: step 25060, loss = 0.86 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:18:51.067565: step 25070, loss = 0.66 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:18:52.353380: step 25080, loss = 0.86 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:18:53.609051: step 25090, loss = 0.74 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:18:54.999176: step 25100, loss = 0.72 (920.8 examples/sec; 0.139 sec/batch)
2017-05-08 19:18:56.196667: step 25110, loss = 0.69 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-08 19:18:57.494202: step 25120, loss = 0.82 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:18:58.836846: step 25130, loss = 0.82 (953.3 examples/sec; 0.134 sec/batch)
2017-05-08 19:19:00.114106: step 25140, loss = 0.73 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:19:01.420236: step 25150, loss = 0.74 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:19:02.711730: step 25160, loss = 0.84 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:19:03.981245: step 25170, loss = 0.81 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:19:05.313787: step 25180, loss = 0.75 (960.6 examples/sec; 0.133 sec/batch)
2017-05-08 19:19:06.609971: step 25190, loss = 0.77 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:19:07.964387: step 25200, loss = 0.90 (945.0 examples/sec; 0.135 sec/batch)
2017-05-08 19:19:09.189583: step 25210, loss = 0.78 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-08 19:19:10.529602: step 25220, loss = 0.68 (955.2 examples/sec; 0.134 sec/batch)
2017-05-08 19:19:11.822003: step 25230, loss = 0.71 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:19:13.151203: step 25240, loss = 0.75 (963.0 examples/sec; 0.133 sec/batch)
2017-05-08 19:19:14.422310: step 25250, loss = 0.84 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:19:15.667780: step 25260, loss = 0.65 (1027.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:19:16.979301: step 25270, loss = 0.90 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:19:18.249120: step 25280, loss = 0.81 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:19:19.522885: step 25290, loss = 0.77 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:19:20.886903: step 25300, loss = 0.67 (938.4 examples/sec; 0.136 sec/batch)
2017-05-08 19:19:22.057201: step 25310, loss = 0.88 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-08 19:19:23.318676: step 25320, loss = 0.81 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:19:24.593344: step 25330, loss = 0.81 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:19:25.879126: step 25340, loss = 0.61 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:19:27.177751: step 25350, loss = 0.87 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:19:28.485880: step 25360, loss = 0.75 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:19:29.805418: step 25370, loss = 0.85 (970.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:19:31.100422: step 25380, loss = 0.88 (988.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:19:32.404653: step 25390, loss = 0.85 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:19:33.781757: step 25400, loss = 0.79 (929.5 examples/sec; 0.138 sec/batch)
2017-05-08 19:19:35.025673: step 25410, loss = 0.80 (1029.0 examples/sec; 0.124 sec/batch)
2017-05-08 19:19:36.334804: step 25420, loss = 0.75 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:19:37.592940: step 25430, loss = 0.75 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:19:38.878604: step 25440, loss = 0.82 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:19:40.143701: step 25450, loss = 0.89 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:19:41.408418: step 25460, loss = 0.71 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:19:42.726978: step 25470, loss = 1.00 (970.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:19:43.993024: step 25480, loss = 0.82 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:19:45.297143: step 25490, loss = 0.75 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:19:46.670783: step 25500, loss = 0.78 (931.8 examples/sec; 0.137 sec/batch)
2017-05-08 19:19:47.880592: step 25510, loss = 0.79 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-08 19:19:49.184399: step 25520, loss = 0.89 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:19:50.494067: step 25530, loss = 0.75 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:19:51.787382: step 25540, loss = 0.74 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:19:53.083209: step 25550, loss = 0.78 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:19:54.385485: step 25560, loss = 0.65 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:19:55.673494: step 25570, loss = 0.73 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:19:56.938048: step 25580, loss = 0.71 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:19:58.214551: step 25590, loss = 0.88 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:19:59.594819: step 25600, loss = 0.72 (927.4 examples/sec; 0.138 sec/batch)
2017-05-08 19:20:00.814219: step 25610, loss = 0.80 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-08 19:20:02.142489: step 25620, loss = 0.78 (963.7 examples/sec; 0.133 sec/batch)
2017-05-08 19:20:03.414631: step 25630, loss = 0.99 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:20:04.695067: step 25640, loss = 0.81 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:20:05.999204: step 25650, loss = 0.79 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:20:07.306285: step 25660, loss = 0.73 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:20:08.608854: step 25670, loss = 0.78 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:20:09.881832: step 25680, loss = 0.71 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:20:11.183270: step 25690, loss = 0.79 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:20:12.590366: step 25700, loss = 0.72 (909.7 examples/sec; 0.141 sec/batch)
2017-05-08 19:20:13.805720: step 25710, loss = 0.69 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-08 19:20:15.084630: step 25720, loss = 0.67 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:20:16.400201: step 25730, loss = 0.73 (973.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:20:17.664044: step 25740, loss = 0.97 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:20:18.924665: step 25750, loss = 0.77 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:20:20.221222: step 25760, loss = 0.70 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:20:21.538523: step 25770, loss = 0.86 (971.7 examples/sec; 0.132 sec/batch)
2017-05-08 19:20:22.827722: step 25780, loss = 0.89 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:20:24.120544: step 25790, loss = 0.77 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:20:25.487870: step 25800, loss = 0.69 (936.1 examples/sec; 0.137 sec/batch)
2017-05-08 19:20:26.674066: step 25810, loss = 0.67 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-08 19:20:27.969512: step 25820, loss = 0.65 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:20:29.262603: step 25830, loss = 0.77 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:20:30.577059: step 25840, loss = 0.83 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:20:31.847382: step 25850, loss = 0.85 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:20:33.159901: step 25860, loss = 0.64 (975.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:20:34.483601: step 25870, loss = 0.75 (967.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:20:35.775182: step 25880, loss = 0.91 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:20:37.052698: step 25890, loss = 0.75 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:20:38.450027: step 25900, loss = 0.68 (916.0 examples/sec; 0.140 sec/batch)
2017-05-08 19:20:39.607838: step 25910, loss = 0.92 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-08 19:20:40.881058: step 25920, loss = 0.83 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:20:42.152254: step 25930, loss = 0.69 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:20:43.440533: step 25940, loss = 0.65 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:20:44.717735: step 25950, loss = 0.85 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:20:46.040903: step 25960, loss = 0.80 (967.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:20:47.348917: step 25970, loss = 0.80 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:20:48.624191: step 25980, loss = 0.94 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:20:49.917505: step 25990, loss = 0.81 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:20:51.284747: step 26000, loss = 0.79 (936.2 examples/sec; 0.137 sec/batch)
2017-05-08 19:20:52.506024: step 26010, loss = 0.88 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-08 19:20:53.806927: step 26020, loss = 0.81 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:20:55.105220: step 26030, loss = 0.79 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:20:56.372768: step 26040, loss = 0.76 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:20:57.644847: step 26050, loss = 0.99 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:20:58.987872: step 26060, loss = 0.81 (953.1 examples/sec; 0.134 sec/batch)
2017-05-08 19:21:00.277362: step 26070, loss = 0.89 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:21:01.552422: step 26080, loss = 0.87 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:21:02.873493: step 26090, loss = 0.77 (968.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:21:04.247541: step 26100, loss = 0.76 (931.6 examples/sec; 0.137 sec/batch)
2017-05-08 19:21:05.418264: step 26110, loss = 0.87 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-08 19:21:06.728821: step 26120, loss = 0.86 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:21:08.024030: step 26130, loss = 0.93 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:21:09.298751: step 26140, loss = 0.60 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:10.567175: step 26150, loss = 0.84 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:11.874865: step 26160, loss = 0.75 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:21:13.164153: step 26170, loss = 0.83 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:21:14.472497: step 26180, loss = 0.87 (978.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:21:15.770932: step 26190, loss = 0.68 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:21:17.176510: step 26200, loss = 0.86 (910.7 examples/sec; 0.141 sec/batch)
2017-05-08 19:21:18.350711: step 26210, loss = 0.72 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-08 19:21:19.669580: step 26220, loss = 0.96 (970.5 examples/sec; 0.132 sec/batch)
2017-05-08 19:21:20.959580: step 26230, loss = 0.85 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:21:22.223694: step 26240, loss = 0.77 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:21:23.506630: step 26250, loss = 0.98 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:21:24.784301: step 26260, loss = 0.85 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:21:26.059260: step 26270, loss = 0.83 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:27.361184: step 26280, loss = 0.95 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:21:28.638659: step 26290, loss = 0.95 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:21:30.050611: step 26300, loss = 0.91 (906.5 examples/sec; 0.141 sec/batch)
2017-05-08 19:21:31.218596: step 26310, loss = 0.76 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-08 19:21:32.533834: step 26320, loss = 0.88 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:21:33.796592: step 26330, loss = 0.97 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:21:35.066580: step 26340, loss = 0.92 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:36.352930: step 26350, loss = 0.82 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:21:37.637501: step 26360, loss = 0.79 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:21:38.930475: step 26370, loss = 0.60 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:21:40.228116: step 26380, loss = 0.84 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:21:41.500050: step 26390, loss = 0.64 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:42.906225: step 26400, loss = 0.66 (910.3 examples/sec; 0.141 sec/batch)
2017-05-08 19:21:44.093730: step 26410, loss = 0.71 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-08 19:21:45.372399: step 26420, loss = 0.76 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:21:46.663882: step 26430, loss = 1.02 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:21:47.937863: step 26440, loss = 0.85 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:49.235411: step 26450, loss = 0.82 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:21:50.527324: step 26460, loss = 0.89 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:21:51.846823: step 26470, loss = 0.74 (970.1 examples/sec; 0.132 sec/batch)
2017-05-08 19:21:53.139857: step 26480, loss = 0.75 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:21:54.456686: step 26490, loss = 0.93 (972.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:21:55.852219: step 26500, loss = 0.74 (917.2 examples/sec; 0.140 sec/batch)
2017-05-08 19:21:57.158047: step 26510, loss = 0.78 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:21:58.398565: step 26520, loss = 0.62 (1031.8 examples/sec; 0.124 sec/batch)
2017-05-08 19:21:59.736619: step 26530, loss = 0.84 (956.6 examples/sec; 0.134 sec/batch)
2017-05-08 19:22:01.022893: step 26540, loss = 0.77 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:22:02.336890: step 26550, loss = 0.65 (974.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:22:03.585718: step 26560, loss = 0.75 (1025.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:22:04.843147: step 26570, loss = 0.76 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:22:06.133494: step 26580, loss = 0.68 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:22:07.446793: step 26590, loss = 0.74 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:22:08.842683: step 26600, loss = 0.92 (917.0 examples/sec; 0.140 sec/batch)
2017-05-08 19:22:10.027941: step 26610, loss = 0.62 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-08 19:22:11.306923: step 26620, loss = 0.76 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:22:12.600943: step 26630, loss = 0.67 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:22:13.893009: step 26640, loss = 0.77 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:22:15.196002: step 26650, loss = 0.95 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:22:16.458924: step 26660, loss = 0.81 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:22:17.748880: step 26670, loss = 0.66 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:22:19.048675: step 26680, loss = 0.76 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:22:20.338016: step 26690, loss = 0.72 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:22:21.722269: step 26700, loss = 0.86 (924.7 examples/sec; 0.138 sec/batch)
2017-05-08 19:22:22.897702: step 26710, loss = 0.92 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-08 19:22:24.164247: step 26720, loss = 0.87 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:22:25.461875: step 26730, loss = 0.78 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:22:26.732400: step 26740, loss = 0.79 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:22:28.043658: step 26750, loss = 0.84 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:22:29.339366: step 26760, loss = 0.80 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:22:30.654554: step 26770, loss = 0.79 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:22:31.956031: step 26780, loss = 0.67 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:22:33.255382: step 26790, loss = 0.77 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:22:34.645826: step 26800, loss = 0.92 (920.6 examples/sec; 0.139 sec/batch)
2017-05-08 19:22:35.821564: step 26810, loss = 0.81 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-08 19:22:37.098834: step 26820, loss = 0.72 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:22:38.373778: step 26830, loss = 0.70 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:22:39.658598: step 26840, loss = 0.74 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:22:40.976168: step 26850, loss = 0.66 (971.5 examples/sec; 0.132 sec/batch)
2017-05-08 19:22:42.256152: step 26860, loss = 0.80 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:22:43.558896: step 26870, loss = 0.93 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:22:44.833486: step 26880, loss = 0.93 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:22:46.147970: step 26890, loss = 0.89 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:22:47.531098: step 26900, loss = 0.66 (925.4 examples/sec; 0.138 sec/batch)
2017-05-08 19:22:48.709396: step 26910, loss = 0.80 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-08 19:22:50.006097: step 26920, loss = 0.64 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:22:51.282738: step 26930, loss = 0.73 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:22:52.600692: step 26940, loss = 0.86 (971.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:22:53.877280: step 26950, loss = 0.80 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:22:55.145140: step 26960, loss = 0.70 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:22:56.412728: step 26970, loss = 0.92 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:22:57.690153: step 26980, loss = 0.88 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:22:58.972145: step 26990, loss = 0.75 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:00.347139: step 27000, loss = 0.74 (930.9 examples/sec; 0.137 sec/batch)
2017-05-08 19:23:01.522112: step 27010, loss = 0.85 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-08 19:23:02.797557: step 27020, loss = 0.62 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:04.053770: step 27030, loss = 0.69 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:23:05.317698: step 27040, loss = 0.69 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:23:06.633825: step 27050, loss = 0.91 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:23:07.934321: step 27060, loss = 0.75 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:23:09.230592: step 27070, loss = 0.82 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:23:10.511254: step 27080, loss = 0.81 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:11.795881: step 27090, loss = 0.73 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:13.201363: step 27100, loss = 0.94 (910.7 examples/sec; 0.141 sec/batch)
2017-05-08 19:23:14.369907: step 27110, loss = 0.67 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-08 19:23:15.678176: step 27120, loss = 0.90 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:23:16.972162: step 27130, loss = 0.74 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:18.260912: step 27140, loss = 1.04 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:19.554190: step 27150, loss = 0.83 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:20.833663: step 27160, loss = 0.65 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:22.121933: step 27170, loss = 0.81 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:23.432773: step 27180, loss = 1.01 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:23:24.789739: step 27190, loss = 0.87 (943.3 examples/sec; 0.136 sec/batch)
2017-05-08 19:23:26.148095: step 27200, loss = 0.86 (942.3 examples/sec; 0.136 sec/batch)
2017-05-08 19:23:27.331902: step 27210, loss = 0.77 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-08 19:23:28.599378: step 27220, loss = 0.70 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:23:29.883137: step 27230, loss = 0.96 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:31.200266: step 27240, loss = 0.98 (971.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:23:32.451376: step 27250, loss = 0.69 (1023.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:23:33.719060: step 27260, loss = 0.81 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:23:35.001721: step 27270, loss = 0.72 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:36.284105: step 27280, loss = 0.72 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:37.573357: step 27290, loss = 0.61 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:38.994711: step 27300, loss = 0.86 (900.5 examples/sec; 0.142 sec/batch)
2017-05-08 19:23:40.192341: step 27310, loss = 0.71 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-08 19:23:41.479409: step 27320, loss = 0.73 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:42.763101: step 27330, loss = 0.72 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:44.053962: step 27340, loss = 0.64 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:45.374162: step 27350, loss = 0.79 (969.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:23:46.654266: step 27360, loss = 0.84 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:47.940062: step 27370, loss = 0.84 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:49.227125: step 27380, loss = 0.98 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:50.507168: step 27390, loss = 0.85 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:51.888730: step 27400, loss = 0.72 (926.5 examples/sec; 0.138 sec/batch)
2017-05-08 19:23:53.120552: step 27410, loss = 0.83 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-08 19:23:54.393093: step 27420, loss = 0.93 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:23:55.709667: step 27430, loss = 0.68 (972.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:23:57.025353: step 27440, loss = 0.69 (972.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:23:58.319243: step 27450, loss = 0.80 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:59.636254: step 27460, loss = 0.82 (971.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:24:00.941766: step 27470, loss = 0.84 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:24:02.244440: step 27480, loss = 0.72 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:24:03.530227: step 27490, loss = 0.72 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:24:04.907286: step 27500, loss = 0.90 (929.5 examples/sec; 0.138 sec/batch)
2017-05-08 19:24:06.117240: step 27510, loss = 0.82 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-08 19:24:07.442544: step 27520, loss = 0.72 (965.8 examples/sec; 0.133 sec/batch)
2017-05-08 19:24:08.739205: step 27530, loss = 0.74 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:24:10.037662: step 27540, loss = 0.73 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:24:11.304878: step 27550, loss = 0.65 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:24:12.584582: step 27560, loss = 0.79 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:24:13.861956: step 27570, loss = 0.68 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:24:15.167116: step 27580, loss = 0.71 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:24:16.465083: step 27590, loss = 0.75 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:24:17.852254: step 27600, loss = 0.73 (922.8 examples/sec; 0.139 sec/batch)
2017-05-08 19:24:19.083405: step 27610, loss = 0.74 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-08 19:24:20.373327: step 27620, loss = 0.72 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:24:21.653234: step 27630, loss = 0.78 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:24:22.952326: step 27640, loss = 0.90 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:24:24.206957: step 27650, loss = 0.81 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:24:25.484748: step 27660, loss = 0.86 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:24:26.765363: step 27670, loss = 0.86 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:24:28.042897: step 27680, loss = 0.52 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:24:29.330606: step 27690, loss = 0.76 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:24:30.719850: step 27700, loss = 0.69 (921.4 examples/sec; 0.139 sec/batch)
2017-05-08 19:24:31.924690: step 27710, loss = 0.83 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-08 19:24:33.222133: step 27720, loss = 0.78 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:24:34.499574: step 27730, loss = 0.56 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:24:35.847784: step 27740, loss = 0.90 (949.4 examples/sec; 0.135 sec/batch)
2017-05-08 19:24:37.148879: step 27750, loss = 0.78 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:24:38.438508: step 27760, loss = 0.82 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:24:39.711913: step 27770, loss = 0.71 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:24:41.049979: step 27780, loss = 0.82 (956.6 examples/sec; 0.134 sec/batch)
2017-05-08 19:24:42.311853: step 27790, loss = 0.79 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:24:43.676223: step 27800, loss = 0.75 (938.2 examples/sec; 0.136 sec/batch)
2017-05-08 19:24:44.860991: step 27810, loss = 0.71 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-08 19:24:46.155879: step 27820, loss = 0.76 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:24:47.442173: step 27830, loss = 0.86 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:24:48.710135: step 27840, loss = 0.61 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:24:50.010360: step 27850, loss = 0.71 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:24:51.295376: step 27860, loss = 0.65 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:24:52.603891: step 27870, loss = 0.82 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:24:53.912123: step 27880, loss = 0.87 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:24:55.187878: step 27890, loss = 0.82 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:24:56.568684: step 27900, loss = 0.82 (927.0 examples/sec; 0.138 sec/batch)
2017-05-08 19:24:57.787197: step 27910, loss = 0.74 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-08 19:24:59.033371: step 27920, loss = 0.85 (1027.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:25:00.315342: step 27930, loss = 0.74 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:01.586408: step 27940, loss = 0.73 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:02.896163: step 27950, loss = 0.88 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:25:04.163656: step 27960, loss = 0.94 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:05.442795: step 27970, loss = 0.90 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:06.700511: step 27980, loss = 0.87 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:25:07.959558: step 27990, loss = 0.69 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:25:09.333598: step 28000, loss = 0.84 (931.6 examples/sec; 0.137 sec/batch)
2017-05-08 19:25:10.554088: step 28010, loss = 0.66 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-08 19:25:11.866885: step 28020, loss = 0.75 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:25:13.199135: step 28030, loss = 0.81 (960.8 examples/sec; 0.133 sec/batch)
2017-05-08 19:25:14.480400: step 28040, loss = 0.81 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:15.760416: step 28050, loss = 0.58 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:17.083613: step 28060, loss = 0.87 (967.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:25:18.384354: step 28070, loss = 0.75 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:25:19.712886: step 28080, loss = 0.91 (963.5 examples/sec; 0.133 sec/batch)
2017-05-08 19:25:21.007852: step 28090, loss = 0.69 (988.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:25:22.427371: step 28100, loss = 0.77 (901.7 examples/sec; 0.142 sec/batch)
2017-05-08 19:25:23.654881: step 28110, loss = 0.90 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-08 19:25:24.976883: step 28120, loss = 0.71 (968.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:25:26.259602: step 28130, loss = 0.66 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:27.558176: step 28140, loss = 0.72 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:25:28.835971: step 28150, loss = 0.77 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:30.118032: step 28160, loss = 0.70 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:31.409057: step 28170, loss = 0.83 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:25:32.682169: step 28180, loss = 0.73 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:33.949686: step 28190, loss = 0.88 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:35.344554: step 28200, loss = 0.88 (917.6 examples/sec; 0.139 sec/batch)
2017-05-08 19:25:36.523765: step 28210, loss = 0.88 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-08 19:25:37.846514: step 28220, loss = 0.78 (967.7 examples/sec; 0.132 sec/batch)
2017-05-08 19:25:39.124433: step 28230, loss = 0.85 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:40.418315: step 28240, loss = 0.85 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:25:41.688733: step 28250, loss = 0.87 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:42.993533: step 28260, loss = 0.70 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:25:44.312822: step 28270, loss = 0.76 (970.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:25:45.597339: step 28280, loss = 0.77 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:46.864115: step 28290, loss = 0.73 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:48.275348: step 28300, loss = 0.81 (907.0 examples/sec; 0.141 sec/batch)
2017-05-08 19:25:49.473581: step 28310, loss = 0.91 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-08 19:25:50.748196: step 28320, loss = 0.72 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:52.025782: step 28330, loss = 0.84 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:53.302079: step 28340, loss = 0.82 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:54.585707: step 28350, loss = 0.73 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:55.854149: step 28360, loss = 0.66 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:57.140033: step 28370, loss = 0.91 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:25:58.424077: step 28380, loss = 0.72 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:59.695347: step 28390, loss = 0.74 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:26:01.052941: step 28400, loss = 0.83 (942.8 examples/sec; 0.136 sec/batch)
2017-05-08 19:26:02.271032: step 28410, loss = 0.84 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-08 19:26:03.543534: step 28420, loss = 0.63 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:26:04.864043: step 28430, loss = 0.79 (969.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:26:06.160905: step 28440, loss = 0.82 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:26:07.426129: step 28450, loss = 0.69 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:26:08.725741: step 28460, loss = 0.64 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:26:10.003976: step 28470, loss = 0.64 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:11.293007: step 28480, loss = 0.79 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:26:12.572126: step 28490, loss = 0.97 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:14.000173: step 28500, loss = 0.78 (896.3 examples/sec; 0.143 sec/batch)
2017-05-08 19:26:15.197907: step 28510, loss = 0.90 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-08 19:26:16.470277: step 28520, loss = 0.76 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:26:17.773552: step 28530, loss = 0.65 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:26:19.031822: step 28540, loss = 0.71 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:26:20.313157: step 28550, loss = 0.82 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:21.637708: step 28560, loss = 0.75 (966.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:26:22.931240: step 28570, loss = 0.87 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:26:24.212563: step 28580, loss = 0.68 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:25.515749: step 28590, loss = 0.75 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:26:26.928461: step 28600, loss = 0.63 (906.1 examples/sec; 0.141 sec/batch)
2017-05-08 19:26:28.223896: step 28610, loss = 1.12 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:26:29.416383: step 28620, loss = 0.84 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-08 19:26:30.718168: step 28630, loss = 0.83 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:26:31.977869: step 28640, loss = 0.87 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:26:33.255024: step 28650, loss = 0.65 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:34.523281: step 28660, loss = 0.83 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:26:35.802845: step 28670, loss = 0.79 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:37.094111: step 28680, loss = 0.96 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:26:38.393023: step 28690, loss = 0.91 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:26:39.760931: step 28700, loss = 0.67 (935.7 examples/sec; 0.137 sec/batch)
2017-05-08 19:26:40.976622: step 28710, loss = 0.83 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-08 19:26:42.283938: step 28720, loss = 0.80 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:26:43.585080: step 28730, loss = 0.84 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:26:44.896060: step 28740, loss = 0.84 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:26:46.181758: step 28750, loss = 0.95 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:26:47.473864: step 28760, loss = 0.69 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:26:48.799173: step 28770, loss = 0.61 (965.8 examples/sec; 0.133 sec/batch)
2017-05-08 19:26:50.070400: step 28780, loss = 1.04 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:26:51.339059: step 28790, loss = 0.64 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:26:52.717899: step 28800, loss = 0.77 (928.3 examples/sec; 0.138 sec/batch)
2017-05-08 19:26:53.910894: step 28810, loss = 0.73 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-08 19:26:55.213762: step 28820, loss = 0.69 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:26:56.473154: step 28830, loss = 0.83 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:26:57.782082: step 28840, loss = 0.90 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:26:59.084576: step 28850, loss = 0.65 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:27:00.378908: step 28860, loss = 0.79 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:27:01.650134: step 28870, loss = 0.80 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:27:02.933346: step 28880, loss = 0.87 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:27:04.234508: step 28890, loss = 0.75 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:27:05.645955: step 28900, loss = 0.77 (906.9 examples/sec; 0.141 sec/batch)
2017-05-08 19:27:06.861111: step 28910, loss = 0.68 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-08 19:27:08.141733: step 28920, loss = 0.84 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:27:09.434365: step 28930, loss = 0.78 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:27:10.726047: step 28940, loss = 0.92 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:27:12.016668: step 28950, loss = 0.74 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:27:13.300587: step 28960, loss = 0.77 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:27:14.597526: step 28970, loss = 0.82 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:27:15.918692: step 28980, loss = 0.80 (968.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:27:17.180158: step 28990, loss = 0.75 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:27:18.575223: step 29000, loss = 0.82 (917.5 examples/sec; 0.140 sec/batch)
2017-05-08 19:27:19.741353: step 29010, loss = 0.80 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-08 19:27:21.040900: step 29020, loss = 0.75 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:27:22.350286: step 29030, loss = 0.75 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:27:23.671550: step 29040, loss = 0.76 (968.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:27:24.957523: step 29050, loss = 0.89 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:27:26.269179: step 29060, loss = 0.75 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:27:27.533257: step 29070, loss = 0.73 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:27:28.780306: step 29080, loss = 0.74 (1026.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:27:30.069477: step 29090, loss = 0.90 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:27:31.463394: step 29100, loss = 0.80 (918.3 examples/sec; 0.139 sec/batch)
2017-05-08 19:27:32.669838: step 29110, loss = 0.89 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-08 19:27:33.950307: step 29120, loss = 0.73 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:27:35.237092: step 29130, loss = 0.71 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:27:36.513135: step 29140, loss = 0.73 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:27:37.789923: step 29150, loss = 0.81 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:27:39.074100: step 29160, loss = 0.89 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:27:40.377329: step 29170, loss = 1.06 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:27:41.648299: step 29180, loss = 0.85 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:27:42.962976: step 29190, loss = 0.74 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:27:44.357186: step 29200, loss = 0.76 (918.1 examples/sec; 0.139 sec/batch)
2017-05-08 19:27:45.573435: step 29210, loss = 0.77 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-08 19:27:46.840359: step 29220, loss = 0.82 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:27:48.114208: step 29230, loss = 0.83 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:27:49.399421: step 29240, loss = 0.91 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:27:50.676558: step 29250, loss = 0.75 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:27:51.961965: step 29260, loss = 0.81 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:27:53.222697: step 29270, loss = 0.79 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:27:54.525809: step 29280, loss = 0.81 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:27:55.831415: step 29290, loss = 0.75 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:27:57.225767: step 29300, loss = 0.67 (918.0 examples/sec; 0.139 sec/batch)
2017-05-08 19:27:58.396817: step 29310, loss = 0.71 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-08 19:27:59.709182: step 29320, loss = 0.83 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:28:01.010472: step 29330, loss = 0.82 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:28:02.376403: step 29340, loss = 0.70 (937.1 examples/sec; 0.137 sec/batch)
2017-05-08 19:28:03.663750: step 29350, loss = 0.84 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:28:04.991392: step 29360, loss = 0.83 (964.1 examples/sec; 0.133 sec/batch)
2017-05-08 19:28:06.270885: step 29370, loss = 0.75 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:28:07.595252: step 29380, loss = 0.77 (966.5 examples/sec; 0.132 sec/batch)
2017-05-08 19:28:08.878088: step 29390, loss = 0.89 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:28:10.242143: step 29400, loss = 0.74 (938.4 examples/sec; 0.136 sec/batch)
2017-05-08 19:28:11.429731: step 29410, loss = 0.77 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-08 19:28:12.711924: step 29420, loss = 0.65 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:28:14.022498: step 29430, loss = 0.90 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:28:15.325602: step 29440, loss = 0.70 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:28:16.601026: step 29450, loss = 0.75 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:28:17.869402: step 29460, loss = 0.76 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:28:19.143067: step 29470, loss = 0.80 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:28:20.426717: step 29480, loss = 0.97 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:28:21.705494: step 29490, loss = 0.85 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:28:23.122544: step 29500, loss = 0.75 (903.3 examples/sec; 0.142 sec/batch)
2017-05-08 19:28:24.291088: step 29510, loss = 0.71 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-08 19:28:25.605582: step 29520, loss = 1.03 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:28:26.886902: step 29530, loss = 0.80 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:28:28.160220: step 29540, loss = 0.75 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:28:29.424037: step 29550, loss = 0.96 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:28:30.756536: step 29560, loss = 0.93 (960.6 examples/sec; 0.133 sec/batch)
2017-05-08 19:28:32.023338: step 29570, loss = 0.88 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:28:33.312102: step 29580, loss = 0.72 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:28:34.592939: step 29590, loss = 0.79 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:28:35.973263: step 29600, loss = 0.91 (927.3 examples/sec; 0.138 sec/batch)
2017-05-08 19:28:37.150711: step 29610, loss = 0.95 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-08 19:28:38.451097: step 29620, loss = 0.77 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:28:39.741000: step 29630, loss = 0.84 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:28:41.034842: step 29640, loss = 0.67 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:28:42.339061: step 29650, loss = 0.86 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:28:43.618976: step 29660, loss = 0.76 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:28:44.918976: step 29670, loss = 0.79 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:28:46.227498: step 29680, loss = 0.85 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:28:47.512774: step 29690, loss = 0.94 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:28:48.903974: step 29700, loss = 0.87 (920.1 examples/sec; 0.139 sec/batch)
2017-05-08 19:28:50.084905: step 29710, loss = 0.72 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-08 19:28:51.399782: step 29720, loss = 0.77 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:28:52.661400: step 29730, loss = 0.74 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:28:53.925115: step 29740, loss = 0.65 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:28:55.224253: step 29750, loss = 0.78 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:28:56.556504: step 29760, loss = 0.87 (960.8 examples/sec; 0.133 sec/batch)
2017-05-08 19:28:57.833305: step 29770, loss = 0.73 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:28:59.107891: step 29780, loss = 0.78 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:29:00.458300: step 29790, loss = 0.80 (947.9 examples/sec; 0.135 sec/batch)
2017-05-08 19:29:01.818504: step 29800, loss = 0.90 (941.0 examples/sec; 0.136 sec/batch)
2017-05-08 19:29:03.002946: step 29810, loss = 0.70 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-08 19:29:04.273949: step 29820, loss = 0.90 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:29:05.566544: step 29830, loss = 0.93 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:29:06.877768: step 29840, loss = 0.80 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:29:08.180893: step 29850, loss = 0.92 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:29:09.478814: step 29860, loss = 0.79 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:29:10.783657: step 29870, loss = 0.66 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:29:12.058631: step 29880, loss = 0.98 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:29:13.320564: step 29890, loss = 0.62 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:29:14.692024: step 29900, loss = 0.69 (933.3 examples/sec; 0.137 sec/batch)
2017-05-08 19:29:15.867149: step 29910, loss = 0.71 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-08 19:29:17.146900: step 29920, loss = 0.66 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:29:18.442188: step 29930, loss = 0.87 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:29:19.709220: step 29940, loss = 0.67 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:29:20.990809: step 29950, loss = 0.81 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:29:22.262045: step 29960, loss = 0.73 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:29:23.517263: step 29970, loss = 0.71 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:29:24.783626: step 29980, loss = 0.82 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:29:26.047621: step 29990, loss = 0.76 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:29:27.406902: step 30000, loss = 0.74 (941.7 examples/sec; 0.136 sec/batch)
2017-05-08 19:29:28.597683: step 30010, loss = 0.82 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-08 19:29:29.885143: step 30020, loss = 0.82 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:29:31.176691: step 30030, loss = 0.78 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:29:32.484661: step 30040, loss = 0.80 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:29:33.785154: step 30050, loss = 0.66 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:29:35.075842: step 30060, loss = 0.89 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:29:36.377927: step 30070, loss = 0.90 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:29:37.656397: step 30080, loss = 0.88 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:29:38.920790: step 30090, loss = 0.71 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:29:40.281105: step 30100, loss = 0.81 (941.0 examples/sec; 0.136 sec/batch)
2017-05-08 19:29:41.509575: step 30110, loss = 0.65 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-08 19:29:42.816537: step 30120, loss = 0.82 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:29:44.154004: step 30130, loss = 0.89 (957.0 examples/sec; 0.134 sec/batch)
2017-05-08 19:29:45.455418: step 30140, loss = 0.86 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:29:46.776850: step 30150, loss = 0.78 (968.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:29:48.089627: step 30160, loss = 0.72 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:29:49.378939: step 30170, loss = 0.73 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:29:50.655114: step 30180, loss = 0.60 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:29:51.945556: step 30190, loss = 0.82 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:29:53.323490: step 30200, loss = 0.67 (928.9 examples/sec; 0.138 sec/batch)
2017-05-08 19:29:54.578420: step 30210, loss = 1.01 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:29:55.883532: step 30220, loss = 0.74 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:29:57.159546: step 30230, loss = 0.70 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:29:58.464353: step 30240, loss = 0.76 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:29:59.758821: step 30250, loss = 0.70 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:30:01.071070: step 30260, loss = 0.78 (975.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:30:02.399488: step 30270, loss = 0.71 (963.6 examples/sec; 0.133 sec/batch)
2017-05-08 19:30:03.666722: step 30280, loss = 0.86 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:30:04.924260: step 30290, loss = 0.82 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:30:06.307174: step 30300, loss = 0.71 (925.6 examples/sec; 0.138 sec/batch)
2017-05-08 19:30:07.498805: step 30310, loss = 0.86 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-08 19:30:08.820636: step 30320, loss = 0.67 (968.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:30:10.123779: step 30330, loss = 0.75 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:30:11.428333: step 30340, loss = 0.69 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:30:12.735891: step 30350, loss = 0.83 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:30:14.017882: step 30360, loss = 0.86 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:30:15.307946: step 30370, loss = 0.82 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:30:16.605245: step 30380, loss = 0.66 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:30:17.935478: step 30390, loss = 0.75 (962.2 examples/sec; 0.133 sec/batch)
2017-05-08 19:30:19.327243: step 30400, loss = 0.88 (919.7 examples/sec; 0.139 sec/batch)
2017-05-08 19:30:20.532016: step 30410, loss = 0.73 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-08 19:30:21.799605: step 30420, loss = 0.86 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:30:23.086195: step 30430, loss = 0.72 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:30:24.400855: step 30440, loss = 0.74 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:30:25.693115: step 30450, loss = 0.75 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:30:26.967213: step 30460, loss = 0.64 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:30:28.230778: step 30470, loss = 0.78 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:30:29.503938: step 30480, loss = 0.65 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:30:30.778820: step 30490, loss = 0.85 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:30:32.148570: step 30500, loss = 0.85 (934.5 examples/sec; 0.137 sec/batch)
2017-05-08 19:30:33.375407: step 30510, loss = 0.85 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-08 19:30:34.688691: step 30520, loss = 0.59 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:30:35.967952: step 30530, loss = 0.75 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:30:37.239395: step 30540, loss = 0.70 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:30:38.515561: step 30550, loss = 0.76 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:30:39.821253: step 30560, loss = 0.83 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:30:41.113561: step 30570, loss = 0.74 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:30:42.400522: step 30580, loss = 0.63 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:30:43.675953: step 30590, loss = 0.76 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:30:45.031967: step 30600, loss = 0.81 (943.9 examples/sec; 0.136 sec/batch)
2017-05-08 19:30:46.214481: step 30610, loss = 0.75 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-08 19:30:47.502152: step 30620, loss = 0.70 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:30:48.762389: step 30630, loss = 0.73 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:30:50.096015: step 30640, loss = 0.79 (959.8 examples/sec; 0.133 sec/batch)
2017-05-08 19:30:51.381889: step 30650, loss = 0.69 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:30:52.643519: step 30660, loss = 0.76 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:30:53.933981: step 30670, loss = 0.81 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:30:55.224306: step 30680, loss = 0.75 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:30:56.492694: step 30690, loss = 0.70 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:30:57.923938: step 30700, loss = 0.69 (894.3 examples/sec; 0.143 sec/batch)
2017-05-08 19:30:59.111009: step 30710, loss = 0.62 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-08 19:31:00.394248: step 30720, loss = 0.87 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:31:01.697030: step 30730, loss = 0.70 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:31:02.982951: step 30740, loss = 0.85 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:31:04.269130: step 30750, loss = 0.77 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:31:05.534562: step 30760, loss = 0.67 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:31:06.815267: step 30770, loss = 0.76 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:31:08.088918: step 30780, loss = 0.77 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:31:09.376961: step 30790, loss = 0.96 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:31:10.792605: step 30800, loss = 0.72 (904.2 examples/sec; 0.142 sec/batch)
2017-05-08 19:31:11.980425: step 30810, loss = 0.75 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-08 19:31:13.278110: step 30820, loss = 0.85 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:31:14.558634: step 30830, loss = 0.89 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:31:15.888700: step 30840, loss = 0.77 (962.4 examples/sec; 0.133 sec/batch)
2017-05-08 19:31:17.169519: step 30850, loss = 0.77 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:31:18.453758: step 30860, loss = 0.79 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:31:19.728225: step 30870, loss = 0.94 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:31:21.008967: step 30880, loss = 0.76 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:31:22.283919: step 30890, loss = 0.84 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:31:23.631841: step 30900, loss = 0.74 (949.6 examples/sec; 0.135 sec/batch)
2017-05-08 19:31:24.886194: step 30910, loss = 0.85 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:31:26.175179: step 30920, loss = 0.85 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:31:27.413229: step 30930, loss = 0.80 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-08 19:31:28.675594: step 30940, loss = 0.81 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:31:30.014437: step 30950, loss = 0.79 (956.1 examples/sec; 0.134 sec/batch)
2017-05-08 19:31:31.284387: step 30960, loss = 0.84 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:31:32.567667: step 30970, loss = 0.74 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:31:33.865436: step 30980, loss = 0.72 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:31:35.160361: step 30990, loss = 1.05 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:31:36.527099: step 31000, loss = 0.74 (936.5 examples/sec; 0.137 sec/batch)
2017-05-08 19:31:37.729064: step 31010, loss = 0.83 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-08 19:31:39.034540: step 31020, loss = 0.76 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:31:40.339886: step 31030, loss = 0.70 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:31:41.618251: step 31040, loss = 0.69 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:31:42.888641: step 31050, loss = 0.78 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:31:44.152337: step 31060, loss = 0.80 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:31:45.443711: step 31070, loss = 0.78 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:31:46.759168: step 31080, loss = 0.72 (973.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:31:48.042554: step 31090, loss = 0.67 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:31:49.439354: step 31100, loss = 0.75 (916.4 examples/sec; 0.140 sec/batch)
2017-05-08 19:31:50.655053: step 31110, loss = 0.95 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-08 19:31:51.938037: step 31120, loss = 0.87 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:31:53.244351: step 31130, loss = 0.73 (979.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:31:54.533096: step 31140, loss = 0.85 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:31:55.799989: step 31150, loss = 0.69 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:31:57.069027: step 31160, loss = 0.69 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:31:58.359685: step 31170, loss = 0.81 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:31:59.651656: step 31180, loss = 0.86 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:32:00.936304: step 31190, loss = 0.80 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:32:02.312780: step 31200, loss = 0.87 (929.9 examples/sec; 0.138 sec/batch)
2017-05-08 19:32:03.481588: step 31210, loss = 0.96 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-08 19:32:04.749331: step 31220, loss = 0.89 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:32:06.005801: step 31230, loss = 0.74 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:32:07.305847: step 31240, loss = 0.83 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:32:08.579063: step 31250, loss = 0.70 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:32:09.881601: step 31260, loss = 0.62 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:32:11.195737: step 31270, loss = 0.73 (974.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:32:12.458284: step 31280, loss = 0.65 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:32:13.737473: step 31290, loss = 0.84 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:32:15.131429: step 31300, loss = 0.83 (918.2 examples/sec; 0.139 sec/batch)
2017-05-08 19:32:16.375383: step 31310, loss = 0.81 (1029.0 examples/sec; 0.124 sec/batch)
2017-05-08 19:32:17.621026: step 31320, loss = 0.73 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:32:18.893139: step 31330, loss = 0.89 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:32:20.160609: step 31340, loss = 0.75 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:32:21.433772: step 31350, loss = 0.69 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:32:22.723766: step 31360, loss = 0.68 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:32:23.984577: step 31370, loss = 0.77 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:32:25.260068: step 31380, loss = 0.76 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:32:26.550564: step 31390, loss = 0.69 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:32:27.912506: step 31400, loss = 0.74 (939.8 examples/sec; 0.136 sec/batch)
2017-05-08 19:32:29.097795: step 31410, loss = 0.82 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-08 19:32:30.372178: step 31420, loss = 0.70 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:32:31.643226: step 31430, loss = 0.79 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:32:32.929876: step 31440, loss = 0.83 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:32:34.224888: step 31450, loss = 0.70 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:32:35.539796: step 31460, loss = 0.64 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:32:36.835111: step 31470, loss = 0.93 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:32:38.135002: step 31480, loss = 0.74 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:32:39.451302: step 31490, loss = 0.85 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:32:40.839191: step 31500, loss = 0.77 (922.3 examples/sec; 0.139 sec/batch)
2017-05-08 19:32:42.036926: step 31510, loss = 0.81 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-08 19:32:43.353965: step 31520, loss = 0.81 (971.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:32:44.641233: step 31530, loss = 0.71 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:32:45.926184: step 31540, loss = 0.74 (996.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:32:47.247425: step 31550, loss = 0.80 (968.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:32:48.565488: step 31560, loss = 0.84 (971.1 examples/sec; 0.132 sec/batch)
2017-05-08 19:32:49.853079: step 31570, loss = 0.91 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:32:51.129905: step 31580, loss = 0.91 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:32:52.431948: step 31590, loss = 0.76 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:32:53.823694: step 31600, loss = 0.89 (919.7 examples/sec; 0.139 sec/batch)
2017-05-08 19:32:55.026904: step 31610, loss = 0.69 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-08 19:32:56.329653: step 31620, loss = 0.94 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:32:57.640250: step 31630, loss = 0.97 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:32:58.909164: step 31640, loss = 0.74 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:33:00.203197: step 31650, loss = 0.65 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:33:01.502776: step 31660, loss = 0.66 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:33:02.798653: step 31670, loss = 0.85 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:33:04.106041: step 31680, loss = 0.77 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:33:05.390756: step 31690, loss = 0.85 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:33:06.771571: step 31700, loss = 0.83 (927.0 examples/sec; 0.138 sec/batch)
2017-05-08 19:33:07.978440: step 31710, loss = 0.81 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-08 19:33:09.254946: step 31720, loss = 0.75 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:33:10.549718: step 31730, loss = 0.82 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:33:11.856280: step 31740, loss = 0.80 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:33:13.148555: step 31750, loss = 0.78 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:33:14.449723: step 31760, loss = 0.77 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:33:15.722578: step 31770, loss = 0.66 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:33:16.986271: step 31780, loss = 0.88 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:33:18.286960: step 31790, loss = 0.74 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:33:19.679792: step 31800, loss = 0.74 (919.0 examples/sec; 0.139 sec/batch)
2017-05-08 19:33:20.906889: step 31810, loss = 0.90 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-08 19:33:22.192816: step 31820, loss = 0.76 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:33:23.495554: step 31830, loss = 0.82 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:33:24.775409: step 31840, loss = 0.71 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:33:26.054850: step 31850, loss = 0.70 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:33:27.342272: step 31860, loss = 0.79 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:33:28.618378: step 31870, loss = 0.59 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:33:29.915327: step 31880, loss = 0.89 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:33:31.224212: step 31890, loss = 0.74 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:33:32.629870: step 31900, loss = 0.77 (910.6 examples/sec; 0.141 sec/batch)
2017-05-08 19:33:33.856637: step 31910, loss = 0.81 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-08 19:33:35.191086: step 31920, loss = 0.76 (959.2 examples/sec; 0.133 sec/batch)
2017-05-08 19:33:36.496694: step 31930, loss = 0.64 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:33:37.792475: step 31940, loss = 0.77 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:33:39.084326: step 31950, loss = 0.76 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:33:40.382127: step 31960, loss = 0.88 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:33:41.671047: step 31970, loss = 0.82 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:33:42.948501: step 31980, loss = 0.78 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:33:44.255781: step 31990, loss = 0.75 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:33:45.627963: step 32000, loss = 0.70 (932.8 examples/sec; 0.137 sec/batch)
2017-05-08 19:33:46.813682: step 32010, loss = 0.74 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-08 19:33:48.078419: step 32020, loss = 0.70 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:33:49.357264: step 32030, loss = 0.70 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:33:50.689804: step 32040, loss = 0.71 (960.6 examples/sec; 0.133 sec/batch)
2017-05-08 19:33:51.997168: step 32050, loss = 0.77 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:33:53.320725: step 32060, loss = 0.66 (967.1 examples/sec; 0.132 sec/batch)
2017-05-08 19:33:54.610138: step 32070, loss = 0.82 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:33:55.882280: step 32080, loss = 0.99 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:33:57.170708: step 32090, loss = 0.85 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:33:58.576691: step 32100, loss = 0.67 (910.4 examples/sec; 0.141 sec/batch)
2017-05-08 19:33:59.807562: step 32110, loss = 0.72 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-08 19:34:01.062929: step 32120, loss = 0.84 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:34:02.377308: step 32130, loss = 0.75 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:34:03.645227: step 32140, loss = 0.76 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:34:04.941137: step 32150, loss = 0.86 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:34:06.243603: step 32160, loss = 0.92 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:34:07.542974: step 32170, loss = 0.74 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:34:08.847146: step 32180, loss = 0.68 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:34:10.159578: step 32190, loss = 0.76 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:34:11.540204: step 32200, loss = 0.86 (927.1 examples/sec; 0.138 sec/batch)
2017-05-08 19:34:12.727782: step 32210, loss = 0.88 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-08 19:34:14.005141: step 32220, loss = 0.88 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:34:15.292675: step 32230, loss = 0.92 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:34:16.547595: step 32240, loss = 0.81 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:34:17.818102: step 32250, loss = 0.69 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:34:19.091651: step 32260, loss = 0.73 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:34:20.360249: step 32270, loss = 0.70 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:34:21.645432: step 32280, loss = 0.76 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:34:22.926330: step 32290, loss = 0.80 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:34:24.302987: step 32300, loss = 0.86 (929.8 examples/sec; 0.138 sec/batch)
2017-05-08 19:34:25.550648: step 32310, loss = 0.71 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:34:26.820393: step 32320, loss = 0.66 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:34:28.126299: step 32330, loss = 0.79 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:34:29.446675: step 32340, loss = 0.67 (969.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:34:30.730391: step 32350, loss = 0.73 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:34:32.013319: step 32360, loss = 0.83 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:34:33.313019: step 32370, loss = 0.75 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:34:34.587183: step 32380, loss = 0.81 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:34:35.934897: step 32390, loss = 0.91 (949.8 examples/sec; 0.135 sec/batch)
2017-05-08 19:34:37.265509: step 32400, loss = 0.75 (962.0 examples/sec; 0.133 sec/batch)
2017-05-08 19:34:38.472742: step 32410, loss = 0.79 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-08 19:34:39.761510: step 32420, loss = 0.72 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:34:41.037784: step 32430, loss = 0.74 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:34:42.302574: step 32440, loss = 0.64 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:34:43.579536: step 32450, loss = 0.71 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:34:44.869110: step 32460, loss = 0.77 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:34:46.126166: step 32470, loss = 0.72 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:34:47.425738: step 32480, loss = 0.65 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:34:48.703863: step 32490, loss = 0.86 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:34:50.079043: step 32500, loss = 0.84 (930.8 examples/sec; 0.138 sec/batch)
2017-05-08 19:34:51.277475: step 32510, loss = 0.76 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-08 19:34:52.562855: step 32520, loss = 0.77 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:34:53.895041: step 32530, loss = 0.76 (960.8 examples/sec; 0.133 sec/batch)
2017-05-08 19:34:55.189688: step 32540, loss = 0.74 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:34:56.489376: step 32550, loss = 0.73 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:34:57.775255: step 32560, loss = 0.88 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:34:59.067124: step 32570, loss = 0.79 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:35:00.364347: step 32580, loss = 0.79 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:35:01.664034: step 32590, loss = 0.77 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:35:03.061826: step 32600, loss = 0.94 (915.7 examples/sec; 0.140 sec/batch)
2017-05-08 19:35:04.324617: step 32610, loss = 0.72 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:35:05.570074: step 32620, loss = 0.82 (1027.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:35:06.882807: step 32630, loss = 0.79 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:35:08.160407: step 32640, loss = 0.71 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:35:09.478297: step 32650, loss = 0.78 (971.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:35:10.792685: step 32660, loss = 0.85 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:35:12.103262: step 32670, loss = 0.77 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:35:13.374855: step 32680, loss = 0.80 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:35:14.640800: step 32690, loss = 0.80 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:35:16.014823: step 32700, loss = 0.74 (931.6 examples/sec; 0.137 sec/batch)
2017-05-08 19:35:17.211636: step 32710, loss = 0.78 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-08 19:35:18.504835: step 32720, loss = 0.72 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:35:19.750355: step 32730, loss = 0.78 (1027.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:35:21.048752: step 32740, loss = 0.88 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:35:22.361347: step 32750, loss = 0.77 (975.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:35:23.644499: step 32760, loss = 0.83 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:35:24.947768: step 32770, loss = 0.76 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:35:26.250028: step 32780, loss = 0.69 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:35:27.532418: step 32790, loss = 0.68 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:35:28.931269: step 32800, loss = 0.67 (915.0 examples/sec; 0.140 sec/batch)
2017-05-08 19:35:30.143161: step 32810, loss = 0.68 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-08 19:35:31.436029: step 32820, loss = 0.81 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:35:32.707195: step 32830, loss = 0.68 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:35:33.986551: step 32840, loss = 0.83 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:35:35.255465: step 32850, loss = 0.65 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:35:36.550639: step 32860, loss = 0.62 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:35:37.843326: step 32870, loss = 0.93 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:35:39.171581: step 32880, loss = 0.77 (963.7 examples/sec; 0.133 sec/batch)
2017-05-08 19:35:40.415553: step 32890, loss = 0.68 (1029.0 examples/sec; 0.124 sec/batch)
2017-05-08 19:35:41.783312: step 32900, loss = 0.77 (935.8 examples/sec; 0.137 sec/batch)
2017-05-08 19:35:42.969274: step 32910, loss = 0.74 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-08 19:35:44.284857: step 32920, loss = 0.88 (973.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:35:45.562845: step 32930, loss = 0.74 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:35:46.844061: step 32940, loss = 0.77 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:35:48.113323: step 32950, loss = 0.65 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:35:49.410829: step 32960, loss = 0.84 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:35:50.697063: step 32970, loss = 0.59 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:35:52.004050: step 32980, loss = 0.64 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:35:53.284480: step 32990, loss = 0.69 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:35:54.646995: step 33000, loss = 0.77 (939.4 examples/sec; 0.136 sec/batch)
2017-05-08 19:35:55.861045: step 33010, loss = 0.76 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-08 19:35:57.148416: step 33020, loss = 0.71 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:35:58.445487: step 33030, loss = 0.82 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:35:59.763759: step 33040, loss = 0.76 (970.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:36:01.069099: step 33050, loss = 0.79 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:36:02.344549: step 33060, loss = 0.85 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:03.628265: step 33070, loss = 0.86 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:04.906310: step 33080, loss = 0.77 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:06.178190: step 33090, loss = 0.78 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:36:07.566848: step 33100, loss = 0.54 (921.8 examples/sec; 0.139 sec/batch)
2017-05-08 19:36:08.764817: step 33110, loss = 0.90 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-08 19:36:10.061829: step 33120, loss = 0.74 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:36:11.342475: step 33130, loss = 0.82 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:12.625705: step 33140, loss = 0.65 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:13.892605: step 33150, loss = 0.76 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:36:15.171646: step 33160, loss = 0.69 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:16.439870: step 33170, loss = 0.79 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:36:17.710916: step 33180, loss = 0.76 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:36:18.985998: step 33190, loss = 0.85 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:20.363080: step 33200, loss = 0.65 (929.5 examples/sec; 0.138 sec/batch)
2017-05-08 19:36:21.581633: step 33210, loss = 0.68 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-08 19:36:22.871839: step 33220, loss = 0.67 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:36:24.142839: step 33230, loss = 0.75 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:36:25.429251: step 33240, loss = 0.81 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:36:26.716130: step 33250, loss = 0.88 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:36:27.999121: step 33260, loss = 0.74 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:29.289298: step 33270, loss = 0.59 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:36:30.575389: step 33280, loss = 0.89 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:36:31.889388: step 33290, loss = 0.66 (974.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:36:33.294750: step 33300, loss = 0.72 (910.8 examples/sec; 0.141 sec/batch)
2017-05-08 19:36:34.492159: step 33310, loss = 0.88 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-08 19:36:35.767280: step 33320, loss = 0.79 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:37.058819: step 33330, loss = 0.72 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:36:38.422955: step 33340, loss = 0.85 (938.3 examples/sec; 0.136 sec/batch)
2017-05-08 19:36:39.671147: step 33350, loss = 0.73 (1025.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:36:40.958805: step 33360, loss = 0.68 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:36:42.263441: step 33370, loss = 0.68 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:36:43.532146: step 33380, loss = 0.83 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:36:44.822115: step 33390, loss = 0.97 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:36:46.197905: step 33400, loss = 0.80 (930.4 examples/sec; 0.138 sec/batch)
2017-05-08 19:36:47.416579: step 33410, loss = 0.90 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-08 19:36:48.710553: step 33420, loss = 0.84 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:36:50.000019: step 33430, loss = 0.79 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:36:51.294344: step 33440, loss = 1.02 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:36:52.544991: step 33450, loss = 1.01 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:36:53.838064: step 33460, loss = 0.74 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:36:55.131832: step 33470, loss = 0.78 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:36:56.431385: step 33480, loss = 0.73 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:36:57.750694: step 33490, loss = 0.74 (970.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:36:59.122408: step 33500, loss = 0.73 (933.1 examples/sec; 0.137 sec/batch)
2017-05-08 19:37:00.394049: step 33510, loss = 0.76 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:37:01.602509: step 33520, loss = 0.78 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-08 19:37:02.862579: step 33530, loss = 0.78 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:37:04.138763: step 33540, loss = 0.78 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:37:05.410277: step 33550, loss = 0.76 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:37:06.708941: step 33560, loss = 0.93 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:37:08.016623: step 33570, loss = 0.69 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:37:09.310517: step 33580, loss = 0.74 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:37:10.609184: step 33590, loss = 0.75 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:37:12.004596: step 33600, loss = 0.69 (917.3 examples/sec; 0.140 sec/batch)
2017-05-08 19:37:13.194815: step 33610, loss = 0.93 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-08 19:37:14.516651: step 33620, loss = 0.75 (968.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:37:15.800627: step 33630, loss = 0.73 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:37:17.078686: step 33640, loss = 0.86 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:37:18.369328: step 33650, loss = 0.82 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:37:19.690963: step 33660, loss = 0.68 (968.5 examples/sec; 0.132 sec/batch)
2017-05-08 19:37:20.975128: step 33670, loss = 0.74 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:37:22.249118: step 33680, loss = 0.77 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:37:23.520824: step 33690, loss = 0.68 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:37:24.891389: step 33700, loss = 0.71 (933.9 examples/sec; 0.137 sec/batch)
2017-05-08 19:37:26.086924: step 33710, loss = 0.74 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-08 19:37:27.357820: step 33720, loss = 0.75 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:37:28.609198: step 33730, loss = 0.85 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:37:29.889167: step 33740, loss = 0.62 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:37:31.176548: step 33750, loss = 0.91 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:37:32.429896: step 33760, loss = 0.77 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:37:33.756165: step 33770, loss = 0.70 (965.1 examples/sec; 0.133 sec/batch)
2017-05-08 19:37:35.052754: step 33780, loss = 0.79 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:37:36.346379: step 33790, loss = 0.77 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:37:37.741067: step 33800, loss = 0.79 (917.8 examples/sec; 0.139 sec/batch)
2017-05-08 19:37:38.955006: step 33810, loss = 0.68 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-08 19:37:40.253720: step 33820, loss = 0.86 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:37:41.534090: step 33830, loss = 0.65 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:37:42.825803: step 33840, loss = 0.65 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:37:44.090872: step 33850, loss = 0.67 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:37:45.385889: step 33860, loss = 0.76 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:37:46.705265: step 33870, loss = 0.81 (970.1 examples/sec; 0.132 sec/batch)
2017-05-08 19:37:47.987214: step 33880, loss = 0.81 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:37:49.271885: step 33890, loss = 0.77 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:37:50.673030: step 33900, loss = 0.81 (913.5 examples/sec; 0.140 sec/batch)
2017-05-08 19:37:51.849002: step 33910, loss = 0.96 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-08 19:37:53.159700: step 33920, loss = 0.74 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:37:54.463110: step 33930, loss = 0.75 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:37:55.736280: step 33940, loss = 0.95 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:37:57.038916: step 33950, loss = 0.72 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:37:58.323591: step 33960, loss = 0.88 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:37:59.642320: step 33970, loss = 0.58 (970.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:38:00.964225: step 33980, loss = 0.71 (968.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:38:02.242403: step 33990, loss = 0.77 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:03.592252: step 34000, loss = 0.76 (948.3 examples/sec; 0.135 sec/batch)
2017-05-08 19:38:04.820657: step 34010, loss = 0.99 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-08 19:38:06.143290: step 34020, loss = 0.82 (967.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:38:07.433260: step 34030, loss = 0.75 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:38:08.680124: step 34040, loss = 0.82 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:38:09.953190: step 34050, loss = 0.72 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:38:11.244364: step 34060, loss = 0.61 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:38:12.549718: step 34070, loss = 0.81 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:38:13.866722: step 34080, loss = 0.87 (971.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:38:15.175370: step 34090, loss = 0.72 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:38:16.529705: step 34100, loss = 0.72 (945.1 examples/sec; 0.135 sec/batch)
2017-05-08 19:38:17.748523: step 34110, loss = 0.84 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-08 19:38:19.015205: step 34120, loss = 0.82 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:38:20.265093: step 34130, loss = 0.73 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:38:21.560122: step 34140, loss = 0.75 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:38:22.831396: step 34150, loss = 0.75 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:38:24.112859: step 34160, loss = 0.76 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:25.385567: step 34170, loss = 0.64 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:38:26.676034: step 34180, loss = 0.92 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:38:27.962810: step 34190, loss = 0.62 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:38:29.330179: step 34200, loss = 0.70 (936.1 examples/sec; 0.137 sec/batch)
2017-05-08 19:38:30.525543: step 34210, loss = 0.86 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-08 19:38:31.808206: step 34220, loss = 0.73 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:33.111574: step 34230, loss = 0.72 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:38:34.447430: step 34240, loss = 0.89 (958.2 examples/sec; 0.134 sec/batch)
2017-05-08 19:38:35.730311: step 34250, loss = 0.72 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:37.036324: step 34260, loss = 0.78 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:38:38.315839: step 34270, loss = 0.75 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:39.591991: step 34280, loss = 0.78 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:40.867776: step 34290, loss = 0.71 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:42.244075: step 34300, loss = 0.80 (930.0 examples/sec; 0.138 sec/batch)
2017-05-08 19:38:43.492504: step 34310, loss = 0.77 (1025.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:38:44.730689: step 34320, loss = 0.70 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-08 19:38:46.002489: step 34330, loss = 0.76 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:38:47.296369: step 34340, loss = 0.82 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:38:48.561532: step 34350, loss = 0.72 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:38:49.853936: step 34360, loss = 0.93 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:38:51.174014: step 34370, loss = 0.74 (969.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:38:52.462791: step 34380, loss = 0.64 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:38:53.738401: step 34390, loss = 0.74 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:55.092638: step 34400, loss = 0.71 (945.2 examples/sec; 0.135 sec/batch)
2017-05-08 19:38:56.261094: step 34410, loss = 0.69 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-08 19:38:57.574034: step 34420, loss = 0.83 (974.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:38:58.854431: step 34430, loss = 0.80 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:39:00.132290: step 34440, loss = 0.74 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:39:01.439076: step 34450, loss = 0.75 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:39:02.738297: step 34460, loss = 0.79 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:39:04.034638: step 34470, loss = 0.77 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:39:05.309117: step 34480, loss = 0.83 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:39:06.586508: step 34490, loss = 0.66 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:39:07.954823: step 34500, loss = 0.76 (935.5 examples/sec; 0.137 sec/batch)
2017-05-08 19:39:09.183835: step 34510, loss = 0.69 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-08 19:39:10.478667: step 34520, loss = 0.80 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:39:11.790910: step 34530, loss = 0.61 (975.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:39:13.074767: step 34540, loss = 0.70 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:39:14.359803: step 34550, loss = 0.86 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:39:15.637166: step 34560, loss = 0.65 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:39:16.894131: step 34570, loss = 0.76 (1018.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:39:18.191726: step 34580, loss = 0.67 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:39:19.506514: step 34590, loss = 0.73 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:39:20.893620: step 34600, loss = 0.87 (922.8 examples/sec; 0.139 sec/batch)
2017-05-08 19:39:22.090144: step 34610, loss = 0.73 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-08 19:39:23.393566: step 34620, loss = 0.90 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:39:24.627792: step 34630, loss = 0.74 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-08 19:39:25.912034: step 34640, loss = 0.90 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:39:27.239607: step 34650, loss = 1.05 (964.2 examples/sec; 0.133 sec/batch)
2017-05-08 19:39:28.563266: step 34660, loss = 0.66 (967.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:39:29.901963: step 34670, loss = 0.71 (956.2 examples/sec; 0.134 sec/batch)
2017-05-08 19:39:31.184687: step 34680, loss = 0.79 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:39:32.448823: step 34690, loss = 0.87 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:39:33.843955: step 34700, loss = 0.72 (917.5 examples/sec; 0.140 sec/batch)
2017-05-08 19:39:35.039799: step 34710, loss = 0.67 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-08 19:39:36.305412: step 34720, loss = 0.76 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:39:37.591658: step 34730, loss = 0.79 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:39:38.873905: step 34740, loss = 0.85 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:39:40.151648: step 34750, loss = 0.66 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:39:41.447594: step 34760, loss = 0.88 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:39:42.737733: step 34770, loss = 0.72 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:39:44.012018: step 34780, loss = 0.87 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:39:45.273016: step 34790, loss = 0.59 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:39:46.627800: step 34800, loss = 0.79 (944.8 examples/sec; 0.135 sec/batch)
2017-05-08 19:39:47.825538: step 34810, loss = 0.81 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-08 19:39:49.124835: step 34820, loss = 0.76 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:39:50.422739: step 34830, loss = 0.82 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:39:51.714816: step 34840, loss = 0.84 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:39:53.015526: step 34850, loss = 0.78 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:39:54.306342: step 34860, loss = 0.70 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:39:55.565649: step 34870, loss = 0.79 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:39:56.853975: step 34880, loss = 0.87 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:39:58.115980: step 34890, loss = 0.66 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:39:59.494039: step 34900, loss = 0.83 (928.8 examples/sec; 0.138 sec/batch)
2017-05-08 19:40:00.676553: step 34910, loss = 0.81 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-08 19:40:01.965207: step 34920, loss = 0.70 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:40:03.261779: step 34930, loss = 0.75 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:40:04.534896: step 34940, loss = 0.74 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:40:05.800340: step 34950, loss = 0.68 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:40:07.080721: step 34960, loss = 0.70 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:40:08.379982: step 34970, loss = 0.71 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:40:09.694196: step 34980, loss = 0.95 (974.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:40:10.970018: step 34990, loss = 0.78 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:40:12.368713: step 35000, loss = 0.87 (915.1 examples/sec; 0.140 sec/batch)
2017-05-08 19:40:13.607317: step 35010, loss = 0.87 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-08 19:40:14.910317: step 35020, loss = 0.83 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:40:16.162417: step 35030, loss = 0.70 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:40:17.466292: step 35040, loss = 0.81 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:40:18.818908: step 35050, loss = 0.80 (946.3 examples/sec; 0.135 sec/batch)
2017-05-08 19:40:20.083042: step 35060, loss = 0.78 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:40:21.411859: step 35070, loss = 1.06 (963.3 examples/sec; 0.133 sec/batch)
2017-05-08 19:40:22.705843: step 35080, loss = 0.72 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:40:23.988605: step 35090, loss = 0.70 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:40:25.358148: step 35100, loss = 0.73 (934.6 examples/sec; 0.137 sec/batch)
2017-05-08 19:40:26.550916: step 35110, loss = 0.79 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-08 19:40:27.821659: step 35120, loss = 0.78 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:40:29.126780: step 35130, loss = 0.74 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:40:30.435975: step 35140, loss = 0.79 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:40:31.734286: step 35150, loss = 0.81 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:40:33.022362: step 35160, loss = 0.70 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:40:34.342234: step 35170, loss = 0.89 (969.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:40:35.661214: step 35180, loss = 0.76 (970.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:40:36.928513: step 35190, loss = 0.79 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:40:38.306799: step 35200, loss = 0.78 (928.7 examples/sec; 0.138 sec/batch)
2017-05-08 19:40:39.506648: step 35210, loss = 0.69 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-08 19:40:40.818503: step 35220, loss = 0.70 (975.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:40:42.075759: step 35230, loss = 0.78 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:40:43.373409: step 35240, loss = 0.75 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:40:44.689720: step 35250, loss = 0.66 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:40:46.016256: step 35260, loss = 0.74 (964.9 examples/sec; 0.133 sec/batch)
2017-05-08 19:40:47.274981: step 35270, loss = 0.83 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:40:48.549419: step 35280, loss = 0.72 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:40:49.809828: step 35290, loss = 0.74 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:40:51.213075: step 35300, loss = 0.89 (912.2 examples/sec; 0.140 sec/batch)
2017-05-08 19:40:52.457723: step 35310, loss = 0.94 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-08 19:40:53.734028: step 35320, loss = 0.73 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:40:54.982683: step 35330, loss = 0.84 (1025.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:40:56.273643: step 35340, loss = 0.79 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:40:57.550740: step 35350, loss = 0.71 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:40:58.823217: step 35360, loss = 0.72 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:41:00.094060: step 35370, loss = 0.87 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:41:01.370766: step 35380, loss = 0.88 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:02.637366: step 35390, loss = 0.73 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:41:04.024841: step 35400, loss = 0.87 (922.5 examples/sec; 0.139 sec/batch)
2017-05-08 19:41:05.235936: step 35410, loss = 0.65 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-08 19:41:06.537885: step 35420, loss = 0.71 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:41:07.814893: step 35430, loss = 0.81 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:09.107389: step 35440, loss = 0.82 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:41:10.402760: step 35450, loss = 0.84 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:41:11.658705: step 35460, loss = 0.69 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:41:12.941782: step 35470, loss = 0.74 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:14.207781: step 35480, loss = 0.72 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:41:15.483790: step 35490, loss = 0.81 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:16.855503: step 35500, loss = 0.76 (933.1 examples/sec; 0.137 sec/batch)
2017-05-08 19:41:18.065427: step 35510, loss = 0.62 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-08 19:41:19.330002: step 35520, loss = 0.82 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:41:20.644209: step 35530, loss = 0.69 (974.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:41:21.952067: step 35540, loss = 0.76 (978.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:41:23.230140: step 35550, loss = 0.88 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:24.494230: step 35560, loss = 0.76 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:41:25.770230: step 35570, loss = 0.73 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:27.057712: step 35580, loss = 0.67 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:41:28.330049: step 35590, loss = 0.76 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:41:29.713809: step 35600, loss = 0.61 (925.0 examples/sec; 0.138 sec/batch)
2017-05-08 19:41:30.926478: step 35610, loss = 0.85 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-08 19:41:32.210170: step 35620, loss = 0.70 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:33.482505: step 35630, loss = 0.75 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:41:34.771939: step 35640, loss = 0.68 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:41:36.066373: step 35650, loss = 0.81 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:41:37.379838: step 35660, loss = 0.92 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:41:38.675001: step 35670, loss = 0.93 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:41:39.984079: step 35680, loss = 0.69 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:41:41.283658: step 35690, loss = 0.74 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:41:42.647144: step 35700, loss = 0.78 (938.8 examples/sec; 0.136 sec/batch)
2017-05-08 19:41:43.866833: step 35710, loss = 0.92 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-08 19:41:45.173814: step 35720, loss = 0.81 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:41:46.469040: step 35730, loss = 0.71 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:41:47.720600: step 35740, loss = 0.71 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:41:49.018728: step 35750, loss = 0.73 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:41:50.295739: step 35760, loss = 0.79 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:51.561546: step 35770, loss = 0.70 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:41:52.848690: step 35780, loss = 0.76 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:41:54.108349: step 35790, loss = 0.94 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:41:55.481810: step 35800, loss = 0.71 (932.0 examples/sec; 0.137 sec/batch)
2017-05-08 19:41:56.734904: step 35810, loss = 0.81 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:41:58.064372: step 35820, loss = 0.73 (962.8 examples/sec; 0.133 sec/batch)
2017-05-08 19:41:59.280468: step 35830, loss = 0.66 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-08 19:42:00.574078: step 35840, loss = 0.69 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:42:01.865523: step 35850, loss = 0.66 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:42:03.135814: step 35860, loss = 0.77 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:42:04.443874: step 35870, loss = 0.76 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:42:05.749937: step 35880, loss = 0.76 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:42:07.056324: step 35890, loss = 0.74 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:42:08.431146: step 35900, loss = 0.85 (931.0 examples/sec; 0.137 sec/batch)
2017-05-08 19:42:09.685526: step 35910, loss = 0.67 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:42:10.935582: step 35920, loss = 0.76 (1024.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:42:12.235907: step 35930, loss = 0.65 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:42:13.536068: step 35940, loss = 0.63 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:42:14.862192: step 35950, loss = 0.69 (965.2 examples/sec; 0.133 sec/batch)
2017-05-08 19:42:16.140292: step 35960, loss = 0.74 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:17.443417: step 35970, loss = 0.80 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:42:18.754341: step 35980, loss = 0.77 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:42:20.051272: step 35990, loss = 0.83 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:42:21.439795: step 36000, loss = 0.68 (921.8 examples/sec; 0.139 sec/batch)
2017-05-08 19:42:22.635343: step 36010, loss = 0.80 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-08 19:42:23.894116: step 36020, loss = 0.72 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:42:25.172365: step 36030, loss = 0.77 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:26.442678: step 36040, loss = 0.67 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:42:27.704104: step 36050, loss = 0.79 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:42:28.996086: step 36060, loss = 0.72 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:42:30.273497: step 36070, loss = 0.64 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:31.557238: step 36080, loss = 0.87 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:32.814724: step 36090, loss = 0.71 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:42:34.188103: step 36100, loss = 0.83 (932.0 examples/sec; 0.137 sec/batch)
2017-05-08 19:42:35.368301: step 36110, loss = 0.80 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-08 19:42:36.636146: step 36120, loss = 0.97 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:42:37.916267: step 36130, loss = 0.76 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:39.199413: step 36140, loss = 0.79 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:40.462385: step 36150, loss = 0.92 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:42:41.733956: step 36160, loss = 0.77 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:42:43.012234: step 36170, loss = 0.72 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:44.302028: step 36180, loss = 0.78 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:42:45.615512: step 36190, loss = 0.63 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:42:47.031001: step 36200, loss = 0.86 (904.3 examples/sec; 0.142 sec/batch)
2017-05-08 19:42:48.189810: step 36210, loss = 0.90 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-08 19:42:49.482782: step 36220, loss = 0.90 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:42:50.782299: step 36230, loss = 0.57 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:42:52.076393: step 36240, loss = 0.71 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:42:53.378156: step 36250, loss = 0.70 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:42:54.658395: step 36260, loss = 0.78 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:55.916109: step 36270, loss = 0.83 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:42:57.200356: step 36280, loss = 0.77 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:58.487335: step 36290, loss = 0.73 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:42:59.841906: step 36300, loss = 0.70 (945.0 examples/sec; 0.135 sec/batch)
2017-05-08 19:43:01.108066: step 36310, loss = 0.81 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:02.369519: step 36320, loss = 0.77 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:43:03.677173: step 36330, loss = 0.84 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:43:04.956160: step 36340, loss = 0.73 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:43:06.246787: step 36350, loss = 0.75 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:43:07.504790: step 36360, loss = 0.77 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:43:08.770199: step 36370, loss = 0.79 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:10.079082: step 36380, loss = 0.72 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:43:11.380806: step 36390, loss = 0.75 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:43:12.774321: step 36400, loss = 0.82 (918.5 examples/sec; 0.139 sec/batch)
2017-05-08 19:43:13.955993: step 36410, loss = 0.63 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-08 19:43:15.239559: step 36420, loss = 0.81 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:43:16.513441: step 36430, loss = 0.68 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:17.839325: step 36440, loss = 0.84 (965.4 examples/sec; 0.133 sec/batch)
2017-05-08 19:43:19.147324: step 36450, loss = 0.58 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:43:20.421962: step 36460, loss = 0.70 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:21.712172: step 36470, loss = 0.78 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:43:22.974274: step 36480, loss = 0.68 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:43:24.245814: step 36490, loss = 0.66 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:25.633654: step 36500, loss = 0.78 (922.3 examples/sec; 0.139 sec/batch)
2017-05-08 19:43:26.837725: step 36510, loss = 0.84 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-08 19:43:28.109647: step 36520, loss = 0.73 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:29.387373: step 36530, loss = 0.69 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:43:30.700848: step 36540, loss = 0.74 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:43:31.967289: step 36550, loss = 0.80 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:33.250665: step 36560, loss = 0.68 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:43:34.531240: step 36570, loss = 0.93 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:43:35.836757: step 36580, loss = 0.63 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:43:37.149402: step 36590, loss = 0.86 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:43:38.561476: step 36600, loss = 0.75 (906.5 examples/sec; 0.141 sec/batch)
2017-05-08 19:43:39.762003: step 36610, loss = 0.81 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-08 19:43:41.029758: step 36620, loss = 0.76 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:42.320240: step 36630, loss = 0.76 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:43:43.625741: step 36640, loss = 0.75 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:43:44.888120: step 36650, loss = 0.83 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:43:46.200358: step 36660, loss = 0.75 (975.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:43:47.486355: step 36670, loss = 0.67 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:43:48.751205: step 36680, loss = 0.80 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:43:50.039261: step 36690, loss = 0.78 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:43:51.411239: step 36700, loss = 0.66 (933.0 examples/sec; 0.137 sec/batch)
2017-05-08 19:43:52.589719: step 36710, loss = 0.75 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-08 19:43:53.854007: step 36720, loss = 0.53 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:43:55.122506: step 36730, loss = 0.83 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:56.410015: step 36740, loss = 0.77 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:43:57.687250: step 36750, loss = 0.71 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:43:58.959334: step 36760, loss = 0.82 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:44:00.220945: step 36770, loss = 0.87 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:44:01.549524: step 36780, loss = 0.70 (963.4 examples/sec; 0.133 sec/batch)
2017-05-08 19:44:02.867808: step 36790, loss = 0.77 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:44:04.218501: step 36800, loss = 0.78 (947.7 examples/sec; 0.135 sec/batch)
2017-05-08 19:44:05.494580: step 36810, loss = 0.80 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:44:06.742601: step 36820, loss = 0.78 (1025.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:44:08.015612: step 36830, loss = 0.67 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:44:09.297802: step 36840, loss = 0.88 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:44:10.598984: step 36850, loss = 0.82 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:44:11.855863: step 36860, loss = 0.64 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:44:13.157649: step 36870, loss = 0.80 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:44:14.435535: step 36880, loss = 0.89 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:44:15.731755: step 36890, loss = 0.67 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:44:17.134513: step 36900, loss = 0.98 (912.5 examples/sec; 0.140 sec/batch)
2017-05-08 19:44:18.330684: step 36910, loss = 0.64 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-08 19:44:19.617065: step 36920, loss = 0.88 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:44:20.923846: step 36930, loss = 0.80 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:44:22.258719: step 36940, loss = 1.08 (958.9 examples/sec; 0.133 sec/batch)
2017-05-08 19:44:23.548239: step 36950, loss = 0.65 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:44:24.828058: step 36960, loss = 0.67 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:44:26.131184: step 36970, loss = 0.66 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:44:27.396039: step 36980, loss = 0.65 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:44:28.680916: step 36990, loss = 0.80 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:44:30.040036: step 37000, loss = 0.76 (941.8 examples/sec; 0.136 sec/batch)
2017-05-08 19:44:31.227369: step 37010, loss = 0.85 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-08 19:44:32.500275: step 37020, loss = 0.85 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:44:33.788247: step 37030, loss = 0.65 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:44:35.101375: step 37040, loss = 0.73 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:44:36.412720: step 37050, loss = 0.69 (976.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:44:37.700033: step 37060, loss = 0.67 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:44:39.008956: step 37070, loss = 0.75 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:44:40.276068: step 37080, loss = 0.88 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:44:41.567922: step 37090, loss = 0.88 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:44:42.945239: step 37100, loss = 0.66 (929.3 examples/sec; 0.138 sec/batch)
2017-05-08 19:44:44.125905: step 37110, loss = 0.68 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-08 19:44:45.414572: step 37120, loss = 0.78 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:44:46.700199: step 37130, loss = 0.60 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:44:48.005298: step 37140, loss = 0.67 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:44:49.335942: step 37150, loss = 0.77 (961.9 examples/sec; 0.133 sec/batch)
2017-05-08 19:44:50.602820: step 37160, loss = 0.80 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:44:51.912731: step 37170, loss = 0.78 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:44:53.191284: step 37180, loss = 0.74 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:44:54.460563: step 37190, loss = 0.69 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:44:55.831110: step 37200, loss = 0.90 (933.9 examples/sec; 0.137 sec/batch)
2017-05-08 19:44:57.022220: step 37210, loss = 0.97 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-08 19:44:58.307224: step 37220, loss = 0.85 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:44:59.610582: step 37230, loss = 0.77 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:45:00.919763: step 37240, loss = 0.78 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:45:02.219795: step 37250, loss = 0.72 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:45:03.490728: step 37260, loss = 0.74 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:45:04.770869: step 37270, loss = 0.75 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:45:06.033376: step 37280, loss = 0.84 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:45:07.311859: step 37290, loss = 0.86 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:45:08.677782: step 37300, loss = 0.79 (937.1 examples/sec; 0.137 sec/batch)
2017-05-08 19:45:09.888208: step 37310, loss = 0.71 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-08 19:45:11.195161: step 37320, loss = 0.80 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:45:12.492478: step 37330, loss = 0.81 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:45:13.767054: step 37340, loss = 0.88 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:45:15.068715: step 37350, loss = 0.82 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:45:16.341560: step 37360, loss = 0.73 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:45:17.612404: step 37370, loss = 0.79 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:45:18.954229: step 37380, loss = 0.76 (953.9 examples/sec; 0.134 sec/batch)
2017-05-08 19:45:20.213173: step 37390, loss = 0.80 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:45:21.598695: step 37400, loss = 0.73 (923.8 examples/sec; 0.139 sec/batch)
2017-05-08 19:45:22.759417: step 37410, loss = 0.75 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-08 19:45:24.037223: step 37420, loss = 0.62 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:45:25.333430: step 37430, loss = 0.79 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:45:26.615533: step 37440, loss = 0.70 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:45:27.870416: step 37450, loss = 0.78 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:45:29.157094: step 37460, loss = 0.79 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:45:30.428404: step 37470, loss = 0.72 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:45:31.735186: step 37480, loss = 0.79 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:45:33.037229: step 37490, loss = 0.75 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:45:34.441391: step 37500, loss = 0.61 (911.6 examples/sec; 0.140 sec/batch)
2017-05-08 19:45:35.660131: step 37510, loss = 0.69 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-08 19:45:36.918549: step 37520, loss = 0.74 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:45:38.241280: step 37530, loss = 0.73 (967.7 examples/sec; 0.132 sec/batch)
2017-05-08 19:45:39.526611: step 37540, loss = 0.87 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:45:40.795008: step 37550, loss = 0.71 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:45:42.072055: step 37560, loss = 0.69 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:45:43.353666: step 37570, loss = 0.84 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:45:44.632386: step 37580, loss = 0.94 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:45:45.957940: step 37590, loss = 0.77 (965.6 examples/sec; 0.133 sec/batch)
2017-05-08 19:45:47.346994: step 37600, loss = 0.82 (921.5 examples/sec; 0.139 sec/batch)
2017-05-08 19:45:48.524489: step 37610, loss = 0.75 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-08 19:45:49.803525: step 37620, loss = 0.74 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:45:51.110878: step 37630, loss = 0.77 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:45:52.368601: step 37640, loss = 0.80 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:45:53.641003: step 37650, loss = 0.75 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:45:54.914119: step 37660, loss = 0.70 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:45:56.187160: step 37670, loss = 0.80 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:45:57.506965: step 37680, loss = 0.78 (969.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:45:58.811005: step 37690, loss = 0.73 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:46:00.200901: step 37700, loss = 0.83 (920.9 examples/sec; 0.139 sec/batch)
2017-05-08 19:46:01.388884: step 37710, loss = 0.79 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-08 19:46:02.698620: step 37720, loss = 0.77 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:46:03.983021: step 37730, loss = 0.76 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:46:05.267848: step 37740, loss = 0.73 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:46:06.567135: step 37750, loss = 0.78 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:46:07.847894: step 37760, loss = 0.73 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:46:09.112314: step 37770, loss = 0.56 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:46:10.420592: step 37780, loss = 0.78 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:46:11.723608: step 37790, loss = 0.69 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:46:13.113555: step 37800, loss = 0.70 (920.9 examples/sec; 0.139 sec/batch)
2017-05-08 19:46:14.318005: step 37810, loss = 0.76 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-08 19:46:15.630661: step 37820, loss = 0.79 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:46:16.915336: step 37830, loss = 0.68 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:46:18.229612: step 37840, loss = 0.80 (973.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:46:19.518184: step 37850, loss = 0.77 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:46:20.808016: step 37860, loss = 0.92 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:46:22.158737: step 37870, loss = 0.72 (947.6 examples/sec; 0.135 sec/batch)
2017-05-08 19:46:23.418702: step 37880, loss = 0.69 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:46:24.692893: step 37890, loss = 0.80 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:46:26.078880: step 37900, loss = 0.82 (923.5 examples/sec; 0.139 sec/batch)
2017-05-08 19:46:27.292154: step 37910, loss = 0.80 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-08 19:46:28.622093: step 37920, loss = 0.92 (962.5 examples/sec; 0.133 sec/batch)
2017-05-08 19:46:29.936047: step 37930, loss = 0.78 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:46:31.205325: step 37940, loss = 0.99 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:46:32.494914: step 37950, loss = 0.78 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:46:33.777030: step 37960, loss = 0.75 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:46:35.055638: step 37970, loss = 0.76 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:46:36.346138: step 37980, loss = 0.64 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:46:37.676046: step 37990, loss = 0.81 (962.5 examples/sec; 0.133 sec/batch)
2017-05-08 19:46:39.084935: step 38000, loss = 0.78 (908.5 examples/sec; 0.141 sec/batch)
2017-05-08 19:46:40.286146: step 38010, loss = 0.60 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-08 19:46:41.602196: step 38020, loss = 0.70 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:46:42.870796: step 38030, loss = 0.69 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:46:44.153012: step 38040, loss = 0.80 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:46:45.423438: step 38050, loss = 0.73 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:46:46.683892: step 38060, loss = 0.84 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:46:47.942737: step 38070, loss = 0.83 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:46:49.264835: step 38080, loss = 0.73 (968.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:46:50.571438: step 38090, loss = 0.66 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:46:51.939281: step 38100, loss = 0.78 (935.8 examples/sec; 0.137 sec/batch)
2017-05-08 19:46:53.138715: step 38110, loss = 0.78 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-08 19:46:54.473187: step 38120, loss = 0.72 (959.2 examples/sec; 0.133 sec/batch)
2017-05-08 19:46:55.754489: step 38130, loss = 0.84 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:46:57.057191: step 38140, loss = 0.74 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:46:58.378354: step 38150, loss = 0.80 (968.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:46:59.669117: step 38160, loss = 0.72 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:00.962046: step 38170, loss = 0.81 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:02.218371: step 38180, loss = 0.65 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:47:03.509035: step 38190, loss = 0.77 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:04.885768: step 38200, loss = 0.75 (929.7 examples/sec; 0.138 sec/batch)
2017-05-08 19:47:06.051845: step 38210, loss = 0.86 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-08 19:47:07.312845: step 38220, loss = 0.88 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:47:08.598163: step 38230, loss = 0.93 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:09.919709: step 38240, loss = 0.77 (968.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:47:11.189675: step 38250, loss = 0.78 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:47:12.505604: step 38260, loss = 0.62 (972.7 examples/sec; 0.132 sec/batch)
2017-05-08 19:47:13.766287: step 38270, loss = 0.75 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:47:15.054076: step 38280, loss = 0.78 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:16.343271: step 38290, loss = 0.91 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:17.746876: step 38300, loss = 0.77 (911.9 examples/sec; 0.140 sec/batch)
2017-05-08 19:47:18.941264: step 38310, loss = 0.73 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-08 19:47:20.227243: step 38320, loss = 0.64 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:21.525058: step 38330, loss = 0.88 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:47:22.834865: step 38340, loss = 0.77 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:47:24.153502: step 38350, loss = 0.86 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 19:47:25.442110: step 38360, loss = 0.69 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:26.729402: step 38370, loss = 0.86 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:27.990086: step 38380, loss = 0.65 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:47:29.252419: step 38390, loss = 0.72 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:47:30.629687: step 38400, loss = 0.77 (929.4 examples/sec; 0.138 sec/batch)
2017-05-08 19:47:31.811914: step 38410, loss = 0.70 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-08 19:47:33.089000: step 38420, loss = 0.78 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:47:34.374605: step 38430, loss = 0.80 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:35.688385: step 38440, loss = 0.60 (974.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:47:37.000301: step 38450, loss = 0.68 (975.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:47:38.364653: step 38460, loss = 0.68 (938.2 examples/sec; 0.136 sec/batch)
2017-05-08 19:47:39.604346: step 38470, loss = 0.70 (1032.5 examples/sec; 0.124 sec/batch)
2017-05-08 19:47:40.883762: step 38480, loss = 0.74 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:47:42.175662: step 38490, loss = 0.81 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:43.565379: step 38500, loss = 1.03 (921.0 examples/sec; 0.139 sec/batch)
2017-05-08 19:47:44.746441: step 38510, loss = 0.83 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-08 19:47:46.056368: step 38520, loss = 0.73 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:47:47.357650: step 38530, loss = 0.75 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:47:48.628408: step 38540, loss = 0.78 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:47:49.899297: step 38550, loss = 0.72 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:47:51.187333: step 38560, loss = 0.67 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:52.463430: step 38570, loss = 0.83 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:47:53.713693: step 38580, loss = 0.69 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:47:54.989999: step 38590, loss = 0.83 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:47:56.341024: step 38600, loss = 0.68 (947.4 examples/sec; 0.135 sec/batch)
2017-05-08 19:47:57.533811: step 38610, loss = 0.85 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-08 19:47:58.837409: step 38620, loss = 0.78 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:48:00.139338: step 38630, loss = 0.68 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:48:01.436762: step 38640, loss = 0.84 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:48:02.709533: step 38650, loss = 0.69 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:48:03.971891: step 38660, loss = 0.86 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:05.237642: step 38670, loss = 0.73 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:48:06.543270: step 38680, loss = 0.80 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:48:07.810080: step 38690, loss = 0.71 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:48:09.198104: step 38700, loss = 0.77 (922.2 examples/sec; 0.139 sec/batch)
2017-05-08 19:48:10.414663: step 38710, loss = 0.80 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-08 19:48:11.697632: step 38720, loss = 0.85 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:48:12.988383: step 38730, loss = 0.60 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:48:14.299968: step 38740, loss = 0.72 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:48:15.569499: step 38750, loss = 0.75 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:48:16.835436: step 38760, loss = 0.71 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:48:18.126515: step 38770, loss = 0.69 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:48:19.413828: step 38780, loss = 0.71 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:48:20.721772: step 38790, loss = 0.82 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:48:22.104872: step 38800, loss = 0.73 (925.5 examples/sec; 0.138 sec/batch)
2017-05-08 19:48:23.317188: step 38810, loss = 0.70 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-08 19:48:24.589063: step 38820, loss = 0.74 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:48:25.852673: step 38830, loss = 0.78 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:27.115962: step 38840, loss = 0.80 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:28.382360: step 38850, loss = 0.66 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:48:29.664392: step 38860, loss = 0.80 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:48:30.920167: step 38870, loss = 0.79 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:32.209021: step 38880, loss = 0.66 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:48:33.480157: step 38890, loss = 0.62 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:48:34.850734: step 38900, loss = 0.81 (933.9 examples/sec; 0.137 sec/batch)
2017-05-08 19:48:36.000647: step 38910, loss = 0.83 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-08 19:48:37.276824: step 38920, loss = 0.84 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:48:38.590358: step 38930, loss = 0.75 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:48:39.889356: step 38940, loss = 0.67 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:48:41.207858: step 38950, loss = 0.73 (970.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:48:42.503696: step 38960, loss = 0.71 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:48:43.789047: step 38970, loss = 0.78 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:48:45.104122: step 38980, loss = 0.65 (973.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:48:46.401141: step 38990, loss = 0.73 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:48:47.765971: step 39000, loss = 0.82 (937.8 examples/sec; 0.136 sec/batch)
2017-05-08 19:48:48.954034: step 39010, loss = 0.83 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-08 19:48:50.298006: step 39020, loss = 0.68 (952.4 examples/sec; 0.134 sec/batch)
2017-05-08 19:48:51.597167: step 39030, loss = 0.74 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:48:52.892635: step 39040, loss = 0.64 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:48:54.161642: step 39050, loss = 0.78 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:48:55.464991: step 39060, loss = 0.72 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:48:56.734930: step 39070, loss = 0.93 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:48:58.036024: step 39080, loss = 0.81 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:48:59.327085: step 39090, loss = 0.73 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:49:00.692293: step 39100, loss = 0.75 (937.6 examples/sec; 0.137 sec/batch)
2017-05-08 19:49:01.888823: step 39110, loss = 0.82 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-08 19:49:03.191375: step 39120, loss = 0.65 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:49:04.512915: step 39130, loss = 0.75 (968.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:49:05.790338: step 39140, loss = 0.75 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:07.069942: step 39150, loss = 0.73 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:08.345120: step 39160, loss = 0.79 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:09.647340: step 39170, loss = 0.89 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:49:10.950927: step 39180, loss = 0.88 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:49:12.251071: step 39190, loss = 0.69 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:49:13.621777: step 39200, loss = 0.73 (933.8 examples/sec; 0.137 sec/batch)
2017-05-08 19:49:14.810770: step 39210, loss = 0.66 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-08 19:49:16.100696: step 39220, loss = 0.78 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:49:17.399330: step 39230, loss = 0.75 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:49:18.688928: step 39240, loss = 0.69 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:49:19.946738: step 39250, loss = 0.62 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:49:21.239512: step 39260, loss = 0.62 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:49:22.513023: step 39270, loss = 0.80 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:49:23.791974: step 39280, loss = 0.68 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:25.107276: step 39290, loss = 0.98 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:49:26.533356: step 39300, loss = 0.61 (897.6 examples/sec; 0.143 sec/batch)
2017-05-08 19:49:27.713888: step 39310, loss = 0.70 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-08 19:49:28.992628: step 39320, loss = 0.75 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:30.273733: step 39330, loss = 0.98 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:31.573099: step 39340, loss = 0.69 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:49:32.855019: step 39350, loss = 0.85 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:34.125771: step 39360, loss = 0.70 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:49:35.418768: step 39370, loss = 0.64 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:49:36.735998: step 39380, loss = 0.90 (971.7 examples/sec; 0.132 sec/batch)
2017-05-08 19:49:38.024867: step 39390, loss = 0.75 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:49:39.419531: step 39400, loss = 0.68 (917.8 examples/sec; 0.139 sec/batch)
2017-05-08 19:49:40.665142: step 39410, loss = 0.78 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:49:41.895355: step 39420, loss = 0.81 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-08 19:49:43.174695: step 39430, loss = 0.65 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:44.462738: step 39440, loss = 0.83 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:49:45.785649: step 39450, loss = 0.69 (967.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:49:47.063011: step 39460, loss = 0.69 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:48.323800: step 39470, loss = 0.63 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:49:49.600698: step 39480, loss = 0.70 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:50.873264: step 39490, loss = 0.81 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:49:52.261159: step 39500, loss = 0.77 (922.3 examples/sec; 0.139 sec/batch)
2017-05-08 19:49:53.472690: step 39510, loss = 0.86 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-08 19:49:54.735156: step 39520, loss = 0.90 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:49:56.010254: step 39530, loss = 0.78 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:57.272107: step 39540, loss = 0.83 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:49:58.551292: step 39550, loss = 0.80 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:59.803359: step 39560, loss = 0.61 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:50:01.068216: step 39570, loss = 0.89 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:50:02.357586: step 39580, loss = 0.77 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:50:03.629713: step 39590, loss = 0.72 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:50:05.021936: step 39600, loss = 0.72 (919.4 examples/sec; 0.139 sec/batch)
2017-05-08 19:50:06.233621: step 39610, loss = 0.83 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-08 19:50:07.515864: step 39620, loss = 0.84 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:08.776437: step 39630, loss = 0.67 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:50:10.028596: step 39640, loss = 0.68 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:50:11.307100: step 39650, loss = 0.82 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:12.582241: step 39660, loss = 0.82 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:13.849141: step 39670, loss = 0.83 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:50:15.147128: step 39680, loss = 0.73 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:50:16.454720: step 39690, loss = 0.80 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:50:17.858779: step 39700, loss = 0.69 (911.6 examples/sec; 0.140 sec/batch)
2017-05-08 19:50:19.062262: step 39710, loss = 0.79 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-08 19:50:20.352739: step 39720, loss = 0.81 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:50:21.638998: step 39730, loss = 0.77 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:50:22.934923: step 39740, loss = 0.67 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:50:24.204551: step 39750, loss = 0.76 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:50:25.497992: step 39760, loss = 0.76 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:50:26.768997: step 39770, loss = 0.65 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:50:28.059877: step 39780, loss = 0.64 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:50:29.364037: step 39790, loss = 0.91 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:50:30.749815: step 39800, loss = 0.77 (923.7 examples/sec; 0.139 sec/batch)
2017-05-08 19:50:31.980142: step 39810, loss = 0.72 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-08 19:50:33.227327: step 39820, loss = 0.68 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:50:34.503618: step 39830, loss = 0.69 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:35.792840: step 39840, loss = 0.66 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:50:37.037667: step 39850, loss = 0.75 (1028.3 examples/sec; 0.124 sec/batch)
2017-05-08 19:50:38.322197: step 39860, loss = 0.66 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:39.576289: step 39870, loss = 0.60 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:50:40.851722: step 39880, loss = 0.69 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:42.128809: step 39890, loss = 0.96 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:43.501632: step 39900, loss = 0.66 (932.4 examples/sec; 0.137 sec/batch)
2017-05-08 19:50:44.711139: step 39910, loss = 0.86 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-08 19:50:45.990691: step 39920, loss = 0.78 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:47.282412: step 39930, loss = 0.67 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:50:48.599053: step 39940, loss = 0.94 (972.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:50:49.877783: step 39950, loss = 0.68 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:51.166688: step 39960, loss = 0.86 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:50:52.426643: step 39970, loss = 0.63 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:50:53.709775: step 39980, loss = 0.72 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:54.992254: step 39990, loss = 0.68 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:56.407919: step 40000, loss = 0.60 (904.2 examples/sec; 0.142 sec/batch)
2017-05-08 19:50:57.614160: step 40010, loss = 0.70 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-08 19:50:58.889386: step 40020, loss = 0.80 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:51:00.181288: step 40030, loss = 0.61 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:51:01.468276: step 40040, loss = 0.73 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:51:02.768803: step 40050, loss = 0.81 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:51:04.072334: step 40060, loss = 0.70 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:51:05.355862: step 40070, loss = 0.80 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:51:06.621632: step 40080, loss = 0.80 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:51:07.884819: step 40090, loss = 0.90 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:51:09.304567: step 40100, loss = 0.79 (901.6 examples/sec; 0.142 sec/batch)
2017-05-08 19:51:10.509730: step 40110, loss = 0.72 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-08 19:51:11.791136: step 40120, loss = 0.93 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:51:13.062049: step 40130, loss = 0.82 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:51:14.347213: step 40140, loss = 0.79 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:51:15.639819: step 40150, loss = 0.83 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:51:16.908177: step 40160, loss = 0.89 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:51:18.216786: step 40170, loss = 0.77 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:51:19.521181: step 40180, loss = 0.74 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:51:20.799643: step 40190, loss = 0.83 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:51:22.176491: step 40200, loss = 1.02 (929.7 examples/sec; 0.138 sec/batch)
2017-05-08 19:51:23.369187: step 40210, loss = 0.77 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-08 19:51:24.656282: step 40220, loss = 0.78 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:51:25.936018: step 40230, loss = 0.73 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:51:27.265329: step 40240, loss = 0.62 (962.9 examples/sec; 0.133 sec/batch)
2017-05-08 19:51:28.552734: step 40250, loss = 0.80 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:51:29.822744: step 40260, loss = 0.89 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:51:31.112853: step 40270, loss = 0.61 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:51:32.406266: step 40280, loss = 0.92 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:51:33.703236: step 40290, loss = 0.65 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:51:35.112622: step 40300, loss = 0.71 (908.2 examples/sec; 0.141 sec/batch)
2017-05-08 19:51:36.334794: step 40310, loss = 0.70 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-08 19:51:37.608235: step 40320, loss = 0.57 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:51:38.934212: step 40330, loss = 0.76 (965.3 examples/sec; 0.133 sec/batch)
2017-05-08 19:51:40.237007: step 40340, loss = 0.67 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:51:41.504527: step 40350, loss = 0.79 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:51:42.774679: step 40360, loss = 0.68 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:51:44.036195: step 40370, loss = 0.68 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:51:45.305246: step 40380, loss = 0.80 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:51:46.586142: step 40390, loss = 0.74 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:51:47.966500: step 40400, loss = 0.83 (927.3 examples/sec; 0.138 sec/batch)
2017-05-08 19:51:49.172334: step 40410, loss = 0.67 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-08 19:51:50.475670: step 40420, loss = 0.64 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:51:51.734366: step 40430, loss = 0.73 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:51:53.016527: step 40440, loss = 0.76 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:51:54.313959: step 40450, loss = 0.92 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:51:55.624978: step 40460, loss = 0.81 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:51:56.912422: step 40470, loss = 0.66 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:51:58.215815: step 40480, loss = 0.82 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:51:59.482498: step 40490, loss = 0.69 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:52:00.856235: step 40500, loss = 0.72 (931.8 examples/sec; 0.137 sec/batch)
2017-05-08 19:52:02.027262: step 40510, loss = 0.70 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-08 19:52:03.334310: step 40520, loss = 0.62 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:52:04.654162: step 40530, loss = 0.79 (969.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:52:05.946314: step 40540, loss = 0.74 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:52:07.215684: step 40550, loss = 0.67 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:52:08.519764: step 40560, loss = 0.84 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:52:09.791740: step 40570, loss = 0.84 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:52:11.066930: step 40580, loss = 0.73 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:12.351215: step 40590, loss = 0.82 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:13.737266: step 40600, loss = 0.81 (923.5 examples/sec; 0.139 sec/batch)
2017-05-08 19:52:14.921803: step 40610, loss = 0.76 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-08 19:52:16.208828: step 40620, loss = 0.83 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:52:17.484385: step 40630, loss = 0.62 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:18.765812: step 40640, loss = 0.81 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:20.029016: step 40650, loss = 0.69 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:52:21.308068: step 40660, loss = 0.90 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:22.612210: step 40670, loss = 0.72 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:52:23.918940: step 40680, loss = 0.70 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:52:25.176391: step 40690, loss = 0.80 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:52:26.531248: step 40700, loss = 0.69 (944.7 examples/sec; 0.135 sec/batch)
2017-05-08 19:52:27.736000: step 40710, loss = 0.71 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-08 19:52:29.055418: step 40720, loss = 0.78 (970.1 examples/sec; 0.132 sec/batch)
2017-05-08 19:52:30.341864: step 40730, loss = 0.75 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:52:31.602910: step 40740, loss = 0.77 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:52:32.880418: step 40750, loss = 0.88 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:34.148013: step 40760, loss = 0.81 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:52:35.430870: step 40770, loss = 0.72 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:36.697776: step 40780, loss = 0.69 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:52:37.970088: step 40790, loss = 0.71 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:52:39.354501: step 40800, loss = 0.74 (924.6 examples/sec; 0.138 sec/batch)
2017-05-08 19:52:40.544114: step 40810, loss = 0.69 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-08 19:52:41.859226: step 40820, loss = 0.88 (973.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:52:43.145225: step 40830, loss = 0.74 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:52:44.402606: step 40840, loss = 0.73 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:52:45.684318: step 40850, loss = 0.79 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:46.947558: step 40860, loss = 0.59 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:52:48.239232: step 40870, loss = 0.78 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:52:49.554252: step 40880, loss = 0.67 (973.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:52:50.862481: step 40890, loss = 0.82 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:52:52.226796: step 40900, loss = 0.67 (938.2 examples/sec; 0.136 sec/batch)
2017-05-08 19:52:53.436437: step 40910, loss = 0.73 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-08 19:52:54.727442: step 40920, loss = 0.70 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:52:56.026925: step 40930, loss = 0.69 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:52:57.318779: step 40940, loss = 0.75 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:52:58.600670: step 40950, loss = 0.77 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:59.875428: step 40960, loss = 0.75 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:53:01.198551: step 40970, loss = 0.66 (967.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:53:02.509492: step 40980, loss = 0.72 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:53:03.820307: step 40990, loss = 0.77 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:53:05.181597: step 41000, loss = 0.87 (940.3 examples/sec; 0.136 sec/batch)
2017-05-08 19:53:06.411877: step 41010, loss = 0.69 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-08 19:53:07.683393: step 41020, loss = 0.77 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:53:08.985700: step 41030, loss = 0.67 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:53:10.260938: step 41040, loss = 0.82 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:53:11.524106: step 41050, loss = 0.82 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:53:12.796767: step 41060, loss = 0.74 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:53:14.126287: step 41070, loss = 0.75 (962.8 examples/sec; 0.133 sec/batch)
2017-05-08 19:53:15.425735: step 41080, loss = 0.68 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:53:16.711245: step 41090, loss = 0.82 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:53:18.092772: step 41100, loss = 0.74 (926.5 examples/sec; 0.138 sec/batch)
2017-05-08 19:53:19.289383: step 41110, loss = 0.77 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-08 19:53:20.588673: step 41120, loss = 0.61 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:53:21.870056: step 41130, loss = 0.62 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:53:23.164386: step 41140, loss = 0.80 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:53:24.463070: step 41150, loss = 0.81 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:53:25.766299: step 41160, loss = 0.71 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:53:27.063541: step 41170, loss = 0.72 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:53:28.348572: step 41180, loss = 0.78 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:53:29.668276: step 41190, loss = 0.81 (969.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:53:31.033119: step 41200, loss = 0.76 (937.8 examples/sec; 0.136 sec/batch)
2017-05-08 19:53:32.208554: step 41210, loss = 0.78 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-08 19:53:33.479573: step 41220, loss = 0.94 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:53:34.761294: step 41230, loss = 0.81 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:53:36.050413: step 41240, loss = 0.76 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:53:37.353389: step 41250, loss = 0.70 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:53:38.662526: step 41260, loss = 0.75 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:53:39.925622: step 41270, loss = 0.68 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:53:41.219216: step 41280, loss = 0.91 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:53:42.481094: step 41290, loss = 0.80 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:53:43.831790: step 41300, loss = 0.75 (947.7 examples/sec; 0.135 sec/batch)
2017-05-08 19:53:45.025845: step 41310, loss = 0.61 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-08 19:53:46.333355: step 41320, loss = 0.62 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:53:47.668290: step 41330, loss = 0.73 (958.8 examples/sec; 0.133 sec/batch)
2017-05-08 19:53:48.942974: step 41340, loss = 0.62 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:53:50.214128: step 41350, loss = 0.67 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:53:51.484001: step 41360, loss = 0.63 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:53:52.781770: step 41370, loss = 0.75 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:53:54.092336: step 41380, loss = 0.84 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:53:55.394008: step 41390, loss = 0.74 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:53:56.771537: step 41400, loss = 0.79 (929.2 examples/sec; 0.138 sec/batch)
2017-05-08 19:53:57.974183: step 41410, loss = 0.70 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-08 19:53:59.246885: step 41420, loss = 0.85 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:54:00.527945: step 41430, loss = 0.81 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:54:01.853920: step 41440, loss = 0.72 (965.3 examples/sec; 0.133 sec/batch)
2017-05-08 19:54:03.155992: step 41450, loss = 0.61 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:54:04.454973: step 41460, loss = 0.77 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:54:05.757897: step 41470, loss = 0.75 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:54:07.036343: step 41480, loss = 0.85 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:54:08.311326: step 41490, loss = 0.73 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:54:09.715278: step 41500, loss = 0.71 (911.7 examples/sec; 0.140 sec/batch)
2017-05-08 19:54:10.902314: step 41510, loss = 0.74 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-08 19:54:12.192924: step 41520, loss = 0.68 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:54:13.480148: step 41530, loss = 0.83 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:54:14.772142: step 41540, loss = 0.78 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:54:16.038033: step 41550, loss = 0.72 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:54:17.310518: step 41560, loss = 0.85 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:54:18.598816: step 41570, loss = 0.75 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:54:19.856982: step 41580, loss = 0.71 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:54:21.118991: step 41590, loss = 0.78 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:54:22.500993: step 41600, loss = 0.76 (926.2 examples/sec; 0.138 sec/batch)
2017-05-08 19:54:23.692830: step 41610, loss = 0.59 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-08 19:54:25.012078: step 41620, loss = 0.69 (970.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:54:26.319365: step 41630, loss = 0.68 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:54:27.590926: step 41640, loss = 0.75 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:54:28.849599: step 41650, loss = 0.74 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:54:30.157079: step 41660, loss = 0.87 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:54:31.451233: step 41670, loss = 0.79 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:54:32.770310: step 41680, loss = 0.82 (970.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:54:34.064762: step 41690, loss = 0.72 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:54:35.446599: step 41700, loss = 0.73 (926.3 examples/sec; 0.138 sec/batch)
2017-05-08 19:54:36.665405: step 41710, loss = 0.79 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-08 19:54:37.982410: step 41720, loss = 0.74 (971.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:54:39.282761: step 41730, loss = 1.06 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:54:40.580943: step 41740, loss = 0.75 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:54:41.881059: step 41750, loss = 0.72 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:54:43.182623: step 41760, loss = 0.85 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:54:44.440991: step 41770, loss = 0.72 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:54:45.742089: step 41780, loss = 0.88 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:54:47.015308: step 41790, loss = 0.81 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:54:48.389737: step 41800, loss = 0.62 (931.3 examples/sec; 0.137 sec/batch)
2017-05-08 19:54:49.607700: step 41810, loss = 0.73 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-08 19:54:50.904894: step 41820, loss = 0.67 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:54:52.183794: step 41830, loss = 0.80 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:54:53.475602: step 41840, loss = 0.88 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:54:54.745247: step 41850, loss = 0.67 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:54:56.003787: step 41860, loss = 0.81 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:54:57.273786: step 41870, loss = 0.83 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:54:58.599787: step 41880, loss = 0.88 (965.3 examples/sec; 0.133 sec/batch)
2017-05-08 19:54:59.869964: step 41890, loss = 0.84 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:55:01.225857: step 41900, loss = 0.87 (944.0 examples/sec; 0.136 sec/batch)
2017-05-08 19:55:02.475583: step 41910, loss = 0.81 (1024.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:55:03.767234: step 41920, loss = 0.82 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:55:05.047524: step 41930, loss = 0.71 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:55:06.327962: step 41940, loss = 0.68 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:55:07.604005: step 41950, loss = 0.82 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:55:08.868511: step 41960, loss = 0.81 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:55:10.134818: step 41970, loss = 0.67 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:55:11.448144: step 41980, loss = 0.66 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:55:12.736588: step 41990, loss = 0.80 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:55:14.131129: step 42000, loss = 0.71 (917.9 examples/sec; 0.139 sec/batch)
2017-05-08 19:55:15.308586: step 42010, loss = 0.72 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-08 19:55:16.573935: step 42020, loss = 0.91 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:55:17.900890: step 42030, loss = 0.81 (964.6 examples/sec; 0.133 sec/batch)
2017-05-08 19:55:19.189429: step 42040, loss = 0.95 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:55:20.454255: step 42050, loss = 0.86 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:55:21.737770: step 42060, loss = 0.73 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:55:23.020370: step 42070, loss = 0.77 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:55:24.299524: step 42080, loss = 0.78 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:55:25.592203: step 42090, loss = 0.74 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:55:27.006122: step 42100, loss = 0.75 (905.3 examples/sec; 0.141 sec/batch)
2017-05-08 19:55:28.217543: step 42110, loss = 0.68 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-08 19:55:29.532020: step 42120, loss = 0.79 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:55:30.814771: step 42130, loss = 1.04 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:55:32.118243: step 42140, loss = 0.66 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:55:33.426509: step 42150, loss = 0.79 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:55:34.714351: step 42160, loss = 0.89 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:55:35.999773: step 42170, loss = 0.73 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:55:37.265022: step 42180, loss = 0.83 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:55:38.552954: step 42190, loss = 0.91 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:55:39.910550: step 42200, loss = 0.58 (942.8 examples/sec; 0.136 sec/batch)
2017-05-08 19:55:41.125616: step 42210, loss = 0.75 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-08 19:55:42.420057: step 42220, loss = 0.70 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:55:43.702446: step 42230, loss = 0.70 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:55:44.975512: step 42240, loss = 0.64 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:55:46.278516: step 42250, loss = 0.84 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:55:47.556332: step 42260, loss = 0.75 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:55:48.849908: step 42270, loss = 0.72 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:55:50.147556: step 42280, loss = 0.81 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:55:51.417541: step 42290, loss = 0.72 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:55:52.819674: step 42300, loss = 0.80 (912.9 examples/sec; 0.140 sec/batch)
2017-05-08 19:55:53.982099: step 42310, loss = 0.88 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-08 19:55:55.240919: step 42320, loss = 0.64 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:55:56.524648: step 42330, loss = 0.68 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:55:57.807671: step 42340, loss = 0.72 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:55:59.097930: step 42350, loss = 0.74 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:56:00.410300: step 42360, loss = 0.67 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:56:01.723983: step 42370, loss = 0.78 (974.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:56:03.003556: step 42380, loss = 0.68 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:56:04.250744: step 42390, loss = 0.71 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:56:05.611050: step 42400, loss = 0.85 (941.0 examples/sec; 0.136 sec/batch)
2017-05-08 19:56:06.789198: step 42410, loss = 0.68 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-08 19:56:08.039223: step 42420, loss = 0.73 (1024.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:56:09.310294: step 42430, loss = 0.70 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:56:10.583879: step 42440, loss = 0.66 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:56:11.860912: step 42450, loss = 0.79 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:56:13.137278: step 42460, loss = 0.85 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:56:14.456628: step 42470, loss = 0.81 (970.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:56:15.738667: step 42480, loss = 0.72 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:56:17.046789: step 42490, loss = 0.62 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:56:18.452919: step 42500, loss = 0.76 (910.3 examples/sec; 0.141 sec/batch)
2017-05-08 19:56:19.647579: step 42510, loss = 0.73 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-08 19:56:20.907800: step 42520, loss = 0.64 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:56:22.189652: step 42530, loss = 0.67 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:56:23.458603: step 42540, loss = 0.53 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:56:24.740929: step 42550, loss = 0.79 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:56:26.022756: step 42560, loss = 0.74 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:56:27.312709: step 42570, loss = 0.75 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:56:28.647585: step 42580, loss = 0.71 (958.9 examples/sec; 0.133 sec/batch)
2017-05-08 19:56:29.946735: step 42590, loss = 0.73 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:56:31.327260: step 42600, loss = 0.75 (927.2 examples/sec; 0.138 sec/batch)
2017-05-08 19:56:32.551891: step 42610, loss = 0.88 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-08 19:56:33.858468: step 42620, loss = 0.67 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:56:35.113943: step 42630, loss = 0.65 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:56:36.395202: step 42640, loss = 0.78 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:56:37.668789: step 42650, loss = 0.70 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:56:38.960503: step 42660, loss = 0.72 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:56:40.263794: step 42670, loss = 0.71 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:56:41.533961: step 42680, loss = 0.63 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:56:42.804581: step 42690, loss = 0.72 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:56:44.161964: step 42700, loss = 0.73 (943.0 examples/sec; 0.136 sec/batch)
2017-05-08 19:56:45.354949: step 42710, loss = 0.66 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-08 19:56:46.645751: step 42720, loss = 0.71 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:56:47.915202: step 42730, loss = 0.77 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:56:49.167595: step 42740, loss = 0.64 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:56:50.456379: step 42750, loss = 0.61 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:56:51.728886: step 42760, loss = 0.80 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:56:53.020925: step 42770, loss = 0.84 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:56:54.313859: step 42780, loss = 0.75 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:56:55.606101: step 42790, loss = 0.68 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:56:57.009332: step 42800, loss = 0.80 (912.2 examples/sec; 0.140 sec/batch)
2017-05-08 19:56:58.211742: step 42810, loss = 0.68 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-08 19:56:59.500711: step 42820, loss = 0.68 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:57:00.792423: step 42830, loss = 0.75 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:57:02.099817: step 42840, loss = 0.69 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:57:03.397974: step 42850, loss = 0.68 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:57:04.663249: step 42860, loss = 0.72 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:57:05.965679: step 42870, loss = 0.87 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:57:07.237145: step 42880, loss = 0.73 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:57:08.496829: step 42890, loss = 0.86 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:57:09.872752: step 42900, loss = 0.98 (930.3 examples/sec; 0.138 sec/batch)
2017-05-08 19:57:11.057389: step 42910, loss = 0.70 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-08 19:57:12.341882: step 42920, loss = 0.80 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:57:13.657169: step 42930, loss = 0.82 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:57:14.950353: step 42940, loss = 0.80 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:57:16.216765: step 42950, loss = 0.88 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:57:17.503922: step 42960, loss = 0.80 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:57:18.811539: step 42970, loss = 0.76 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:57:20.090438: step 42980, loss = 0.68 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:57:21.374218: step 42990, loss = 0.68 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:57:22.751334: step 43000, loss = 0.86 (929.5 examples/sec; 0.138 sec/batch)
2017-05-08 19:57:23.934136: step 43010, loss = 0.77 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-08 19:57:25.204366: step 43020, loss = 0.66 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:57:26.482370: step 43030, loss = 0.88 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:57:27.762060: step 43040, loss = 0.70 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:57:29.045560: step 43050, loss = 0.72 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:57:30.352751: step 43060, loss = 0.72 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:57:31.632628: step 43070, loss = 0.71 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:57:32.933953: step 43080, loss = 0.64 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:57:34.207165: step 43090, loss = 0.84 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:57:35.548176: step 43100, loss = 0.71 (954.5 examples/sec; 0.134 sec/batch)
2017-05-08 19:57:36.841080: step 43110, loss = 0.68 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:57:38.104055: step 43120, loss = 0.79 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:57:39.340416: step 43130, loss = 0.75 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-08 19:57:40.585625: step 43140, loss = 0.64 (1027.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:57:41.875733: step 43150, loss = 0.67 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:57:43.161161: step 43160, loss = 0.82 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:57:44.476754: step 43170, loss = 0.69 (972.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:57:45.778519: step 43180, loss = 0.77 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:57:47.076151: step 43190, loss = 0.73 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:57:48.444091: step 43200, loss = 0.81 (935.7 examples/sec; 0.137 sec/batch)
2017-05-08 19:57:49.660883: step 43210, loss = 0.67 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-08 19:57:50.976182: step 43220, loss = 0.71 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:57:52.286151: step 43230, loss = 0.63 (977.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:57:53.607525: step 43240, loss = 0.71 (968.7 examples/sec; 0.132 sec/batch)
2017-05-08 19:57:54.904044: step 43250, loss = 0.77 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:57:56.202147: step 43260, loss = 0.89 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:57:57.478642: step 43270, loss = 0.74 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:57:58.750921: step 43280, loss = 0.66 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:00.044877: step 43290, loss = 0.81 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:58:01.446780: step 43300, loss = 0.74 (913.0 examples/sec; 0.140 sec/batch)
2017-05-08 19:58:02.696592: step 43310, loss = 0.84 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:58:03.976621: step 43320, loss = 0.75 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:05.258938: step 43330, loss = 0.73 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:06.525851: step 43340, loss = 0.65 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:07.783284: step 43350, loss = 0.65 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:58:09.081348: step 43360, loss = 0.68 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:58:10.354417: step 43370, loss = 0.77 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:11.614376: step 43380, loss = 0.74 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:58:12.883774: step 43390, loss = 0.68 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:14.262539: step 43400, loss = 0.75 (928.4 examples/sec; 0.138 sec/batch)
2017-05-08 19:58:15.477925: step 43410, loss = 0.75 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-08 19:58:16.800058: step 43420, loss = 0.81 (968.1 examples/sec; 0.132 sec/batch)
2017-05-08 19:58:18.080092: step 43430, loss = 0.76 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:19.371288: step 43440, loss = 0.72 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:58:20.636443: step 43450, loss = 0.92 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:21.922364: step 43460, loss = 0.89 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:58:23.177278: step 43470, loss = 0.94 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:58:24.458554: step 43480, loss = 0.67 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:25.757607: step 43490, loss = 0.65 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:58:27.134161: step 43500, loss = 0.80 (929.9 examples/sec; 0.138 sec/batch)
2017-05-08 19:58:28.317117: step 43510, loss = 0.89 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-08 19:58:29.595145: step 43520, loss = 0.67 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:30.873554: step 43530, loss = 0.80 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:32.142543: step 43540, loss = 0.93 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:33.413232: step 43550, loss = 0.69 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:34.694467: step 43560, loss = 0.87 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:35.963548: step 43570, loss = 0.70 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:37.272827: step 43580, loss = 0.81 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:58:38.592505: step 43590, loss = 0.68 (969.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:58:39.982990: step 43600, loss = 0.76 (920.5 examples/sec; 0.139 sec/batch)
2017-05-08 19:58:41.180044: step 43610, loss = 0.79 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-08 19:58:42.449035: step 43620, loss = 0.68 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:43.748416: step 43630, loss = 0.87 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:58:45.027533: step 43640, loss = 0.72 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:46.306292: step 43650, loss = 0.75 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:47.575972: step 43660, loss = 0.73 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:48.854311: step 43670, loss = 0.82 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:50.129794: step 43680, loss = 0.80 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:51.407248: step 43690, loss = 0.72 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:52.770728: step 43700, loss = 0.74 (938.8 examples/sec; 0.136 sec/batch)
2017-05-08 19:58:54.021893: step 43710, loss = 0.67 (1023.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:58:55.254209: step 43720, loss = 0.84 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-08 19:58:56.547973: step 43730, loss = 0.73 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:58:57.813421: step 43740, loss = 0.76 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:59.080836: step 43750, loss = 0.71 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:59:00.398907: step 43760, loss = 0.74 (971.1 examples/sec; 0.132 sec/batch)
2017-05-08 19:59:01.647614: step 43770, loss = 0.75 (1025.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:59:02.926744: step 43780, loss = 0.67 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:04.200399: step 43790, loss = 0.71 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:59:05.581812: step 43800, loss = 0.67 (926.6 examples/sec; 0.138 sec/batch)
2017-05-08 19:59:06.779947: step 43810, loss = 0.60 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-08 19:59:08.053524: step 43820, loss = 0.62 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:59:09.333700: step 43830, loss = 0.79 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:10.639836: step 43840, loss = 0.68 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:59:11.938322: step 43850, loss = 0.83 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:59:13.253360: step 43860, loss = 0.76 (973.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:59:14.521964: step 43870, loss = 0.81 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:59:15.833405: step 43880, loss = 0.67 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:59:17.108560: step 43890, loss = 0.99 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:18.501383: step 43900, loss = 0.79 (919.0 examples/sec; 0.139 sec/batch)
2017-05-08 19:59:19.711061: step 43910, loss = 0.81 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-08 19:59:21.003754: step 43920, loss = 0.77 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:59:22.293370: step 43930, loss = 0.71 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:59:23.562012: step 43940, loss = 0.75 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:59:24.858013: step 43950, loss = 0.92 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:59:26.111623: step 43960, loss = 0.89 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:59:27.378321: step 43970, loss = 0.78 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:59:28.662909: step 43980, loss = 0.85 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:29.980784: step 43990, loss = 0.85 (971.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:59:31.360363: step 44000, loss = 0.89 (927.8 examples/sec; 0.138 sec/batch)
2017-05-08 19:59:32.551075: step 44010, loss = 0.75 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-08 19:59:33.828947: step 44020, loss = 0.79 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:35.119216: step 44030, loss = 0.79 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:59:36.379092: step 44040, loss = 0.74 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:59:37.663573: step 44050, loss = 0.82 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:38.952705: step 44060, loss = 0.71 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:59:40.251729: step 44070, loss = 0.63 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:59:41.545706: step 44080, loss = 0.66 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:59:42.824034: step 44090, loss = 0.80 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:44.198767: step 44100, loss = 0.81 (931.1 examples/sec; 0.137 sec/batch)
2017-05-08 19:59:45.392214: step 44110, loss = 0.82 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-08 19:59:46.669536: step 44120, loss = 0.79 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:47.968616: step 44130, loss = 0.67 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:59:49.297679: step 44140, loss = 0.65 (963.1 examples/sec; 0.133 sec/batch)
2017-05-08 19:59:50.578659: step 44150, loss = 0.76 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:51.876292: step 44160, loss = 0.82 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:59:53.146020: step 44170, loss = 0.73 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:59:54.445387: step 44180, loss = 0.65 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:59:55.719712: step 44190, loss = 0.76 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:59:57.074330: step 44200, loss = 0.76 (944.9 examples/sec; 0.135 sec/batch)
2017-05-08 19:59:58.256621: step 44210, loss = 0.73 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-08 19:59:59.532404: step 44220, loss = 0.60 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:00:00.792046: step 44230, loss = 0.69 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-08 20:00:02.086267: step 44240, loss = 0.87 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:00:03.364861: step 44250, loss = 0.80 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 20:00:04.683116: step 44260, loss = 0.81 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 20:00:05.959724: step 44270, loss = 0.63 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 20:00:07.233259: step 44280, loss = 0.73 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:00:08.509803: step 44290, loss = 0.68 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 20:00:09.901212: step 44300, loss = 0.85 (919.9 examples/sec; 0.139 sec/batch)
2017-05-08 20:00:11.074437: step 44310, loss = 0.76 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-08 20:00:12.344924: step 44320, loss = 0.75 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 20:00:13.634295: step 44330, loss = 0.87 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 20:00:14.949724: step 44340, loss = 0.68 (973.1 examples/sec; 0.132 sec/batch)
2017-05-08 20:00:16.254763: step 44350, loss = 0.67 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 20:00:17.556771: step 44360, loss = 0.79 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 20:00:18.878739: step 44370, loss = 0.74 (968.3 examples/sec; 0.132 sec/batch)
2017-05-08 20:00:20.161397: step 44380, loss = 0.61 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 20:00:21.468442: step 44390, loss = 0.72 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 20:00:22.831132: step 44400, loss = 0.84 (939.3 examples/sec; 0.136 sec/batch)
2017-05-08 20:00:24.026343: step 44410, loss = 1.02 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-08 20:00:25.325257: step 44420, loss = 0.77 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 20:00:26.604985: step 44430, loss = 0.89 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:00:27.899709: step 44440, loss = 0.67 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 20:00:29.192696: step 44450, loss = 0.68 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:00:30.479113: step 44460, loss = 0.75 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:00:31.788867: step 44470, loss = 0.66 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 20:00:33.055697: step 44480, loss = 0.73 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:00:34.317950: step 44490, loss = 0.70 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-08 20:00:35.692892: step 44500, loss = 0.73 (930.9 examples/sec; 0.137 sec/batch)
2017-05-08 20:00:36.890699: step 44510, loss = 0.81 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-08 20:00:38.144073: step 44520, loss = 0.84 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-08 20:00:39.409965: step 44530, loss = 0.65 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 20:00:40.717340: step 44540, loss = 0.77 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 20:00:42.013056: step 44550, loss = 0.69 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 20:00:43.279880: step 44560, loss = 0.82 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:00:44.548491: step 44570, loss = 0.67 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:00:45.849794: step 44580, loss = 0.76 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 20:00:47.123251: step 44590, loss = 0.71 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:00:48.481559: step 44600, loss = 0.74 (942.3 examples/sec; 0.136 sec/batch)
2017-05-08 20:00:49.670158: step 44610, loss = 0.80 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-08 20:00:50.948136: step 44620, loss = 0.75 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 20:00:52.215913: step 44630, loss = 0.72 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:00:53.468145: step 44640, loss = 0.71 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-08 20:00:54.760263: step 44650, loss = 0.73 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 20:00:56.044134: step 44660, loss = 0.79 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 20:00:57.321674: step 44670, loss = 0.77 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 20:00:58.597231: step 44680, loss = 0.80 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 20:00:59.862485: step 44690, loss = 0.92 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:01:01.246941: step 44700, loss = 0.62 (924.6 examples/sec; 0.138 sec/batch)
2017-05-08 20:01:02.512409: step 44710, loss = 0.67 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 20:01:03.765635: step 44720, loss = 0.61 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-08 20:01:05.088391: step 44730, loss = 0.78 (967.7 examples/sec; 0.132 sec/batch)
2017-05-08 20:01:06.380571: step 44740, loss = 0.68 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 20:01:07.664793: step 44750, loss = 0.85 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 20:01:08.939088: step 44760, loss = 0.72 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 20:01:10.213864: step 44770, loss = 0.61 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:01:11.477021: step 44780, loss = 0.70 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-08 20:01:12.725840: step 44790, loss = 0.83 (1025.0 examples/sec; 0.125 sec/batch)
2017-05-08 20:01:14.089688: step 44800, loss = 0.85 (938.5 examples/sec; 0.136 sec/batch)
2017-05-08 20:01:15.271924: step 44810, loss = 0.80 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-08 20:01:16.543738: step 44820, loss = 0.64 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:01:17.851004: step 44830, loss = 0.72 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 20:01:19.123887: step 44840, loss = 0.81 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:01:20.447300: step 44850, loss = 0.62 (967.2 examples/sec; 0.132 sec/batch)
2017-05-08 20:01:21.767682: step 44860, loss = 0.88 (969.4 examples/sec; 0.132 sec/batch)
2017-05-08 20:01:23.028892: step 44870, loss = 0.66 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:01:24.309113: step 44880, loss = 0.91 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:01:25.597580: step 44890, loss = 0.70 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 20:01:27.011534: step 44900, loss = 0.92 (905.3 examples/sec; 0.141 sec/batch)
2017-05-08 20:01:28.213113: step 44910, loss = 0.63 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-08 20:01:29.494959: step 44920, loss = 0.74 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 20:01:30.776208: step 44930, loss = 0.71 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 20:01:32.055777: step 44940, loss = 0.83 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:01:33.407546: step 44950, loss = 0.82 (946.9 examples/sec; 0.135 sec/batch)
2017-05-08 20:01:34.696083: step 44960, loss = 0.67 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 20:01:35.961410: step 44970, loss = 0.67 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:01:37.239763: step 44980, loss = 0.69 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:01:38.528984: step 44990, loss = 0.74 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 20:01:39.933062: step 45000, loss = 0.67 (911.6 examples/sec; 0.140 sec/batch)
2017-05-08 20:01:41.126016: step 45010, loss = 0.78 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-08 20:01:42.447160: step 45020, loss = 0.66 (968.9 examples/sec; 0.132 sec/batch)
2017-05-08 20:01:43.724798: step 45030, loss = 0.72 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:01:45.029119: step 45040, loss = 0.62 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 20:01:46.320994: step 45050, loss = 0.76 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 20:01:47.618498: step 45060, loss = 0.65 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 20:01:48.948220: step 45070, loss = 0.63 (962.6 examples/sec; 0.133 sec/batch)
2017-05-08 20:01:50.240693: step 45080, loss = 0.78 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 20:01:51.566179: step 45090, loss = 0.73 (965.7 examples/sec; 0.133 sec/batch)
2017-05-08 20:01:52.965092: step 45100, loss = 0.77 (915.0 examples/sec; 0.140 sec/batch)
2017-05-08 20:01:54.147737: step 45110, loss = 0.75 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-08 20:01:55.401885: step 45120, loss = 0.82 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-08 20:01:56.698536: step 45130, loss = 0.77 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 20:01:57.983560: step 45140, loss = 0.68 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 20:01:59.270865: step 45150, loss = 0.62 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 20:02:00.551711: step 45160, loss = 0.73 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:01.803549: step 45170, loss = 0.81 (1022.5 examples/sec; 0.125 sec/batch)
2017-05-08 20:02:03.068784: step 45180, loss = 0.88 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:02:04.343283: step 45190, loss = 0.76 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 20:02:05.713095: step 45200, loss = 0.70 (934.4 examples/sec; 0.137 sec/batch)
2017-05-08 20:02:06.907253: step 45210, loss = 0.74 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-08 20:02:08.184785: step 45220, loss = 0.79 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:09.464682: step 45230, loss = 0.72 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:10.720937: step 45240, loss = 0.95 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:02:12.022890: step 45250, loss = 0.73 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 20:02:13.299662: step 45260, loss = 0.91 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:14.583894: step 45270, loss = 0.81 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:15.845135: step 45280, loss = 0.86 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:02:17.123818: step 45290, loss = 0.71 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:18.524635: step 45300, loss = 0.69 (913.7 examples/sec; 0.140 sec/batch)
2017-05-08 20:02:19.724299: step 45310, loss = 0.69 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-08 20:02:21.036279: step 45320, loss = 0.73 (975.6 examples/sec; 0.131 sec/batch)
2017-05-08 20:02:22.318399: step 45330, loss = 0.62 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:23.605727: step 45340, loss = 0.79 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 20:02:24.881716: step 45350, loss = 0.70 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:26.167560: step 45360, loss = 0.68 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 20:02:27.481440: step 45370, loss = 0.84 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 20:02:28.758573: step 45380, loss = 0.60 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:30.038555: step 45390, loss = 0.67 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:31.411687: step 45400, loss = 0.64 (932.2 examples/sec; 0.137 sec/batch)
2017-05-08 20:02:32.578962: step 45410, loss = 0.67 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-08 20:02:33.867773: step 45420, loss = 0.88 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 20:02:35.176753: step 45430, loss = 0.64 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 20:02:36.458863: step 45440, loss = 0.79 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:37.726805: step 45450, loss = 0.76 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 20:02:39.024948: step 45460, loss = 0.68 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 20:02:40.289557: step 45470, loss = 0.73 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 20:02:41.614586: step 45480, loss = 0.79 (966.0 examples/sec; 0.133 sec/batch)
2017-05-08 20:02:42.893295: step 45490, loss = 0.59 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:44.298810: step 45500, loss = 0.82 (910.7 examples/sec; 0.141 sec/batch)
2017-05-08 20:02:45.517033: step 45510, loss = 0.71 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-08 20:02:46.785512: step 45520, loss = 0.60 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:02:48.046101: step 45530, loss = 0.74 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 20:02:49.361864: step 45540, loss = 0.80 (972.8 examples/sec; 0.132 sec/batch)
2017-05-08 20:02:50.674833: step 45550, loss = 0.69 (974.9 examples/sec; 0.131 sec/batch)
2017-05-08 20:02:51.955518: step 45560, loss = 0.78 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:53.250038: step 45570, loss = 0.88 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 20:02:54.543292: step 45580, loss = 0.66 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 20:02:55.858792: step 45590, loss = 0.75 (973.0 examples/sec; 0.132 sec/batch)
2017-05-08 20:02:57.236333: step 45600, loss = 0.66 (929.2 examples/sec; 0.138 sec/batch)
2017-05-08 20:02:58.473704: step 45610, loss = 0.77 (1034.5 examples/sec; 0.124 sec/batch)
2017-05-08 20:02:59.779735: step 45620, loss = 0.79 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 20:03:01.053494: step 45630, loss = 0.70 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 20:03:02.374183: step 45640, loss = 0.75 (969.2 examples/sec; 0.132 sec/batch)
2017-05-08 20:03:03.643783: step 45650, loss = 0.67 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 20:03:04.905272: step 45660, loss = 0.70 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-08 20:03:06.175566: step 45670, loss = 0.80 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:03:07.471729: step 45680, loss = 0.71 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 20:03:08.746058: step 45690, loss = 0.68 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 20:03:10.142155: step 45700, loss = 0.79 (916.8 examples/sec; 0.140 sec/batch)
2017-05-08 20:03:11.357827: step 45710, loss = 0.75 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-08 20:03:12.595749: step 45720, loss = 0.70 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-08 20:03:13.862695: step 45730, loss = 0.65 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 20:03:15.131833: step 45740, loss = 0.67 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:03:16.396875: step 45750, loss = 0.79 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:03:17.735652: step 45760, loss = 0.70 (956.1 examples/sec; 0.134 sec/batch)
2017-05-08 20:03:19.016429: step 45770, loss = 0.67 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 20:03:20.325442: step 45780, loss = 0.76 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 20:03:21.599913: step 45790, loss = 0.71 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 20:03:22.972475: step 45800, loss = 0.78 (932.6 examples/sec; 0.137 sec/batch)
2017-05-08 20:03:24.186473: step 45810, loss = 0.78 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-08 20:03:25.473065: step 45820, loss = 0.84 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 20:03:26.763976: step 45830, loss = 0.80 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 20:03:28.058624: step 45840, loss = 0.95 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 20:03:29.337259: step 45850, loss = 0.58 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 20:03:30.660159: step 45860, loss = 0.77 (967.6 examples/sec; 0.132 sec/batch)
2017-05-08 20:03:31.926999: step 45870, loss = 0.75 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:03:33.238912: step 45880, loss = 0.75 (975.7 examples/sec; 0.131 sec/batch)
2017-05-08 20:03:34.531815: step 45890, loss = 0.68 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:03:35.902997: step 45900, loss = 0.75 (933.5 examples/sec; 0.137 sec/batch)
2017-05-08 20:03:37.127917: step 45910, loss = 0.68 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-08 20:03:38.429363: step 45920, loss = 0.62 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 20:03:39.720970: step 45930, loss = 0.69 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:03:41.002091: step 45940, loss = 0.83 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 20:03:42.308264: step 45950, loss = 0.67 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 20:03:43.572011: step 45960, loss = 0.79 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:03:44.857127: step 45970, loss = 0.63 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:03:46.115426: step 45980, loss = 0.65 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-08 20:03:47.376339: step 45990, loss = 0.75 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 20:03:48.739983: step 46000, loss = 0.78 (938.7 examples/sec; 0.136 sec/batch)
2017-05-08 20:03:49.937834: step 46010, loss = 0.86 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-08 20:03:51.222177: step 46020, loss = 0.80 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 20:03:52.491307: step 46030, loss = 0.91 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:03:53.755912: step 46040, loss = 0.71 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 20:03:55.037157: step 46050, loss = 0.74 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 20:03:56.360925: step 46060, loss = 0.63 (966.9 examples/sec; 0.132 sec/batch)
2017-05-08 20:03:57.646119: step 46070, loss = 0.74 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:03:58.948841: step 46080, loss = 0.76 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 20:04:00.208905: step 46090, loss = 0.71 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:04:01.607146: step 46100, loss = 0.67 (915.4 examples/sec; 0.140 sec/batch)
2017-05-08 20:04:02.816083: step 46110, loss = 0.70 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-08 20:04:04.070585: step 46120, loss = 0.83 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-08 20:04:05.367546: step 46130, loss = 0.66 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 20:04:06.682023: step 46140, loss = 0.72 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 20:04:07.966652: step 46150, loss = 0.88 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 20:04:09.253550: step 46160, loss = 0.75 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 20:04:10.535057: step 46170, loss = 0.72 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:04:11.808957: step 46180, loss = 0.73 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:04:13.101946: step 46190, loss = 0.76 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 20:04:14.470261: step 46200, loss = 0.57 (935.5 examples/sec; 0.137 sec/batch)
2017-05-08 20:04:15.695091: step 46210, loss = 0.80 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-08 20:04:16.999093: step 46220, loss = 0.52 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 20:04:18.299792: step 46230, loss = 0.71 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 20:04:19.594255: step 46240, loss = 0.95 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 20:04:20.831329: step 46250, loss = 0.86 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-08 20:04:22.118835: step 46260, loss = 0.82 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 20:04:23.386687: step 46270, loss = 0.76 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:04:24.639887: step 46280, loss = 0.75 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-08 20:04:25.930703: step 46290, loss = 0.85 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 20:04:27.316753: step 46300, loss = 0.60 (923.5 examples/sec; 0.139 sec/batch)
2017-05-08 20:04:28.525731: step 46310, loss = 0.76 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-08 20:04:29.836215: step 46320, loss = 0.81 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 20:04:31.122863: step 46330, loss = 0.66 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 20:04:32.389622: step 46340, loss = 0.84 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 20:04:33.663809: step 46350, loss = 0.79 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:04:34.950700: step 46360, loss = 0.68 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 20:04:36.278715: step 46370, loss = 0.90 (963.8 examples/sec; 0.133 sec/batch)
2017-05-08 20:04:37.587800: step 46380, loss = 0.78 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 20:04:38.873473: step 46390, loss = 0.80 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 20:04:40.244434: step 46400, loss = 0.80 (933.6 examples/sec; 0.137 sec/batch)
2017-05-08 20:04:41.465700: step 46410, loss = 0.70 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-08 20:04:42.761856: step 46420, loss = 0.62 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 20:04:44.059587: step 46430, loss = 0.55 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 20:04:45.374309: step 46440, loss = 0.75 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 20:04:46.685798: step 46450, loss = 0.77 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 20:04:47.968252: step 46460, loss = 0.57 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 20:04:49.242109: step 46470, loss = 0.73 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:04:50.516879: step 46480, loss = 0.93 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:04:51.804555: step 46490, loss = 0.70 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:04:53.180577: step 46500, loss = 0.67 (930.2 examples/sec; 0.138 sec/batch)
2017-05-08 20:04:54.376784: step 46510, loss = 0.88 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-08 20:04:55.644589: step 46520, loss = 0.72 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:04:56.927658: step 46530, loss = 0.74 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 20:04:58.253797: step 46540, loss = 0.76 (965.2 examples/sec; 0.133 sec/batch)
2017-05-08 20:04:59.535923: step 46550, loss = 0.82 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:00.803780: step 46560, loss = 0.88 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:05:02.099441: step 46570, loss = 0.72 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 20:05:03.388490: step 46580, loss = 0.58 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:05:04.660774: step 46590, loss = 0.81 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:05:06.045835: step 46600, loss = 0.66 (924.1 examples/sec; 0.139 sec/batch)
2017-05-08 20:05:07.251738: step 46610, loss = 0.80 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-08 20:05:08.550203: step 46620, loss = 0.79 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 20:05:09.854846: step 46630, loss = 0.76 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 20:05:11.125511: step 46640, loss = 0.75 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 20:05:12.393241: step 46650, loss = 0.75 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:05:13.668403: step 46660, loss = 0.61 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:14.947532: step 46670, loss = 0.87 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:16.225473: step 46680, loss = 0.85 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:17.504481: step 46690, loss = 0.64 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:18.877085: step 46700, loss = 0.81 (932.5 examples/sec; 0.137 sec/batch)
2017-05-08 20:05:20.050986: step 46710, loss = 0.70 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-08 20:05:21.359856: step 46720, loss = 0.83 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 20:05:22.657660: step 46730, loss = 0.74 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 20:05:23.951095: step 46740, loss = 0.70 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 20:05:25.286067: step 46750, loss = 0.73 (958.8 examples/sec; 0.133 sec/batch)
2017-05-08 20:05:26.565812: step 46760, loss = 0.77 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:27.855848: step 46770, loss = 0.84 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 20:05:29.140499: step 46780, loss = 0.83 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:30.423591: step 46790, loss = 0.59 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:31.790268: step 46800, loss = 0.70 (936.6 examples/sec; 0.137 sec/batch)
2017-05-08 20:05:32.963886: step 46810, loss = 0.67 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-08 20:05:34.269781: step 46820, loss = 0.78 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 20:05:35.578835: step 46830, loss = 0.83 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 20:05:36.869628: step 46840, loss = 0.63 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 20:05:38.172814: step 46850, loss = 0.55 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 20:05:39.451271: step 46860, loss = 0.82 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:40.715528: step 46870, loss = 0.60 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 20:05:41.962313: step 46880, loss = 0.65 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-08 20:05:43.232534: step 46890, loss = 0.67 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:05:44.608066: step 46900, loss = 0.77 (930.5 examples/sec; 0.138 sec/batch)
2017-05-08 20:05:45.867896: step 46910, loss = 0.72 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 20:05:47.157565: step 46920, loss = 0.73 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 20:05:48.454240: step 46930, loss = 0.57 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 20:05:49.728684: step 46940, loss = 0.71 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:05:51.040536: step 46950, loss = 0.73 (975.7 examples/sec; 0.131 sec/batch)
2017-05-08 20:05:52.313968: step 46960, loss = 0.71 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 20:05:53.618490: step 46970, loss = 0.81 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 20:05:54.939992: step 46980, loss = 0.84 (968.6 examples/sec; 0.132 sec/batch)
2017-05-08 20:05:56.192284: step 46990, loss = 0.72 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-08 20:05:57.548156: step 47000, loss = 0.77 (944.1 examples/sec; 0.136 sec/batch)
2017-05-08 20:05:58.753148: step 47010, loss = 0.74 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-08 20:06:00.033095: step 47020, loss = 0.72 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 20:06:01.356343: step 47030, loss = 0.73 (967.3 examples/sec; 0.132 sec/batch)
2017-05-08 20:06:02.638560: step 47040, loss = 0.80 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:06:03.917610: step 47050, loss = 0.83 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 20:06:05.201672: step 47060, loss = 0.74 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:06:06.515713: step 47070, loss = 0.89 (974.1 examples/sec; 0.131 sec/batch)
2017-05-08 20:06:07.835756: step 47080, loss = 0.66 (969.7 examples/sec; 0.132 sec/batch)
2017-05-08 20:06:09.112688: step 47090, loss = 0.82 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 20:06:10.476606: step 47100, loss = 0.76 (938.5 examples/sec; 0.136 sec/batch)
2017-05-08 20:06:11.715908: step 47110, loss = 0.80 (1032.8 examples/sec; 0.124 sec/batch)
2017-05-08 20:06:12.988300: step 47120, loss = 0.68 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:14.315685: step 47130, loss = 0.69 (964.3 examples/sec; 0.133 sec/batch)
2017-05-08 20:06:15.587127: step 47140, loss = 0.64 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:16.853677: step 47150, loss = 0.70 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:18.128087: step 47160, loss = 0.65 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:19.435130: step 47170, loss = 0.81 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 20:06:20.740361: step 47180, loss = 0.73 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 20:06:22.023650: step 47190, loss = 0.73 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 20:06:23.382336: step 47200, loss = 0.73 (942.1 examples/sec; 0.136 sec/batch)
2017-05-08 20:06:24.584360: step 47210, loss = 0.87 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-08 20:06:25.894931: step 47220, loss = 0.75 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 20:06:27.199641: step 47230, loss = 0.68 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 20:06:28.468817: step 47240, loss = 0.85 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:29.746177: step 47250, loss = 0.66 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 20:06:31.029725: step 47260, loss = 0.61 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:06:32.313357: step 47270, loss = 0.65 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:06:33.567317: step 47280, loss = 0.69 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-08 20:06:34.842218: step 47290, loss = 0.82 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:36.209065: step 47300, loss = 0.65 (936.5 examples/sec; 0.137 sec/batch)
2017-05-08 20:06:37.393439: step 47310, loss = 0.63 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-08 20:06:38.702587: step 47320, loss = 0.74 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 20:06:40.006735: step 47330, loss = 0.99 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 20:06:41.312001: step 47340, loss = 0.60 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 20:06:42.613503: step 47350, loss = 0.75 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 20:06:43.907992: step 47360, loss = 0.84 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 20:06:45.199408: step 47370, loss = 0.68 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 20:06:46.473411: step 47380, loss = 0.68 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:47.742902: step 47390, loss = 0.68 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:49.136688: step 47400, loss = 0.85 (918.4 examples/sec; 0.139 sec/batch)
2017-05-08 20:06:50.317020: step 47410, loss = 0.73 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-08 20:06:51.605234: step 47420, loss = 0.79 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 20:06:52.912197: step 47430, loss = 0.80 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 20:06:54.193367: step 47440, loss = 0.72 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 20:06:55.478194: step 47450, loss = 0.69 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:06:56.781691: step 47460, loss = 0.59 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 20:06:58.092168: step 47470, loss = 0.82 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 20:06:59.392824: step 47480, loss = 0.83 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 20:07:00.676361: step 47490, loss = 0.69 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:07:02.070859: step 47500, loss = 0.68 (917.9 examples/sec; 0.139 sec/batch)
2017-05-08 20:07:03.261424: step 47510, loss = 0.76 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-08 20:07:04.567958: step 47520, loss = 0.64 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 20:07:05.851241: step 47530, loss = 0.72 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 20:07:07.114926: step 47540, loss = 0.67 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:07:08.384235: step 47550, loss = 0.84 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:07:09.660075: step 47560, loss = 0.74 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:07:10.942162: step 47570, loss = 0.84 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 20:07:12.211662: step 47580, loss = 0.75 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 20:07:13.489943: step 47590, loss = 0.73 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:07:14.867489: step 47600, loss = 0.69 (929.2 examples/sec; 0.138 sec/batch)
2017-05-08 20:07:16.039779: step 47610, loss = 0.68 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-08 20:07:17.323536: step 47620, loss = 0.67 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 20:07:18.591803: step 47630, loss = 0.72 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 20:07:19.867584: step 47640, loss = 0.67 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:07:21.163919: step 47650, loss = 0.54 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 20:07:22.461024: step 47660, loss = 0.73 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 20:07:23.749070: step 47670, loss = 0.86 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 20:07:25.049069: step 47680, loss = 0.66 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 20:07:26.348640: step 47690, loss = 0.75 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 20:07:27.693551: step 47700, loss = 0.72 (951.7 examples/sec; 0.134 sec/batch)
2017-05-08 20:07:28.879516: step 47710, loss = 0.86 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-08 20:07:30.150062: step 47720, loss = 0.72 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:07:31.420666: step 47730, loss = 0.72 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:07:32.692233: step 47740, loss = 0.80 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:07:34.011095: step 47750, loss = 0.59 (970.5 examples/sec; 0.132 sec/batch)
2017-05-08 20:07:35.305251: step 47760, loss = 0.75 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 20:07:36.583338: step 47770, loss = 0.71 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 20:07:37.870796: step 47780, loss = 0.74 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 20:07:39.198267: step 47790, loss = 0.61 (964.2 examples/sec; 0.133 sec/batch)
2017-05-08 20:07:40.569644: step 47800, loss = 0.74 (933.4 examples/sec; 0.137 sec/batch)
2017-05-08 20:07:41.765388: step 47810, loss = 0.60 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-08 20:07:43.071909: step 47820, loss = 0.74 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 20:07:44.361062: step 47830, loss = 0.84 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 20:07:45.646490: step 47840, loss = 0.77 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 20:07:46.943352: step 47850, loss = 0.65 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 20:07:48.253317: step 47860, loss = 0.59 (977.1 examples/sec; 0.131 sec/batch)
2017-05-08 20:07:49.553532: step 47870, loss = 0.68 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 20:07:50.837377: step 47880, loss = 0.77 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 20:07:52.117619: step 47890, loss = 0.72 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:07:53.491928: step 47900, loss = 0.75 (931.4 examples/sec; 0.137 sec/batch)
2017-05-08 20:07:54.684539: step 47910, loss = 0.79 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-08 20:07:55.966716: step 47920, loss = 0.76 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:07:57.283963: step 47930, loss = 0.77 (971.7 examples/sec; 0.132 sec/batch)
2017-05-08 20:07:58.587930: step 47940, loss = 0.64 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 20:07:59.846068: step 47950, loss = 0.77 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:01.128216: step 47960, loss = 0.78 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:08:02.412059: step 47970, loss = 0.77 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 20:08:03.701413: step 47980, loss = 0.67 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 20:08:05.020652: step 47990, loss = 0.74 (970.3 examples/sec; 0.132 sec/batch)
2017-05-08 20:08:06.387200: step 48000, loss = 0.88 (936.7 examples/sec; 0.137 sec/batch)
2017-05-08 20:08:07.566605: step 48010, loss = 0.60 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-08 20:08:08.832387: step 48020, loss = 0.83 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:10.136138: step 48030, loss = 0.67 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 20:08:11.417580: step 48040, loss = 0.67 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 20:08:12.675090: step 48050, loss = 0.78 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:13.938627: step 48060, loss = 0.64 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:15.217422: step 48070, loss = 0.85 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 20:08:16.471802: step 48080, loss = 0.57 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-08 20:08:17.738249: step 48090, loss = 0.78 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:19.117665: step 48100, loss = 0.66 (927.9 examples/sec; 0.138 sec/batch)
2017-05-08 20:08:20.388580: step 48110, loss = 0.73 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:21.657119: step 48120, loss = 0.67 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:22.916443: step 48130, loss = 0.71 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:24.188871: step 48140, loss = 0.81 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:25.461094: step 48150, loss = 0.72 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:26.736644: step 48160, loss = 0.68 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 20:08:28.020217: step 48170, loss = 0.95 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:08:29.316421: step 48180, loss = 0.73 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 20:08:30.635116: step 48190, loss = 0.62 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 20:08:31.990306: step 48200, loss = 0.65 (944.5 examples/sec; 0.136 sec/batch)
2017-05-08 20:08:33.211335: step 48210, loss = 0.87 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-08 20:08:34.471206: step 48220, loss = 0.77 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:35.779400: step 48230, loss = 0.76 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 20:08:37.074374: step 48240, loss = 0.78 (988.4 examples/sec; 0.129 sec/batch)
2017-05-08 20:08:38.369499: step 48250, loss = 0.84 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 20:08:39.635316: step 48260, loss = 0.62 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:40.910680: step 48270, loss = 0.75 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 20:08:42.203732: step 48280, loss = 0.77 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 20:08:43.487626: step 48290, loss = 0.84 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 20:08:44.880627: step 48300, loss = 0.58 (918.9 examples/sec; 0.139 sec/batch)
2017-05-08 20:08:46.115867: step 48310, loss = 0.80 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-08 20:08:47.417357: step 48320, loss = 0.84 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 20:08:48.668947: step 48330, loss = 0.86 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-08 20:08:49.939260: step 48340, loss = 0.72 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:51.234046: step 48350, loss = 0.67 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 20:08:52.483983: step 48360, loss = 0.69 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-08 20:08:53.762751: step 48370, loss = 0.85 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 20:08:55.025262: step 48380, loss = 0.67 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:56.314981: step 48390, loss = 0.82 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 20:08:57.686990: step 48400, loss = 0.67 (932.9 examples/sec; 0.137 sec/batch)
2017-05-08 20:08:58.885366: step 48410, loss = 0.81 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-08 20:09:00.168514: step 48420, loss = 0.69 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 20:09:01.445747: step 48430, loss = 0.65 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:09:02.714563: step 48440, loss = 0.88 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:03.997702: step 48450, loss = 0.79 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 20:09:05.324391: step 48460, loss = 0.66 (964.8 examples/sec; 0.133 sec/batch)
2017-05-08 20:09:06.613113: step 48470, loss = 0.66 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 20:09:07.902403: step 48480, loss = 0.72 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 20:09:09.175163: step 48490, loss = 0.70 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:10.542685: step 48500, loss = 0.85 (936.0 examples/sec; 0.137 sec/batch)
2017-05-08 20:09:11.777303: step 48510, loss = 0.86 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-08 20:09:12.996928: step 48520, loss = 0.71 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-08 20:09:14.268038: step 48530, loss = 0.91 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:15.532217: step 48540, loss = 0.80 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 20:09:16.826319: step 48550, loss = 0.71 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 20:09:18.163310: step 48560, loss = 0.57 (957.4 examples/sec; 0.134 sec/batch)
2017-05-08 20:09:19.462801: step 48570, loss = 0.62 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 20:09:20.760977: step 48580, loss = 0.72 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 20:09:22.068528: step 48590, loss = 0.80 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 20:09:23.474257: step 48600, loss = 0.71 (910.6 examples/sec; 0.141 sec/batch)
2017-05-08 20:09:24.716968: step 48610, loss = 0.70 (1030.0 examples/sec; 0.124 sec/batch)
2017-05-08 20:09:26.018421: step 48620, loss = 0.89 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 20:09:27.321820: step 48630, loss = 0.69 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 20:09:28.631205: step 48640, loss = 0.69 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 20:09:29.898559: step 48650, loss = 0.68 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:31.193367: step 48660, loss = 0.65 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 20:09:32.493223: step 48670, loss = 0.94 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 20:09:33.809628: step 48680, loss = 0.62 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 20:09:35.074661: step 48690, loss = 0.82 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:36.480259: step 48700, loss = 0.81 (910.6 examples/sec; 0.141 sec/batch)
2017-05-08 20:09:37.719013: step 48710, loss = 0.61 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-08 20:09:38.961714: step 48720, loss = 0.73 (1030.0 examples/sec; 0.124 sec/batch)
2017-05-08 20:09:40.253369: step 48730, loss = 0.78 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:09:41.560838: step 48740, loss = 0.64 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 20:09:42.917807: step 48750, loss = 0.76 (943.3 examples/sec; 0.136 sec/batch)
2017-05-08 20:09:44.154330: step 48760, loss = 0.80 (1035.2 examples/sec; 0.124 sec/batch)
2017-05-08 20:09:45.406883: step 48770, loss = 0.69 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-08 20:09:46.678062: step 48780, loss = 0.66 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:47.959672: step 48790, loss = 0.75 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 20:09:49.348725: step 48800, loss = 0.71 (921.5 examples/sec; 0.139 sec/batch)
2017-05-08 20:09:50.547722: step 48810, loss = 0.61 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-08 20:09:51.822625: step 48820, loss = 0.63 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:53.095082: step 48830, loss = 1.01 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:54.395735: step 48840, loss = 0.73 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 20:09:55.712479: step 48850, loss = 0.57 (972.1 examples/sec; 0.132 sec/batch)
2017-05-08 20:09:56.978044: step 48860, loss = 0.76 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:58.266910: step 48870, loss = 0.75 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 20:09:59.555821: step 48880, loss = 0.77 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 20:10:00.873945: step 48890, loss = 0.68 (971.1 examples/sec; 0.132 sec/batch)
2017-05-08 20:10:02.277296: step 48900, loss = 0.79 (912.1 examples/sec; 0.140 sec/batch)
2017-05-08 20:10:03.483401: step 48910, loss = 0.65 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-08 20:10:04.765268: step 48920, loss = 0.71 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 20:10:06.038136: step 48930, loss = 0.67 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:10:07.330882: step 48940, loss = 0.73 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 20:10:08.642033: step 48950, loss = 0.68 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 20:10:09.906202: step 48960, loss = 0.84 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 20:10:11.200495: step 48970, loss = 0.75 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:10:12.483882: step 48980, loss = 0.71 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 20:10:13.807762: step 48990, loss = 0.68 (966.9 examples/sec; 0.132 sec/batch)
2017-05-08 20:10:15.211028: step 49000, loss = 0.59 (912.2 examples/sec; 0.140 sec/batch)
2017-05-08 20:10:16.458209: step 49010, loss = 0.79 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-08 20:10:17.743337: step 49020, loss = 0.66 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:10:19.025866: step 49030, loss = 0.76 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 20:10:20.324038: step 49040, loss = 0.66 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 20:10:21.611668: step 49050, loss = 0.67 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 20:10:22.925687: step 49060, loss = 0.70 (974.1 examples/sec; 0.131 sec/batch)
2017-05-08 20:10:24.228310: step 49070, loss = 0.72 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 20:10:25.528655: step 49080, loss = 0.79 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 20:10:26.843307: step 49090, loss = 0.78 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 20:10:28.221219: step 49100, loss = 0.83 (928.9 examples/sec; 0.138 sec/batch)
2017-05-08 20:10:29.411251: step 49110, loss = 0.62 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-08 20:10:30.664198: step 49120, loss = 0.78 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-08 20:10:31.935979: step 49130, loss = 0.70 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 20:10:33.243732: step 49140, loss = 0.74 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 20:10:34.536857: step 49150, loss = 0.77 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 20:10:35.804162: step 49160, loss = 0.78 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:10:37.105006: step 49170, loss = 0.64 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 20:10:38.376889: step 49180, loss = 0.77 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:10:39.632049: step 49190, loss = 0.73 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-08 20:10:41.029027: step 49200, loss = 0.67 (916.3 examples/sec; 0.140 sec/batch)
2017-05-08 20:10:42.226719: step 49210, loss = 0.75 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-08 20:10:43.487118: step 49220, loss = 0.70 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 20:10:44.789244: step 49230, loss = 0.76 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 20:10:46.063223: step 49240, loss = 0.91 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:10:47.349708: step 49250, loss = 0.65 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:10:48.658702: step 49260, loss = 0.70 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 20:10:49.950518: step 49270, loss = 0.81 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 20:10:51.228425: step 49280, loss = 0.63 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 20:10:52.425218: step 49290, loss = 0.76 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-08 20:10:53.429595: step 49300, loss = 0.84 (1274.4 examples/sec; 0.100 sec/batch)
2017-05-08 20:10:54.228487: step 49310, loss = 0.72 (1602.2 examples/sec; 0.080 sec/batch)
2017-05-08 20:10:55.058996: step 49320, loss = 0.62 (1541.2 examples/sec; 0.083 sec/batch)
2017-05-08 20:10:55.869751: step 49330, loss = 0.65 (1578.8 examples/sec; 0.081 sec/batch)
2017-05-08 20:10:56.683612: step 49340, loss = 0.61 (1572.8 examples/sec; 0.081 sec/batch)
2017-05-08 20:10:57.506398: step 49350, loss = 0.67 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-08 20:10:58.355722: step 49360, loss = 0.66 (1507.1 examples/sec; 0.085 sec/batch)
2017-05-08 20:10:59.202280: step 49370, loss = 0.64 (1512.0 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:00.051086: step 49380, loss = 0.66 (1508.0 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:00.898563: step 49390, loss = 0.69 (1510.4 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:01.868044: step 49400, loss = 0.68 (1320.3 examples/sec; 0.097 sec/batch)
2017-05-08 20:11:02.682621: step 49410, loss = 0.68 (1571.4 examples/sec; 0.081 sec/batch)
2017-05-08 20:11:03.505024: step 49420, loss = 0.77 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-08 20:11:04.309701: step 49430, loss = 0.64 (1590.7 examples/sec; 0.080 sec/batch)
2017-05-08 20:11:05.129519: step 49440, loss = 0.77 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-08 20:11:05.975358: step 49450, loss = 0.75 (1513.3 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:06.829685: step 49460, loss = 0.68 (1498.2 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:07.675064: step 49470, loss = 0.69 (1514.1 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:08.529480: step 49480, loss = 0.72 (1498.2 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:09.385830: step 49490, loss = 0.74 (1494.7 examples/sec; 0.086 sec/batch)
2017-05-08 20:11:10.385033: step 49500, loss = 0.82 (1281.0 examples/sec; 0.100 sec/batch)
2017-05-08 20:11:11.189889: step 49510, loss = 0.89 (1590.4 examples/sec; 0.080 sec/batch)
2017-05-08 20:11:12.011033: step 49520, loss = 0.62 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-08 20:11:12.825175: step 49530, loss = 0.71 (1572.2 examples/sec; 0.081 sec/batch)
2017-05-08 20:11:13.650932: step 49540, loss = 0.70 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-08 20:11:14.482775: step 49550, loss = 0.58 (1538.7 examples/sec; 0.083 sec/batch)
2017-05-08 20:11:15.327635: step 49560, loss = 0.72 (1515.0 examples/sec; 0.084 sec/batch)
2017-05-08 20:11:16.177105: step 49570, loss = 0.68 (1506.8 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:17.031394: step 49580, loss = 0.68 (1498.3 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:17.884069: step 49590, loss = 0.80 (1501.2 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:18.839530: step 49600, loss = 0.74 (1339.7 examples/sec; 0.096 sec/batch)
2017-05-08 20:11:19.667621: step 49610, loss = 0.76 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-08 20:11:20.495004: step 49620, loss = 0.71 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-08 20:11:21.330133: step 49630, loss = 0.78 (1532.7 examples/sec; 0.084 sec/batch)
2017-05-08 20:11:22.149623: step 49640, loss = 0.83 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-08 20:11:23.000757: step 49650, loss = 0.87 (1503.9 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:23.850070: step 49660, loss = 0.71 (1507.1 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:24.706555: step 49670, loss = 0.87 (1494.5 examples/sec; 0.086 sec/batch)
2017-05-08 20:11:25.555773: step 49680, loss = 0.67 (1507.3 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:26.411991: step 49690, loss = 0.68 (1494.9 examples/sec; 0.086 sec/batch)
2017-05-08 20:11:27.373043: step 49700, loss = 0.62 (1331.9 examples/sec; 0.096 sec/batch)
2017-05-08 20:11:28.185983: step 49710, loss = 0.59 (1574.5 examples/sec; 0.081 sec/batch)
2017-05-08 20:11:28.986862: step 49720, loss = 0.72 (1598.2 examples/sec; 0.080 sec/batch)
2017-05-08 20:11:29.835998: step 49730, loss = 0.63 (1507.4 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:30.693680: step 49740, loss = 0.79 (1492.4 examples/sec; 0.086 sec/batch)
2017-05-08 20:11:31.541378: step 49750, loss = 0.74 (1510.0 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:32.401056: step 49760, loss = 0.67 (1488.9 examples/sec; 0.086 sec/batch)
2017-05-08 20:11:33.251969: step 49770, loss = 0.74 (1504.3 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:34.103725: step 49780, loss = 0.83 (1502.8 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:34.966516: step 49790, loss = 0.77 (1483.6 examples/sec; 0.086 sec/batch)
2017-05-08 20:11:35.914231: step 49800, loss = 0.66 (1350.6 examples/sec; 0.095 sec/batch)
2017-05-08 20:11:36.714650: step 49810, loss = 0.78 (1599.2 examples/sec; 0.080 sec/batch)
2017-05-08 20:11:37.538898: step 49820, loss = 0.69 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-08 20:11:38.382428: step 49830, loss = 0.72 (1517.4 examples/sec; 0.084 sec/batch)
2017-05-08 20:11:39.238364: step 49840, loss = 0.70 (1495.4 examples/sec; 0.086 sec/batch)
2017-05-08 20:11:40.094614: step 49850, loss = 0.64 (1494.9 examples/sec; 0.086 sec/batch)
2017-05-08 20:11:40.956505: step 49860, loss = 0.73 (1485.1 examples/sec; 0.086 sec/batch)
2017-05-08 20:11:41.811216: step 49870, loss = 0.78 (1497.6 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:42.671817: step 49880, loss = 0.76 (1487.3 examples/sec; 0.086 sec/batch)
2017-05-08 20:11:43.575630: step 49890, loss = 0.70 (1416.2 examples/sec; 0.090 sec/batch)
2017-05-08 20:11:44.485765: step 49900, loss = 0.69 (1406.4 examples/sec; 0.091 sec/batch)
2017-05-08 20:11:45.300957: step 49910, loss = 0.62 (1570.2 examples/sec; 0.082 sec/batch)
2017-05-08 20:11:46.116247: step 49920, loss = 0.61 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-08 20:11:46.982690: step 49930, loss = 0.70 (1477.3 examples/sec; 0.087 sec/batch)
2017-05-08 20:11:47.830193: step 49940, loss = 0.79 (1510.3 examples/sec; 0.085 sec/batch)
2017-05-08 20:11:48.694710: step 49950, loss = 0.64 (1480.6 examples/sec; 0.086 sec/batch)
2017-05-08 20:11:49.559781: step 49960, loss = 0.82 (1479.6 examples/sec; 0.087 sec/batch)
2017-05-08 20:11:50.422267: step 49970, loss = 0.59 (1484.1 examples/sec; 0.086 sec/batch)
2017-05-08 20:11:51.281095: step 49980, loss = 0.62 (1490.4 examples/sec; 0.086 sec/batch)
2017-05-08 20:11:52.167661: step 49990, loss = 0.66 (1443.8 examples/sec; 0.089 sec/batch)
2017-05-08 20:11:53.080492: step 50000, loss = 0.71 (1402.2 examples/sec; 0.091 sec/batch)
--- 6455.27221298 seconds ---
