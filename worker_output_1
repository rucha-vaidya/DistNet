I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:04:00.0
Total memory: 7.92GiB
Free memory: 7.81GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x45e6c00
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Quadro K620
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:03:00.0
Total memory: 1.95GiB
Free memory: 1.87GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Ignoring gpu device (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0) with Cuda multiprocessor count: 3. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
Connecting to port  15691
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-04 21:48:34.334277: step 0, loss = 4.67 (101.3 examples/sec; 1.264 sec/batch)
2017-05-04 21:48:53.909505: step 10, loss = 4.67 (65.4 examples/sec; 1.958 sec/batch)
2017-05-04 21:48:55.103913: step 20, loss = 4.65 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-04 21:48:56.304041: step 30, loss = 4.62 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-04 21:48:57.490298: step 40, loss = 4.61 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:48:58.665046: step 50, loss = 4.54 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 21:48:59.839525: step 60, loss = 4.44 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:01.030708: step 70, loss = 4.44 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-04 21:49:02.201409: step 80, loss = 4.45 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:03.396765: step 90, loss = 4.46 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:04.597330: step 100, loss = 4.38 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:05.764278: step 110, loss = 4.32 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:06.947486: step 120, loss = 4.33 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:08.136866: step 130, loss = 4.29 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-04 21:49:09.290709: step 140, loss = 4.39 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-04 21:49:10.458947: step 150, loss = 4.19 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:11.640284: step 160, loss = 4.31 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:12.814148: step 170, loss = 4.38 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:13.991532: step 180, loss = 4.35 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:15.171758: step 190, loss = 4.22 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:16.343863: step 200, loss = 4.16 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:17.542802: step 210, loss = 4.24 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:18.724206: step 220, loss = 4.18 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:19.924833: step 230, loss = 4.21 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:21.129785: step 240, loss = 4.13 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:22.335104: step 250, loss = 4.40 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-04 21:49:23.533212: step 260, loss = 4.04 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:24.722083: step 270, loss = 4.03 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-04 21:49:25.924289: step 280, loss = 4.24 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:27.114964: step 290, loss = 4.20 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:49:28.324334: step 300, loss = 4.05 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 21:49:29.506681: step 310, loss = 4.48 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:30.694816: step 320, loss = 4.05 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-04 21:49:31.903732: step 330, loss = 4.06 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 21:49:33.093721: step 340, loss = 4.06 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-04 21:49:34.297618: step 350, loss = 4.06 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-04 21:49:35.482172: step 360, loss = 4.04 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:36.663802: step 370, loss = 4.10 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:37.844489: step 380, loss = 3.87 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:39.020901: step 390, loss = 4.05 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:40.215187: step 400, loss = 3.94 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:49:41.399028: step 410, loss = 4.08 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:42.566605: step 420, loss = 3.95 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:43.752287: step 430, loss = 3.93 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-04 21:49:44.934036: step 440, loss = 4.00 (1083.1 examples/sec; 0.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 13 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
118 sec/batch)
2017-05-04 21:49:46.107522: step 450, loss = 4.06 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:47.275539: step 460, loss = 3.95 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:48.444039: step 470, loss = 3.93 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:49.625458: step 480, loss = 3.95 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:50.799680: step 490, loss = 3.93 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:51.966380: step 500, loss = 4.00 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:53.139708: step 510, loss = 4.12 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 21:49:54.316676: step 520, loss = 3.87 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:55.494724: step 530, loss = 3.91 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:56.656340: step 540, loss = 4.10 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-04 21:49:57.837017: step 550, loss = 3.88 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-04 21:49:59.043974: step 560, loss = 3.87 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-04 21:50:00.221587: step 570, loss = 3.71 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:01.401160: step 580, loss = 3.91 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:02.605278: step 590, loss = 3.95 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 21:50:03.782799: step 600, loss = 3.88 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:04.970697: step 610, loss = 3.85 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:06.133974: step 620, loss = 3.94 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 21:50:07.309549: step 630, loss = 3.72 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:08.487261: step 640, loss = 3.91 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:09.661966: step 650, loss = 3.94 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:10.854334: step 660, loss = 3.93 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:12.013942: step 670, loss = 3.84 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-04 21:50:13.190720: step 680, loss = 3.78 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:14.360423: step 690, loss = 3.66 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:15.532779: step 700, loss = 3.86 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:16.690840: step 710, loss = 3.62 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-04 21:50:17.856166: step 720, loss = 3.76 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:19.044545: step 730, loss = 3.99 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:20.232490: step 740, loss = 3.76 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:21.420391: step 750, loss = 3.88 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:22.583495: step 760, loss = 3.83 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-04 21:50:23.766287: step 770, loss = 3.70 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:24.947822: step 780, loss = 3.83 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:26.122668: step 790, loss = 3.91 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:27.310818: step 800, loss = 3.82 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:28.482041: step 810, loss = 3.72 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:29.652565: step 820, loss = 3.80 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:30.809886: step 830, loss = 3.61 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-04 21:50:31.988117: step 840, loss = 3.77 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:33.150309: step 850, loss = 3.88 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-04 21:50:34.317215: step 860, loss = 3.47 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:35.501464: step 870, loss = 3.73 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:36.688270: step 880, loss = 3.69 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:37.872356: step 890, loss = 3.69 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:39.044087: step 900, loss = 3.71 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:40.222491: step 910, loss = 3.77 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:41.378191: step 920, loss = 3.63 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-04 21:50:42.555047: step 930, loss = 3.67 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:43.735417: step 940, loss = 3.46 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:44.908475: step 950, loss = 3.65 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:46.075077: step 960, loss = 3.61 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:47.236895: step 970, loss = 3.58 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-04 21:50:48.427411: step 980, loss = 3.72 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:49.732063: step 990, loss = 3.64 (981.1 examples/sec; 0.130 sec/batch)
2017-05-04 21:50:50.774862: step 1000, loss = 3.59 (1227.5 examples/sec; 0.104 sec/batch)
2017-05-04 21:50:51.965556: step 1010, loss = 3.75 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:53.143459: step 1020, loss = 3.95 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:54.307473: step 1030, loss = 3.50 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-04 21:50:55.494378: step 1040, loss = 3.83 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 21:50:56.678168: step 1050, loss = 3.73 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-04 21:50:57.848108: step 1060, loss = 3.59 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 21:50:59.020706: step 1070, loss = 3.58 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:00.196360: step 1080, loss = 3.54 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:01.369440: step 1090, loss = 3.46 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:02.555776: step 1100, loss = 3.43 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:51:03.740096: step 1110, loss = 3.65 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:04.921840: step 1120, loss = 3.46 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:06.095885: step 1130, loss = 3.60 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:07.270497: step 1140, loss = 3.68 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:08.418837: step 1150, loss = 3.59 (1114.7 examples/sec; 0.115 sec/batch)
2017-05-04 21:51:09.583961: step 1160, loss = 3.53 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:10.753875: step 1170, loss = 3.51 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:11.941201: step 1180, loss = 3.43 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:51:13.108585: step 1190, loss = 3.53 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:14.260849: step 1200, loss = 3.59 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-04 21:51:15.434383: step 1210, loss = 3.51 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:16.609343: step 1220, loss = 3.44 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:17.768514: step 1230, loss = 3.45 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-04 21:51:18.965461: step 1240, loss = 3.67 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:51:20.144671: step 1250, loss = 3.24 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:21.321275: step 1260, loss = 3.52 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:22.503890: step 1270, loss = 3.57 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:23.701164: step 1280, loss = 3.61 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-04 21:51:24.888563: step 1290, loss = 3.54 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:51:26.067096: step 1300, loss = 3.38 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:27.239992: step 1310, loss = 3.62 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:28.422458: step 1320, loss = 3.37 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:29.577714: step 1330, loss = 3.44 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-04 21:51:30.760002: step 1340, loss = 3.49 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:31.921547: step 1350, loss = 3.36 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-04 21:51:33.109894: step 1360, loss = 3.39 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-04 21:51:34.268371: step 1370, loss = 3.50 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-04 21:51:35.447487: step 1380, loss = 3.51 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:36.647767: step 1390, loss = 3.36 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:51:37.822082: step 1400, loss = 3.38 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:38.991297: step 1410, loss = 3.27 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:40.176526: step 1420, loss = 3.28 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:51:41.341034: step 1430, loss = 3.42 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-04 21:51:42.531772: step 1440, loss = 3.68 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:51:43.723492: step 1450, loss = 3.47 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-04 21:51:44.881722: step 1460, loss = 3.49 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-04 21:51:46.024838: step 1470, loss = 3.46 (1119.7 examples/sec; 0.114 sec/batch)
2017-05-04 21:51:47.191539: step 1480, loss = 3.41 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:48.375631: step 1490, loss = 3.44 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:49.537722: step 1500, loss = 3.24 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-04 21:51:50.712058: step 1510, loss = 3.40 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:51.894823: step 1520, loss = 3.43 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:53.065214: step 1530, loss = 3.34 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:54.217423: step 1540, loss = 3.34 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-04 21:51:55.394317: step 1550, loss = 3.35 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:51:56.568443: step 1560, loss = 3.20 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:57.734487: step 1570, loss = 3.32 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-04 21:51:58.929476: step 1580, loss = 3.41 (1071.1 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:00.129731: step 1590, loss = 3.20 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:52:01.301074: step 1600, loss = 3.53 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:02.482552: step 1610, loss = 3.15 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:03.651192: step 1620, loss = 3.38 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:04.822748: step 1630, loss = 3.14 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:05.977822: step 1640, loss = 3.42 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-04 21:52:07.161875: step 1650, loss = 3.15 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:08.355998: step 1660, loss = 3.32 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:09.532132: step 1670, loss = 3.28 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:10.709432: step 1680, loss = 3.28 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:11.911774: step 1690, loss = 3.35 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-04 21:52:13.098405: step 1700, loss = 3.28 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:14.250308: step 1710, loss = 3.38 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-04 21:52:15.435024: step 1720, loss = 3.22 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:16.612667: step 1730, loss = 3.14 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:17.777985: step 1740, loss = 3.38 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:18.970450: step 1750, loss = 3.29 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:20.153828: step 1760, loss = 3.11 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:21.324158: step 1770, loss = 3.32 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:22.493443: step 1780, loss = 3.16 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:23.695325: step 1790, loss = 3.19 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-04 21:52:24.909488: step 1800, loss = 3.28 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 21:52:26.078001: step 1810, loss = 3.13 (1095.4 examples/sec; 0.117 sec/batch)E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 25 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU

2017-05-04 21:52:27.258972: step 1820, loss = 3.23 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:28.452649: step 1830, loss = 3.29 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:29.644688: step 1840, loss = 3.09 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:30.835723: step 1850, loss = 3.13 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:32.039707: step 1860, loss = 3.39 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 21:52:33.255129: step 1870, loss = 3.34 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-04 21:52:34.459671: step 1880, loss = 3.06 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 21:52:35.649618: step 1890, loss = 3.18 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:36.842981: step 1900, loss = 3.15 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:38.007297: step 1910, loss = 3.15 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-04 21:52:39.187477: step 1920, loss = 3.13 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:40.367291: step 1930, loss = 3.34 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:41.539301: step 1940, loss = 3.02 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:42.707832: step 1950, loss = 3.12 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:43.911966: step 1960, loss = 3.17 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-04 21:52:45.106805: step 1970, loss = 3.05 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:46.385516: step 1980, loss = 3.10 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-04 21:52:47.443280: step 1990, loss = 3.03 (1210.2 examples/sec; 0.106 sec/batch)
2017-05-04 21:52:48.625339: step 2000, loss = 3.15 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:49.790994: step 2010, loss = 3.10 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:50.980756: step 2020, loss = 3.15 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:52:52.153710: step 2030, loss = 3.13 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:53.338117: step 2040, loss = 3.00 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:54.508038: step 2050, loss = 2.96 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:55.692945: step 2060, loss = 3.10 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:56.866687: step 2070, loss = 3.25 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 21:52:58.042956: step 2080, loss = 3.13 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:52:59.240123: step 2090, loss = 3.17 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-04 21:53:00.415952: step 2100, loss = 3.20 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:01.603119: step 2110, loss = 3.08 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:02.782110: step 2120, loss = 3.09 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:03.989926: step 2130, loss = 3.14 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 21:53:05.177608: step 2140, loss = 3.18 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:06.355425: step 2150, loss = 3.23 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:07.537452: step 2160, loss = 3.14 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:08.713427: step 2170, loss = 3.08 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:09.877427: step 2180, loss = 3.26 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-04 21:53:11.063813: step 2190, loss = 3.07 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:12.225202: step 2200, loss = 3.11 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-04 21:53:13.417282: step 2210, loss = 3.01 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:14.603318: step 2220, loss = 3.01 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:15.787107: step 2230, loss = 3.04 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:16.965865: step 2240, loss = 3.12 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:18.130445: step 2250, loss = 3.08 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-04 21:53:19.322516: step 2260, loss = 2.92 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:20.502399: step 2270, loss = 2.94 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:21.670561: step 2280, loss = 3.15 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:22.827930: step 2290, loss = 3.17 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-04 21:53:24.015132: step 2300, loss = 3.25 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:25.195524: step 2310, loss = 3.14 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:26.380693: step 2320, loss = 3.05 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:27.572961: step 2330, loss = 3.03 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:28.753518: step 2340, loss = 3.07 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:29.924331: step 2350, loss = 3.02 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:31.109517: step 2360, loss = 2.90 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:32.283006: step 2370, loss = 3.01 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:33.452800: step 2380, loss = 2.84 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:34.626113: step 2390, loss = 3.25 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:35.802680: step 2400, loss = 3.08 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:36.970688: step 2410, loss = 2.98 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:38.135790: step 2420, loss = 3.10 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:39.319067: step 2430, loss = 3.03 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:40.501407: step 2440, loss = 3.14 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:41.678054: step 2450, loss = 2.96 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:42.862298: step 2460, loss = 3.06 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:44.038887: step 2470, loss = 3.01 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:45.213603: step 2480, loss = 3.20 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:46.383164: step 2490, loss = 3.03 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:47.550539: step 2500, loss = 2.93 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-04 21:53:48.750288: step 2510, loss = 2.97 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-04 21:53:49.941975: step 2520, loss = 3.04 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:51.121466: step 2530, loss = 3.01 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:52.298832: step 2540, loss = 3.06 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:53.493394: step 2550, loss = 3.31 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:54.677728: step 2560, loss = 2.95 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:53:55.865177: step 2570, loss = 2.94 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:53:57.061811: step 2580, loss = 2.90 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-04 21:53:58.259991: step 2590, loss = 3.00 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-04 21:53:59.459431: step 2600, loss = 3.03 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-04 21:54:00.673857: step 2610, loss = 2.95 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 21:54:01.867256: step 2620, loss = 3.05 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-04 21:54:03.078330: step 2630, loss = 3.10 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 21:54:04.284789: step 2640, loss = 3.05 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-04 21:54:05.486188: step 2650, loss = 3.05 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:54:06.672704: step 2660, loss = 2.96 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:54:07.866992: step 2670, loss = 2.95 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-04 21:54:09.046192: step 2680, loss = 2.81 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:54:10.232043: step 2690, loss = 2.93 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 21:54:11.404022: step 2700, loss = 2.81 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:54:12.569493: step 2710, loss = 2.86 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-04 21:54:13.748791: step 2720, loss = 2.91 (1085.4 examples/sec; 0.118 sec/batch)
2E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 36 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
017-05-04 21:54:14.930157: step 2730, loss = 3.05 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:54:16.131003: step 2740, loss = 2.82 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-04 21:54:17.321803: step 2750, loss = 2.86 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:54:18.533638: step 2760, loss = 3.12 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 21:54:19.744274: step 2770, loss = 2.98 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 21:54:20.950014: step 2780, loss = 3.15 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-04 21:54:22.154173: step 2790, loss = 3.00 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-04 21:54:23.367876: step 2800, loss = 2.90 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-04 21:54:24.575222: step 2810, loss = 2.89 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-04 21:54:25.779594: step 2820, loss = 2.89 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-04 21:54:27.002080: step 2830, loss = 2.82 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 21:54:28.236569: step 2840, loss = 2.96 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:29.464672: step 2850, loss = 2.95 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:30.691149: step 2860, loss = 2.85 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:31.926531: step 2870, loss = 3.00 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-04 21:54:33.155995: step 2880, loss = 3.06 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:34.376307: step 2890, loss = 2.91 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 21:54:35.605801: step 2900, loss = 2.96 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:36.843552: step 2910, loss = 2.81 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-04 21:54:38.053345: step 2920, loss = 2.94 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 21:54:39.283574: step 2930, loss = 2.86 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:40.520885: step 2940, loss = 3.02 (1034.5 examples/sec; 0.124 sec/batch)
2017-05-04 21:54:41.754559: step 2950, loss = 2.95 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:42.979592: step 2960, loss = 2.87 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:44.329122: step 2970, loss = 2.86 (948.5 examples/sec; 0.135 sec/batch)
2017-05-04 21:54:45.434322: step 2980, loss = 3.07 (1158.2 examples/sec; 0.111 sec/batch)
2017-05-04 21:54:46.661942: step 2990, loss = 2.85 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:47.900151: step 3000, loss = 2.92 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-04 21:54:49.145340: step 3010, loss = 3.00 (1028.0 examples/sec; 0.125 sec/batch)
2017-05-04 21:54:50.365525: step 3020, loss = 2.96 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 21:54:51.588478: step 3030, loss = 2.82 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 21:54:52.831831: step 3040, loss = 2.86 (1029.5 examples/sec; 0.124 sec/batch)
2017-05-04 21:54:54.062600: step 3050, loss = 2.86 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:55.293315: step 3060, loss = 2.92 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:56.532382: step 3070, loss = 2.77 (1033.0 examples/sec; 0.124 sec/batch)
2017-05-04 21:54:57.757742: step 3080, loss = 2.86 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-04 21:54:58.994633: step 3090, loss = 2.86 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-04 21:55:00.217813: step 3100, loss = 2.79 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:01.444863: step 3110, loss = 2.94 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-04 21:55:02.693517: step 3120, loss = 3.02 (1025.1 examples/sec; 0.125 sec/batch)
2017-05-04 21:55:03.932333: step 3130, loss = 2.90 (1033.2 examples/sec; 0.124 sec/batch)
2017-05-04 21:55:05.160183: step 3140, loss = 2.93 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-04 21:55:06.396068: step 3150, loss = 2.99 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-04 21:55:07.635935: step 3160, loss = 3.03 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-04 21:55:08.866501: step 3170, loss = 2.91 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-04 21:55:10.035082: step 3180, loss = 2.86 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:55:11.247093: step 3190, loss = 2.69 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:12.448098: step 3200, loss = 2.72 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:13.647281: step 3210, loss = 3.06 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:14.863101: step 3220, loss = 3.05 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:16.053404: step 3230, loss = 2.96 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-04 21:55:17.257103: step 3240, loss = 2.76 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:18.481974: step 3250, loss = 2.94 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:19.702269: step 3260, loss = 2.81 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:20.919468: step 3270, loss = 2.92 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:22.105794: step 3280, loss = 2.70 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:55:23.322368: step 3290, loss = 3.01 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:24.529185: step 3300, loss = 2.79 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:25.732922: step 3310, loss = 2.93 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:26.924956: step 3320, loss = 2.81 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:55:28.129992: step 3330, loss = 2.75 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:29.316807: step 3340, loss = 2.98 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-04 21:55:30.515446: step 3350, loss = 2.58 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:31.712221: step 3360, loss = 2.89 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:32.940130: step 3370, loss = 2.76 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-04 21:55:34.158371: step 3380, loss = 2.84 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:35.374132: step 3390, loss = 2.75 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:36.576398: step 3400, loss = 2.54 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:37.767426: step 3410, loss = 2.89 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-04 21:55:38.965965: step 3420, loss = 2.76 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:40.145978: step 3430, loss = 2.66 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:55:41.346144: step 3440, loss = 2.82 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:42.525829: step 3450, loss = 2.84 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-04 21:55:43.714596: step 3460, loss = 2.63 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-04 21:55:44.921109: step 3470, loss = 2.85 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:46.129698: step 3480, loss = 2.88 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:47.339628: step 3490, loss = 2.81 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 21:55:48.560358: step 3500, loss = 2.96 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:49.783576: step 3510, loss = 2.60 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:51.017207: step 3520, loss = 2.99 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-04 21:55:52.250795: step 3530, loss = 2.66 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-04 21:55:53.454183: step 3540, loss = 2.77 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 21:55:54.677396: step 3550, loss = 2.74 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:55.900674: step 3560, loss = 2.88 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:57.119742: step 3570, loss = 2.79 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:58.337295: step 3580, loss = 2.68 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 21:55:59.558132: step 3590, loss = 2.53 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:00.788991: step 3600, loss = 2.71 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:02.027340: step 3610, loss = 2.70 (1033.6 examples/sec; 0.124 sec/batch)
2017-05-04 21:56:03.273273: step 3620, loss = 2.88 (1027.3 examples/sec; 0.125 sec/batch)
2017-05-04 21:56:04.478757: step 3630, loss = 2.90 (1061.8 examples/sec; 0.121 sec/batch)
2017E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 46 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
-05-04 21:56:05.716332: step 3640, loss = 2.57 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-04 21:56:06.946136: step 3650, loss = 2.67 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:08.191398: step 3660, loss = 2.79 (1027.9 examples/sec; 0.125 sec/batch)
2017-05-04 21:56:09.421655: step 3670, loss = 2.74 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:10.657579: step 3680, loss = 2.70 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-04 21:56:11.886334: step 3690, loss = 2.92 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:13.114682: step 3700, loss = 2.95 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:14.340182: step 3710, loss = 2.61 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:15.567393: step 3720, loss = 2.67 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:16.798084: step 3730, loss = 2.64 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:18.020432: step 3740, loss = 2.72 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:19.252498: step 3750, loss = 2.71 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:20.490683: step 3760, loss = 2.50 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-04 21:56:21.675928: step 3770, loss = 2.72 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:56:22.910627: step 3780, loss = 2.92 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:24.134639: step 3790, loss = 2.97 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:25.351528: step 3800, loss = 2.66 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:26.573958: step 3810, loss = 2.70 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:27.804444: step 3820, loss = 2.69 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:29.017548: step 3830, loss = 2.70 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:56:30.235257: step 3840, loss = 2.61 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:31.453452: step 3850, loss = 2.74 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 21:56:32.638063: step 3860, loss = 2.84 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:56:33.839204: step 3870, loss = 2.89 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 21:56:35.053580: step 3880, loss = 2.77 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 21:56:36.267856: step 3890, loss = 2.69 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:56:37.470036: step 3900, loss = 2.63 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 21:56:38.674084: step 3910, loss = 2.72 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 21:56:39.870661: step 3920, loss = 2.69 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-04 21:56:41.102797: step 3930, loss = 2.72 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:42.286039: step 3940, loss = 2.76 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:56:43.517378: step 3950, loss = 2.69 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-04 21:56:44.835731: step 3960, loss = 2.58 (970.9 examples/sec; 0.132 sec/batch)
2017-05-04 21:56:45.911524: step 3970, loss = 2.72 (1189.8 examples/sec; 0.108 sec/batch)
2017-05-04 21:56:47.122090: step 3980, loss = 2.76 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 21:56:48.324461: step 3990, loss = 2.68 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-04 21:56:49.526748: step 4000, loss = 2.80 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-04 21:56:50.736298: step 4010, loss = 2.61 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 21:56:51.929769: step 4020, loss = 2.73 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-04 21:56:53.131427: step 4030, loss = 2.57 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 21:56:54.299001: step 4040, loss = 2.69 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 21:56:55.484104: step 4050, loss = 2.64 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 21:56:56.664972: step 4060, loss = 2.62 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:56:57.840831: step 4070, loss = 2.61 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 21:56:59.015056: step 4080, loss = 2.70 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 21:57:00.202387: step 4090, loss = 2.58 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:01.372928: step 4100, loss = 2.82 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 21:57:02.533503: step 4110, loss = 2.73 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-04 21:57:03.726144: step 4120, loss = 2.77 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:04.908156: step 4130, loss = 2.73 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:06.079068: step 4140, loss = 2.66 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 21:57:07.272294: step 4150, loss = 2.52 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:08.433186: step 4160, loss = 2.63 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 21:57:09.593760: step 4170, loss = 2.57 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-04 21:57:10.770377: step 4180, loss = 2.59 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:11.950310: step 4190, loss = 2.78 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:13.130863: step 4200, loss = 2.71 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:14.289875: step 4210, loss = 2.75 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-04 21:57:15.471290: step 4220, loss = 2.70 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:16.652629: step 4230, loss = 2.61 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:17.848599: step 4240, loss = 2.59 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-04 21:57:19.044989: step 4250, loss = 2.66 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 21:57:20.228769: step 4260, loss = 2.65 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:21.407588: step 4270, loss = 2.59 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:22.621887: step 4280, loss = 2.84 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:57:23.805730: step 4290, loss = 2.80 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:25.003082: step 4300, loss = 2.63 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-04 21:57:26.194730: step 4310, loss = 2.61 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:27.375446: step 4320, loss = 2.59 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:28.569614: step 4330, loss = 2.46 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:29.755160: step 4340, loss = 2.52 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:30.954695: step 4350, loss = 2.64 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-04 21:57:32.135368: step 4360, loss = 2.47 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:33.366828: step 4370, loss = 2.59 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-04 21:57:34.574844: step 4380, loss = 2.62 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 21:57:35.770663: step 4390, loss = 2.73 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:57:36.992294: step 4400, loss = 2.57 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-04 21:57:38.176561: step 4410, loss = 2.69 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:57:39.391774: step 4420, loss = 2.49 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 21:57:40.615282: step 4430, loss = 2.68 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 21:57:41.820301: step 4440, loss = 2.52 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-04 21:57:43.041954: step 4450, loss = 2.61 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-04 21:57:44.270026: step 4460, loss = 2.53 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 21:57:45.477861: step 4470, loss = 2.68 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-04 21:57:46.693827: step 4480, loss = 2.52 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 21:57:47.885438: step 4490, loss = 2.50 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:49.075301: step 4500, loss = 2.65 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:57:50.287205: step 4510, loss = 2.63 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 21:57:51.485919: step 4520, loss = 2.62 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-04 21:57:52.700199: step 4530, loss = 2.53 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:57:53.918615: step 4540, loss = 2.52 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 21:57:55.131333: step 4550, loss = 2.59 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-04 21:57:56.331295: step 4560, loss = 2.52 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-04 21:57:57.537199: step 4570, loss = 2.57 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 21:57:58.753326: step 4580, loss = 2.66 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 21:57:59.985974: step 4590, loss = 2.53 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-04 21:58:01.201704: step 4600, loss = 2.74 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-04 21:58:02.440020: step 4610, loss = 2.56 (1033.7 examples/sec; 0.124 sec/batch)
2017-05-04 21:58:03.661509: step 4620, loss = 2.57 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 21:58:04.874598: step 4630, loss = 2.65 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 21:58:06.095364: step 4640, loss = 2.48 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 21:58:07.332925: step 4650, loss = 2.55 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-04 21:58:08.567422: step 4660, loss = 2.59 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-04 21:58:09.774567: step 4670, loss = 2.41 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-04 21:58:10.990285: step 4680, loss = 2.51 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-04 21:58:12.218137: step 4690, loss = 2.44 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-04 21:58:13.445617: step 4700, loss = 2.64 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-04 21:58:14.677799: step 4710, loss = 2.50 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 21:58:15.904129: step 4720, loss = 2.37 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 21:58:17.160954: step 4730, loss = 2.63 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-04 21:58:18.388890: step 4740, loss = 2.47 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-04 21:58:19.622141: step 4750, loss = 2.52 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-04 21:58:20.825527: step 4760, loss = 2.63 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 21:58:22.030654: step 4770, loss = 2.49 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:58:23.233222: step 4780, loss = 2.76 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:58:24.425918: step 4790, loss = 2.56 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-04 21:58:25.616415: step 4800, loss = 2.50 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-04 21:58:26.808162: step 4810, loss = 2.69 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-04 21:58:27.997377: step 4820, loss = 2.62 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-04 21:58:29.220821: step 4830, loss = 2.57 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 21:58:30.412886: step 4840, loss = 2.70 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:58:31.585332: step 4850, loss = 2.58 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-04 21:58:32.774771: step 4860, loss = 2.52 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 21:58:33.965691: step 4870, loss = 2.57 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 21:58:35.195428: step 4880, loss = 2.59 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 21:58:36.384514: step 4890, loss = 2.48 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-04 21:58:37.583906: step 4900, loss = 2.54 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-04 21:58:38.791782: step 4910, loss = 2.56 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-04 21:58:39.981530: step 4920, loss = 2.57 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-04 21:58:41.168696: step 4930, loss = 2.44 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-04 21:58:42.342600: step 4940, loss = 2.49 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 21:58:43.636565: step 4950, loss = 2.60 (989.2 examples/sec; 0.129 sec/batch)
2017-05-04 21:58:44.692343: step 4960, loss = 2.50 (1212.4 examples/sec; 0.106 sec/batch)
2017-05-04 21:58:45.863429: step 4970, loss = 2.54 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 21:58:47.054792: step 4980, loss = 2.65 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-04 21:58:48.233668: step 4990, loss = 2.50 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 21:58:49.404636: step 5000, loss = 2.55 (10E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 58 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
93.1 examples/sec; 0.117 sec/batch)
2017-05-04 21:58:50.572917: step 5010, loss = 2.64 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 21:58:51.740178: step 5020, loss = 2.54 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-04 21:58:52.921963: step 5030, loss = 2.87 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 21:58:54.091072: step 5040, loss = 2.64 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 21:58:55.287579: step 5050, loss = 2.49 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-04 21:58:56.509303: step 5060, loss = 2.42 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 21:58:57.711826: step 5070, loss = 2.55 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 21:58:58.954254: step 5080, loss = 2.33 (1030.3 examples/sec; 0.124 sec/batch)
2017-05-04 21:59:00.188007: step 5090, loss = 2.51 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-04 21:59:01.407641: step 5100, loss = 2.54 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:02.651254: step 5110, loss = 2.67 (1029.3 examples/sec; 0.124 sec/batch)
2017-05-04 21:59:03.872638: step 5120, loss = 2.41 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:05.115421: step 5130, loss = 2.72 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-04 21:59:06.327208: step 5140, loss = 2.54 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:07.675855: step 5150, loss = 2.48 (949.1 examples/sec; 0.135 sec/batch)
2017-05-04 21:59:08.796038: step 5160, loss = 2.68 (1142.7 examples/sec; 0.112 sec/batch)
2017-05-04 21:59:10.021686: step 5170, loss = 2.41 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-04 21:59:11.259272: step 5180, loss = 2.59 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-04 21:59:12.504470: step 5190, loss = 2.64 (1028.0 examples/sec; 0.125 sec/batch)
2017-05-04 21:59:13.674167: step 5200, loss = 2.55 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 21:59:14.915945: step 5210, loss = 2.56 (1030.8 examples/sec; 0.124 sec/batch)
2017-05-04 21:59:16.161443: step 5220, loss = 2.59 (1027.7 examples/sec; 0.125 sec/batch)
2017-05-04 21:59:17.384611: step 5230, loss = 2.51 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:18.600535: step 5240, loss = 2.47 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:19.848864: step 5250, loss = 2.39 (1025.4 examples/sec; 0.125 sec/batch)
2017-05-04 21:59:21.057375: step 5260, loss = 2.70 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:22.280904: step 5270, loss = 2.49 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:23.498584: step 5280, loss = 2.63 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:24.737253: step 5290, loss = 2.39 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-04 21:59:25.942877: step 5300, loss = 2.47 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:27.194493: step 5310, loss = 2.43 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-04 21:59:28.407546: step 5320, loss = 2.63 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:29.623568: step 5330, loss = 2.53 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:30.859418: step 5340, loss = 2.46 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-04 21:59:32.041615: step 5350, loss = 2.54 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 21:59:33.268936: step 5360, loss = 2.49 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 21:59:34.481566: step 5370, loss = 2.38 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:35.726298: step 5380, loss = 2.48 (1028.3 examples/sec; 0.124 sec/batch)
2017-05-04 21:59:36.947657: step 5390, loss = 2.54 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:38.156579: step 5400, loss = 2.54 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:39.356319: step 5410, loss = 2.70 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-04 21:59:40.576615: step 5420, loss = 2.39 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:41.789405: step 5430, loss = 2.37 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:42.998956: step 5440, loss = 2.52 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:44.212384: step 5450, loss = 2.48 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:45.406043: step 5460, loss = 2.38 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-04 21:59:46.617317: step 5470, loss = 2.45 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:47.841503: step 5480, loss = 2.36 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:49.065995: step 5490, loss = 2.60 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:50.274620: step 5500, loss = 2.64 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:51.490582: step 5510, loss = 2.52 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:52.703391: step 5520, loss = 2.56 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:53.927559: step 5530, loss = 2.36 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:55.151821: step 5540, loss = 2.48 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:56.360413: step 5550, loss = 2.42 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 21:59:57.576110: step 5560, loss = 2.73 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-04 21:59:58.834507: step 5570, loss = 2.47 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-04 22:00:00.072272: step 5580, loss = 2.35 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:01.318443: step 5590, loss = 2.45 (1027.1 examples/sec; 0.125 sec/batch)
2017-05-04 22:00:02.555996: step 5600, loss = 2.38 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:03.777171: step 5610, loss = 2.82 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:00:05.019198: step 5620, loss = 2.49 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:06.249283: step 5630, loss = 2.37 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:07.489949: step 5640, loss = 2.38 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:08.728571: step 5650, loss = 2.36 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:09.948173: step 5660, loss = 2.42 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:00:11.198675: step 5670, loss = 2.56 (1023.6 examples/sec; 0.125 sec/batch)
2017-05-04 22:00:12.404265: step 5680, loss = 2.52 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:00:13.642948: step 5690, loss = 2.46 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:14.867663: step 5700, loss = 2.55 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:00:16.102317: step 5710, loss = 2.40 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:17.341649: step 5720, loss = 2.44 (1032.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:18.575596: step 5730, loss = 2.39 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:19.812010: step 5740, loss = 2.60 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:21.004397: step 5750, loss = 2.63 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:00:22.226427: step 5760, loss = 2.54 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:00:23.458823: step 5770, loss = 2.45 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:24.682539: step 5780, loss = 2.61 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:00:25.929099: step 5790, loss = 2.47 (1026.8 examples/sec; 0.125 sec/batch)
2017-05-04 22:00:27.159632: step 5800, loss = 2.67 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:28.394364: step 5810, loss = 2.54 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:29.622410: step 5820, loss = 2.56 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:30.856704: step 5830, loss = 2.25 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:32.060294: step 5840, loss = 2.42 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:00:33.303383: step 5850, loss = 2.40 (1029.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:34.505547: step 5860, loss = 2.40 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:00:35.755987: step 5870, loss = 2.39 (1023.6 examples/sec; 0.125 sec/batch)
2017-05-04 22:00:36.995869: step 5880, loss = 2.41 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:38.210704: step 5890, loss = 2.49 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:00:39.429551: step 5900, loss = 2.59 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:00:40.678227: step 5910, loss = 2.44 (1025.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 68 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
1 examples/sec; 0.125 sec/batch)
2017-05-04 22:00:41.866354: step 5920, loss = 2.36 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:00:43.118850: step 5930, loss = 2.37 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-04 22:00:44.471389: step 5940, loss = 2.34 (946.4 examples/sec; 0.135 sec/batch)
2017-05-04 22:00:45.561667: step 5950, loss = 2.74 (1174.0 examples/sec; 0.109 sec/batch)
2017-05-04 22:00:46.775805: step 5960, loss = 2.28 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:00:48.041543: step 5970, loss = 2.33 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-04 22:00:49.248581: step 5980, loss = 2.47 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:00:50.474329: step 5990, loss = 2.42 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:51.707120: step 6000, loss = 2.41 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:52.934617: step 6010, loss = 2.53 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:54.170145: step 6020, loss = 2.45 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:00:55.396882: step 6030, loss = 2.45 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:56.625503: step 6040, loss = 2.37 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:00:57.886125: step 6050, loss = 2.32 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-04 22:00:59.108446: step 6060, loss = 2.34 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:00.333499: step 6070, loss = 2.67 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:01.554380: step 6080, loss = 2.34 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:02.786427: step 6090, loss = 2.39 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:04.026420: step 6100, loss = 2.44 (1032.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:05.264541: step 6110, loss = 2.26 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:06.484852: step 6120, loss = 2.39 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:07.734958: step 6130, loss = 2.27 (1023.9 examples/sec; 0.125 sec/batch)
2017-05-04 22:01:09.040676: step 6140, loss = 2.40 (980.3 examples/sec; 0.131 sec/batch)
2017-05-04 22:01:10.152120: step 6150, loss = 2.59 (1151.7 examples/sec; 0.111 sec/batch)
2017-05-04 22:01:11.379211: step 6160, loss = 2.66 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:12.637196: step 6170, loss = 2.43 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-04 22:01:13.856715: step 6180, loss = 2.19 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:15.079835: step 6190, loss = 2.45 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:16.311352: step 6200, loss = 2.62 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:17.553180: step 6210, loss = 2.66 (1030.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:18.787049: step 6220, loss = 2.56 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:20.032664: step 6230, loss = 2.25 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-04 22:01:21.247692: step 6240, loss = 2.17 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:22.482807: step 6250, loss = 2.47 (1036.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:23.718896: step 6260, loss = 2.38 (1035.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:24.923664: step 6270, loss = 2.45 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:01:26.145905: step 6280, loss = 2.42 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:27.376039: step 6290, loss = 2.33 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:28.597739: step 6300, loss = 2.57 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:29.830858: step 6310, loss = 2.35 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:31.049510: step 6320, loss = 2.39 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:32.270315: step 6330, loss = 2.71 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:33.478059: step 6340, loss = 2.46 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:01:34.706941: step 6350, loss = 2.73 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:35.949501: step 6360, loss = 2.38 (1030.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:37.180198: step 6370, loss = 2.31 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:38.393208: step 6380, loss = 2.41 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:01:39.632389: step 6390, loss = 2.52 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:40.870818: step 6400, loss = 2.44 (1033.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:01:42.072138: step 6410, loss = 2.28 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:01:43.289647: step 6420, loss = 2.43 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:44.515923: step 6430, loss = 2.42 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:45.739376: step 6440, loss = 2.39 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:46.954793: step 6450, loss = 2.41 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:01:48.168272: step 6460, loss = 2.40 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:01:49.397878: step 6470, loss = 2.34 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:50.626599: step 6480, loss = 2.61 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:51.882208: step 6490, loss = 2.38 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-04 22:01:53.095431: step 6500, loss = 2.43 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:01:54.323389: step 6510, loss = 2.38 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:55.550445: step 6520, loss = 2.51 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:01:56.797009: step 6530, loss = 2.33 (1026.8 examples/sec; 0.125 sec/batch)
2017-05-04 22:01:57.986743: step 6540, loss = 2.37 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:01:59.217408: step 6550, loss = 2.20 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:00.436984: step 6560, loss = 2.47 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:01.661877: step 6570, loss = 2.29 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:02.910874: step 6580, loss = 2.35 (1024.8 examples/sec; 0.125 sec/batch)
2017-05-04 22:02:04.133954: step 6590, loss = 2.61 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:05.348740: step 6600, loss = 2.38 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:02:06.573097: step 6610, loss = 2.49 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:07.787818: step 6620, loss = 2.44 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:02:09.039759: step 6630, loss = 2.43 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-04 22:02:10.255569: step 6640, loss = 2.60 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:11.495639: step 6650, loss = 2.30 (1032.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:12.728388: step 6660, loss = 2.32 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:13.956575: step 6670, loss = 2.56 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:15.191584: step 6680, loss = 2.39 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:16.424793: step 6690, loss = 2.47 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:17.624591: step 6700, loss = 2.19 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:02:18.852567: step 6710, loss = 2.47 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:20.097410: step 6720, loss = 2.36 (1028.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:21.328992: step 6730, loss = 2.25 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:22.512949: step 6740, loss = 2.26 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:02:23.750746: step 6750, loss = 2.56 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:24.964318: step 6760, loss = 2.50 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:02:26.185405: step 6770, loss = 2.31 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:27.414750: step 6780, loss = 2.45 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:28.612793: step 6790, loss = 2.44 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:02:29.801003: step 6800, loss = 2.39 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:02:31.017652: step 6810, loss = 2.51 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:32.248407: step 6820, loss = 2.30 (1040.0 exE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 78 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
amples/sec; 0.123 sec/batch)
2017-05-04 22:02:33.459624: step 6830, loss = 2.38 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:02:34.696327: step 6840, loss = 2.52 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:35.911435: step 6850, loss = 2.51 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:37.152933: step 6860, loss = 2.42 (1031.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:38.386678: step 6870, loss = 2.58 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:39.613172: step 6880, loss = 2.58 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:40.857182: step 6890, loss = 2.34 (1028.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:42.065132: step 6900, loss = 2.58 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:02:43.301875: step 6910, loss = 2.39 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:44.521080: step 6920, loss = 2.25 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:45.901916: step 6930, loss = 2.32 (927.0 examples/sec; 0.138 sec/batch)
2017-05-04 22:02:46.974110: step 6940, loss = 2.32 (1193.8 examples/sec; 0.107 sec/batch)
2017-05-04 22:02:48.233608: step 6950, loss = 2.33 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-04 22:02:49.474627: step 6960, loss = 2.24 (1031.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:50.708884: step 6970, loss = 2.31 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:51.953831: step 6980, loss = 2.29 (1028.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:02:53.185407: step 6990, loss = 2.34 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:54.412031: step 7000, loss = 2.40 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:02:55.628422: step 7010, loss = 2.42 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:02:56.836715: step 7020, loss = 2.35 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:02:58.081890: step 7030, loss = 2.15 (1028.0 examples/sec; 0.125 sec/batch)
2017-05-04 22:02:59.310309: step 7040, loss = 2.43 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:00.543819: step 7050, loss = 2.35 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:01.766297: step 7060, loss = 2.44 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:02.988137: step 7070, loss = 2.31 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:04.215690: step 7080, loss = 2.47 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:05.431808: step 7090, loss = 2.42 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:06.659774: step 7100, loss = 2.23 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:07.903558: step 7110, loss = 2.26 (1029.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:09.101384: step 7120, loss = 2.20 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:03:10.422552: step 7130, loss = 2.27 (968.8 examples/sec; 0.132 sec/batch)
2017-05-04 22:03:11.510328: step 7140, loss = 2.37 (1176.7 examples/sec; 0.109 sec/batch)
2017-05-04 22:03:12.744397: step 7150, loss = 2.46 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:13.969598: step 7160, loss = 2.26 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:15.195314: step 7170, loss = 2.38 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:16.392294: step 7180, loss = 2.34 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:03:17.648811: step 7190, loss = 2.46 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-04 22:03:18.881776: step 7200, loss = 2.39 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:20.102036: step 7210, loss = 2.28 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:21.338441: step 7220, loss = 2.34 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:22.557002: step 7230, loss = 2.21 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:23.796775: step 7240, loss = 2.23 (1032.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:25.026853: step 7250, loss = 2.25 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:26.232419: step 7260, loss = 2.43 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:03:27.460971: step 7270, loss = 2.49 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:28.699317: step 7280, loss = 2.53 (1033.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:29.934813: step 7290, loss = 2.38 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:31.172264: step 7300, loss = 2.41 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:32.398992: step 7310, loss = 2.38 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:33.616915: step 7320, loss = 2.32 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:34.811116: step 7330, loss = 2.34 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:03:36.043712: step 7340, loss = 2.52 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:37.271709: step 7350, loss = 2.33 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:38.481531: step 7360, loss = 2.42 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:03:39.701773: step 7370, loss = 2.30 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:40.947154: step 7380, loss = 2.28 (1027.8 examples/sec; 0.125 sec/batch)
2017-05-04 22:03:42.178129: step 7390, loss = 2.27 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:43.393407: step 7400, loss = 2.36 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:44.633262: step 7410, loss = 2.35 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:45.863707: step 7420, loss = 2.27 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:47.091753: step 7430, loss = 2.53 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:48.319327: step 7440, loss = 2.34 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:49.550909: step 7450, loss = 2.35 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:50.792984: step 7460, loss = 2.22 (1030.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:52.035720: step 7470, loss = 2.31 (1030.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:03:53.238878: step 7480, loss = 2.37 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:03:54.453138: step 7490, loss = 2.17 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:03:55.668686: step 7500, loss = 2.34 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:56.898002: step 7510, loss = 2.25 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:03:58.119172: step 7520, loss = 2.41 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:03:59.308851: step 7530, loss = 2.27 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:04:00.542100: step 7540, loss = 2.31 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:01.763056: step 7550, loss = 2.39 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:03.016485: step 7560, loss = 2.27 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-04 22:04:04.261319: step 7570, loss = 2.55 (1028.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:04:05.483479: step 7580, loss = 2.45 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:06.714337: step 7590, loss = 2.30 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:07.927944: step 7600, loss = 2.14 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:04:09.148330: step 7610, loss = 2.27 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:10.382159: step 7620, loss = 2.22 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:11.616021: step 7630, loss = 2.35 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:12.848602: step 7640, loss = 2.40 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:14.086103: step 7650, loss = 2.22 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:04:15.300637: step 7660, loss = 2.52 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:04:16.521551: step 7670, loss = 2.34 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:17.746108: step 7680, loss = 2.34 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:18.974645: step 7690, loss = 2.42 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:20.194560: step 7700, loss = 2.24 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:21.428187: step 7710, loss = 2.31 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:22.666502: step 7720, loss = 2.11 (1033.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:04:23.874980: step 7730, loss = 2.15 (1059.2 examplE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 88 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
es/sec; 0.121 sec/batch)
2017-05-04 22:04:25.089496: step 7740, loss = 2.43 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:04:26.326572: step 7750, loss = 2.32 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:04:27.531503: step 7760, loss = 2.54 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:04:28.769497: step 7770, loss = 2.36 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:04:29.982108: step 7780, loss = 2.29 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:04:31.240684: step 7790, loss = 2.13 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-04 22:04:32.456660: step 7800, loss = 2.41 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:33.683073: step 7810, loss = 2.28 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:34.910411: step 7820, loss = 2.26 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:36.162001: step 7830, loss = 2.31 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-04 22:04:37.363522: step 7840, loss = 2.26 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:04:38.609199: step 7850, loss = 2.30 (1027.5 examples/sec; 0.125 sec/batch)
2017-05-04 22:04:39.807844: step 7860, loss = 2.40 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:04:41.032946: step 7870, loss = 2.34 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:42.262561: step 7880, loss = 2.29 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:43.490495: step 7890, loss = 2.28 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:44.717786: step 7900, loss = 2.35 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:45.941453: step 7910, loss = 2.33 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:47.280069: step 7920, loss = 2.10 (956.2 examples/sec; 0.134 sec/batch)
2017-05-04 22:04:48.361409: step 7930, loss = 2.36 (1183.7 examples/sec; 0.108 sec/batch)
2017-05-04 22:04:49.578045: step 7940, loss = 2.36 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:50.822818: step 7950, loss = 2.46 (1028.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:04:52.051291: step 7960, loss = 2.30 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:53.261153: step 7970, loss = 2.22 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:04:54.489390: step 7980, loss = 2.18 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:55.718462: step 7990, loss = 2.44 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:56.944872: step 8000, loss = 2.28 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:04:58.163374: step 8010, loss = 2.41 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:04:59.386756: step 8020, loss = 2.30 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:00.610392: step 8030, loss = 2.39 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:01.858658: step 8040, loss = 2.45 (1025.4 examples/sec; 0.125 sec/batch)
2017-05-04 22:05:03.085084: step 8050, loss = 2.20 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:04.318543: step 8060, loss = 2.62 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:05.548051: step 8070, loss = 2.27 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:06.784077: step 8080, loss = 2.27 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:05:08.014772: step 8090, loss = 2.30 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:09.232488: step 8100, loss = 2.34 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:10.452802: step 8110, loss = 2.42 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:11.751473: step 8120, loss = 2.41 (985.6 examples/sec; 0.130 sec/batch)
2017-05-04 22:05:12.877787: step 8130, loss = 2.32 (1136.5 examples/sec; 0.113 sec/batch)
2017-05-04 22:05:14.099745: step 8140, loss = 2.26 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:15.338106: step 8150, loss = 2.33 (1033.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:05:16.554742: step 8160, loss = 2.30 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:17.786959: step 8170, loss = 2.27 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:19.024861: step 8180, loss = 2.28 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:05:20.237790: step 8190, loss = 2.28 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:05:21.455605: step 8200, loss = 2.53 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:22.686036: step 8210, loss = 2.19 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:23.915316: step 8220, loss = 2.28 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:25.153062: step 8230, loss = 2.38 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:05:26.372394: step 8240, loss = 2.29 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:27.630356: step 8250, loss = 2.25 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-04 22:05:28.854068: step 8260, loss = 2.28 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:30.071649: step 8270, loss = 2.18 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:31.315469: step 8280, loss = 2.27 (1029.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:05:32.545395: step 8290, loss = 2.40 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:33.770300: step 8300, loss = 2.28 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:34.988720: step 8310, loss = 2.22 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:36.206645: step 8320, loss = 2.30 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:37.410693: step 8330, loss = 2.30 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:05:38.620471: step 8340, loss = 2.34 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:05:39.850882: step 8350, loss = 2.27 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:41.068353: step 8360, loss = 2.26 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:42.276687: step 8370, loss = 2.30 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:05:43.463659: step 8380, loss = 2.32 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:05:44.670184: step 8390, loss = 2.26 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:05:45.872036: step 8400, loss = 2.20 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:05:47.071769: step 8410, loss = 2.24 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:05:48.278405: step 8420, loss = 2.47 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:05:49.523677: step 8430, loss = 2.25 (1027.9 examples/sec; 0.125 sec/batch)
2017-05-04 22:05:50.740821: step 8440, loss = 2.36 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:51.974497: step 8450, loss = 2.20 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:05:53.197985: step 8460, loss = 2.49 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:54.435200: step 8470, loss = 2.25 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:05:55.650304: step 8480, loss = 2.36 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:05:56.910396: step 8490, loss = 2.19 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-04 22:05:58.107762: step 8500, loss = 2.27 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:05:59.351815: step 8510, loss = 2.22 (1028.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:00.567047: step 8520, loss = 2.45 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:01.759532: step 8530, loss = 2.20 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:06:02.994480: step 8540, loss = 2.33 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:04.249444: step 8550, loss = 2.21 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-04 22:06:05.469870: step 8560, loss = 2.25 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:06.718219: step 8570, loss = 2.40 (1025.4 examples/sec; 0.125 sec/batch)
2017-05-04 22:06:07.930736: step 8580, loss = 2.49 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:06:09.183528: step 8590, loss = 2.32 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-04 22:06:10.382102: step 8600, loss = 2.59 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:06:11.628601: step 8610, loss = 2.36 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-04 22:06:12.866576: step 8620, loss = 2.24 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:14.090775: step 8630, loss = 2.21 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:15.325733: step 8640, loss = 2.24 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:16.537529: step 8650, loss = 2.36 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:06:17.755557: step 8660, loss = 2.11 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:18.989046: step 8670, loss = 2.29 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:20.224341: step 8680, loss = 2.24 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:21.455881: step 8690, loss = 2.15 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:22.681823: step 8700, loss = 2.18 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:23.900246: step 8710, loss = 2.12 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:25.088779: step 8720, loss = 2.12 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:06:26.331745: step 8730, loss = 2.32 (1029.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:27.572472: step 8740, loss = 2.13 (1031.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:28.800557: step 8750, loss = 2.26 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:30.050706: step 8760, loss = 2.22 (1023.9 examples/sec; 0.125 sec/batch)
2017-05-04 22:06:31.288102: step 8770, loss = 2.18 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:32.513115: step 8780, loss = 2.31 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:33.738704: step 8790, loss = 2.40 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:34.965900: step 8800, loss = 2.43 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:36.203149: step 8810, loss = 2.38 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:37.434307: step 8820, loss = 2.44 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:38.672328: step 8830, loss = 2.22 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:39.892735: step 8840, loss = 2.37 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:41.131435: step 8850, loss = 2.20 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:42.340406: step 8860, loss = 2.21 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:06:43.591370: step 8870, loss = 2.48 (1023.2 examples/sec; 0.125 sec/batch)
2017-05-04 22:06:44.819210: step 8880, loss = 2.33 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:46.036122: step 8890, loss = 2.26 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:47.253752: step 8900, loss = 2.36 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:48.620072: step 8910, loss = 2.42 (936.8 examples/sec; 0.137 sec/batch)
2017-05-04 22:06:49.669714: step 8920, loss = 2.21 (1219.5 examples/sec; 0.105 sec/batch)
2017-05-04 22:06:50.951986: step 8930, loss = 2.34 (998.2 examples/sec; 0.128 sec/batch)
2017-05-04 22:06:52.154081: step 8940, loss = 2.59 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:06:53.385807: step 8950, loss = 2.38 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:06:54.639402: step 8960, loss = 2.15 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-04 22:06:55.877397: step 8970, loss = 2.18 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:57.095003: step 8980, loss = 2.10 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:06:58.332892: step 8990, loss = 2.26 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:06:59.551866: step 9000, loss = 2.17 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:00.776004: step 9010, loss = 2.30 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:01.999782: step 9020, loss = 2.33 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:03.231044: step 9030, loss = 2.26 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:04.448942: step 9040, loss = 2.21 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:05.673800: step 9050, loss = 2.28 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:06.898020: step 9060, loss = 2.22 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:08.130423: step 9070, loss = 2.25 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:09.360345: step 9080, loss = 2.18 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:10.580044: step 9090, loss = 2.36 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:11.799917:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 98 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
 step 9100, loss = 2.25 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:13.125045: step 9110, loss = 2.33 (965.9 examples/sec; 0.133 sec/batch)
2017-05-04 22:07:14.218160: step 9120, loss = 2.20 (1171.0 examples/sec; 0.109 sec/batch)
2017-05-04 22:07:15.451095: step 9130, loss = 2.09 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:16.677587: step 9140, loss = 2.36 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:17.907038: step 9150, loss = 2.13 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:19.135507: step 9160, loss = 2.33 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:20.358754: step 9170, loss = 2.21 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:21.592326: step 9180, loss = 2.34 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:22.839193: step 9190, loss = 2.29 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-04 22:07:24.068839: step 9200, loss = 2.26 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:25.306771: step 9210, loss = 2.29 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:07:26.530613: step 9220, loss = 2.42 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:27.763722: step 9230, loss = 2.25 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:28.996066: step 9240, loss = 2.14 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:30.243167: step 9250, loss = 2.30 (1026.4 examples/sec; 0.125 sec/batch)
2017-05-04 22:07:31.456102: step 9260, loss = 2.28 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:07:32.694738: step 9270, loss = 2.28 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:07:33.909159: step 9280, loss = 2.10 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:07:35.153647: step 9290, loss = 2.30 (1028.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:07:36.386366: step 9300, loss = 2.14 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:37.593645: step 9310, loss = 2.22 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:07:38.806427: step 9320, loss = 2.32 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:07:40.053054: step 9330, loss = 2.23 (1026.8 examples/sec; 0.125 sec/batch)
2017-05-04 22:07:41.286403: step 9340, loss = 2.17 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:42.521675: step 9350, loss = 2.33 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:07:43.722972: step 9360, loss = 2.33 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:07:44.975364: step 9370, loss = 2.21 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-04 22:07:46.201028: step 9380, loss = 2.25 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:47.446838: step 9390, loss = 2.16 (1027.4 examples/sec; 0.125 sec/batch)
2017-05-04 22:07:48.665271: step 9400, loss = 2.27 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:49.877819: step 9410, loss = 2.32 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:07:51.114061: step 9420, loss = 2.21 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:07:52.338346: step 9430, loss = 2.21 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:07:53.565503: step 9440, loss = 2.26 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:54.773037: step 9450, loss = 2.37 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:07:56.005026: step 9460, loss = 2.27 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:07:57.245362: step 9470, loss = 2.30 (1032.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:07:58.450521: step 9480, loss = 2.10 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:07:59.684454: step 9490, loss = 2.31 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:00.913087: step 9500, loss = 2.21 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:02.095013: step 9510, loss = 2.05 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:08:03.335063: step 9520, loss = 2.26 (1032.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:04.586185: step 9530, loss = 2.19 (1023.1 examples/sec; 0.125 sec/batch)
2017-05-04 22:08:05.798696: step 9540, loss = 2.20 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:08:07.046734: step 9550, loss = 2.43 (1025.6 examples/sec; 0.125 sec/batch)
2017-05-04 22:08:08.268606: step 9560, loss = 2.27 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:09.520904: step 9570, loss = 2.19 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-04 22:08:10.734895: step 9580, loss = 2.24 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:08:11.965456: step 9590, loss = 2.39 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:13.183704: step 9600, loss = 2.11 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:14.430144: step 9610, loss = 2.14 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-04 22:08:15.650689: step 9620, loss = 2.50 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:16.881713: step 9630, loss = 2.35 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:18.080875: step 9640, loss = 2.35 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:08:19.335005: step 9650, loss = 2.26 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-04 22:08:20.570037: step 9660, loss = 2.39 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:21.787695: step 9670, loss = 2.25 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:23.001213: step 9680, loss = 2.39 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:08:24.270063: step 9690, loss = 2.33 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-04 22:08:25.485495: step 9700, loss = 2.26 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:26.683487: step 9710, loss = 2.25 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:08:27.900877: step 9720, loss = 2.30 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:29.116657: step 9730, loss = 2.27 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:30.312662: step 9740, loss = 2.23 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:08:31.552596: step 9750, loss = 2.24 (1032.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:32.774932: step 9760, loss = 2.56 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:34.009643: step 9770, loss = 2.43 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:35.229302: step 9780, loss = 2.42 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:36.436755: step 9790, loss = 2.33 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:08:37.620439: step 9800, loss = 2.13 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:08:38.846769: step 9810, loss = 2.34 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:40.079369: step 9820, loss = 2.23 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:41.332903: step 9830, loss = 2.15 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-04 22:08:42.533621: step 9840, loss = 2.24 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:08:43.769268: step 9850, loss = 2.36 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:44.972982: step 9860, loss = 2.37 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:08:46.202843: step 9870, loss = 2.31 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:47.414356: step 9880, loss = 2.39 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:08:48.667326: step 9890, loss = 2.12 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-04 22:08:49.972850: step 9900, loss = 2.29 (980.4 examples/sec; 0.131 sec/batch)
2017-05-04 22:08:51.076322: step 9910, loss = 2.23 (1160.0 examples/sec; 0.110 sec/batch)
2017-05-04 22:08:52.307834: step 9920, loss = 2.26 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:08:53.561318: step 9930, loss = 2.16 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-04 22:08:54.776903: step 9940, loss = 2.15 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:55.993549: step 9950, loss = 2.17 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:08:57.230984: step 9960, loss = 2.15 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:08:58.432752: step 9970, loss = 2.18 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:08:59.651689: step 9980, loss = 2.32 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:00.898996: step 9990, loss = 2.32 (1026.2 examples/sec; 0.125 sec/batch)
2017-05-04 22:09:02.110384: step 10000, loss = 2.06 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:09:03.372661: stE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 109 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
ep 10010, loss = 2.26 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-04 22:09:04.584893: step 10020, loss = 2.13 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:09:05.829636: step 10030, loss = 2.22 (1028.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:09:07.046849: step 10040, loss = 2.51 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:08.265360: step 10050, loss = 2.08 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:09.499953: step 10060, loss = 2.23 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:10.721771: step 10070, loss = 2.25 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:11.955281: step 10080, loss = 2.08 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:13.197154: step 10090, loss = 2.37 (1030.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:09:14.512857: step 10100, loss = 2.19 (972.9 examples/sec; 0.132 sec/batch)
2017-05-04 22:09:15.628258: step 10110, loss = 2.14 (1147.6 examples/sec; 0.112 sec/batch)
2017-05-04 22:09:16.848936: step 10120, loss = 2.37 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:18.088850: step 10130, loss = 2.18 (1032.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:09:19.298471: step 10140, loss = 2.21 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:09:20.533810: step 10150, loss = 2.24 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:09:21.775371: step 10160, loss = 2.02 (1031.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:09:23.008149: step 10170, loss = 2.18 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:24.222741: step 10180, loss = 2.46 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:09:25.454545: step 10190, loss = 2.46 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:26.680365: step 10200, loss = 2.37 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:27.905138: step 10210, loss = 2.28 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:29.135991: step 10220, loss = 2.23 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:30.365246: step 10230, loss = 2.33 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:31.595152: step 10240, loss = 2.22 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:32.808199: step 10250, loss = 2.13 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:09:34.014833: step 10260, loss = 2.33 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:09:35.254166: step 10270, loss = 2.14 (1032.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:09:36.489580: step 10280, loss = 2.09 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:09:37.734487: step 10290, loss = 2.25 (1028.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:09:38.924146: step 10300, loss = 2.18 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:09:40.149867: step 10310, loss = 2.08 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:41.356738: step 10320, loss = 2.33 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:09:42.575660: step 10330, loss = 2.32 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:43.802339: step 10340, loss = 2.22 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:45.021458: step 10350, loss = 2.14 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:46.243791: step 10360, loss = 2.27 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:47.472320: step 10370, loss = 2.46 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:48.686552: step 10380, loss = 2.14 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:09:49.940343: step 10390, loss = 2.17 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-04 22:09:51.159561: step 10400, loss = 2.29 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:52.415964: step 10410, loss = 2.23 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-04 22:09:53.624085: step 10420, loss = 2.27 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:09:54.860319: step 10430, loss = 2.29 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:09:56.080419: step 10440, loss = 2.20 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:57.315152: step 10450, loss = 2.29 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:09:58.533997: step 10460, loss = 2.29 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:09:59.762923: step 10470, loss = 2.30 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:00.996897: step 10480, loss = 2.43 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:02.215380: step 10490, loss = 2.19 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:03.392691: step 10500, loss = 2.37 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:10:04.647117: step 10510, loss = 2.20 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-04 22:10:05.870756: step 10520, loss = 2.32 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:07.105357: step 10530, loss = 2.39 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:08.310706: step 10540, loss = 2.11 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:10:09.564356: step 10550, loss = 2.09 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-04 22:10:10.784995: step 10560, loss = 2.30 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:12.014859: step 10570, loss = 2.19 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:13.252914: step 10580, loss = 2.11 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:14.483330: step 10590, loss = 2.05 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:15.721317: step 10600, loss = 2.40 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:16.949438: step 10610, loss = 2.16 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:18.153700: step 10620, loss = 2.07 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:10:19.411329: step 10630, loss = 2.25 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-04 22:10:20.641384: step 10640, loss = 2.22 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:21.848452: step 10650, loss = 2.26 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:10:23.075533: step 10660, loss = 2.10 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:24.312151: step 10670, loss = 2.24 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:25.533764: step 10680, loss = 2.21 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:26.755540: step 10690, loss = 2.09 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:27.959804: step 10700, loss = 2.14 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:10:29.189491: step 10710, loss = 2.36 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:30.409994: step 10720, loss = 2.25 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:31.647192: step 10730, loss = 2.05 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:32.886015: step 10740, loss = 2.31 (1033.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:34.112107: step 10750, loss = 2.07 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:35.330688: step 10760, loss = 2.28 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:36.562332: step 10770, loss = 2.23 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:37.780946: step 10780, loss = 2.33 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:39.026804: step 10790, loss = 2.06 (1027.4 examples/sec; 0.125 sec/batch)
2017-05-04 22:10:40.243916: step 10800, loss = 2.16 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:41.482725: step 10810, loss = 2.24 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:42.690316: step 10820, loss = 2.17 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:10:43.937526: step 10830, loss = 2.19 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-04 22:10:45.156943: step 10840, loss = 2.28 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:46.374053: step 10850, loss = 2.25 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:47.598680: step 10860, loss = 2.09 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:10:48.841536: step 10870, loss = 2.31 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:50.055935: step 10880, loss = 2.25 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:10:51.392433: step 10890, loss = 2.29 (957.7 examples/sec; 0.134 sec/batch)
2017-05-04 22:10:52.467292: step 10900, loss = 2.17 (1190.9 examples/sec; 0.107 sec/batch)
2017-05-04 22:10:53.706177: step 1E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 119 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
0910, loss = 2.18 (1033.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:54.910780: step 10920, loss = 2.23 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:10:56.143361: step 10930, loss = 2.21 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:57.374973: step 10940, loss = 2.36 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:10:58.611995: step 10950, loss = 2.26 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:10:59.841416: step 10960, loss = 2.24 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:01.058914: step 10970, loss = 2.36 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:02.293137: step 10980, loss = 2.34 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:03.507531: step 10990, loss = 2.18 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:11:04.736036: step 11000, loss = 2.22 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:05.950331: step 11010, loss = 2.21 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:11:07.204779: step 11020, loss = 2.02 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-04 22:11:08.430430: step 11030, loss = 2.15 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:09.666191: step 11040, loss = 2.28 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:11:10.906277: step 11050, loss = 2.15 (1032.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:11:12.128219: step 11060, loss = 2.37 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:13.352730: step 11070, loss = 2.22 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:14.580900: step 11080, loss = 2.19 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:15.898527: step 11090, loss = 2.26 (971.5 examples/sec; 0.132 sec/batch)
2017-05-04 22:11:17.022714: step 11100, loss = 2.18 (1138.6 examples/sec; 0.112 sec/batch)
2017-05-04 22:11:18.258987: step 11110, loss = 2.18 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:11:19.477497: step 11120, loss = 2.22 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:20.732794: step 11130, loss = 2.22 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-04 22:11:21.933729: step 11140, loss = 2.10 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:11:23.168409: step 11150, loss = 2.20 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:24.383931: step 11160, loss = 2.44 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:25.637631: step 11170, loss = 2.24 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-04 22:11:26.840257: step 11180, loss = 2.22 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:11:28.073969: step 11190, loss = 2.29 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:29.287188: step 11200, loss = 2.18 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:11:30.506771: step 11210, loss = 2.39 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:31.749864: step 11220, loss = 2.35 (1029.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:11:32.984212: step 11230, loss = 2.22 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:34.180475: step 11240, loss = 2.20 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:11:35.419703: step 11250, loss = 2.21 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:11:36.641475: step 11260, loss = 2.22 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:37.881644: step 11270, loss = 2.11 (1032.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:11:39.091805: step 11280, loss = 2.27 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:11:40.285823: step 11290, loss = 2.21 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:11:41.528390: step 11300, loss = 1.99 (1030.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:11:42.761131: step 11310, loss = 2.06 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:43.977929: step 11320, loss = 2.14 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:11:45.226641: step 11330, loss = 2.11 (1025.1 examples/sec; 0.125 sec/batch)
2017-05-04 22:11:46.441297: step 11340, loss = 2.41 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:11:47.697878: step 11350, loss = 2.27 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-04 22:11:48.905648: step 11360, loss = 2.24 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:11:50.131988: step 11370, loss = 2.08 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:51.341608: step 11380, loss = 2.16 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:11:52.569455: step 11390, loss = 2.34 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:53.811674: step 11400, loss = 2.18 (1030.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:11:55.041365: step 11410, loss = 2.40 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:56.273983: step 11420, loss = 2.18 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:57.503268: step 11430, loss = 2.42 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:11:58.689489: step 11440, loss = 2.15 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:11:59.932647: step 11450, loss = 2.10 (1029.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:01.170418: step 11460, loss = 2.19 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:02.391076: step 11470, loss = 2.21 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:03.625191: step 11480, loss = 2.08 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:04.820560: step 11490, loss = 2.17 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:12:06.033139: step 11500, loss = 2.22 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:12:07.269768: step 11510, loss = 2.12 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:08.479134: step 11520, loss = 2.15 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:12:09.723678: step 11530, loss = 2.22 (1028.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:10.942585: step 11540, loss = 2.22 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:12.193396: step 11550, loss = 2.10 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-04 22:12:13.379129: step 11560, loss = 2.03 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:12:14.604429: step 11570, loss = 2.00 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:15.841639: step 11580, loss = 2.21 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:17.070788: step 11590, loss = 2.24 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:18.304006: step 11600, loss = 2.19 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:19.543977: step 11610, loss = 1.99 (1032.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:20.763841: step 11620, loss = 2.28 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:21.981180: step 11630, loss = 2.17 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:23.202342: step 11640, loss = 2.39 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:24.443159: step 11650, loss = 2.12 (1031.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:25.647000: step 11660, loss = 2.07 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:12:26.870740: step 11670, loss = 2.06 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:28.107904: step 11680, loss = 2.27 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:29.292441: step 11690, loss = 2.13 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:12:30.521826: step 11700, loss = 2.49 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:31.749510: step 11710, loss = 2.29 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:32.967829: step 11720, loss = 2.27 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:34.210953: step 11730, loss = 2.23 (1029.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:35.449211: step 11740, loss = 2.25 (1033.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:36.663598: step 11750, loss = 2.23 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:12:37.880408: step 11760, loss = 2.14 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:39.126056: step 11770, loss = 2.08 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-04 22:12:40.354923: step 11780, loss = 2.26 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:41.593620: step 11790, loss = 2.26 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:42.824516: step 11800, loss = 2.17 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:44.048816: step 1181E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 129 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
0, loss = 2.09 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:45.268697: step 11820, loss = 2.08 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:12:46.478466: step 11830, loss = 2.08 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:12:47.684040: step 11840, loss = 2.11 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:12:48.916285: step 11850, loss = 2.27 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:50.123509: step 11860, loss = 2.21 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:12:51.349607: step 11870, loss = 2.28 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:52.678620: step 11880, loss = 2.30 (963.1 examples/sec; 0.133 sec/batch)
2017-05-04 22:12:53.767095: step 11890, loss = 2.25 (1176.0 examples/sec; 0.109 sec/batch)
2017-05-04 22:12:55.008200: step 11900, loss = 2.35 (1031.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:56.241811: step 11910, loss = 2.32 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:12:57.453551: step 11920, loss = 2.23 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:12:58.691611: step 11930, loss = 2.17 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:12:59.900369: step 11940, loss = 2.30 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:13:01.140040: step 11950, loss = 2.27 (1032.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:13:02.349504: step 11960, loss = 2.10 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:13:03.601127: step 11970, loss = 2.13 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-04 22:13:04.813551: step 11980, loss = 2.23 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:13:06.030042: step 11990, loss = 2.16 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:07.267884: step 12000, loss = 2.06 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:13:08.485364: step 12010, loss = 2.20 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:09.700777: step 12020, loss = 2.27 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:10.929059: step 12030, loss = 2.40 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:12.156512: step 12040, loss = 2.19 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:13.388670: step 12050, loss = 2.09 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:14.590458: step 12060, loss = 2.15 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:13:15.820388: step 12070, loss = 2.27 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:17.150148: step 12080, loss = 2.13 (962.6 examples/sec; 0.133 sec/batch)
2017-05-04 22:13:18.251601: step 12090, loss = 2.12 (1162.1 examples/sec; 0.110 sec/batch)
2017-05-04 22:13:19.485547: step 12100, loss = 2.09 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:20.733832: step 12110, loss = 2.21 (1025.4 examples/sec; 0.125 sec/batch)
2017-05-04 22:13:21.980172: step 12120, loss = 2.14 (1027.0 examples/sec; 0.125 sec/batch)
2017-05-04 22:13:23.209095: step 12130, loss = 2.28 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:24.463902: step 12140, loss = 2.06 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-04 22:13:25.687031: step 12150, loss = 2.91 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:26.903824: step 12160, loss = 2.16 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:28.143594: step 12170, loss = 2.22 (1032.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:13:29.351750: step 12180, loss = 2.24 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:13:30.569103: step 12190, loss = 2.17 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:31.766270: step 12200, loss = 2.21 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:13:32.984263: step 12210, loss = 2.11 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:34.196080: step 12220, loss = 2.20 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:13:35.407761: step 12230, loss = 2.39 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:13:36.631403: step 12240, loss = 2.16 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:37.854371: step 12250, loss = 2.12 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:39.063945: step 12260, loss = 2.12 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:13:40.291961: step 12270, loss = 2.09 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:41.487195: step 12280, loss = 2.12 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:13:42.735521: step 12290, loss = 2.03 (1025.4 examples/sec; 0.125 sec/batch)
2017-05-04 22:13:43.937665: step 12300, loss = 2.05 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:13:45.177732: step 12310, loss = 2.15 (1032.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:13:46.389451: step 12320, loss = 2.12 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:13:47.649856: step 12330, loss = 2.23 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-04 22:13:48.869050: step 12340, loss = 2.20 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:50.077052: step 12350, loss = 2.34 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:13:51.306322: step 12360, loss = 2.22 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:52.555171: step 12370, loss = 2.20 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-04 22:13:53.740883: step 12380, loss = 2.23 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:13:54.980528: step 12390, loss = 2.14 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:13:56.206554: step 12400, loss = 2.17 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:13:57.429500: step 12410, loss = 2.20 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:13:58.638282: step 12420, loss = 2.12 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:13:59.875372: step 12430, loss = 2.15 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:14:01.097948: step 12440, loss = 2.27 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:14:02.294315: step 12450, loss = 2.24 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:03.523936: step 12460, loss = 2.18 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:14:04.753542: step 12470, loss = 2.24 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:14:05.952901: step 12480, loss = 2.27 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:07.174426: step 12490, loss = 2.05 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:14:08.403201: step 12500, loss = 2.08 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:14:09.610730: step 12510, loss = 2.10 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:14:10.812376: step 12520, loss = 2.22 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:12.010187: step 12530, loss = 2.12 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:13.232692: step 12540, loss = 2.23 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:14:14.422814: step 12550, loss = 2.13 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:14:15.623730: step 12560, loss = 2.27 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:16.828614: step 12570, loss = 2.29 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:18.005652: step 12580, loss = 2.14 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:14:19.196563: step 12590, loss = 2.20 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:14:20.392654: step 12600, loss = 2.06 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:21.590031: step 12610, loss = 2.18 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:22.803219: step 12620, loss = 2.45 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:14:24.022310: step 12630, loss = 2.13 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:14:25.238444: step 12640, loss = 2.22 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:14:26.438645: step 12650, loss = 2.20 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:27.674071: step 12660, loss = 2.19 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:14:28.877658: step 12670, loss = 2.20 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:30.064107: step 12680, loss = 2.16 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:14:31.282077: step 12690, loss = 2.26 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:14:32.498543: step 12700, loss = 2.12 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:14:33.718937: step 12710, loss = 2.16 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:14:34.950613: step 12720, loss = 2.40 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:14:36.159754: step 12730, loss = 2.28 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:14:37.333429: step 12740, loss = 2.15 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:14:38.546887: step 12750, loss = 2.09 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:14:39.736478: step 12760, loss = 2.18 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:14:40.929970: step 12770, loss = 2.07 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:14:42.076113: step 12780, loss = 2.19 (1116.8 examples/sec; 0.115 sec/batch)
2017-05-04 22:14:43.241658: step 12790, loss = 2.12 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:14:44.423542: step 12800, loss = 2.32 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:14:45.593333: step 12810, loss = 2.18 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:14:46.759862: step 12820, loss = 2.21 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:14:47.956979: step 12830, loss = 2.19 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:14:49.126784: step 12840, loss = 2.16 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:14:50.297720: step 12850, loss = 2.10 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:14:51.461319: step 12860, loss = 2.13 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:14:52.741303: step 12870, loss = 2.05 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-04 22:14:53.788171: step 12880, loss = 2.31 (1222.7 examples/sec; 0.105 sec/batch)
2017-05-04 22:14:54.977689: step 12890, loss = 2.20 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:14:56.146376: step 12900, loss = 2.24 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:14:57.335134: step 12910, loss = 2.30 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:14:58.516060: step 12920, loss = 2.17 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:14:59.692360: step 12930, loss = 2.03 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:00.866100: step 12940, loss = 2.01 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:02.059382: step 12950, loss = 2.42 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:03.250193: step 12960, loss = 2.13 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:04.423489: step 12970, loss = 2.54 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:05.589461: step 12980, loss = 2.20 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:06.764627: step 12990, loss = 2.20 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:07.949347: step 13000, loss = 2.19 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:09.115755: step 13010, loss = 2.19 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:10.287645: step 13020, loss = 2.17 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:11.458293: step 13030, loss = 2.24 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:12.606627: step 13040, loss = 2.07 (1114.7 examples/sec; 0.115 sec/batch)
2017-05-04 22:15:13.765278: step 13050, loss = 2.04 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:15:14.929006: step 13060, loss = 2.18 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:15:16.137301: step 13070, loss = 2.16 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:15:17.287580: step 13080, loss = 2.40 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-04 22:15:18.467709: step 13090, loss = 2.03 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:19.642435: step 13100, loss = 2.13 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:20.838319: step 13110, loss = 2.03 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:15:22.024770: step 13120, loss = 2.10 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:23.231652: step 13130, loss = 2.37 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:15:24.414321: step 13140, loss = 2.05 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:25.600361: step 13150, loss = 2.12 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:26.793591: step 13160, loE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 140 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
ss = 2.08 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:28.000432: step 13170, loss = 2.23 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:15:29.203534: step 13180, loss = 2.38 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:15:30.389107: step 13190, loss = 2.23 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:31.572326: step 13200, loss = 2.19 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:32.779787: step 13210, loss = 2.04 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:15:33.944944: step 13220, loss = 2.24 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:35.146179: step 13230, loss = 2.26 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:15:36.347962: step 13240, loss = 2.15 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:15:37.531010: step 13250, loss = 2.16 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:38.704910: step 13260, loss = 2.01 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:39.883825: step 13270, loss = 2.04 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:41.060785: step 13280, loss = 2.21 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:42.236315: step 13290, loss = 2.19 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:43.412645: step 13300, loss = 2.08 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:44.593694: step 13310, loss = 2.04 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:45.761721: step 13320, loss = 2.16 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:46.935166: step 13330, loss = 2.41 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:48.096035: step 13340, loss = 2.25 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:15:49.282300: step 13350, loss = 2.17 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:50.452656: step 13360, loss = 2.08 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:51.640577: step 13370, loss = 2.39 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:52.823929: step 13380, loss = 2.08 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:54.007346: step 13390, loss = 2.27 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:55.191808: step 13400, loss = 2.19 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:15:56.379912: step 13410, loss = 2.28 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:15:57.551458: step 13420, loss = 2.07 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:58.717472: step 13430, loss = 1.99 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:15:59.897356: step 13440, loss = 2.09 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:01.075043: step 13450, loss = 2.20 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:02.250773: step 13460, loss = 2.14 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:03.448007: step 13470, loss = 2.13 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:16:04.619687: step 13480, loss = 2.18 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:05.802928: step 13490, loss = 2.25 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:06.972937: step 13500, loss = 2.32 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:08.160821: step 13510, loss = 2.27 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:09.346191: step 13520, loss = 2.27 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:10.531953: step 13530, loss = 2.21 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:11.736686: step 13540, loss = 2.29 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:16:12.917174: step 13550, loss = 2.15 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:14.104810: step 13560, loss = 2.13 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:15.297345: step 13570, loss = 2.13 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:16.467221: step 13580, loss = 2.07 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:17.634759: step 13590, loss = 2.13 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:18.833284: step 13600, loss = 2.17 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:16:20.015319: step 13610, loss = 2.10 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:21.194169: step 13620, loss = 2.12 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:22.362264: step 13630, loss = 2.28 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:23.525334: step 13640, loss = 2.08 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:16:24.707588: step 13650, loss = 2.14 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:25.871645: step 13660, loss = 2.27 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:16:27.046019: step 13670, loss = 2.32 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:28.233411: step 13680, loss = 2.15 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:29.426951: step 13690, loss = 2.25 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:30.611105: step 13700, loss = 2.03 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:31.788094: step 13710, loss = 2.14 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:32.984469: step 13720, loss = 2.07 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:16:34.173916: step 13730, loss = 2.12 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:16:35.379424: step 13740, loss = 2.42 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:16:36.555998: step 13750, loss = 2.15 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:37.735081: step 13760, loss = 2.15 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:38.908038: step 13770, loss = 2.13 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:40.083890: step 13780, loss = 1.96 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:41.266532: step 13790, loss = 2.25 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:42.435888: step 13800, loss = 2.07 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:43.609881: step 13810, loss = 2.19 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:44.776915: step 13820, loss = 1.89 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:45.934619: step 13830, loss = 2.14 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:16:47.112199: step 13840, loss = 2.07 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:48.295818: step 13850, loss = 2.26 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:49.562125: step 13860, loss = 2.15 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-04 22:16:50.658938: step 13870, loss = 2.25 (1167.0 examples/sec; 0.110 sec/batch)
2017-05-04 22:16:51.838015: step 13880, loss = 2.02 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:53.010721: step 13890, loss = 2.30 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:54.176748: step 13900, loss = 2.12 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:55.353661: step 13910, loss = 2.08 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:56.535324: step 13920, loss = 1.99 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:16:57.702728: step 13930, loss = 2.13 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:16:58.881114: step 13940, loss = 2.24 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:00.057707: step 13950, loss = 1.98 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:01.244801: step 13960, loss = 2.11 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:17:02.429576: step 13970, loss = 2.07 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:03.602030: step 13980, loss = 2.17 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:04.781720: step 13990, loss = 2.32 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:05.926020: step 14000, loss = 2.10 (1118.6 examples/sec; 0.114 sec/batch)
2017-05-04 22:17:07.107093: step 14010, loss = 2.06 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:08.297136: step 14020, loss = 2.28 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:17:09.465328: step 14030, loss = 2.16 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:10.637036: step 14040, loss = 2.13 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:11.805539: step 14050, loss = 2.23 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:12.973082: step 14060, lossE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 151 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
 = 2.14 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:14.120775: step 14070, loss = 2.29 (1115.3 examples/sec; 0.115 sec/batch)
2017-05-04 22:17:15.288848: step 14080, loss = 2.26 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:16.455070: step 14090, loss = 2.08 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:17.627116: step 14100, loss = 1.90 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:18.821674: step 14110, loss = 2.07 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:17:19.987487: step 14120, loss = 2.20 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:21.166155: step 14130, loss = 2.27 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:22.323100: step 14140, loss = 2.25 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:17:23.487292: step 14150, loss = 2.28 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:17:24.670498: step 14160, loss = 2.17 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:25.816335: step 14170, loss = 2.37 (1117.1 examples/sec; 0.115 sec/batch)
2017-05-04 22:17:27.004276: step 14180, loss = 1.97 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:17:28.187215: step 14190, loss = 2.16 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:29.381534: step 14200, loss = 2.22 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:17:30.599032: step 14210, loss = 1.99 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:17:31.789149: step 14220, loss = 2.26 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:17:33.003928: step 14230, loss = 2.12 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:17:34.205104: step 14240, loss = 2.16 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:17:35.420312: step 14250, loss = 2.35 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:17:36.594817: step 14260, loss = 2.09 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:17:37.774618: step 14270, loss = 2.13 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:38.985340: step 14280, loss = 2.08 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:17:40.190035: step 14290, loss = 2.02 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:17:41.393118: step 14300, loss = 2.42 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:17:42.600940: step 14310, loss = 2.04 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:17:43.787671: step 14320, loss = 2.23 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:17:44.992129: step 14330, loss = 2.05 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:17:46.184510: step 14340, loss = 2.41 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:17:47.384849: step 14350, loss = 2.03 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:17:48.569349: step 14360, loss = 2.13 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:17:49.776191: step 14370, loss = 2.22 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:17:51.001325: step 14380, loss = 2.32 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:17:52.198789: step 14390, loss = 2.15 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:17:53.414522: step 14400, loss = 2.23 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:17:54.623460: step 14410, loss = 2.20 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:17:55.846811: step 14420, loss = 2.10 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:17:57.067953: step 14430, loss = 2.37 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:17:58.288793: step 14440, loss = 1.98 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:17:59.536376: step 14450, loss = 2.24 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-04 22:18:00.744435: step 14460, loss = 2.33 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:18:01.932301: step 14470, loss = 2.11 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:18:03.164547: step 14480, loss = 2.17 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:18:04.394971: step 14490, loss = 2.07 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:18:05.595514: step 14500, loss = 2.19 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:18:06.839177: step 14510, loss = 2.18 (1029.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:18:08.070997: step 14520, loss = 2.09 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:18:09.309457: step 14530, loss = 2.10 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:18:10.519452: step 14540, loss = 2.02 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:18:11.743499: step 14550, loss = 2.10 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:18:12.991281: step 14560, loss = 2.24 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-04 22:18:14.195660: step 14570, loss = 2.28 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:18:15.410089: step 14580, loss = 2.28 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:18:16.623577: step 14590, loss = 2.00 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:18:17.816643: step 14600, loss = 2.11 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:18:19.057938: step 14610, loss = 2.06 (1031.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:18:20.266282: step 14620, loss = 2.05 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:18:21.505307: step 14630, loss = 2.11 (1033.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:18:22.711880: step 14640, loss = 2.17 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:18:23.936398: step 14650, loss = 2.21 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:18:25.135278: step 14660, loss = 2.06 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:18:26.362660: step 14670, loss = 2.13 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:18:27.562718: step 14680, loss = 2.16 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:18:28.796062: step 14690, loss = 2.18 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:18:30.017109: step 14700, loss = 1.90 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:18:31.219924: step 14710, loss = 2.13 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:18:32.420460: step 14720, loss = 2.05 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:18:33.631849: step 14730, loss = 2.03 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:18:34.860520: step 14740, loss = 2.22 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:18:36.083604: step 14750, loss = 2.07 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:18:37.292005: step 14760, loss = 2.09 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:18:38.493526: step 14770, loss = 2.12 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:18:39.691340: step 14780, loss = 2.10 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:18:40.897609: step 14790, loss = 2.24 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:18:42.075588: step 14800, loss = 2.22 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:18:43.258863: step 14810, loss = 1.99 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:18:44.436713: step 14820, loss = 1.99 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:18:45.603393: step 14830, loss = 2.07 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:18:46.785708: step 14840, loss = 2.22 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:18:48.059347: step 14850, loss = 2.10 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-04 22:18:49.123638: step 14860, loss = 2.19 (1202.7 examples/sec; 0.106 sec/batch)
2017-05-04 22:18:50.312477: step 14870, loss = 2.03 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:18:51.465169: step 14880, loss = 2.15 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-04 22:18:52.648368: step 14890, loss = 2.20 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:18:53.829910: step 14900, loss = 2.39 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:18:55.017089: step 14910, loss = 2.16 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:18:56.200487: step 14920, loss = 2.26 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:18:57.394392: step 14930, loss = 2.05 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:18:58.565824: step 14940, loss = 2.01 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:18:59.753106: step 14950, loss = 2.12 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:19:00.952850: step 14960, loss =E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 163 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
 2.23 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:02.157483: step 14970, loss = 2.14 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:03.382652: step 14980, loss = 2.11 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:19:04.605329: step 14990, loss = 2.25 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:05.810071: step 15000, loss = 2.23 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:07.013966: step 15010, loss = 2.12 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:08.215099: step 15020, loss = 1.98 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:09.399031: step 15030, loss = 2.37 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:19:10.571174: step 15040, loss = 2.27 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:19:11.840048: step 15050, loss = 2.11 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-04 22:19:12.965687: step 15060, loss = 2.04 (1137.1 examples/sec; 0.113 sec/batch)
2017-05-04 22:19:14.142574: step 15070, loss = 2.28 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:19:15.338305: step 15080, loss = 2.20 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:16.558766: step 15090, loss = 2.06 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:17.754385: step 15100, loss = 2.04 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:18.961059: step 15110, loss = 2.17 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:20.179879: step 15120, loss = 2.12 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:21.400429: step 15130, loss = 2.12 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:22.626293: step 15140, loss = 2.13 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:19:23.836633: step 15150, loss = 2.31 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:25.052015: step 15160, loss = 2.16 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:26.247179: step 15170, loss = 2.13 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:27.462757: step 15180, loss = 2.29 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:28.679590: step 15190, loss = 2.30 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:29.878482: step 15200, loss = 2.05 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:31.086641: step 15210, loss = 2.35 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:32.308697: step 15220, loss = 2.08 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:33.518593: step 15230, loss = 2.15 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:34.727787: step 15240, loss = 2.13 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:35.936191: step 15250, loss = 2.23 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:37.159384: step 15260, loss = 2.21 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:38.389944: step 15270, loss = 2.06 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:19:39.594438: step 15280, loss = 2.14 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:40.811382: step 15290, loss = 2.12 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:42.010949: step 15300, loss = 2.19 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:43.243391: step 15310, loss = 2.35 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:19:44.468039: step 15320, loss = 2.10 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:45.697366: step 15330, loss = 2.15 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:19:46.930244: step 15340, loss = 2.20 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:19:48.149834: step 15350, loss = 2.22 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:49.356446: step 15360, loss = 2.17 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:50.577722: step 15370, loss = 2.21 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:19:51.807203: step 15380, loss = 1.95 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:19:53.013787: step 15390, loss = 2.11 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:19:54.205519: step 15400, loss = 2.15 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:19:55.409917: step 15410, loss = 2.06 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:19:56.600399: step 15420, loss = 2.13 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:19:57.779299: step 15430, loss = 1.99 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:19:58.986805: step 15440, loss = 2.17 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:20:00.170005: step 15450, loss = 2.10 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:01.371615: step 15460, loss = 2.06 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:02.543906: step 15470, loss = 2.33 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:20:03.718877: step 15480, loss = 2.17 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:20:04.917372: step 15490, loss = 2.23 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:06.110648: step 15500, loss = 2.15 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:07.309986: step 15510, loss = 2.26 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:08.506726: step 15520, loss = 2.14 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:09.706947: step 15530, loss = 2.13 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:10.884651: step 15540, loss = 2.06 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:12.078718: step 15550, loss = 1.99 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:13.261945: step 15560, loss = 2.33 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:14.456633: step 15570, loss = 2.12 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:15.659169: step 15580, loss = 2.36 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:16.865105: step 15590, loss = 2.24 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:20:18.066793: step 15600, loss = 2.13 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:19.272212: step 15610, loss = 2.26 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:20:20.441443: step 15620, loss = 2.14 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:20:21.603340: step 15630, loss = 2.37 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:20:22.766463: step 15640, loss = 2.10 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:20:23.936277: step 15650, loss = 2.14 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:20:25.121672: step 15660, loss = 2.03 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:26.296285: step 15670, loss = 2.08 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:20:27.473273: step 15680, loss = 2.09 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:28.658397: step 15690, loss = 2.24 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:29.813999: step 15700, loss = 2.04 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:20:30.997058: step 15710, loss = 2.13 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:32.167212: step 15720, loss = 2.16 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:20:33.339620: step 15730, loss = 2.30 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:20:34.508280: step 15740, loss = 2.21 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:20:35.699738: step 15750, loss = 2.12 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:36.879271: step 15760, loss = 2.25 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:38.054868: step 15770, loss = 2.15 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:39.239875: step 15780, loss = 2.48 (1080.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:40.437584: step 15790, loss = 2.17 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:41.620956: step 15800, loss = 2.13 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:42.824033: step 15810, loss = 2.19 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:20:44.036694: step 15820, loss = 2.29 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:20:45.242297: step 15830, loss = 2.25 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:20:46.555398: step 15840, loss = 2.23 (974.8 examples/sec; 0.131 sec/batch)
2017-05-04 22:20:47.649012: step 15850, loss = 2.21 (1170.4 examples/sec; 0.109 sec/batch)
2017-05-04 22:20:48.874294: step 15860, loss = 2.31 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:20:50.069277: step 15870, loss = 2.03 (1071.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:51.278575: step 15880, loss = 2.22 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:20:52.470586: step 15890, loss = 2.10 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:53.660192: step 15900, loss = 2.16 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:54.849664: step 15910, loss = 2.11 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:56.029067: step 15920, loss = 2.22 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:57.223010: step 15930, loss = 2.10 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:20:58.403621: step 15940, loss = 2.16 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:20:59.586843: step 15950, loss = 2.38 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:00.770639: step 15960, loss = 1.92 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:01.929983: step 15970, loss = 2.10 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:21:03.110069: step 15980, loss = 2.30 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:04.283001: step 15990, loss = 2.04 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:21:05.459221: step 16000, loss = 2.07 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:06.624357: step 16010, loss = 2.23 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:21:07.797555: step 16020, loss = 2.07 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:21:09.001536: step 16030, loss = 2.07 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:21:10.170026: step 16040, loss = 2.20 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:21:11.327087: step 16050, loss = 2.05 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:21:12.505312: step 16060, loss = 2.37 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:13.688710: step 16070, loss = 2.16 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:14.840878: step 16080, loss = 2.27 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:21:16.017956: step 16090, loss = 2.20 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:17.198606: step 16100, loss = 2.25 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:18.362352: step 16110, loss = 2.03 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:21:19.541143: step 16120, loss = 2.40 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:20.732722: step 16130, loss = 2.04 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:21:21.914088: step 16140, loss = 2.05 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:23.091801: step 16150, loss = 2.19 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:24.268406: step 16160, loss = 2.34 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:25.446007: step 16170, loss = 2.08 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:26.615272: step 16180, loss = 2.06 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:21:27.792736: step 16190, loss = 2.13 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:28.989024: step 16200, loss = 2.11 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:21:30.159771: step 16210, loss = 2.04 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:21:31.356630: step 16220, loss = 2.30 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:21:32.555380: step 16230, loss = 2.17 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:21:33.764598: step 16240, loss = 2.17 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:21:34.995676: step 16250, loss = 2.09 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:21:36.217971: step 16260, loss = 2.22 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:21:37.457427: step 16270, loss = 2.21 (1032.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:21:38.647636: step 16280, loss = 2.10 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:21:39.864165: step 16290, loss = 2.24 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:21:41.094993: step 16300, loss = 2.17 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:21:42.297646: step 16310, loss = 2.1E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 174 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
3 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:21:43.513706: step 16320, loss = 2.23 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:21:44.703846: step 16330, loss = 2.18 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:21:45.881108: step 16340, loss = 2.02 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:47.074559: step 16350, loss = 2.03 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:21:48.283960: step 16360, loss = 2.10 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:21:49.472664: step 16370, loss = 2.17 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:21:50.672545: step 16380, loss = 2.17 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:21:51.872220: step 16390, loss = 2.18 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:21:53.083136: step 16400, loss = 2.07 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:21:54.258784: step 16410, loss = 2.16 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:55.438085: step 16420, loss = 2.17 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:21:56.640113: step 16430, loss = 2.14 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:21:57.826889: step 16440, loss = 2.01 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:21:59.026391: step 16450, loss = 1.97 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:22:00.210727: step 16460, loss = 2.13 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:01.376856: step 16470, loss = 2.06 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:02.547067: step 16480, loss = 2.05 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:03.729447: step 16490, loss = 2.18 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:04.916405: step 16500, loss = 2.08 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:22:06.098135: step 16510, loss = 2.13 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:07.282176: step 16520, loss = 2.37 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:08.459222: step 16530, loss = 2.22 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:09.619473: step 16540, loss = 2.07 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:22:10.812006: step 16550, loss = 2.06 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:22:12.005623: step 16560, loss = 2.24 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:22:13.191978: step 16570, loss = 2.04 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:22:14.360655: step 16580, loss = 2.20 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:15.547849: step 16590, loss = 2.05 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:22:16.756986: step 16600, loss = 1.97 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:22:17.934636: step 16610, loss = 1.94 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:19.166922: step 16620, loss = 2.04 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:22:20.384114: step 16630, loss = 2.28 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:22:21.589414: step 16640, loss = 2.38 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:22:22.810078: step 16650, loss = 2.27 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:22:24.046843: step 16660, loss = 2.04 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:22:25.258274: step 16670, loss = 2.01 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:22:26.457020: step 16680, loss = 2.22 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:22:27.680190: step 16690, loss = 2.12 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:22:28.893523: step 16700, loss = 2.24 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:22:30.089510: step 16710, loss = 1.98 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:22:31.290924: step 16720, loss = 2.18 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:22:32.473957: step 16730, loss = 2.14 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:33.656140: step 16740, loss = 2.19 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:34.840288: step 16750, loss = 2.12 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:36.022188: step 16760, loss = 2.21 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:37.198549: step 16770, loss = 2.22 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:38.348168: step 16780, loss = 2.12 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-04 22:22:39.522133: step 16790, loss = 2.18 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:40.707317: step 16800, loss = 2.10 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:22:41.859409: step 16810, loss = 2.14 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:22:43.033787: step 16820, loss = 2.32 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:44.300440: step 16830, loss = 2.06 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-04 22:22:45.378400: step 16840, loss = 1.86 (1187.4 examples/sec; 0.108 sec/batch)
2017-05-04 22:22:46.553923: step 16850, loss = 1.98 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:47.726040: step 16860, loss = 2.18 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:48.904481: step 16870, loss = 2.17 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:50.082616: step 16880, loss = 2.38 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:51.276184: step 16890, loss = 2.27 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:22:52.444569: step 16900, loss = 2.23 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:22:53.602377: step 16910, loss = 2.17 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:22:54.766191: step 16920, loss = 2.13 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:22:55.951452: step 16930, loss = 2.09 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:22:57.131454: step 16940, loss = 2.09 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:22:58.321300: step 16950, loss = 2.11 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:22:59.504461: step 16960, loss = 2.22 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:00.687845: step 16970, loss = 2.20 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:01.836997: step 16980, loss = 2.11 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-04 22:23:03.014565: step 16990, loss = 2.14 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:04.184247: step 17000, loss = 2.03 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:05.341347: step 17010, loss = 2.08 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:23:06.508389: step 17020, loss = 2.27 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:07.686835: step 17030, loss = 2.14 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:08.860416: step 17040, loss = 2.15 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:10.030861: step 17050, loss = 2.33 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:11.208873: step 17060, loss = 2.15 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:12.394567: step 17070, loss = 2.01 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:23:13.547828: step 17080, loss = 2.05 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-04 22:23:14.711604: step 17090, loss = 2.06 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:23:15.890558: step 17100, loss = 2.21 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:17.078384: step 17110, loss = 2.14 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:23:18.240800: step 17120, loss = 2.01 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:23:19.401140: step 17130, loss = 1.91 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:23:20.583342: step 17140, loss = 2.13 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:21.759743: step 17150, loss = 2.26 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:22.931561: step 17160, loss = 1.95 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:24.134904: step 17170, loss = 2.23 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:23:25.319986: step 17180, loss = 2.19 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:23:26.495094: step 17190, loss = 2.04 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:27.670385: step 17200, loss = 1.93 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:28.857469: step 17210, loss = 1.97 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 185 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
(1078.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:23:30.044311: step 17220, loss = 2.05 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:23:31.218544: step 17230, loss = 2.03 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:32.404204: step 17240, loss = 2.06 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:23:33.580089: step 17250, loss = 2.02 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:34.775613: step 17260, loss = 2.28 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:23:35.941997: step 17270, loss = 1.98 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:37.090480: step 17280, loss = 2.05 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-04 22:23:38.234692: step 17290, loss = 2.21 (1118.7 examples/sec; 0.114 sec/batch)
2017-05-04 22:23:39.409162: step 17300, loss = 2.28 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:40.583875: step 17310, loss = 2.22 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:41.765112: step 17320, loss = 2.10 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:42.955879: step 17330, loss = 2.07 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:23:44.111169: step 17340, loss = 1.91 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:23:45.290903: step 17350, loss = 2.02 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:46.450848: step 17360, loss = 2.18 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:23:47.614432: step 17370, loss = 2.13 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:23:48.789711: step 17380, loss = 2.31 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:49.957016: step 17390, loss = 2.20 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:51.124080: step 17400, loss = 2.00 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:52.313036: step 17410, loss = 2.20 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:23:53.469973: step 17420, loss = 2.05 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:23:54.631454: step 17430, loss = 2.25 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:23:55.773826: step 17440, loss = 2.10 (1120.5 examples/sec; 0.114 sec/batch)
2017-05-04 22:23:56.953767: step 17450, loss = 2.15 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:23:58.128493: step 17460, loss = 2.28 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:23:59.301389: step 17470, loss = 2.04 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:00.466305: step 17480, loss = 2.15 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:01.628519: step 17490, loss = 2.13 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:02.801579: step 17500, loss = 1.89 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:03.957547: step 17510, loss = 2.16 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:05.135976: step 17520, loss = 2.22 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:06.318278: step 17530, loss = 2.17 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:07.499520: step 17540, loss = 2.12 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:08.679907: step 17550, loss = 2.20 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:09.852095: step 17560, loss = 2.18 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:11.032923: step 17570, loss = 2.10 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:12.227710: step 17580, loss = 2.19 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:24:13.410434: step 17590, loss = 2.09 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:14.571569: step 17600, loss = 2.11 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:15.743827: step 17610, loss = 2.05 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:16.916692: step 17620, loss = 2.06 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:18.079843: step 17630, loss = 2.13 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:19.255734: step 17640, loss = 2.12 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:20.423026: step 17650, loss = 2.13 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:21.578316: step 17660, loss = 2.20 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:22.738151: step 17670, loss = 2.13 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:23.917460: step 17680, loss = 2.21 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:25.083701: step 17690, loss = 2.18 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:26.241649: step 17700, loss = 2.04 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:27.414981: step 17710, loss = 2.15 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:28.575394: step 17720, loss = 2.04 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:29.738337: step 17730, loss = 2.16 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:30.901164: step 17740, loss = 1.92 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:32.054326: step 17750, loss = 2.03 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:24:33.228552: step 17760, loss = 2.08 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:34.390270: step 17770, loss = 2.15 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:35.557826: step 17780, loss = 2.11 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:36.728760: step 17790, loss = 2.39 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:37.908336: step 17800, loss = 2.04 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:39.085390: step 17810, loss = 2.22 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:40.380410: step 17820, loss = 2.14 (988.4 examples/sec; 0.130 sec/batch)
2017-05-04 22:24:41.440196: step 17830, loss = 2.10 (1207.8 examples/sec; 0.106 sec/batch)
2017-05-04 22:24:42.601509: step 17840, loss = 2.17 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:43.784721: step 17850, loss = 1.95 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:44.965554: step 17860, loss = 2.11 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:24:46.123209: step 17870, loss = 2.04 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:47.276448: step 17880, loss = 2.29 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-04 22:24:48.437274: step 17890, loss = 2.06 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:49.605721: step 17900, loss = 2.09 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:50.776580: step 17910, loss = 2.22 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:51.939810: step 17920, loss = 2.15 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:24:53.105838: step 17930, loss = 2.07 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:54.298581: step 17940, loss = 2.06 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:24:55.467612: step 17950, loss = 2.14 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:56.640810: step 17960, loss = 2.04 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:57.811584: step 17970, loss = 2.20 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:24:59.009351: step 17980, loss = 2.09 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:25:00.228965: step 17990, loss = 2.26 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:01.449868: step 18000, loss = 2.08 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:02.672396: step 18010, loss = 2.05 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:03.894563: step 18020, loss = 2.08 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:05.128901: step 18030, loss = 1.93 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:06.343986: step 18040, loss = 2.00 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:07.560184: step 18050, loss = 2.02 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:08.772644: step 18060, loss = 2.20 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:25:09.994548: step 18070, loss = 2.36 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:11.215395: step 18080, loss = 2.09 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:12.459771: step 18090, loss = 2.10 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:25:13.654275: step 18100, loss = 2.06 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:25:14.885364: step 18110, loss = 1.97 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:16.102550: step 18120, loss = 2.13 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:17.327583: step 18130, loss = 2.10 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:18.544926: step 18140, loss = 2.22 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:19.762032: step 18150, loss = 2.13 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:21.002854: step 18160, loss = 2.14 (1031.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:25:22.207286: step 18170, loss = 2.30 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:25:23.427182: step 18180, loss = 2.03 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:24.630786: step 18190, loss = 2.24 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:25:25.847299: step 18200, loss = 2.26 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:27.075567: step 18210, loss = 2.25 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:28.248127: step 18220, loss = 2.01 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:25:29.454975: step 18230, loss = 2.08 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:25:30.672046: step 18240, loss = 2.06 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:31.903546: step 18250, loss = 2.00 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:33.142072: step 18260, loss = 2.05 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:25:34.356613: step 18270, loss = 1.98 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:25:35.566438: step 18280, loss = 2.17 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:25:36.799755: step 18290, loss = 2.20 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:38.004800: step 18300, loss = 2.04 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:25:39.237147: step 18310, loss = 2.15 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:40.455487: step 18320, loss = 2.02 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:41.670138: step 18330, loss = 2.45 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:25:42.877794: step 18340, loss = 2.03 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:25:44.096058: step 18350, loss = 2.18 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:45.292512: step 18360, loss = 2.15 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:25:46.510841: step 18370, loss = 2.23 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:25:47.744766: step 18380, loss = 2.03 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:48.953165: step 18390, loss = 2.06 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:25:50.181021: step 18400, loss = 2.00 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:51.380741: step 18410, loss = 2.02 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:25:52.562369: step 18420, loss = 2.13 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:25:53.792669: step 18430, loss = 2.07 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:55.023828: step 18440, loss = 2.07 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:56.252004: step 18450, loss = 2.04 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:25:57.460667: step 18460, loss = 2.06 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:25:58.667600: step 18470, loss = 2.11 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:25:59.894336: step 18480, loss = 2.29 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:01.113874: step 18490, loss = 2.42 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:02.338275: step 18500, loss = 2.06 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:03.563363: step 18510, loss = 1.97 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:04.776753: step 18520, loss = 2.14 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:05.998320: step 18530, loss = 2.09 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:07.218058: step 18540, loss = 2.06 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:08.435983: step 18550, loss = 2.11 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:09.644618: step 18560, loss = 2.17 (105E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 196 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
9.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:10.867763: step 18570, loss = 1.96 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:12.079997: step 18580, loss = 1.99 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:13.291372: step 18590, loss = 2.19 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:14.508690: step 18600, loss = 2.14 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:15.726942: step 18610, loss = 2.34 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:16.924988: step 18620, loss = 2.26 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:26:18.127188: step 18630, loss = 2.25 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:26:19.327061: step 18640, loss = 2.20 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:26:20.544004: step 18650, loss = 2.14 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:21.715986: step 18660, loss = 2.23 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:26:22.903132: step 18670, loss = 2.03 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:26:24.098258: step 18680, loss = 2.18 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:26:25.284881: step 18690, loss = 2.09 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:26:26.464128: step 18700, loss = 2.12 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:26:27.673632: step 18710, loss = 2.06 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:28.877613: step 18720, loss = 2.03 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:26:30.073902: step 18730, loss = 2.10 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:26:31.267717: step 18740, loss = 2.09 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:26:32.490895: step 18750, loss = 2.05 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:33.712084: step 18760, loss = 2.19 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:34.916532: step 18770, loss = 2.05 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:26:36.131448: step 18780, loss = 2.08 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:37.354998: step 18790, loss = 2.09 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:38.585579: step 18800, loss = 1.98 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:39.900025: step 18810, loss = 2.02 (973.8 examples/sec; 0.131 sec/batch)
2017-05-04 22:26:41.013093: step 18820, loss = 2.12 (1150.0 examples/sec; 0.111 sec/batch)
2017-05-04 22:26:42.224896: step 18830, loss = 2.11 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:43.444993: step 18840, loss = 2.10 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:44.673362: step 18850, loss = 2.17 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:45.874114: step 18860, loss = 1.99 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:26:47.086449: step 18870, loss = 2.00 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:48.324075: step 18880, loss = 2.07 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:26:49.545288: step 18890, loss = 2.16 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:50.771268: step 18900, loss = 2.11 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:51.993911: step 18910, loss = 2.13 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:53.225321: step 18920, loss = 2.15 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:54.440464: step 18930, loss = 2.22 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:55.652960: step 18940, loss = 2.17 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:26:56.869583: step 18950, loss = 2.17 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:26:58.104391: step 18960, loss = 2.04 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:26:59.335645: step 18970, loss = 2.05 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:27:00.555944: step 18980, loss = 2.00 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:01.760120: step 18990, loss = 2.19 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:27:02.962988: step 19000, loss = 2.14 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:27:04.169662: step 19010, loss = 2.21 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:05.383854: step 19020, loss = 2.17 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:06.607900: step 19030, loss = 2.18 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:07.833547: step 19040, loss = 2.10 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:27:09.056133: step 19050, loss = 2.14 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:10.267400: step 19060, loss = 2.03 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:11.495296: step 19070, loss = 2.11 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:27:12.707376: step 19080, loss = 2.14 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:13.912746: step 19090, loss = 2.00 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:15.127631: step 19100, loss = 2.20 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:16.347099: step 19110, loss = 2.13 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:17.547262: step 19120, loss = 2.01 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:27:18.767894: step 19130, loss = 2.31 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:19.988364: step 19140, loss = 2.06 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:21.205845: step 19150, loss = 2.12 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:22.429876: step 19160, loss = 1.95 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:23.641058: step 19170, loss = 2.10 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:24.866632: step 19180, loss = 2.01 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:27:26.084521: step 19190, loss = 2.19 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:27.298774: step 19200, loss = 2.05 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:28.483471: step 19210, loss = 2.15 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:27:29.701898: step 19220, loss = 1.98 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:30.930289: step 19230, loss = 2.10 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:27:32.150628: step 19240, loss = 2.19 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:33.390855: step 19250, loss = 2.13 (1032.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:27:34.599388: step 19260, loss = 2.08 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:35.823836: step 19270, loss = 1.99 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:37.055094: step 19280, loss = 2.03 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:27:38.272742: step 19290, loss = 1.99 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:39.477907: step 19300, loss = 2.21 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:40.699334: step 19310, loss = 1.85 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:41.899344: step 19320, loss = 2.06 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:27:43.117873: step 19330, loss = 2.00 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:44.341725: step 19340, loss = 2.07 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:45.558718: step 19350, loss = 2.25 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:46.775601: step 19360, loss = 2.21 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:48.011280: step 19370, loss = 2.06 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:27:49.230446: step 19380, loss = 2.29 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:50.443864: step 19390, loss = 1.92 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:51.657005: step 19400, loss = 1.97 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:52.853038: step 19410, loss = 2.22 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:27:54.068211: step 19420, loss = 1.99 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:55.292866: step 19430, loss = 2.31 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:27:56.493752: step 19440, loss = 2.17 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:27:57.704493: step 19450, loss = 1.95 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:27:58.919621: step 19460, loss = 2.08 (1053.4E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 206 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:00.132721: step 19470, loss = 2.19 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:28:01.350807: step 19480, loss = 2.13 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:02.605516: step 19490, loss = 2.16 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-04 22:28:03.816403: step 19500, loss = 2.03 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:28:05.039682: step 19510, loss = 2.15 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:06.255461: step 19520, loss = 2.12 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:07.480384: step 19530, loss = 2.10 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:08.699954: step 19540, loss = 1.97 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:09.906381: step 19550, loss = 2.31 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:28:11.127834: step 19560, loss = 1.98 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:12.349832: step 19570, loss = 2.14 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:13.571587: step 19580, loss = 1.96 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:14.811562: step 19590, loss = 2.03 (1032.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:28:16.031308: step 19600, loss = 2.25 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:17.230927: step 19610, loss = 2.11 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:28:18.449184: step 19620, loss = 2.09 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:19.662285: step 19630, loss = 1.99 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:28:20.890914: step 19640, loss = 2.18 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:22.087566: step 19650, loss = 2.07 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:28:23.316825: step 19660, loss = 2.06 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:24.543810: step 19670, loss = 1.93 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:25.774650: step 19680, loss = 2.02 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:26.980208: step 19690, loss = 2.14 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:28:28.209083: step 19700, loss = 2.03 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:29.424366: step 19710, loss = 2.15 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:30.638315: step 19720, loss = 2.06 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:28:31.853799: step 19730, loss = 2.02 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:33.088341: step 19740, loss = 2.22 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:34.307491: step 19750, loss = 2.35 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:35.547575: step 19760, loss = 2.35 (1032.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:28:36.755000: step 19770, loss = 2.13 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:28:37.958625: step 19780, loss = 2.10 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:28:39.184595: step 19790, loss = 2.12 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:40.491421: step 19800, loss = 2.15 (979.5 examples/sec; 0.131 sec/batch)
2017-05-04 22:28:41.601262: step 19810, loss = 2.28 (1153.3 examples/sec; 0.111 sec/batch)
2017-05-04 22:28:42.819088: step 19820, loss = 2.09 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:44.027833: step 19830, loss = 2.19 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:28:45.244624: step 19840, loss = 2.21 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:46.461215: step 19850, loss = 1.98 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:47.680389: step 19860, loss = 2.05 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:48.931000: step 19870, loss = 2.06 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-04 22:28:50.145144: step 19880, loss = 1.94 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:28:51.377014: step 19890, loss = 2.20 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:28:52.613753: step 19900, loss = 2.18 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:28:53.801330: step 19910, loss = 2.13 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:28:55.023280: step 19920, loss = 2.28 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:56.238856: step 19930, loss = 2.20 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:57.463852: step 19940, loss = 1.96 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:58.681882: step 19950, loss = 2.15 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:28:59.909152: step 19960, loss = 2.07 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:29:01.149558: step 19970, loss = 2.19 (1031.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:29:02.367213: step 19980, loss = 1.97 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:03.585004: step 19990, loss = 2.32 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:04.765139: step 20000, loss = 2.11 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:29:05.983042: step 20010, loss = 2.06 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:07.224499: step 20020, loss = 2.01 (1031.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:29:08.452670: step 20030, loss = 2.00 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:29:09.675442: step 20040, loss = 2.07 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:10.887828: step 20050, loss = 2.24 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:12.118846: step 20060, loss = 2.03 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:29:13.354185: step 20070, loss = 2.42 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:29:14.584191: step 20080, loss = 2.05 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:29:15.803439: step 20090, loss = 2.26 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:17.024500: step 20100, loss = 2.07 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:18.232376: step 20110, loss = 2.31 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:19.446671: step 20120, loss = 2.18 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:20.678282: step 20130, loss = 2.31 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:29:21.872501: step 20140, loss = 2.24 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:29:23.103413: step 20150, loss = 2.18 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:29:24.319779: step 20160, loss = 2.19 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:25.526330: step 20170, loss = 2.32 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:26.750453: step 20180, loss = 2.15 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:27.965119: step 20190, loss = 2.19 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:29.152453: step 20200, loss = 2.17 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:29:30.374810: step 20210, loss = 1.94 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:31.595804: step 20220, loss = 2.13 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:32.830334: step 20230, loss = 2.11 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:29:34.068433: step 20240, loss = 2.05 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:29:35.284634: step 20250, loss = 2.06 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:36.486794: step 20260, loss = 1.99 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:29:37.714362: step 20270, loss = 2.08 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:29:38.949712: step 20280, loss = 2.25 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:29:40.174757: step 20290, loss = 2.01 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:29:41.391620: step 20300, loss = 2.23 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:42.595411: step 20310, loss = 2.00 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:29:43.827592: step 20320, loss = 2.09 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:29:45.053617: step 20330, loss = 2.17 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:29:46.265848: step 20340, loss = 1.94 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:47.503544: step 20350, loss = 2.16 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:29:48.721015: step 20360, loss = 2.24 (1051.4 exE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 216 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
amples/sec; 0.122 sec/batch)
2017-05-04 22:29:49.943498: step 20370, loss = 2.05 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:51.179359: step 20380, loss = 2.11 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:29:52.402862: step 20390, loss = 2.08 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:53.581275: step 20400, loss = 1.95 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:29:54.794670: step 20410, loss = 2.13 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:56.015463: step 20420, loss = 2.02 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:57.232526: step 20430, loss = 2.14 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:29:58.437856: step 20440, loss = 2.04 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:29:59.675616: step 20450, loss = 2.31 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:30:00.910448: step 20460, loss = 2.18 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:02.095484: step 20470, loss = 2.11 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:30:03.325811: step 20480, loss = 2.10 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:04.552187: step 20490, loss = 2.11 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:05.753766: step 20500, loss = 2.11 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:30:06.978606: step 20510, loss = 2.13 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:08.217817: step 20520, loss = 2.02 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:30:09.438954: step 20530, loss = 2.19 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:10.679935: step 20540, loss = 2.07 (1031.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:30:11.907009: step 20550, loss = 2.20 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:13.134764: step 20560, loss = 1.97 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:14.342150: step 20570, loss = 2.00 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:30:15.561270: step 20580, loss = 1.98 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:16.761615: step 20590, loss = 2.11 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:30:17.948692: step 20600, loss = 2.13 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:30:19.170894: step 20610, loss = 2.15 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:20.395522: step 20620, loss = 2.17 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:21.611262: step 20630, loss = 2.11 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:22.840995: step 20640, loss = 2.08 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:24.071267: step 20650, loss = 2.00 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:25.298049: step 20660, loss = 2.12 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:26.522472: step 20670, loss = 2.18 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:27.761086: step 20680, loss = 1.93 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:30:28.984526: step 20690, loss = 1.89 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:30.204173: step 20700, loss = 2.25 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:31.428302: step 20710, loss = 2.09 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:32.645750: step 20720, loss = 2.25 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:33.866023: step 20730, loss = 2.08 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:35.082879: step 20740, loss = 2.03 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:36.300024: step 20750, loss = 2.06 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:37.517875: step 20760, loss = 2.12 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:38.734873: step 20770, loss = 2.07 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:39.978632: step 20780, loss = 1.90 (1029.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:30:41.289689: step 20790, loss = 2.08 (976.3 examples/sec; 0.131 sec/batch)
2017-05-04 22:30:42.393629: step 20800, loss = 2.05 (1159.5 examples/sec; 0.110 sec/batch)
2017-05-04 22:30:43.620604: step 20810, loss = 2.24 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:44.845019: step 20820, loss = 2.02 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:30:46.057037: step 20830, loss = 2.18 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:30:47.292599: step 20840, loss = 2.10 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:30:48.526434: step 20850, loss = 2.20 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:49.721136: step 20860, loss = 2.20 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:30:50.951528: step 20870, loss = 2.01 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:52.157801: step 20880, loss = 2.15 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:30:53.367523: step 20890, loss = 2.11 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:30:54.569443: step 20900, loss = 1.96 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:30:55.794873: step 20910, loss = 2.33 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:30:57.008875: step 20920, loss = 2.15 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:30:58.223560: step 20930, loss = 2.07 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:30:59.445584: step 20940, loss = 2.06 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:00.664985: step 20950, loss = 2.01 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:01.869688: step 20960, loss = 1.99 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:31:03.080724: step 20970, loss = 2.14 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:31:04.291831: step 20980, loss = 2.11 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:31:05.492803: step 20990, loss = 1.95 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:31:06.684217: step 21000, loss = 2.15 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:31:07.871218: step 21010, loss = 1.99 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:31:09.076975: step 21020, loss = 2.10 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:31:10.254626: step 21030, loss = 2.13 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:31:11.449745: step 21040, loss = 2.20 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:31:12.670502: step 21050, loss = 2.09 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:13.843656: step 21060, loss = 2.17 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:31:15.021512: step 21070, loss = 2.05 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:31:16.195468: step 21080, loss = 2.12 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:31:17.372837: step 21090, loss = 2.05 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:31:18.560056: step 21100, loss = 2.07 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:31:19.731203: step 21110, loss = 2.20 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:31:20.927514: step 21120, loss = 2.29 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:31:22.106167: step 21130, loss = 2.02 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:31:23.286652: step 21140, loss = 2.03 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:31:24.470589: step 21150, loss = 2.04 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:31:25.639823: step 21160, loss = 2.08 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:31:26.846523: step 21170, loss = 2.01 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:31:28.044609: step 21180, loss = 2.10 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:31:29.273080: step 21190, loss = 2.17 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:31:30.485473: step 21200, loss = 2.22 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:31:31.710114: step 21210, loss = 2.09 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:32.934440: step 21220, loss = 2.12 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:34.156695: step 21230, loss = 2.15 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:35.389331: step 21240, loss = 2.17 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:31:36.618217: step 21250, loss = 2.17 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:31:37.822639: step 21260, loss = 2.20 (1062.7 exampE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 226 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
les/sec; 0.120 sec/batch)
2017-05-04 22:31:39.054739: step 21270, loss = 2.17 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:31:40.281806: step 21280, loss = 1.99 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:31:41.505865: step 21290, loss = 2.26 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:42.723467: step 21300, loss = 2.21 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:43.957647: step 21310, loss = 2.13 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:31:45.174988: step 21320, loss = 2.21 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:46.378862: step 21330, loss = 2.08 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:31:47.598758: step 21340, loss = 2.36 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:48.808476: step 21350, loss = 2.20 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:31:50.028583: step 21360, loss = 2.00 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:51.253374: step 21370, loss = 2.05 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:31:52.480330: step 21380, loss = 2.18 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:31:53.661147: step 21390, loss = 2.10 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:31:54.902102: step 21400, loss = 2.12 (1031.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:31:56.127321: step 21410, loss = 2.15 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:31:57.341048: step 21420, loss = 2.11 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:31:58.549813: step 21430, loss = 2.04 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:31:59.786365: step 21440, loss = 1.98 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:32:01.019847: step 21450, loss = 2.12 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:02.226189: step 21460, loss = 2.19 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:03.435818: step 21470, loss = 2.06 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:04.664290: step 21480, loss = 2.13 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:05.879522: step 21490, loss = 2.01 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:07.092632: step 21500, loss = 2.17 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:08.301160: step 21510, loss = 2.15 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:09.523334: step 21520, loss = 1.97 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:10.758514: step 21530, loss = 2.05 (1036.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:32:11.971868: step 21540, loss = 2.06 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:13.197333: step 21550, loss = 2.13 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:14.401494: step 21560, loss = 2.21 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:32:15.619446: step 21570, loss = 2.17 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:16.822678: step 21580, loss = 2.25 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:32:18.017699: step 21590, loss = 2.08 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:32:19.228803: step 21600, loss = 2.22 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:20.443115: step 21610, loss = 2.11 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:21.651297: step 21620, loss = 2.08 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:22.891744: step 21630, loss = 2.09 (1031.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:32:24.096123: step 21640, loss = 2.12 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:32:25.324364: step 21650, loss = 2.17 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:26.528650: step 21660, loss = 2.09 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:32:27.755697: step 21670, loss = 2.12 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:28.966713: step 21680, loss = 2.12 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:30.186187: step 21690, loss = 2.07 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:31.398091: step 21700, loss = 1.95 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:32.602017: step 21710, loss = 2.06 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:32:33.810432: step 21720, loss = 1.89 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:35.031854: step 21730, loss = 2.19 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:36.258647: step 21740, loss = 2.07 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:37.478589: step 21750, loss = 2.26 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:38.690424: step 21760, loss = 2.19 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:39.907461: step 21770, loss = 2.11 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:41.201068: step 21780, loss = 2.00 (989.5 examples/sec; 0.129 sec/batch)
2017-05-04 22:32:42.299657: step 21790, loss = 2.07 (1165.1 examples/sec; 0.110 sec/batch)
2017-05-04 22:32:43.531775: step 21800, loss = 2.00 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:44.753884: step 21810, loss = 1.90 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:45.968442: step 21820, loss = 2.19 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:47.202710: step 21830, loss = 2.14 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:48.432971: step 21840, loss = 2.29 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:49.656499: step 21850, loss = 2.27 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:32:50.868310: step 21860, loss = 2.13 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:52.078222: step 21870, loss = 2.17 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:53.305366: step 21880, loss = 1.88 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:54.512189: step 21890, loss = 1.95 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:32:55.739666: step 21900, loss = 1.98 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:56.968900: step 21910, loss = 2.09 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:58.194648: step 21920, loss = 2.09 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:32:59.399348: step 21930, loss = 2.21 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:33:00.624474: step 21940, loss = 2.07 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:33:01.831190: step 21950, loss = 2.12 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:03.057414: step 21960, loss = 2.07 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:33:04.295899: step 21970, loss = 2.17 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:33:05.486331: step 21980, loss = 2.01 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:33:06.705025: step 21990, loss = 2.05 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:07.927214: step 22000, loss = 2.31 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:09.153182: step 22010, loss = 2.10 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:33:10.364150: step 22020, loss = 2.12 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:11.570471: step 22030, loss = 2.15 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:12.785035: step 22040, loss = 1.94 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:13.987134: step 22050, loss = 2.24 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:33:15.205357: step 22060, loss = 2.19 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:16.435501: step 22070, loss = 2.04 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:33:17.636937: step 22080, loss = 2.07 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:33:18.873759: step 22090, loss = 2.16 (1034.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:33:20.094718: step 22100, loss = 2.10 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:21.314678: step 22110, loss = 2.11 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:22.520920: step 22120, loss = 2.05 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:23.749033: step 22130, loss = 2.06 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:33:24.956300: step 22140, loss = 2.15 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:26.172010: step 22150, loss = 2.12 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:27.386020: step 22160, loss = 2.08 (1054.3 examplesE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 236 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
/sec; 0.121 sec/batch)
2017-05-04 22:33:28.621170: step 22170, loss = 2.32 (1036.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:33:29.839272: step 22180, loss = 2.07 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:31.056143: step 22190, loss = 2.05 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:32.275301: step 22200, loss = 1.95 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:33.491723: step 22210, loss = 1.96 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:34.715322: step 22220, loss = 1.94 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:35.952304: step 22230, loss = 2.07 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:33:37.171929: step 22240, loss = 2.03 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:38.404884: step 22250, loss = 2.02 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:33:39.625163: step 22260, loss = 2.15 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:40.846789: step 22270, loss = 1.98 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:42.062481: step 22280, loss = 2.13 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:43.293820: step 22290, loss = 2.15 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:33:44.500902: step 22300, loss = 2.05 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:45.707450: step 22310, loss = 1.92 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:46.926902: step 22320, loss = 2.11 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:48.144639: step 22330, loss = 2.13 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:49.372783: step 22340, loss = 1.95 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:33:50.593253: step 22350, loss = 2.14 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:33:51.797650: step 22360, loss = 2.14 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:33:53.040865: step 22370, loss = 2.24 (1029.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:33:54.203890: step 22380, loss = 2.06 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:33:55.416219: step 22390, loss = 2.03 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:56.628029: step 22400, loss = 2.27 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:33:57.818471: step 22410, loss = 2.09 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:33:59.036920: step 22420, loss = 2.06 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:34:00.259634: step 22430, loss = 2.08 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:34:01.467057: step 22440, loss = 2.17 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:02.663630: step 22450, loss = 1.91 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:03.866188: step 22460, loss = 2.19 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:05.085053: step 22470, loss = 2.15 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:34:06.281949: step 22480, loss = 2.05 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:07.467922: step 22490, loss = 2.24 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:34:08.668154: step 22500, loss = 2.12 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:09.854623: step 22510, loss = 2.06 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:34:11.055799: step 22520, loss = 2.11 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:12.242701: step 22530, loss = 2.14 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:34:13.420712: step 22540, loss = 2.07 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:34:14.626695: step 22550, loss = 2.04 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:15.828617: step 22560, loss = 2.32 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:17.051279: step 22570, loss = 2.16 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:34:18.229026: step 22580, loss = 2.21 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:34:19.441678: step 22590, loss = 2.04 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:20.651719: step 22600, loss = 2.09 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:21.849932: step 22610, loss = 2.02 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:23.039048: step 22620, loss = 2.27 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:34:24.237734: step 22630, loss = 2.01 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:25.425070: step 22640, loss = 2.05 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:34:26.601064: step 22650, loss = 2.22 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:34:27.773494: step 22660, loss = 2.12 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:34:28.957317: step 22670, loss = 2.04 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:34:30.150993: step 22680, loss = 2.10 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:34:31.345143: step 22690, loss = 2.16 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:34:32.546377: step 22700, loss = 2.13 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:33.735704: step 22710, loss = 2.00 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:34:34.941769: step 22720, loss = 2.00 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:36.170916: step 22730, loss = 1.96 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:34:37.380592: step 22740, loss = 2.08 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:38.568710: step 22750, loss = 2.17 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:34:39.775358: step 22760, loss = 2.19 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:41.095647: step 22770, loss = 2.06 (969.5 examples/sec; 0.132 sec/batch)
2017-05-04 22:34:42.211110: step 22780, loss = 1.91 (1147.5 examples/sec; 0.112 sec/batch)
2017-05-04 22:34:43.427335: step 22790, loss = 2.11 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:34:44.635977: step 22800, loss = 2.23 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:45.843969: step 22810, loss = 2.04 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:47.041654: step 22820, loss = 2.02 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:48.253390: step 22830, loss = 2.03 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:34:49.454167: step 22840, loss = 2.18 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:50.670890: step 22850, loss = 2.06 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:34:51.863899: step 22860, loss = 1.87 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:34:53.091282: step 22870, loss = 2.11 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:34:54.295667: step 22880, loss = 2.08 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:34:55.486107: step 22890, loss = 1.99 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:34:56.676332: step 22900, loss = 1.99 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:34:57.851688: step 22910, loss = 2.11 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:34:59.022089: step 22920, loss = 2.06 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:35:00.222788: step 22930, loss = 2.33 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:01.412944: step 22940, loss = 2.15 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:35:02.608416: step 22950, loss = 2.06 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:03.814679: step 22960, loss = 2.21 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:35:04.998683: step 22970, loss = 2.17 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:35:06.181714: step 22980, loss = 2.14 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:35:07.368286: step 22990, loss = 2.27 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:35:08.555567: step 23000, loss = 2.08 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:35:09.725652: step 23010, loss = 2.09 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:35:10.926210: step 23020, loss = 1.87 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:12.113153: step 23030, loss = 1.95 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:35:13.312384: step 23040, loss = 2.06 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:14.503626: step 23050, loss = 1.96 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:35:15.716599: step 23060, loss = 2.05 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:35:16.907598: step 23070, loss = 2.16 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:35:18.100973: step 23080, loss = 2.06 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:35:19.301586: step 23090, loss = 2.09 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:20.504471: step 23100, loss = 2.22 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:21.708884: step 23110, loss = 1.91 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:22.931940: step 23120, loss = 1.97 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:24.153872: step 23130, loss = 2.06 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:25.384195: step 23140, loss = 2.27 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:35:26.599417: step 23150, loss = 2.08 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:27.817653: step 23160, loss = 2.11 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:29.021171: step 23170, loss = 1.93 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:30.241037: step 23180, loss = 2.09 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:31.478187: step 23190, loss = 1.96 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:35:32.706312: step 23200, loss = 2.09 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:35:33.909825: step 23210, loss = 2.11 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:35.144483: step 23220, loss = 2.28 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:35:36.365996: step 23230, loss = 1.94 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:37.584799: step 23240, loss = 2.05 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:38.802976: step 23250, loss = 2.23 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:40.024123: step 23260, loss = 1.95 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:41.244009: step 23270, loss = 2.13 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:42.448938: step 23280, loss = 2.01 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:43.668047: step 23290, loss = 2.16 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:44.874449: step 23300, loss = 2.20 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:35:46.092720: step 23310, loss = 2.07 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:47.303794: step 23320, loss = 1.98 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:35:48.535592: step 23330, loss = 2.16 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:35:49.751894: step 23340, loss = 1.93 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:50.963302: step 23350, loss = 2.10 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:35:52.187349: step 23360, loss = 2.12 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:53.383104: step 23370, loss = 2.08 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:35:54.599496: step 23380, loss = 2.14 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:55.821307: step 23390, loss = 1.97 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:35:57.050652: step 23400, loss = 2.14 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:35:58.259505: step 23410, loss = 2.07 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:35:59.473619: step 23420, loss = 1.95 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:36:00.685573: step 23430, loss = 2.07 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:36:01.898804: step 23440, loss = 2.10 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:36:03.123607: step 23450, loss = 2.11 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:04.342028: step 23460, loss = 2.12 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:05.541966: step 23470, loss = 2.15 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:36:06.759173: step 23480, loss = 2.01 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:07.991308: step 23490, loss = 2.12 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:36:09.209252: step 23500, loss = 1.94 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:10.427489: step 23510, loss = 2.06 (1050.7 examples/secE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 247 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
; 0.122 sec/batch)
2017-05-04 22:36:11.664819: step 23520, loss = 2.00 (1034.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:36:12.884294: step 23530, loss = 2.13 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:14.081919: step 23540, loss = 2.00 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:36:15.300539: step 23550, loss = 2.06 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:16.514509: step 23560, loss = 1.97 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:36:17.683742: step 23570, loss = 1.98 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:36:18.918158: step 23580, loss = 2.17 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:36:20.144945: step 23590, loss = 2.10 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:36:21.357746: step 23600, loss = 2.07 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:36:22.568602: step 23610, loss = 2.12 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:36:23.792009: step 23620, loss = 2.07 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:25.022041: step 23630, loss = 2.15 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:36:26.236054: step 23640, loss = 1.89 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:36:27.472625: step 23650, loss = 2.09 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:36:28.686116: step 23660, loss = 2.16 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:36:29.880970: step 23670, loss = 2.14 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:36:31.092937: step 23680, loss = 1.92 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:36:32.313415: step 23690, loss = 2.14 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:33.513386: step 23700, loss = 2.10 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:36:34.732725: step 23710, loss = 2.21 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:35.957157: step 23720, loss = 2.05 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:36:37.192694: step 23730, loss = 2.21 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:36:38.388985: step 23740, loss = 2.08 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:36:39.599656: step 23750, loss = 2.11 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:36:40.919185: step 23760, loss = 1.97 (970.0 examples/sec; 0.132 sec/batch)
2017-05-04 22:36:42.006503: step 23770, loss = 2.14 (1177.2 examples/sec; 0.109 sec/batch)
2017-05-04 22:36:43.233066: step 23780, loss = 2.13 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:36:44.441472: step 23790, loss = 2.17 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:36:45.635087: step 23800, loss = 2.06 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:36:46.827491: step 23810, loss = 2.08 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:36:48.011133: step 23820, loss = 1.99 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:36:49.206355: step 23830, loss = 1.90 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:36:50.360537: step 23840, loss = 2.16 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:36:51.513711: step 23850, loss = 2.29 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:36:52.672715: step 23860, loss = 2.16 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:36:53.822975: step 23870, loss = 2.08 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-04 22:36:54.982939: step 23880, loss = 2.00 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:36:56.165793: step 23890, loss = 2.23 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:36:57.339775: step 23900, loss = 2.30 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:36:58.497490: step 23910, loss = 2.01 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:36:59.675118: step 23920, loss = 2.07 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:00.860698: step 23930, loss = 2.00 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:37:02.015099: step 23940, loss = 2.23 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-04 22:37:03.188114: step 23950, loss = 2.17 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:04.366584: step 23960, loss = 2.09 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:05.521349: step 23970, loss = 2.08 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-04 22:37:06.690681: step 23980, loss = 2.02 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:07.869997: step 23990, loss = 2.02 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:09.047424: step 24000, loss = 2.04 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:10.211860: step 24010, loss = 2.11 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:37:11.377221: step 24020, loss = 2.16 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:12.554686: step 24030, loss = 1.95 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:13.708324: step 24040, loss = 1.96 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-04 22:37:14.895362: step 24050, loss = 2.15 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:37:16.051979: step 24060, loss = 2.04 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:37:17.235150: step 24070, loss = 2.13 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:18.388327: step 24080, loss = 2.13 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:37:19.551939: step 24090, loss = 2.22 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:37:20.724624: step 24100, loss = 2.14 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:21.892696: step 24110, loss = 2.07 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:23.064305: step 24120, loss = 2.03 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:24.229816: step 24130, loss = 2.15 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:25.416001: step 24140, loss = 2.05 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:37:26.581941: step 24150, loss = 2.04 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:27.765363: step 24160, loss = 2.01 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:28.940387: step 24170, loss = 2.26 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:30.091812: step 24180, loss = 1.92 (1111.7 examples/sec; 0.115 sec/batch)
2017-05-04 22:37:31.262367: step 24190, loss = 2.25 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:32.425357: step 24200, loss = 2.04 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:37:33.568413: step 24210, loss = 1.92 (1119.8 examples/sec; 0.114 sec/batch)
2017-05-04 22:37:34.763788: step 24220, loss = 2.07 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:37:35.912099: step 24230, loss = 2.29 (1114.7 examples/sec; 0.115 sec/batch)
2017-05-04 22:37:37.098659: step 24240, loss = 2.10 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:37:38.257931: step 24250, loss = 1.97 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:37:39.410440: step 24260, loss = 2.09 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-04 22:37:40.581692: step 24270, loss = 2.22 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:41.732557: step 24280, loss = 2.10 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-04 22:37:42.922792: step 24290, loss = 2.26 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:37:44.119805: step 24300, loss = 2.03 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:37:45.305847: step 24310, loss = 2.18 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:37:46.497640: step 24320, loss = 2.05 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:37:47.707198: step 24330, loss = 2.18 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:37:48.917540: step 24340, loss = 1.95 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:37:50.098219: step 24350, loss = 2.20 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:51.290149: step 24360, loss = 1.93 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:37:52.486751: step 24370, loss = 2.33 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:37:53.677262: step 24380, loss = 1.91 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:37:54.850214: step 24390, loss = 2.05 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:37:56.026734: step 24400, loss = 2.10 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:37:57.209659: step 24410, loss = 2.22 (1082.1 examples/sec; 0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 258 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
.118 sec/batch)
2017-05-04 22:37:58.369416: step 24420, loss = 1.99 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:37:59.526331: step 24430, loss = 2.13 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:38:00.692875: step 24440, loss = 1.90 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:01.831696: step 24450, loss = 2.31 (1124.0 examples/sec; 0.114 sec/batch)
2017-05-04 22:38:03.006624: step 24460, loss = 1.95 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:04.173802: step 24470, loss = 1.98 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:05.327767: step 24480, loss = 2.02 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-04 22:38:06.495051: step 24490, loss = 2.34 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:07.661073: step 24500, loss = 2.16 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:08.823846: step 24510, loss = 2.12 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:38:09.984340: step 24520, loss = 2.20 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:38:11.171414: step 24530, loss = 2.13 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:12.349104: step 24540, loss = 1.93 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:13.519500: step 24550, loss = 2.18 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:14.721587: step 24560, loss = 1.94 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:38:15.896301: step 24570, loss = 1.97 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:17.086411: step 24580, loss = 2.06 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:18.257991: step 24590, loss = 2.17 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:19.444743: step 24600, loss = 2.07 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:20.617931: step 24610, loss = 2.31 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:21.805417: step 24620, loss = 2.02 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:22.998130: step 24630, loss = 1.95 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:24.191252: step 24640, loss = 2.05 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:25.362547: step 24650, loss = 2.15 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:26.532385: step 24660, loss = 2.08 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:27.721594: step 24670, loss = 2.03 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:28.898003: step 24680, loss = 2.02 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:30.055163: step 24690, loss = 2.02 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:38:31.224055: step 24700, loss = 2.02 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:32.381950: step 24710, loss = 2.15 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:38:33.555087: step 24720, loss = 2.13 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:34.776569: step 24730, loss = 2.12 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:38:35.929354: step 24740, loss = 2.03 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-04 22:38:37.211552: step 24750, loss = 2.18 (998.3 examples/sec; 0.128 sec/batch)
2017-05-04 22:38:38.277754: step 24760, loss = 1.96 (1200.5 examples/sec; 0.107 sec/batch)
2017-05-04 22:38:39.458022: step 24770, loss = 2.19 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:40.635321: step 24780, loss = 2.10 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:41.794260: step 24790, loss = 2.03 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:38:42.996758: step 24800, loss = 2.07 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:38:44.167148: step 24810, loss = 2.00 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:45.333784: step 24820, loss = 2.25 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:46.520691: step 24830, loss = 2.08 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:47.693478: step 24840, loss = 2.12 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:48.871111: step 24850, loss = 2.23 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:50.041742: step 24860, loss = 2.16 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:38:51.229288: step 24870, loss = 2.03 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:52.410841: step 24880, loss = 2.09 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:53.562796: step 24890, loss = 2.07 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-04 22:38:54.760524: step 24900, loss = 1.99 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:38:55.974939: step 24910, loss = 2.02 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:38:57.166419: step 24920, loss = 2.18 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:38:58.351263: step 24930, loss = 2.12 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:38:59.548380: step 24940, loss = 2.25 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:00.834588: step 24950, loss = 1.96 (995.2 examples/sec; 0.129 sec/batch)
2017-05-04 22:39:01.944869: step 24960, loss = 2.16 (1152.9 examples/sec; 0.111 sec/batch)
2017-05-04 22:39:03.160038: step 24970, loss = 2.11 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:39:04.376012: step 24980, loss = 2.14 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:39:05.569750: step 24990, loss = 2.19 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:39:06.750050: step 25000, loss = 1.97 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:39:07.945447: step 25010, loss = 1.96 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:09.143526: step 25020, loss = 2.07 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:10.346078: step 25030, loss = 2.07 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:11.535610: step 25040, loss = 2.12 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:39:12.731696: step 25050, loss = 2.01 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:13.922751: step 25060, loss = 1.95 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:39:15.116354: step 25070, loss = 2.08 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:39:16.301325: step 25080, loss = 1.89 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:39:17.470590: step 25090, loss = 1.98 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:39:18.660254: step 25100, loss = 1.87 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:39:19.856806: step 25110, loss = 2.01 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:21.062961: step 25120, loss = 2.18 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:22.265263: step 25130, loss = 2.10 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:23.477773: step 25140, loss = 2.05 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:24.673256: step 25150, loss = 2.15 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:25.874771: step 25160, loss = 2.08 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:27.090050: step 25170, loss = 2.02 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:39:28.297590: step 25180, loss = 2.23 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:29.504308: step 25190, loss = 2.04 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:30.691552: step 25200, loss = 2.21 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:39:31.868823: step 25210, loss = 2.04 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:39:33.069503: step 25220, loss = 2.19 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:34.267729: step 25230, loss = 2.13 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:35.473224: step 25240, loss = 2.01 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:36.670185: step 25250, loss = 2.01 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:37.869980: step 25260, loss = 1.95 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:39.084255: step 25270, loss = 1.95 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:40.311704: step 25280, loss = 2.10 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:39:41.511805: step 25290, loss = 2.21 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:42.705219: step 25300, loss = 2.28 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:39:43.938601: step 25310, loss = 2.09 (1037.8 examples/sec; 0.123E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 269 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
 sec/batch)
2017-05-04 22:39:45.149736: step 25320, loss = 1.99 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:46.347319: step 25330, loss = 1.87 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:47.594723: step 25340, loss = 2.33 (1026.1 examples/sec; 0.125 sec/batch)
2017-05-04 22:39:48.809546: step 25350, loss = 2.12 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:50.002561: step 25360, loss = 2.07 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:39:51.246005: step 25370, loss = 1.98 (1029.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:39:52.444715: step 25380, loss = 2.23 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:39:53.667074: step 25390, loss = 1.97 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:39:54.903776: step 25400, loss = 2.16 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-04 22:39:56.116932: step 25410, loss = 2.15 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:57.358348: step 25420, loss = 2.04 (1031.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:39:58.572818: step 25430, loss = 2.03 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:39:59.789610: step 25440, loss = 2.02 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:01.014059: step 25450, loss = 1.98 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:02.215441: step 25460, loss = 2.35 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:40:03.447696: step 25470, loss = 2.05 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:40:04.645548: step 25480, loss = 1.98 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:40:05.845724: step 25490, loss = 1.94 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:40:07.089553: step 25500, loss = 2.19 (1029.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:40:08.305123: step 25510, loss = 2.09 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:09.515469: step 25520, loss = 2.11 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:10.732392: step 25530, loss = 2.26 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:11.939218: step 25540, loss = 2.00 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:13.144901: step 25550, loss = 2.25 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:14.353211: step 25560, loss = 2.08 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:15.577410: step 25570, loss = 2.07 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:16.778492: step 25580, loss = 2.14 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:40:17.958019: step 25590, loss = 1.92 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:40:19.149789: step 25600, loss = 1.99 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:40:20.352497: step 25610, loss = 2.11 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:40:21.559048: step 25620, loss = 2.02 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:22.787317: step 25630, loss = 2.24 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:40:24.015042: step 25640, loss = 2.03 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:40:25.252018: step 25650, loss = 2.06 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:40:26.424376: step 25660, loss = 1.92 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:40:27.660223: step 25670, loss = 2.12 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:40:28.904622: step 25680, loss = 2.09 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:40:30.103843: step 25690, loss = 2.10 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:40:31.307743: step 25700, loss = 2.09 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:40:32.545536: step 25710, loss = 2.00 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:40:33.739499: step 25720, loss = 2.12 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:40:34.963735: step 25730, loss = 1.94 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:36.328440: step 25740, loss = 2.29 (937.9 examples/sec; 0.136 sec/batch)
2017-05-04 22:40:37.393384: step 25750, loss = 2.07 (1201.9 examples/sec; 0.106 sec/batch)
2017-05-04 22:40:38.596732: step 25760, loss = 2.00 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:40:39.823112: step 25770, loss = 2.21 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:40:41.037508: step 25780, loss = 2.14 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:42.258373: step 25790, loss = 2.09 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:43.453068: step 25800, loss = 1.95 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:40:44.707829: step 25810, loss = 1.94 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-04 22:40:45.881080: step 25820, loss = 2.28 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:40:47.121056: step 25830, loss = 2.08 (1032.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:40:48.316291: step 25840, loss = 2.20 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:40:49.526446: step 25850, loss = 1.98 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:50.730962: step 25860, loss = 2.03 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:40:51.970506: step 25870, loss = 2.04 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:40:53.168334: step 25880, loss = 2.04 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:40:54.376837: step 25890, loss = 2.22 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:55.591593: step 25900, loss = 1.95 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:56.813366: step 25910, loss = 2.05 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:40:58.025470: step 25920, loss = 2.03 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:40:59.252848: step 25930, loss = 2.01 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:41:00.591949: step 25940, loss = 2.19 (955.9 examples/sec; 0.134 sec/batch)
2017-05-04 22:41:01.664018: step 25950, loss = 1.91 (1194.0 examples/sec; 0.107 sec/batch)
2017-05-04 22:41:02.867080: step 25960, loss = 2.12 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:41:04.113972: step 25970, loss = 2.03 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-04 22:41:05.305835: step 25980, loss = 2.02 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:41:06.493922: step 25990, loss = 2.09 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:41:07.697093: step 26000, loss = 1.88 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:41:08.874186: step 26010, loss = 2.08 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:10.056176: step 26020, loss = 2.07 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:11.242949: step 26030, loss = 2.25 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:41:12.447202: step 26040, loss = 2.06 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:41:13.631321: step 26050, loss = 2.03 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:14.808974: step 26060, loss = 2.16 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:15.992051: step 26070, loss = 2.00 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:17.163543: step 26080, loss = 2.23 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:18.342374: step 26090, loss = 2.09 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:19.510374: step 26100, loss = 2.06 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:20.689994: step 26110, loss = 2.13 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:21.830139: step 26120, loss = 2.03 (1122.7 examples/sec; 0.114 sec/batch)
2017-05-04 22:41:23.003369: step 26130, loss = 2.15 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:24.212399: step 26140, loss = 2.10 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:41:25.383349: step 26150, loss = 2.12 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:26.564548: step 26160, loss = 1.98 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:27.729179: step 26170, loss = 2.07 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:28.914048: step 26180, loss = 2.02 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:30.073785: step 26190, loss = 2.01 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:31.232154: step 26200, loss = 2.20 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:32.402056: step 26210, loss = 2.07 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:33.553446: step 26220, loss = 2.10 (1111.7 examples/sec; 0.115 sec/batch)
2017-05-04 22:41:34.712193: step 26230, loss = 2.14 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:35.869410: step 26240, loss = 2.07 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:37.042827: step 26250, loss = 2.07 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:38.213629: step 26260, loss = 2.10 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:39.382630: step 26270, loss = 2.03 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:40.538454: step 26280, loss = 2.10 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:41.717369: step 26290, loss = 2.07 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:42.871225: step 26300, loss = 2.03 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-04 22:41:44.055949: step 26310, loss = 2.07 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:45.226693: step 26320, loss = 2.04 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:46.371657: step 26330, loss = 2.17 (1117.9 examples/sec; 0.114 sec/batch)
2017-05-04 22:41:47.535975: step 26340, loss = 1.92 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:41:48.704713: step 26350, loss = 2.07 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:49.896349: step 26360, loss = 2.03 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:41:51.105105: step 26370, loss = 2.04 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:41:52.298168: step 26380, loss = 2.13 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:41:53.470195: step 26390, loss = 2.03 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:54.640598: step 26400, loss = 1.98 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:55.808017: step 26410, loss = 2.14 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:56.974676: step 26420, loss = 2.11 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:41:58.151736: step 26430, loss = 2.14 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:41:59.327748: step 26440, loss = 2.04 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:00.505270: step 26450, loss = 2.09 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:01.663424: step 26460, loss = 2.27 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:42:02.843341: step 26470, loss = 1.95 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:04.013839: step 26480, loss = 2.08 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:42:05.166966: step 26490, loss = 2.16 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:42:06.313300: step 26500, loss = 2.02 (1116.6 examples/sec; 0.115 sec/batch)
2017-05-04 22:42:07.488298: step 26510, loss = 2.14 (1089.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:08.670924: step 26520, loss = 2.11 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:09.833456: step 26530, loss = 1.97 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:42:11.012692: step 26540, loss = 2.04 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:12.188490: step 26550, loss = 1.96 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:13.361426: step 26560, loss = 1.92 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:42:14.506181: step 26570, loss = 2.12 (1118.1 examples/sec; 0.114 sec/batch)
2017-05-04 22:42:15.669079: step 26580, loss = 2.08 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:42:16.840885: step 26590, loss = 2.11 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:42:17.998069: step 26600, loss = 2.00 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:42:19.169577: step 26610, loss = 1.85 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:42:20.334925: step 26620, loss = 1.96 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:42:21.487822: step 26630, loss = 2.10 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-04 22:42:22.634876: step 26640, loss = 1.99 (1115.9 examples/sec; 0.115 sec/batch)
2017-05-04 22:42:23.792461: step 26650, loss = 2.02 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:42:24.961140: step 26660, loss = 1.96 (1095.3 examples/sec; 0.117 sec/E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 280 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
batch)
2017-05-04 22:42:26.113292: step 26670, loss = 2.24 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:42:27.300547: step 26680, loss = 1.91 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:42:28.482662: step 26690, loss = 2.02 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:42:29.655428: step 26700, loss = 1.98 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:42:30.848357: step 26710, loss = 2.00 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:42:32.011898: step 26720, loss = 2.15 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:42:33.304839: step 26730, loss = 2.08 (990.0 examples/sec; 0.129 sec/batch)
2017-05-04 22:42:34.376114: step 26740, loss = 1.94 (1194.8 examples/sec; 0.107 sec/batch)
2017-05-04 22:42:35.604878: step 26750, loss = 2.08 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:42:36.829990: step 26760, loss = 2.08 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:42:38.044874: step 26770, loss = 2.24 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:42:39.252686: step 26780, loss = 2.05 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:42:40.479526: step 26790, loss = 2.19 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:42:41.689121: step 26800, loss = 2.16 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:42:42.904696: step 26810, loss = 2.04 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:42:44.126530: step 26820, loss = 2.14 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:42:45.331545: step 26830, loss = 2.17 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:42:46.521278: step 26840, loss = 2.14 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:42:47.720612: step 26850, loss = 2.02 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:42:48.931533: step 26860, loss = 1.93 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:42:50.125635: step 26870, loss = 2.03 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:42:51.329563: step 26880, loss = 2.11 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:42:52.525193: step 26890, loss = 2.12 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:42:53.699737: step 26900, loss = 1.99 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:42:54.892292: step 26910, loss = 2.20 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:42:56.080928: step 26920, loss = 2.15 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:42:57.316381: step 26930, loss = 2.03 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:42:58.422970: step 26940, loss = 2.25 (1156.7 examples/sec; 0.111 sec/batch)
2017-05-04 22:42:59.603809: step 26950, loss = 2.31 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:00.780179: step 26960, loss = 2.23 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:01.963299: step 26970, loss = 2.03 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:03.179879: step 26980, loss = 1.95 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:43:04.378528: step 26990, loss = 2.01 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:43:05.565484: step 27000, loss = 2.02 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:43:06.761150: step 27010, loss = 2.14 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:43:07.947612: step 27020, loss = 2.17 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:43:09.131601: step 27030, loss = 2.07 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:10.304204: step 27040, loss = 2.00 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:11.501200: step 27050, loss = 2.15 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:43:12.692748: step 27060, loss = 2.13 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:43:13.865433: step 27070, loss = 2.17 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:15.040386: step 27080, loss = 1.90 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:16.232141: step 27090, loss = 2.05 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:43:17.405517: step 27100, loss = 2.04 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:18.572297: step 27110, loss = 2.11 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:19.747047: step 27120, loss = 2.07 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:20.930258: step 27130, loss = 2.04 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:22.100589: step 27140, loss = 2.03 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:23.285070: step 27150, loss = 1.93 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:24.458737: step 27160, loss = 2.08 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:25.651888: step 27170, loss = 2.11 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:43:26.821014: step 27180, loss = 1.98 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:27.986859: step 27190, loss = 2.11 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:29.156448: step 27200, loss = 2.16 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:30.317357: step 27210, loss = 2.08 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:43:31.474198: step 27220, loss = 1.98 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:43:32.648446: step 27230, loss = 2.38 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:33.808649: step 27240, loss = 1.99 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:43:34.985544: step 27250, loss = 2.19 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:36.138760: step 27260, loss = 2.03 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-04 22:43:37.295629: step 27270, loss = 1.94 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:43:38.449511: step 27280, loss = 2.09 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-04 22:43:39.626612: step 27290, loss = 1.90 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:40.801145: step 27300, loss = 2.20 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:41.954758: step 27310, loss = 1.98 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-04 22:43:43.122219: step 27320, loss = 1.97 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:44.286362: step 27330, loss = 2.16 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:43:45.450468: step 27340, loss = 2.04 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:43:46.607480: step 27350, loss = 2.06 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:43:47.771178: step 27360, loss = 2.06 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:43:48.957364: step 27370, loss = 2.07 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:43:50.115077: step 27380, loss = 2.02 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:43:51.279187: step 27390, loss = 2.16 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:43:52.462349: step 27400, loss = 2.00 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:53.631332: step 27410, loss = 2.06 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:43:54.812244: step 27420, loss = 2.06 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:55.995354: step 27430, loss = 2.15 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:57.175487: step 27440, loss = 2.06 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:58.352045: step 27450, loss = 2.21 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:43:59.546458: step 27460, loss = 2.14 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:44:00.733985: step 27470, loss = 2.09 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:44:01.892749: step 27480, loss = 2.15 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:03.074544: step 27490, loss = 2.09 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:04.232417: step 27500, loss = 2.10 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:05.395695: step 27510, loss = 2.03 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:06.564998: step 27520, loss = 2.21 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:07.729016: step 27530, loss = 2.17 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:08.909788: step 27540, loss = 2.07 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:10.060293: step 27550, loss = 2.00 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-04 22:44:11.220963: step 27560, loss = 1.95 (1102.8 examples/sec; 0.116 sec/batE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 291 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
ch)
2017-05-04 22:44:12.389711: step 27570, loss = 2.08 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:13.551137: step 27580, loss = 2.03 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:14.717861: step 27590, loss = 2.02 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:15.888764: step 27600, loss = 2.18 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:17.057679: step 27610, loss = 2.07 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:18.220427: step 27620, loss = 2.05 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:19.403607: step 27630, loss = 2.14 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:20.574571: step 27640, loss = 1.98 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:21.728650: step 27650, loss = 1.86 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-04 22:44:22.894799: step 27660, loss = 2.06 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:24.068993: step 27670, loss = 2.09 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:25.257396: step 27680, loss = 2.02 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:44:26.432961: step 27690, loss = 2.12 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:27.580253: step 27700, loss = 2.23 (1115.7 examples/sec; 0.115 sec/batch)
2017-05-04 22:44:28.757419: step 27710, loss = 2.07 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:30.012259: step 27720, loss = 1.97 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-04 22:44:31.100194: step 27730, loss = 2.01 (1176.5 examples/sec; 0.109 sec/batch)
2017-05-04 22:44:32.267034: step 27740, loss = 1.97 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:33.432636: step 27750, loss = 1.79 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:34.596548: step 27760, loss = 2.04 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:35.770387: step 27770, loss = 2.15 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:36.954777: step 27780, loss = 2.00 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:38.115909: step 27790, loss = 2.21 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:39.304875: step 27800, loss = 1.96 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:44:40.489047: step 27810, loss = 2.08 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:41.681828: step 27820, loss = 1.97 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:44:42.901200: step 27830, loss = 2.20 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:44:44.123073: step 27840, loss = 1.93 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:44:45.311368: step 27850, loss = 2.07 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:44:46.507166: step 27860, loss = 2.25 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:44:47.699142: step 27870, loss = 2.14 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:44:48.890021: step 27880, loss = 2.06 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:44:50.068088: step 27890, loss = 2.05 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:51.251544: step 27900, loss = 1.94 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:52.408854: step 27910, loss = 2.14 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:53.562642: step 27920, loss = 2.11 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-04 22:44:54.731943: step 27930, loss = 2.09 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:55.900001: step 27940, loss = 1.98 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:44:57.081298: step 27950, loss = 2.05 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:44:58.237917: step 27960, loss = 1.94 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:44:59.411010: step 27970, loss = 1.98 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:00.573762: step 27980, loss = 2.14 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:01.770905: step 27990, loss = 1.87 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:45:02.952258: step 28000, loss = 2.05 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:04.120976: step 28010, loss = 2.13 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:05.294940: step 28020, loss = 2.04 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:06.471696: step 28030, loss = 2.14 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:07.634002: step 28040, loss = 2.06 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:08.803499: step 28050, loss = 2.12 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:09.967591: step 28060, loss = 1.90 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:11.136001: step 28070, loss = 2.05 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:12.296172: step 28080, loss = 2.00 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:13.460453: step 28090, loss = 2.08 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:14.632184: step 28100, loss = 2.06 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:15.800853: step 28110, loss = 1.98 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:16.970186: step 28120, loss = 2.22 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:18.151866: step 28130, loss = 2.13 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:19.336492: step 28140, loss = 2.01 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:20.523452: step 28150, loss = 2.04 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:45:21.715780: step 28160, loss = 2.05 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:45:22.909491: step 28170, loss = 2.12 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:45:24.090488: step 28180, loss = 2.13 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:25.260660: step 28190, loss = 2.00 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:26.418933: step 28200, loss = 2.14 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:27.605156: step 28210, loss = 2.07 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:45:28.793680: step 28220, loss = 2.17 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:45:29.951641: step 28230, loss = 2.02 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:31.112167: step 28240, loss = 2.00 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:32.270563: step 28250, loss = 2.01 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:33.434168: step 28260, loss = 1.94 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:34.598795: step 28270, loss = 2.03 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:35.765887: step 28280, loss = 2.06 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:36.923304: step 28290, loss = 1.96 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:38.080599: step 28300, loss = 2.13 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:39.259679: step 28310, loss = 2.20 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:40.433323: step 28320, loss = 2.02 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:41.600981: step 28330, loss = 2.17 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:42.774563: step 28340, loss = 2.11 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:43.948344: step 28350, loss = 2.13 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:45.114207: step 28360, loss = 1.96 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:46.270949: step 28370, loss = 2.03 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:47.442810: step 28380, loss = 2.05 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:48.609287: step 28390, loss = 2.06 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:49.766293: step 28400, loss = 2.12 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:50.944644: step 28410, loss = 2.20 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:52.100081: step 28420, loss = 2.06 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:53.270510: step 28430, loss = 2.02 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:54.444907: step 28440, loss = 2.05 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:45:55.584767: step 28450, loss = 2.06 (1122.9 examples/sec; 0.114 sec/batch)
2017-05-04 22:45:56.745325: step 28460, loss = 2.09 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:45:57.926730: step 28470, loss = 2.13 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:45:59.105855: step 28480, loss = 1.99 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:00.298739: step 28490, loss = 2.09 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:46:01.469076: step 28500, loss = 2.06 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:02.635833: step 28510, loss = 2.09 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:03.808699: step 28520, loss = 2.03 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:04.993668: step 28530, loss = 2.05 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:06.136164: step 28540, loss = 2.15 (1120.4 examples/sec; 0.114 sec/batch)
2017-05-04 22:46:07.299935: step 28550, loss = 2.06 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:08.471511: step 28560, loss = 2.07 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:09.640434: step 28570, loss = 1.98 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:10.803495: step 28580, loss = 2.08 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:11.975605: step 28590, loss = 1.80 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:13.139808: step 28600, loss = 2.09 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:14.303575: step 28610, loss = 2.00 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:15.474994: step 28620, loss = 1.96 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:16.648022: step 28630, loss = 2.25 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:17.814147: step 28640, loss = 1.88 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:19.002391: step 28650, loss = 2.17 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:46:20.173912: step 28660, loss = 1.99 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:21.355036: step 28670, loss = 2.09 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:22.500405: step 28680, loss = 2.00 (1117.5 examples/sec; 0.115 sec/batch)
2017-05-04 22:46:23.676192: step 28690, loss = 2.13 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:24.837682: step 28700, loss = 2.07 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:26.096528: step 28710, loss = 2.05 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-04 22:46:27.145537: step 28720, loss = 2.02 (1220.2 examples/sec; 0.105 sec/batch)
2017-05-04 22:46:28.313502: step 28730, loss = 2.22 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:29.470503: step 28740, loss = 2.09 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:30.638090: step 28750, loss = 2.09 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:31.818987: step 28760, loss = 2.07 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:32.988302: step 28770, loss = 2.00 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:34.146957: step 28780, loss = 1.95 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:35.345080: step 28790, loss = 2.09 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:46:36.545731: step 28800, loss = 2.13 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:46:37.713837: step 28810, loss = 2.04 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:38.907301: step 28820, loss = 2.08 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:46:40.101502: step 28830, loss = 2.04 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:46:41.292737: step 28840, loss = 2.15 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:46:42.482991: step 28850, loss = 1.89 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:46:43.664261: step 28860, loss = 2.11 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:44.833489: step 28870, loss = 2.11 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:46.002526: step 28880, loss = 2.27 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:47.175120: step 28890, loss = 1.99 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:48.351815: step 28900, loss = 2.15 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:49.514007: step 28910, loss = 1.90 (1101.4 examples/sec; 0.116 sec/batch)E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 302 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU

2017-05-04 22:46:50.671112: step 28920, loss = 2.04 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:51.848071: step 28930, loss = 2.20 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:46:53.018660: step 28940, loss = 2.10 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:46:54.181717: step 28950, loss = 1.99 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:55.378829: step 28960, loss = 2.04 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:46:56.569949: step 28970, loss = 2.01 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:46:57.729103: step 28980, loss = 2.07 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:46:58.911922: step 28990, loss = 2.01 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:00.075140: step 29000, loss = 2.00 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:47:01.237745: step 29010, loss = 2.00 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:47:02.406271: step 29020, loss = 1.92 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:03.578040: step 29030, loss = 2.08 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:04.768065: step 29040, loss = 2.06 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:05.950888: step 29050, loss = 2.14 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:07.120971: step 29060, loss = 1.99 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:08.310702: step 29070, loss = 2.05 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:09.476164: step 29080, loss = 2.16 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:10.664880: step 29090, loss = 1.90 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:11.881436: step 29100, loss = 1.98 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:47:13.086420: step 29110, loss = 2.10 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:47:14.290606: step 29120, loss = 2.19 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:47:15.490256: step 29130, loss = 2.02 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:47:16.677498: step 29140, loss = 2.18 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:17.836141: step 29150, loss = 2.12 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:47:19.014747: step 29160, loss = 2.01 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:20.190863: step 29170, loss = 1.97 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:21.383186: step 29180, loss = 2.05 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:22.570133: step 29190, loss = 2.22 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:23.756736: step 29200, loss = 2.06 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:24.949046: step 29210, loss = 1.87 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:26.123988: step 29220, loss = 2.06 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:27.318459: step 29230, loss = 2.12 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:28.504506: step 29240, loss = 2.10 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:29.676160: step 29250, loss = 2.02 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:30.875336: step 29260, loss = 2.07 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:47:32.064301: step 29270, loss = 2.24 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:33.244451: step 29280, loss = 2.16 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:34.424850: step 29290, loss = 2.12 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:35.597180: step 29300, loss = 2.07 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:36.780209: step 29310, loss = 2.04 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:37.936459: step 29320, loss = 2.09 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:47:39.103837: step 29330, loss = 2.11 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:40.289623: step 29340, loss = 2.06 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:47:41.455040: step 29350, loss = 2.21 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:42.603455: step 29360, loss = 2.09 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-04 22:47:43.773597: step 29370, loss = 2.27 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:44.942084: step 29380, loss = 2.03 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:46.105471: step 29390, loss = 1.93 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:47:47.270721: step 29400, loss = 2.05 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:48.435180: step 29410, loss = 1.98 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:47:49.585959: step 29420, loss = 2.00 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-04 22:47:50.766791: step 29430, loss = 1.98 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:51.930074: step 29440, loss = 2.11 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:47:53.105300: step 29450, loss = 2.15 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:54.264029: step 29460, loss = 1.97 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:47:55.444923: step 29470, loss = 2.17 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:47:56.617338: step 29480, loss = 2.16 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:47:57.776082: step 29490, loss = 1.98 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:47:58.944997: step 29500, loss = 2.12 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:00.124417: step 29510, loss = 2.04 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:01.307145: step 29520, loss = 2.04 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:02.486829: step 29530, loss = 2.01 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:03.654654: step 29540, loss = 1.95 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:04.837310: step 29550, loss = 2.02 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:05.995617: step 29560, loss = 2.02 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:48:07.172709: step 29570, loss = 2.05 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:08.373801: step 29580, loss = 1.97 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:48:09.568790: step 29590, loss = 2.00 (1071.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:48:10.765759: step 29600, loss = 2.08 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:48:11.982153: step 29610, loss = 2.05 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:48:13.186073: step 29620, loss = 2.08 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:48:14.387501: step 29630, loss = 2.11 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:48:15.569725: step 29640, loss = 2.00 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:16.742130: step 29650, loss = 2.06 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:17.927532: step 29660, loss = 2.03 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:48:19.103778: step 29670, loss = 2.13 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:20.266865: step 29680, loss = 2.06 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:48:21.435611: step 29690, loss = 2.07 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:22.710953: step 29700, loss = 2.10 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-04 22:48:23.774365: step 29710, loss = 1.98 (1203.7 examples/sec; 0.106 sec/batch)
2017-05-04 22:48:24.968763: step 29720, loss = 2.05 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:48:26.122292: step 29730, loss = 2.11 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-04 22:48:27.281872: step 29740, loss = 2.04 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:48:28.449396: step 29750, loss = 2.06 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:29.599668: step 29760, loss = 1.98 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-04 22:48:30.760546: step 29770, loss = 2.09 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 22:48:31.937650: step 29780, loss = 2.02 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:33.106317: step 29790, loss = 2.16 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:34.294470: step 29800, loss = 2.02 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:48:35.505285: step 29810, loss = 2.07 (1057.1 examples/sec; 0.121 sec/batch)
2E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 314 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
017-05-04 22:48:36.674101: step 29820, loss = 1.96 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:37.820168: step 29830, loss = 1.95 (1116.9 examples/sec; 0.115 sec/batch)
2017-05-04 22:48:38.994579: step 29840, loss = 2.07 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:40.167385: step 29850, loss = 2.00 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:41.341886: step 29860, loss = 2.18 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:42.501096: step 29870, loss = 2.02 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:48:43.672971: step 29880, loss = 2.17 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:44.822142: step 29890, loss = 2.08 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-04 22:48:45.993178: step 29900, loss = 2.14 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:47.142890: step 29910, loss = 2.01 (1113.3 examples/sec; 0.115 sec/batch)
2017-05-04 22:48:48.311684: step 29920, loss = 2.06 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:48:49.470994: step 29930, loss = 1.93 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:48:50.663823: step 29940, loss = 2.09 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:48:51.846033: step 29950, loss = 2.31 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:48:53.052644: step 29960, loss = 2.10 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:48:54.243651: step 29970, loss = 1.97 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:48:55.438040: step 29980, loss = 1.95 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:48:56.658834: step 29990, loss = 2.09 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:48:57.849389: step 30000, loss = 2.17 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:48:59.073606: step 30010, loss = 2.06 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:49:00.282942: step 30020, loss = 1.95 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:49:01.497403: step 30030, loss = 2.06 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:49:02.710079: step 30040, loss = 2.06 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:49:03.908641: step 30050, loss = 2.13 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:49:05.098074: step 30060, loss = 1.88 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:06.286777: step 30070, loss = 2.03 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:07.469407: step 30080, loss = 2.02 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:08.679403: step 30090, loss = 1.97 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:49:09.869501: step 30100, loss = 2.06 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:11.082364: step 30110, loss = 1.92 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:49:12.317644: step 30120, loss = 2.04 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:49:13.492521: step 30130, loss = 2.11 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:49:14.689352: step 30140, loss = 1.97 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:49:15.907570: step 30150, loss = 2.23 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:49:17.123438: step 30160, loss = 1.92 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:49:18.317596: step 30170, loss = 2.07 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:19.502646: step 30180, loss = 1.89 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:20.709373: step 30190, loss = 2.03 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:49:21.873095: step 30200, loss = 2.21 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:49:23.061787: step 30210, loss = 2.15 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:24.232922: step 30220, loss = 2.01 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:49:25.416474: step 30230, loss = 2.02 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:26.601428: step 30240, loss = 1.95 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:27.788484: step 30250, loss = 2.03 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:28.977937: step 30260, loss = 2.21 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:30.157761: step 30270, loss = 1.88 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:31.338306: step 30280, loss = 1.98 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:32.533747: step 30290, loss = 1.92 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:49:33.730812: step 30300, loss = 2.03 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:49:34.938105: step 30310, loss = 2.19 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:49:36.131959: step 30320, loss = 2.16 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:37.323303: step 30330, loss = 2.14 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:38.505146: step 30340, loss = 2.00 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:39.686715: step 30350, loss = 2.18 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:40.877790: step 30360, loss = 2.17 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:42.034190: step 30370, loss = 1.99 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:49:43.213655: step 30380, loss = 2.11 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:44.385924: step 30390, loss = 2.09 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:49:45.557833: step 30400, loss = 2.12 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:49:46.748125: step 30410, loss = 2.22 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:47.923163: step 30420, loss = 1.92 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:49.110844: step 30430, loss = 2.07 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:50.248923: step 30440, loss = 2.19 (1124.7 examples/sec; 0.114 sec/batch)
2017-05-04 22:49:51.433881: step 30450, loss = 2.03 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:52.592005: step 30460, loss = 2.20 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:49:53.753100: step 30470, loss = 1.97 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:49:54.939030: step 30480, loss = 2.11 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:49:56.111072: step 30490, loss = 2.00 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:49:57.287635: step 30500, loss = 1.94 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:58.465462: step 30510, loss = 2.04 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:49:59.634345: step 30520, loss = 1.93 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:00.804827: step 30530, loss = 1.93 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:01.992447: step 30540, loss = 2.12 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:50:03.186859: step 30550, loss = 2.05 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:50:04.347014: step 30560, loss = 2.08 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:05.513195: step 30570, loss = 2.25 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:06.673392: step 30580, loss = 2.16 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:07.841505: step 30590, loss = 2.18 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:09.010596: step 30600, loss = 2.12 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:10.160829: step 30610, loss = 2.04 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-04 22:50:11.325272: step 30620, loss = 2.08 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:12.503956: step 30630, loss = 2.21 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:50:13.654348: step 30640, loss = 2.09 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-04 22:50:14.809594: step 30650, loss = 1.95 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:15.969975: step 30660, loss = 2.02 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:17.159136: step 30670, loss = 1.93 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:50:18.298062: step 30680, loss = 2.00 (1123.9 examples/sec; 0.114 sec/batch)
2017-05-04 22:50:19.553349: step 30690, loss = 1.86 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-04 22:50:20.628549: step 30700, loss = 1.99 (1190.5 examples/sec; 0.108 sec/batch)
2017-05-04 22:50:21.789030: step 30710, loss = 2.19 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:22.948492: step 30720, loss = 2.00 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:24.118784: step 30730, loss = 2.06 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:25.274863: step 30740, loss = 2.33 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:26.440969: step 30750, loss = 2.10 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:27.587766: step 30760, loss = 2.07 (1116.1 examples/sec; 0.115 sec/batch)
2017-05-04 22:50:28.768962: step 30770, loss = 2.10 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:50:29.927508: step 30780, loss = 2.02 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:31.113548: step 30790, loss = 1.99 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:50:32.282921: step 30800, loss = 2.10 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:33.448370: step 30810, loss = 1.98 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:34.605209: step 30820, loss = 2.04 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:35.758846: step 30830, loss = 2.05 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-04 22:50:36.929985: step 30840, loss = 2.00 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:38.087149: step 30850, loss = 2.10 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:39.256298: step 30860, loss = 2.17 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:40.430773: step 30870, loss = 2.01 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:41.584953: step 30880, loss = 1.95 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-04 22:50:42.767416: step 30890, loss = 1.91 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:50:43.907936: step 30900, loss = 2.19 (1122.3 examples/sec; 0.114 sec/batch)
2017-05-04 22:50:45.091348: step 30910, loss = 2.05 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:50:46.264780: step 30920, loss = 1.97 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:47.439969: step 30930, loss = 2.00 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-04 22:50:48.620149: step 30940, loss = 1.91 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:50:49.768875: step 30950, loss = 1.98 (1114.3 examples/sec; 0.115 sec/batch)
2017-05-04 22:50:50.931768: step 30960, loss = 2.11 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:52.116573: step 30970, loss = 2.14 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:50:53.272300: step 30980, loss = 1.97 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:54.446086: step 30990, loss = 2.13 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:55.619766: step 31000, loss = 1.99 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:50:56.800107: step 31010, loss = 2.16 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:50:57.955436: step 31020, loss = 2.25 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:50:59.122390: step 31030, loss = 2.17 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:00.297118: step 31040, loss = 2.10 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:01.486556: step 31050, loss = 2.00 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:02.672840: step 31060, loss = 2.09 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:03.879954: step 31070, loss = 2.07 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:51:05.065167: step 31080, loss = 2.06 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:06.248903: step 31090, loss = 2.15 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:07.456185: step 31100, loss = 2.13 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:51:08.646302: step 31110, loss = 1.97 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:09.840095: step 31120, loss = 2.10 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:11.055156: step 31130, loss = 2.05 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:51:12.260552: step 31140, loss = 2.01 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:51:13.447063: step 31150, loss = 2.05 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:14.621756: step 31160, loss = 2.05 (1089.6 examples/sec; 0.117 sec/batch)
2017E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 325 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
-05-04 22:51:15.808960: step 31170, loss = 1.90 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:16.990910: step 31180, loss = 2.02 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:18.153092: step 31190, loss = 1.98 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-04 22:51:19.342117: step 31200, loss = 2.07 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:20.509135: step 31210, loss = 1.99 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:21.659874: step 31220, loss = 2.07 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-04 22:51:22.840391: step 31230, loss = 2.04 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:24.006845: step 31240, loss = 1.92 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:25.170303: step 31250, loss = 1.95 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 22:51:26.326904: step 31260, loss = 2.09 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-04 22:51:27.479714: step 31270, loss = 2.00 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-04 22:51:28.641973: step 31280, loss = 2.07 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-04 22:51:29.822859: step 31290, loss = 2.21 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:30.988577: step 31300, loss = 2.10 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:32.154395: step 31310, loss = 1.93 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:33.325297: step 31320, loss = 2.09 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:34.490047: step 31330, loss = 2.10 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-04 22:51:35.672537: step 31340, loss = 1.98 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:36.845004: step 31350, loss = 2.08 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:38.024359: step 31360, loss = 2.27 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:39.211384: step 31370, loss = 1.97 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:40.374936: step 31380, loss = 2.06 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 22:51:41.560363: step 31390, loss = 2.11 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:42.742463: step 31400, loss = 1.97 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-04 22:51:43.910878: step 31410, loss = 2.06 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:45.080085: step 31420, loss = 1.86 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:46.249211: step 31430, loss = 2.11 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:47.415147: step 31440, loss = 1.99 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:48.579893: step 31450, loss = 1.98 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-04 22:51:49.747416: step 31460, loss = 2.13 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:50.945468: step 31470, loss = 1.99 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:51:52.138636: step 31480, loss = 2.06 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:51:53.306852: step 31490, loss = 1.80 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:54.477781: step 31500, loss = 2.11 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:55.651983: step 31510, loss = 2.06 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:56.825043: step 31520, loss = 1.96 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:57.991505: step 31530, loss = 2.30 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:51:59.177330: step 31540, loss = 2.02 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:52:00.375057: step 31550, loss = 2.20 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:52:01.578908: step 31560, loss = 2.04 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:52:02.801633: step 31570, loss = 2.09 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:04.028706: step 31580, loss = 2.07 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:05.250011: step 31590, loss = 2.17 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:06.450072: step 31600, loss = 2.15 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:52:07.670040: step 31610, loss = 1.98 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:08.875299: step 31620, loss = 2.12 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:10.104443: step 31630, loss = 1.90 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:11.329204: step 31640, loss = 2.20 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:12.549443: step 31650, loss = 2.13 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:13.746215: step 31660, loss = 2.11 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:52:14.980871: step 31670, loss = 2.03 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:16.292767: step 31680, loss = 1.98 (975.7 examples/sec; 0.131 sec/batch)
2017-05-04 22:52:17.406604: step 31690, loss = 2.01 (1149.2 examples/sec; 0.111 sec/batch)
2017-05-04 22:52:18.599774: step 31700, loss = 1.98 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:52:19.832070: step 31710, loss = 2.09 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:21.042520: step 31720, loss = 2.05 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:22.252934: step 31730, loss = 1.97 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:23.462302: step 31740, loss = 2.03 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:24.686790: step 31750, loss = 1.89 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:25.885098: step 31760, loss = 2.17 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:52:27.112189: step 31770, loss = 2.19 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:28.328294: step 31780, loss = 1.98 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:29.534891: step 31790, loss = 1.96 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:30.758270: step 31800, loss = 1.94 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:31.985626: step 31810, loss = 2.06 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:33.212546: step 31820, loss = 2.05 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:34.438512: step 31830, loss = 1.97 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:35.651808: step 31840, loss = 1.91 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:36.864793: step 31850, loss = 2.13 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:38.059355: step 31860, loss = 2.05 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:52:39.295850: step 31870, loss = 2.01 (1035.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:52:40.607731: step 31880, loss = 2.14 (975.7 examples/sec; 0.131 sec/batch)
2017-05-04 22:52:41.707740: step 31890, loss = 1.97 (1163.6 examples/sec; 0.110 sec/batch)
2017-05-04 22:52:42.928491: step 31900, loss = 2.14 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:44.142130: step 31910, loss = 1.94 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:45.367502: step 31920, loss = 2.06 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:52:46.581062: step 31930, loss = 2.10 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:47.795311: step 31940, loss = 2.28 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:49.037791: step 31950, loss = 1.91 (1030.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:52:50.256600: step 31960, loss = 2.00 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:51.474995: step 31970, loss = 1.92 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:52:52.678321: step 31980, loss = 2.07 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:52:53.888972: step 31990, loss = 2.05 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:55.127221: step 32000, loss = 2.09 (1033.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:52:56.336184: step 32010, loss = 2.19 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:52:57.573965: step 32020, loss = 1.96 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:52:58.770737: step 32030, loss = 1.93 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:52:59.990365: step 32040, loss = 2.11 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:01.217660: step 32050, loss = 2.11 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:02.413091: step 32060, loss = 2.09 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 335 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
04 22:53:03.634658: step 32070, loss = 1.98 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:04.807726: step 32080, loss = 2.03 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 22:53:05.995421: step 32090, loss = 1.98 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:53:07.218223: step 32100, loss = 2.10 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:08.440475: step 32110, loss = 2.12 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:09.650007: step 32120, loss = 1.91 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:10.870063: step 32130, loss = 2.20 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:12.099413: step 32140, loss = 2.08 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:13.325690: step 32150, loss = 1.89 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:14.534343: step 32160, loss = 2.05 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:15.750656: step 32170, loss = 2.14 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:16.975893: step 32180, loss = 2.17 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:18.178283: step 32190, loss = 2.08 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:53:19.392625: step 32200, loss = 2.02 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:20.623377: step 32210, loss = 2.23 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:21.810717: step 32220, loss = 1.93 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-04 22:53:23.049911: step 32230, loss = 2.06 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:53:24.259444: step 32240, loss = 2.20 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:25.455338: step 32250, loss = 2.06 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:53:26.648957: step 32260, loss = 2.13 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:53:27.903274: step 32270, loss = 1.98 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-04 22:53:29.099882: step 32280, loss = 1.98 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:53:30.330090: step 32290, loss = 2.34 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:31.560718: step 32300, loss = 2.08 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:32.790819: step 32310, loss = 2.12 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:33.968655: step 32320, loss = 2.03 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-04 22:53:35.198549: step 32330, loss = 2.00 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:36.431846: step 32340, loss = 2.10 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:37.654614: step 32350, loss = 2.01 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:38.873551: step 32360, loss = 2.03 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:40.092626: step 32370, loss = 2.09 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:41.307176: step 32380, loss = 2.07 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:42.514552: step 32390, loss = 2.16 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:43.710935: step 32400, loss = 1.89 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 22:53:44.918213: step 32410, loss = 2.01 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:46.126182: step 32420, loss = 1.96 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:47.373989: step 32430, loss = 2.16 (1025.8 examples/sec; 0.125 sec/batch)
2017-05-04 22:53:48.591156: step 32440, loss = 1.92 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:49.805455: step 32450, loss = 2.09 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:51.013429: step 32460, loss = 2.13 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:53:52.242512: step 32470, loss = 1.97 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:53:53.430523: step 32480, loss = 2.19 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-04 22:53:54.677831: step 32490, loss = 2.13 (1026.2 examples/sec; 0.125 sec/batch)
2017-05-04 22:53:55.860727: step 32500, loss = 1.91 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-04 22:53:57.078635: step 32510, loss = 2.18 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:53:58.263330: step 32520, loss = 2.16 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 22:53:59.486415: step 32530, loss = 2.17 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:00.693163: step 32540, loss = 2.00 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:01.930717: step 32550, loss = 2.10 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:54:03.157896: step 32560, loss = 2.20 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:04.392299: step 32570, loss = 2.10 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:05.601099: step 32580, loss = 2.08 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:06.808063: step 32590, loss = 1.97 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:08.028640: step 32600, loss = 1.96 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:09.272299: step 32610, loss = 2.12 (1029.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:54:10.485099: step 32620, loss = 2.12 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:11.714721: step 32630, loss = 1.96 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:12.928092: step 32640, loss = 2.17 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:14.136499: step 32650, loss = 2.03 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:15.370866: step 32660, loss = 1.93 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:16.671771: step 32670, loss = 1.96 (983.9 examples/sec; 0.130 sec/batch)
2017-05-04 22:54:17.757410: step 32680, loss = 2.17 (1179.0 examples/sec; 0.109 sec/batch)
2017-05-04 22:54:18.983205: step 32690, loss = 2.08 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:20.206382: step 32700, loss = 2.03 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:21.423968: step 32710, loss = 2.09 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:22.634879: step 32720, loss = 1.93 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:23.871963: step 32730, loss = 2.03 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:54:25.079903: step 32740, loss = 1.93 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:26.286678: step 32750, loss = 2.06 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:27.504952: step 32760, loss = 2.02 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:28.714441: step 32770, loss = 2.04 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:29.933646: step 32780, loss = 2.02 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:31.172004: step 32790, loss = 1.95 (1033.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:54:32.389569: step 32800, loss = 2.14 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:33.625348: step 32810, loss = 2.18 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:54:34.832776: step 32820, loss = 2.05 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:36.066343: step 32830, loss = 2.15 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:37.270917: step 32840, loss = 1.88 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:54:38.497369: step 32850, loss = 2.17 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:39.713468: step 32860, loss = 1.91 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:41.018698: step 32870, loss = 2.02 (980.7 examples/sec; 0.131 sec/batch)
2017-05-04 22:54:42.139005: step 32880, loss = 2.04 (1142.5 examples/sec; 0.112 sec/batch)
2017-05-04 22:54:43.381451: step 32890, loss = 1.89 (1030.2 examples/sec; 0.124 sec/batch)
2017-05-04 22:54:44.587355: step 32900, loss = 1.94 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:45.787614: step 32910, loss = 1.98 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:54:47.017387: step 32920, loss = 2.08 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:54:48.237806: step 32930, loss = 1.90 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:49.447485: step 32940, loss = 2.18 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:50.636844: step 32950, loss = 2.04 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:54:51.850861: step 32960, loss = 2.00 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 2E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 345 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
2:54:53.060784: step 32970, loss = 1.84 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:54.274950: step 32980, loss = 1.98 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:55.498385: step 32990, loss = 2.14 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:56.707078: step 33000, loss = 2.02 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:54:57.922196: step 33010, loss = 1.85 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:54:59.147224: step 33020, loss = 1.86 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:00.369734: step 33030, loss = 2.03 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:01.584003: step 33040, loss = 2.16 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:02.833262: step 33050, loss = 2.02 (1024.6 examples/sec; 0.125 sec/batch)
2017-05-04 22:55:04.043814: step 33060, loss = 1.99 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:05.283681: step 33070, loss = 2.12 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:55:06.486407: step 33080, loss = 1.90 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:55:07.689738: step 33090, loss = 2.11 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:55:08.895260: step 33100, loss = 2.11 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:10.125976: step 33110, loss = 2.03 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:11.343409: step 33120, loss = 1.85 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:12.571905: step 33130, loss = 2.08 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:13.803358: step 33140, loss = 2.02 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:15.020075: step 33150, loss = 2.12 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:16.238726: step 33160, loss = 1.84 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:17.454103: step 33170, loss = 2.06 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:18.680926: step 33180, loss = 2.14 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:19.903435: step 33190, loss = 2.10 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:21.104690: step 33200, loss = 2.06 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:55:22.337615: step 33210, loss = 2.04 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:23.545184: step 33220, loss = 1.97 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:24.767546: step 33230, loss = 2.09 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:25.970330: step 33240, loss = 2.03 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:55:27.169221: step 33250, loss = 2.05 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:55:28.404622: step 33260, loss = 1.98 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:55:29.588092: step 33270, loss = 2.14 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:55:30.801656: step 33280, loss = 2.12 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:32.039734: step 33290, loss = 2.17 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:55:33.255618: step 33300, loss = 2.15 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:34.481293: step 33310, loss = 2.37 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:35.689085: step 33320, loss = 2.10 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:36.916430: step 33330, loss = 1.89 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:38.121371: step 33340, loss = 2.10 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:55:39.365845: step 33350, loss = 1.98 (1028.5 examples/sec; 0.124 sec/batch)
2017-05-04 22:55:40.572037: step 33360, loss = 2.06 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:41.793789: step 33370, loss = 2.00 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:43.001393: step 33380, loss = 2.11 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:44.228704: step 33390, loss = 1.96 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:45.440319: step 33400, loss = 2.01 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:46.659170: step 33410, loss = 2.08 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:47.868010: step 33420, loss = 2.09 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:49.071946: step 33430, loss = 1.88 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:55:50.273637: step 33440, loss = 2.23 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:55:51.490721: step 33450, loss = 2.12 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:52.723451: step 33460, loss = 2.19 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:53.918941: step 33470, loss = 2.14 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:55:55.136200: step 33480, loss = 1.79 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:56.344336: step 33490, loss = 2.11 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:55:57.564851: step 33500, loss = 2.06 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:55:58.796932: step 33510, loss = 1.91 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:55:59.996555: step 33520, loss = 2.06 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-04 22:56:01.245089: step 33530, loss = 2.07 (1025.2 examples/sec; 0.125 sec/batch)
2017-05-04 22:56:02.431708: step 33540, loss = 2.20 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 22:56:03.668076: step 33550, loss = 2.31 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:56:04.889690: step 33560, loss = 2.04 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:06.094326: step 33570, loss = 2.03 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:56:07.326403: step 33580, loss = 2.17 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:08.554703: step 33590, loss = 1.99 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:09.775903: step 33600, loss = 1.93 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:10.993195: step 33610, loss = 2.11 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:12.215800: step 33620, loss = 2.19 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:13.435330: step 33630, loss = 2.05 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:14.660034: step 33640, loss = 2.03 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:15.859308: step 33650, loss = 2.12 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:56:17.184015: step 33660, loss = 2.04 (966.3 examples/sec; 0.132 sec/batch)
2017-05-04 22:56:18.285617: step 33670, loss = 2.09 (1161.9 examples/sec; 0.110 sec/batch)
2017-05-04 22:56:19.508561: step 33680, loss = 2.05 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:20.733071: step 33690, loss = 1.99 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:21.949855: step 33700, loss = 1.95 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:23.194603: step 33710, loss = 1.89 (1028.3 examples/sec; 0.124 sec/batch)
2017-05-04 22:56:24.412784: step 33720, loss = 2.11 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:25.639588: step 33730, loss = 2.04 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:26.874326: step 33740, loss = 2.22 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:28.088882: step 33750, loss = 1.93 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:56:29.310913: step 33760, loss = 2.23 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:30.529661: step 33770, loss = 2.12 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:31.758565: step 33780, loss = 2.00 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:32.991515: step 33790, loss = 2.01 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:34.180922: step 33800, loss = 2.12 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:56:35.412007: step 33810, loss = 2.12 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:36.617124: step 33820, loss = 1.94 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:56:37.824141: step 33830, loss = 1.99 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:56:39.053274: step 33840, loss = 1.97 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:40.290521: step 33850, loss = 1.97 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:56:41.587813: step 33860, loss = 2.03 (986.7 examples/sec; 0.130 sec/batch)
2017-05-04 22:56E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 355 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
:42.672282: step 33870, loss = 2.04 (1180.3 examples/sec; 0.108 sec/batch)
2017-05-04 22:56:43.902758: step 33880, loss = 2.06 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:45.136603: step 33890, loss = 1.96 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:46.313136: step 33900, loss = 1.98 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 22:56:47.535444: step 33910, loss = 1.99 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:48.756707: step 33920, loss = 1.94 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:49.967077: step 33930, loss = 2.06 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:56:51.188701: step 33940, loss = 2.20 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:52.412974: step 33950, loss = 2.03 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:53.622813: step 33960, loss = 2.06 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:56:54.835855: step 33970, loss = 1.88 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:56:56.065919: step 33980, loss = 2.04 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:56:57.286889: step 33990, loss = 2.10 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:56:58.490556: step 34000, loss = 1.95 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:56:59.718673: step 34010, loss = 2.03 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:00.928389: step 34020, loss = 1.97 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:02.140920: step 34030, loss = 1.99 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:03.347605: step 34040, loss = 1.93 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:04.557609: step 34050, loss = 2.04 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:05.740271: step 34060, loss = 2.01 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 22:57:06.973933: step 34070, loss = 2.14 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:08.191723: step 34080, loss = 2.14 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:09.417097: step 34090, loss = 2.05 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:10.609142: step 34100, loss = 2.16 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-04 22:57:11.854627: step 34110, loss = 2.06 (1027.7 examples/sec; 0.125 sec/batch)
2017-05-04 22:57:13.064406: step 34120, loss = 2.14 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:14.268383: step 34130, loss = 2.32 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 22:57:15.504279: step 34140, loss = 2.00 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:57:16.729518: step 34150, loss = 2.06 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:17.936443: step 34160, loss = 2.20 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:19.142364: step 34170, loss = 2.19 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:20.348668: step 34180, loss = 2.15 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:21.572086: step 34190, loss = 2.14 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:22.768662: step 34200, loss = 2.02 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-04 22:57:24.006788: step 34210, loss = 1.95 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:57:25.223256: step 34220, loss = 2.04 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:26.433225: step 34230, loss = 2.06 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:27.666683: step 34240, loss = 1.95 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:28.909749: step 34250, loss = 2.04 (1029.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:57:30.080493: step 34260, loss = 2.09 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-04 22:57:31.294488: step 34270, loss = 2.15 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:32.523303: step 34280, loss = 1.99 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:33.735042: step 34290, loss = 2.00 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:34.961305: step 34300, loss = 2.11 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:36.176265: step 34310, loss = 2.05 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:37.385189: step 34320, loss = 2.12 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:38.614768: step 34330, loss = 2.06 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:39.839880: step 34340, loss = 1.84 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:41.061987: step 34350, loss = 1.93 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:42.252136: step 34360, loss = 1.95 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:57:43.481655: step 34370, loss = 2.05 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:44.684107: step 34380, loss = 1.99 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:57:45.893643: step 34390, loss = 1.97 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:47.124507: step 34400, loss = 1.95 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:48.352265: step 34410, loss = 2.05 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:49.552711: step 34420, loss = 2.12 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:57:50.777389: step 34430, loss = 1.86 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:51.990898: step 34440, loss = 2.10 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:53.218837: step 34450, loss = 2.02 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:54.393503: step 34460, loss = 1.99 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:57:55.622664: step 34470, loss = 1.92 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:57:56.837223: step 34480, loss = 2.17 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:57:58.054650: step 34490, loss = 2.07 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:57:59.264367: step 34500, loss = 2.07 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:00.476348: step 34510, loss = 1.85 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:01.697083: step 34520, loss = 1.98 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:02.929404: step 34530, loss = 1.95 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:04.161571: step 34540, loss = 2.23 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:05.395727: step 34550, loss = 2.07 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:06.605950: step 34560, loss = 1.94 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:07.829210: step 34570, loss = 1.87 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:09.070865: step 34580, loss = 2.16 (1030.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:58:10.291084: step 34590, loss = 2.10 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:11.493512: step 34600, loss = 2.21 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-04 22:58:12.731943: step 34610, loss = 2.08 (1033.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:58:13.906593: step 34620, loss = 2.10 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 22:58:15.133894: step 34630, loss = 2.13 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:16.359120: step 34640, loss = 2.06 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:17.700626: step 34650, loss = 2.01 (954.2 examples/sec; 0.134 sec/batch)
2017-05-04 22:58:18.774739: step 34660, loss = 1.97 (1191.7 examples/sec; 0.107 sec/batch)
2017-05-04 22:58:20.001047: step 34670, loss = 2.06 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:21.217556: step 34680, loss = 1.95 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:22.427834: step 34690, loss = 1.91 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:23.647282: step 34700, loss = 2.31 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:24.886570: step 34710, loss = 1.86 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:58:26.100080: step 34720, loss = 2.06 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:27.332269: step 34730, loss = 1.94 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:28.527137: step 34740, loss = 2.12 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:58:29.746053: step 34750, loss = 2.14 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:30.982303: step 34760, loss = 2.07 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-04 22:58:32E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 366 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
.202972: step 34770, loss = 1.84 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:33.421086: step 34780, loss = 1.92 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:34.627310: step 34790, loss = 2.03 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:58:35.823761: step 34800, loss = 2.04 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:58:37.044100: step 34810, loss = 2.05 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:38.244464: step 34820, loss = 1.96 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:58:39.476973: step 34830, loss = 2.05 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:40.680439: step 34840, loss = 2.02 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:58:41.969088: step 34850, loss = 2.09 (993.3 examples/sec; 0.129 sec/batch)
2017-05-04 22:58:43.087329: step 34860, loss = 2.17 (1144.6 examples/sec; 0.112 sec/batch)
2017-05-04 22:58:44.304164: step 34870, loss = 1.98 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:45.537714: step 34880, loss = 2.15 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:46.738942: step 34890, loss = 1.91 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 22:58:47.970348: step 34900, loss = 2.02 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-04 22:58:49.189988: step 34910, loss = 1.89 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:50.388045: step 34920, loss = 2.20 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:58:51.632215: step 34930, loss = 1.97 (1028.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:58:52.872665: step 34940, loss = 2.17 (1031.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:58:54.108644: step 34950, loss = 2.09 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-04 22:58:55.332513: step 34960, loss = 1.95 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:56.553051: step 34970, loss = 1.99 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:58:57.748891: step 34980, loss = 2.14 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:58:58.985715: step 34990, loss = 2.14 (1034.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:59:00.188927: step 35000, loss = 1.96 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-04 22:59:01.413293: step 35010, loss = 1.95 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:02.627670: step 35020, loss = 1.97 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:03.865965: step 35030, loss = 2.02 (1033.7 examples/sec; 0.124 sec/batch)
2017-05-04 22:59:05.101606: step 35040, loss = 1.95 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-04 22:59:06.270670: step 35050, loss = 2.09 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 22:59:07.482644: step 35060, loss = 2.05 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:08.705687: step 35070, loss = 1.96 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:09.904820: step 35080, loss = 2.06 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:59:11.148943: step 35090, loss = 2.00 (1028.8 examples/sec; 0.124 sec/batch)
2017-05-04 22:59:12.372167: step 35100, loss = 2.18 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:13.561150: step 35110, loss = 1.90 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:59:14.790884: step 35120, loss = 2.10 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:16.038083: step 35130, loss = 2.09 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-04 22:59:17.249939: step 35140, loss = 2.15 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:18.483179: step 35150, loss = 2.07 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:19.689418: step 35160, loss = 2.03 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:20.930803: step 35170, loss = 2.03 (1031.1 examples/sec; 0.124 sec/batch)
2017-05-04 22:59:22.119031: step 35180, loss = 2.06 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:59:23.320433: step 35190, loss = 1.98 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-04 22:59:24.529315: step 35200, loss = 1.83 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:25.756676: step 35210, loss = 2.16 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:26.960519: step 35220, loss = 2.01 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:59:28.189362: step 35230, loss = 2.12 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:29.402205: step 35240, loss = 1.93 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:30.591533: step 35250, loss = 2.00 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-04 22:59:31.804808: step 35260, loss = 2.04 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:33.018219: step 35270, loss = 2.25 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:34.226580: step 35280, loss = 2.05 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:35.451963: step 35290, loss = 2.16 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:36.675992: step 35300, loss = 1.98 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:37.902054: step 35310, loss = 2.05 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:39.117115: step 35320, loss = 2.14 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:40.326287: step 35330, loss = 1.81 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:41.510798: step 35340, loss = 2.04 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 22:59:42.705363: step 35350, loss = 2.10 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-04 22:59:43.928011: step 35360, loss = 1.98 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:45.148421: step 35370, loss = 2.08 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:46.345428: step 35380, loss = 2.42 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-04 22:59:47.563765: step 35390, loss = 2.00 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:48.787912: step 35400, loss = 2.13 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:50.007794: step 35410, loss = 2.07 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:51.222535: step 35420, loss = 1.92 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:52.455217: step 35430, loss = 2.26 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-04 22:59:53.656888: step 35440, loss = 2.16 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 22:59:54.878695: step 35450, loss = 2.09 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 22:59:56.092680: step 35460, loss = 1.94 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:57.301355: step 35470, loss = 2.01 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-04 22:59:58.495879: step 35480, loss = 1.88 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-04 22:59:59.718859: step 35490, loss = 2.07 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:00.942974: step 35500, loss = 1.87 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:02.163509: step 35510, loss = 2.00 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:03.375332: step 35520, loss = 2.04 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:04.580341: step 35530, loss = 2.03 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:05.781825: step 35540, loss = 2.21 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:00:06.993781: step 35550, loss = 1.88 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:08.211446: step 35560, loss = 2.16 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:09.443865: step 35570, loss = 2.02 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:00:10.640401: step 35580, loss = 1.99 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:00:11.848214: step 35590, loss = 2.06 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:13.072328: step 35600, loss = 2.06 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:14.269886: step 35610, loss = 1.85 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:00:15.489393: step 35620, loss = 1.93 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:16.712437: step 35630, loss = 1.99 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:18.025557: step 35640, loss = 1.95 (974.8 examples/sec; 0.131 sec/batch)
2017-05-04 23:00:19.109711: step 35650, loss = 2.12 (1180.6 examples/sec; 0.108 sec/batch)
2017-05-04 23:00:20.310563: step 35660, loss = 2.05 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:00:21.541846: step 35670, loss = 2.12 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:00:22.745442: step 35680, loss = 1.92 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:00:23.964532: step 35690, loss = 2.04 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:25.174980: step 35700, loss = 2.12 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:26.373331: step 35710, loss = 1.99 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:00:27.582011: step 35720, loss = 2.04 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:28.829029: step 35730, loss = 1.98 (1026.4 examples/sec; 0.125 sec/batch)
2017-05-04 23:00:30.024064: step 35740, loss = 1.95 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:00:31.252344: step 35750, loss = 2.02 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:00:32.455715: step 35760, loss = 2.12 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:00:33.665417: step 35770, loss = 2.17 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:34.884949: step 35780, loss = 2.05 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:36.099878: step 35790, loss = 2.10 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:37.308581: step 35800, loss = 2.23 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:38.525332: step 35810, loss = 2.14 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:39.747708: step 35820, loss = 2.04 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:40.961636: step 35830, loss = 2.03 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:42.245483: step 35840, loss = 1.77 (997.0 examples/sec; 0.128 sec/batch)
2017-05-04 23:00:43.372397: step 35850, loss = 1.95 (1135.8 examples/sec; 0.113 sec/batch)
2017-05-04 23:00:44.572531: step 35860, loss = 2.23 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:00:45.788463: step 35870, loss = 2.02 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:47.004613: step 35880, loss = 1.90 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:00:48.232488: step 35890, loss = 1.92 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:00:49.425906: step 35900, loss = 1.93 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:00:50.635279: step 35910, loss = 2.11 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:51.843916: step 35920, loss = 1.95 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:53.103676: step 35930, loss = 1.86 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-04 23:00:54.290598: step 35940, loss = 2.04 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:00:55.525204: step 35950, loss = 2.11 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:00:56.735962: step 35960, loss = 2.08 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:57.944698: step 35970, loss = 1.99 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:00:59.146125: step 35980, loss = 1.94 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:01:00.392613: step 35990, loss = 1.85 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-04 23:01:01.583905: step 36000, loss = 2.17 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:01:02.796534: step 36010, loss = 1.80 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:04.004823: step 36020, loss = 2.07 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:05.225009: step 36030, loss = 2.02 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:06.408499: step 36040, loss = 1.93 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:01:07.640594: step 36050, loss = 1.96 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:01:08.875423: step 36060, loss = 2.02 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:01:10.073622: step 36070, loss = 1.91 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:01:11.287870: step 36080, loss = 1.93 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:12.514867: step 36090, loss = 2.17 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:01:13.724341: step 36100, loss = 2.10 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:14.934877: step 36110, loss = 1.97 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:16.16514E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 376 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
4: step 36120, loss = 1.86 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:01:17.382991: step 36130, loss = 1.94 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:18.593912: step 36140, loss = 2.08 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:19.812814: step 36150, loss = 2.12 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:21.016374: step 36160, loss = 1.92 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:01:22.241985: step 36170, loss = 2.02 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:01:23.457654: step 36180, loss = 2.08 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:24.671501: step 36190, loss = 2.02 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:25.860770: step 36200, loss = 2.04 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:01:27.086524: step 36210, loss = 1.83 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:01:28.288041: step 36220, loss = 2.05 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:01:29.523401: step 36230, loss = 2.13 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:01:30.727775: step 36240, loss = 2.05 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:01:31.955793: step 36250, loss = 2.04 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:01:33.162674: step 36260, loss = 1.88 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:34.382754: step 36270, loss = 2.01 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:35.598083: step 36280, loss = 1.97 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:36.823186: step 36290, loss = 2.15 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:01:38.021003: step 36300, loss = 2.06 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:01:39.251082: step 36310, loss = 1.95 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:01:40.472225: step 36320, loss = 2.14 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:41.691085: step 36330, loss = 1.96 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:42.920581: step 36340, loss = 2.03 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:01:44.121775: step 36350, loss = 1.95 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:01:45.337415: step 36360, loss = 1.89 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:46.558071: step 36370, loss = 2.07 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:47.781506: step 36380, loss = 2.03 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:48.992976: step 36390, loss = 2.15 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:50.203948: step 36400, loss = 2.08 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:51.422875: step 36410, loss = 1.93 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:52.636303: step 36420, loss = 1.96 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:01:53.863342: step 36430, loss = 2.02 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:01:55.053582: step 36440, loss = 2.08 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:01:56.273629: step 36450, loss = 2.04 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:01:57.475336: step 36460, loss = 2.08 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:01:58.703120: step 36470, loss = 2.02 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:01:59.921070: step 36480, loss = 2.00 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:01.147196: step 36490, loss = 2.09 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:02:02.352778: step 36500, loss = 2.01 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:03.568981: step 36510, loss = 2.00 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:04.791381: step 36520, loss = 1.97 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:05.988305: step 36530, loss = 2.22 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:02:07.222528: step 36540, loss = 1.87 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:02:08.451392: step 36550, loss = 2.06 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:02:09.648527: step 36560, loss = 2.04 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:02:10.883865: step 36570, loss = 1.96 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-04 23:02:12.084580: step 36580, loss = 2.10 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:02:13.304704: step 36590, loss = 2.22 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:14.514208: step 36600, loss = 1.91 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:15.735254: step 36610, loss = 2.01 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:16.949404: step 36620, loss = 2.12 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:18.228531: step 36630, loss = 2.23 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-04 23:02:19.336032: step 36640, loss = 2.00 (1155.8 examples/sec; 0.111 sec/batch)
2017-05-04 23:02:20.558035: step 36650, loss = 2.09 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:21.776592: step 36660, loss = 1.96 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:22.993981: step 36670, loss = 1.95 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:24.204131: step 36680, loss = 1.99 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:25.422446: step 36690, loss = 2.08 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:26.639434: step 36700, loss = 2.06 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:27.849053: step 36710, loss = 1.95 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:29.059979: step 36720, loss = 1.82 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:30.305652: step 36730, loss = 2.01 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-04 23:02:31.510782: step 36740, loss = 1.97 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:32.743201: step 36750, loss = 2.33 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:02:33.931022: step 36760, loss = 1.97 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:02:35.159131: step 36770, loss = 1.96 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:02:36.367167: step 36780, loss = 2.03 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:37.585269: step 36790, loss = 2.00 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:38.806587: step 36800, loss = 1.87 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:40.023299: step 36810, loss = 1.98 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:41.244469: step 36820, loss = 2.02 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:02:42.482078: step 36830, loss = 2.15 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-04 23:02:43.623949: step 36840, loss = 1.93 (1121.0 examples/sec; 0.114 sec/batch)
2017-05-04 23:02:44.825753: step 36850, loss = 2.13 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:02:46.012672: step 36860, loss = 1.96 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:02:47.221957: step 36870, loss = 2.11 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:02:48.398062: step 36880, loss = 1.84 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:02:49.554025: step 36890, loss = 1.91 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:02:50.724387: step 36900, loss = 2.03 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:02:51.911123: step 36910, loss = 2.00 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:02:53.082797: step 36920, loss = 2.07 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:02:54.266098: step 36930, loss = 2.16 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:02:55.451694: step 36940, loss = 1.78 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:02:56.637773: step 36950, loss = 1.86 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:02:57.799469: step 36960, loss = 2.10 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:02:58.967695: step 36970, loss = 2.17 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:03:00.139665: step 36980, loss = 2.20 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:03:01.322134: step 36990, loss = 1.95 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:02.490102: step 37000, loss = 1.95 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:03:03.676727: step 37010, loss = 2.02 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:03:04.853415:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 386 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
 step 37020, loss = 1.93 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:06.035917: step 37030, loss = 1.89 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:07.219570: step 37040, loss = 2.11 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:08.404062: step 37050, loss = 2.06 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:09.582019: step 37060, loss = 2.07 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:10.767875: step 37070, loss = 2.02 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:03:11.967244: step 37080, loss = 1.90 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:03:13.170824: step 37090, loss = 1.92 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:03:14.349427: step 37100, loss = 1.96 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:15.529300: step 37110, loss = 2.12 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:16.705913: step 37120, loss = 2.03 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:17.880805: step 37130, loss = 2.09 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:03:19.049161: step 37140, loss = 2.02 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:03:20.267186: step 37150, loss = 2.05 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:03:21.455076: step 37160, loss = 2.09 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:03:22.650083: step 37170, loss = 1.95 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:03:23.863640: step 37180, loss = 2.06 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:03:25.054577: step 37190, loss = 2.08 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:03:26.247334: step 37200, loss = 2.03 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:03:27.433033: step 37210, loss = 2.13 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:03:28.620949: step 37220, loss = 2.03 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:03:29.779703: step 37230, loss = 1.96 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:03:30.949398: step 37240, loss = 2.18 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:03:32.142569: step 37250, loss = 2.04 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:03:33.317750: step 37260, loss = 2.17 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:03:34.487390: step 37270, loss = 1.90 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:03:35.683044: step 37280, loss = 2.01 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:03:36.876882: step 37290, loss = 2.04 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:03:38.070444: step 37300, loss = 2.26 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:03:39.289535: step 37310, loss = 1.98 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:03:40.505469: step 37320, loss = 2.09 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:03:41.722262: step 37330, loss = 1.95 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:03:42.961982: step 37340, loss = 2.15 (1032.5 examples/sec; 0.124 sec/batch)
2017-05-04 23:03:44.199879: step 37350, loss = 2.11 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-04 23:03:45.427584: step 37360, loss = 2.09 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:03:46.636120: step 37370, loss = 2.14 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:03:47.870721: step 37380, loss = 1.97 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:03:49.097148: step 37390, loss = 2.01 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:03:50.317750: step 37400, loss = 2.00 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:03:51.540884: step 37410, loss = 2.09 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:03:52.762207: step 37420, loss = 1.95 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:03:53.926296: step 37430, loss = 2.07 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:03:55.175940: step 37440, loss = 2.04 (1024.3 examples/sec; 0.125 sec/batch)
2017-05-04 23:03:56.414904: step 37450, loss = 2.05 (1033.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:03:57.625650: step 37460, loss = 2.09 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:03:58.860678: step 37470, loss = 2.02 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-04 23:04:00.090412: step 37480, loss = 2.06 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:01.314366: step 37490, loss = 2.04 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:02.551129: step 37500, loss = 1.98 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-04 23:04:03.796964: step 37510, loss = 1.90 (1027.4 examples/sec; 0.125 sec/batch)
2017-05-04 23:04:05.029392: step 37520, loss = 2.28 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:06.259082: step 37530, loss = 1.98 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:07.479636: step 37540, loss = 2.15 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:08.705772: step 37550, loss = 2.04 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:09.913523: step 37560, loss = 2.01 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:04:11.141084: step 37570, loss = 2.16 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:12.381337: step 37580, loss = 2.03 (1032.0 examples/sec; 0.124 sec/batch)
2017-05-04 23:04:13.590516: step 37590, loss = 2.12 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:04:14.818361: step 37600, loss = 2.07 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:16.038393: step 37610, loss = 2.03 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:17.343662: step 37620, loss = 2.06 (980.6 examples/sec; 0.131 sec/batch)
2017-05-04 23:04:18.427879: step 37630, loss = 1.96 (1180.6 examples/sec; 0.108 sec/batch)
2017-05-04 23:04:19.674507: step 37640, loss = 2.15 (1026.8 examples/sec; 0.125 sec/batch)
2017-05-04 23:04:20.907853: step 37650, loss = 2.07 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:22.115322: step 37660, loss = 2.08 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:04:23.339911: step 37670, loss = 2.09 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:24.572092: step 37680, loss = 1.93 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:25.791510: step 37690, loss = 1.91 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:27.010133: step 37700, loss = 2.04 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:28.223043: step 37710, loss = 2.04 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:04:29.444208: step 37720, loss = 2.19 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:30.657456: step 37730, loss = 1.99 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:04:31.883496: step 37740, loss = 2.12 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:33.109735: step 37750, loss = 1.99 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:34.321818: step 37760, loss = 2.04 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:04:35.554931: step 37770, loss = 1.89 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:36.780055: step 37780, loss = 1.89 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:38.006136: step 37790, loss = 2.05 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:39.236149: step 37800, loss = 1.92 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:40.453698: step 37810, loss = 2.24 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:41.630496: step 37820, loss = 1.94 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:04:42.849830: step 37830, loss = 2.15 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:44.083560: step 37840, loss = 2.01 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:45.315369: step 37850, loss = 1.91 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:46.546976: step 37860, loss = 2.11 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:47.755503: step 37870, loss = 2.08 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:04:48.974288: step 37880, loss = 2.10 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:50.195276: step 37890, loss = 2.04 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:51.428442: step 37900, loss = 2.04 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:52.654653: step 37910, loss = 2.07 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:53.873084: stE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 396 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
ep 37920, loss = 2.09 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:04:55.114971: step 37930, loss = 2.00 (1030.7 examples/sec; 0.124 sec/batch)
2017-05-04 23:04:56.341198: step 37940, loss = 1.89 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:04:57.553065: step 37950, loss = 1.95 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:04:58.769094: step 37960, loss = 2.12 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:00.011531: step 37970, loss = 1.99 (1030.2 examples/sec; 0.124 sec/batch)
2017-05-04 23:05:01.244919: step 37980, loss = 1.89 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:02.469421: step 37990, loss = 2.09 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:03.695451: step 38000, loss = 2.00 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:04.929947: step 38010, loss = 2.03 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:06.119428: step 38020, loss = 2.04 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:05:07.333895: step 38030, loss = 2.14 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:08.553725: step 38040, loss = 2.10 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:09.776346: step 38050, loss = 2.16 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:10.993674: step 38060, loss = 1.95 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:12.217943: step 38070, loss = 1.99 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:13.417759: step 38080, loss = 1.94 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:05:14.626047: step 38090, loss = 2.08 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:15.855162: step 38100, loss = 2.15 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:17.079810: step 38110, loss = 2.13 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:18.280868: step 38120, loss = 1.90 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:05:19.519142: step 38130, loss = 1.91 (1033.7 examples/sec; 0.124 sec/batch)
2017-05-04 23:05:20.744637: step 38140, loss = 2.01 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:21.955539: step 38150, loss = 1.98 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:23.178510: step 38160, loss = 2.11 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:24.400733: step 38170, loss = 2.04 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:25.632408: step 38180, loss = 1.97 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:26.842354: step 38190, loss = 1.98 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:28.073182: step 38200, loss = 2.14 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:29.302803: step 38210, loss = 2.12 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:30.477360: step 38220, loss = 2.02 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:05:31.683642: step 38230, loss = 2.02 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:32.912092: step 38240, loss = 1.98 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:34.135203: step 38250, loss = 2.14 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:35.375089: step 38260, loss = 1.94 (1032.3 examples/sec; 0.124 sec/batch)
2017-05-04 23:05:36.588028: step 38270, loss = 1.84 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:37.800545: step 38280, loss = 1.95 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:39.023665: step 38290, loss = 2.05 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:40.238718: step 38300, loss = 2.00 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:41.459079: step 38310, loss = 2.14 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:42.676793: step 38320, loss = 1.95 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:43.902273: step 38330, loss = 1.98 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:45.105610: step 38340, loss = 1.94 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:05:46.306613: step 38350, loss = 2.05 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:05:47.521706: step 38360, loss = 2.06 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:48.744350: step 38370, loss = 2.05 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:49.955078: step 38380, loss = 2.10 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:51.180917: step 38390, loss = 2.00 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:05:52.395141: step 38400, loss = 2.05 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:53.615460: step 38410, loss = 2.07 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:54.824788: step 38420, loss = 1.98 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:56.035907: step 38430, loss = 2.04 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:05:57.254844: step 38440, loss = 2.08 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:05:58.453597: step 38450, loss = 2.01 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:05:59.694957: step 38460, loss = 2.05 (1031.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:06:00.923644: step 38470, loss = 2.05 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:06:02.134442: step 38480, loss = 2.05 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:06:03.352811: step 38490, loss = 1.98 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:06:04.570374: step 38500, loss = 2.04 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:06:05.779539: step 38510, loss = 2.22 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:06:06.993511: step 38520, loss = 2.18 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:06:08.220366: step 38530, loss = 2.04 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:06:09.429489: step 38540, loss = 2.01 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:06:10.614831: step 38550, loss = 2.06 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:06:11.838878: step 38560, loss = 2.04 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:06:13.044528: step 38570, loss = 2.06 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:06:14.230674: step 38580, loss = 2.02 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:06:15.402531: step 38590, loss = 2.08 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:16.584389: step 38600, loss = 2.00 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:17.843514: step 38610, loss = 1.95 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-04 23:06:18.924066: step 38620, loss = 2.00 (1184.6 examples/sec; 0.108 sec/batch)
2017-05-04 23:06:20.098093: step 38630, loss = 1.96 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:21.276296: step 38640, loss = 1.96 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:22.428422: step 38650, loss = 1.94 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-04 23:06:23.602719: step 38660, loss = 2.15 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:24.791564: step 38670, loss = 1.92 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:06:25.958860: step 38680, loss = 2.11 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:27.142083: step 38690, loss = 2.02 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:28.318031: step 38700, loss = 2.02 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:29.488401: step 38710, loss = 1.93 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:30.677690: step 38720, loss = 1.86 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:06:31.854375: step 38730, loss = 2.15 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:33.018405: step 38740, loss = 2.04 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:06:34.184505: step 38750, loss = 1.92 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:35.346608: step 38760, loss = 2.16 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:06:36.524362: step 38770, loss = 2.00 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:37.684345: step 38780, loss = 2.01 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:06:38.859223: step 38790, loss = 1.97 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:40.028711: step 38800, loss = 2.00 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:41.204357: step 38810, loss = 2.03 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:42.383551: step 38820, loss = 2.02 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:43.564453: step 38830, loss = 1.97 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:44.747652: step 38840, loss = 1.91 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:45.900531: step 38850, loss = 1.96 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:06:47.052661: step 38860, loss = 1.97 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-04 23:06:48.222278: step 38870, loss = 1.94 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:49.377510: step 38880, loss = 2.04 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:06:50.550195: step 38890, loss = 1.96 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:51.736355: step 38900, loss = 2.16 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:06:52.897477: step 38910, loss = 1.97 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:06:54.082369: step 38920, loss = 1.99 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:55.262641: step 38930, loss = 2.19 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:56.437327: step 38940, loss = 1.83 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:57.609700: step 38950, loss = 2.01 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:06:58.787731: step 38960, loss = 1.97 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:06:59.959968: step 38970, loss = 2.03 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:07:01.155610: step 38980, loss = 1.99 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:02.365399: step 38990, loss = 2.14 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:07:03.558452: step 39000, loss = 1.96 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:07:04.753028: step 39010, loss = 1.91 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:07:05.924015: step 39020, loss = 1.97 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:07:07.123984: step 39030, loss = 1.87 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:08.323756: step 39040, loss = 1.98 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:09.508933: step 39050, loss = 1.96 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:07:10.711959: step 39060, loss = 1.97 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:11.915953: step 39070, loss = 1.90 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:13.117336: step 39080, loss = 2.11 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:14.318141: step 39090, loss = 2.14 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:15.519480: step 39100, loss = 2.05 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:16.708159: step 39110, loss = 2.25 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:07:17.864705: step 39120, loss = 1.88 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:07:19.037012: step 39130, loss = 1.98 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:07:20.206401: step 39140, loss = 1.99 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:07:21.380634: step 39150, loss = 2.01 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:07:22.544860: step 39160, loss = 2.10 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:07:23.701958: step 39170, loss = 2.03 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:07:24.870891: step 39180, loss = 2.01 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:07:26.025272: step 39190, loss = 1.89 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:07:27.217348: step 39200, loss = 1.84 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:07:28.392971: step 39210, loss = 2.08 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:07:29.593452: step 39220, loss = 2.13 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:30.787873: step 39230, loss = 1.90 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:07:31.994209: step 39240, loss = 2.10 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:07:33.212633: step 39250, loss = 1.93 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:07:34.436330: step 39260, loss = 2.01 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:07:35.654815: step E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 408 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
39270, loss = 1.87 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:07:36.877002: step 39280, loss = 1.94 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:07:38.086859: step 39290, loss = 2.05 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:07:39.308520: step 39300, loss = 1.98 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:07:40.510748: step 39310, loss = 2.00 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:41.713358: step 39320, loss = 2.15 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:42.923596: step 39330, loss = 2.23 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:07:44.125081: step 39340, loss = 1.94 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:45.322643: step 39350, loss = 1.90 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:46.512351: step 39360, loss = 1.95 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:07:47.708763: step 39370, loss = 2.08 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:48.919214: step 39380, loss = 2.05 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:07:50.129681: step 39390, loss = 1.98 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:07:51.361303: step 39400, loss = 2.24 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:07:52.572789: step 39410, loss = 2.22 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:07:53.763977: step 39420, loss = 2.00 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:07:54.964100: step 39430, loss = 1.88 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:07:56.150046: step 39440, loss = 1.95 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:07:57.326169: step 39450, loss = 2.03 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:07:58.497472: step 39460, loss = 1.92 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:07:59.675130: step 39470, loss = 2.03 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:00.842493: step 39480, loss = 2.02 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:02.016694: step 39490, loss = 2.18 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:03.180424: step 39500, loss = 2.12 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:08:04.342470: step 39510, loss = 2.03 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:08:05.497081: step 39520, loss = 2.07 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:08:06.669518: step 39530, loss = 2.21 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:07.854808: step 39540, loss = 2.05 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:08:09.027614: step 39550, loss = 1.88 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:10.211626: step 39560, loss = 2.06 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:11.379988: step 39570, loss = 2.07 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:12.548461: step 39580, loss = 2.05 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:13.704651: step 39590, loss = 1.85 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:08:14.982019: step 39600, loss = 1.98 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-04 23:08:16.059561: step 39610, loss = 2.00 (1187.9 examples/sec; 0.108 sec/batch)
2017-05-04 23:08:17.235357: step 39620, loss = 2.06 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:18.408543: step 39630, loss = 1.99 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:19.576914: step 39640, loss = 2.03 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:20.751503: step 39650, loss = 2.01 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:21.927591: step 39660, loss = 1.95 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:23.100792: step 39670, loss = 1.94 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:24.300360: step 39680, loss = 2.06 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:08:25.468867: step 39690, loss = 2.08 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:26.629210: step 39700, loss = 1.82 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:08:27.805662: step 39710, loss = 2.11 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:28.989300: step 39720, loss = 2.06 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:30.157259: step 39730, loss = 1.92 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:31.336760: step 39740, loss = 2.05 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:32.507295: step 39750, loss = 2.02 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:33.653601: step 39760, loss = 2.08 (1116.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:08:34.928819: step 39770, loss = 1.93 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-04 23:08:36.103401: step 39780, loss = 1.95 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:37.306370: step 39790, loss = 2.25 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:08:38.485509: step 39800, loss = 2.04 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:39.695485: step 39810, loss = 1.87 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:08:40.878845: step 39820, loss = 1.81 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:42.053012: step 39830, loss = 2.07 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:43.246887: step 39840, loss = 2.01 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:08:44.408715: step 39850, loss = 1.90 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:08:45.591523: step 39860, loss = 1.97 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:46.788016: step 39870, loss = 1.92 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:08:47.998912: step 39880, loss = 2.09 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:08:49.208143: step 39890, loss = 2.13 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:08:50.389369: step 39900, loss = 2.09 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:08:51.607991: step 39910, loss = 1.91 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:08:52.807525: step 39920, loss = 2.00 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:08:53.972442: step 39930, loss = 2.24 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:08:55.129934: step 39940, loss = 2.01 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:08:56.292357: step 39950, loss = 2.04 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:08:57.459366: step 39960, loss = 2.15 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:08:58.622845: step 39970, loss = 2.22 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:08:59.786437: step 39980, loss = 1.96 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:09:00.969944: step 39990, loss = 1.97 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:02.155526: step 40000, loss = 1.89 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:03.330618: step 40010, loss = 2.13 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:04.499976: step 40020, loss = 1.96 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:05.672826: step 40030, loss = 2.04 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:06.850478: step 40040, loss = 2.00 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:08.015577: step 40050, loss = 2.04 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:09.194368: step 40060, loss = 2.08 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:10.369277: step 40070, loss = 2.12 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:11.550773: step 40080, loss = 2.03 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:12.755666: step 40090, loss = 1.94 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:09:13.935327: step 40100, loss = 1.91 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:15.131365: step 40110, loss = 2.02 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:09:16.327214: step 40120, loss = 1.83 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:09:17.528436: step 40130, loss = 1.91 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:09:18.720734: step 40140, loss = 1.92 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:19.899965: step 40150, loss = 2.08 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:21.095346: step 40160, loss = 2.28 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:09:22.257397: step 40E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 421 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
170, loss = 2.05 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:09:23.418932: step 40180, loss = 2.05 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:09:24.582544: step 40190, loss = 1.96 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:09:25.733577: step 40200, loss = 2.10 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-04 23:09:26.904313: step 40210, loss = 1.99 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:28.077867: step 40220, loss = 2.02 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:29.257505: step 40230, loss = 2.06 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:30.403060: step 40240, loss = 2.02 (1117.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:09:31.582808: step 40250, loss = 2.24 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:32.769074: step 40260, loss = 1.92 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:33.938644: step 40270, loss = 1.89 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:35.095178: step 40280, loss = 2.13 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:09:36.289303: step 40290, loss = 2.15 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:37.491793: step 40300, loss = 1.95 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:09:38.659902: step 40310, loss = 1.88 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:39.853615: step 40320, loss = 2.13 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:41.031524: step 40330, loss = 1.94 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:42.240993: step 40340, loss = 1.85 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:09:43.437137: step 40350, loss = 2.05 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:09:44.610166: step 40360, loss = 1.88 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:45.774293: step 40370, loss = 2.13 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:09:46.942390: step 40380, loss = 1.87 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:09:48.127103: step 40390, loss = 2.19 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:49.325549: step 40400, loss = 2.12 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:09:50.505095: step 40410, loss = 1.95 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:51.700651: step 40420, loss = 2.24 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:09:52.896539: step 40430, loss = 2.06 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:09:54.089719: step 40440, loss = 2.27 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:55.277011: step 40450, loss = 2.19 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:09:56.474223: step 40460, loss = 2.01 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:09:57.649276: step 40470, loss = 2.02 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:09:58.857470: step 40480, loss = 2.06 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:10:00.077437: step 40490, loss = 1.91 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:10:01.275769: step 40500, loss = 2.12 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:10:02.478600: step 40510, loss = 2.03 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:10:03.656917: step 40520, loss = 1.94 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:04.839953: step 40530, loss = 1.98 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:06.005000: step 40540, loss = 1.89 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:07.183182: step 40550, loss = 2.05 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:08.349878: step 40560, loss = 2.19 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:09.514729: step 40570, loss = 2.19 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:10.662199: step 40580, loss = 2.19 (1115.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:10:11.928623: step 40590, loss = 2.05 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-04 23:10:12.995505: step 40600, loss = 1.93 (1199.8 examples/sec; 0.107 sec/batch)
2017-05-04 23:10:14.167432: step 40610, loss = 2.06 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:15.337813: step 40620, loss = 2.08 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:16.498670: step 40630, loss = 2.01 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:17.678528: step 40640, loss = 1.97 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:18.838916: step 40650, loss = 2.17 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:19.989680: step 40660, loss = 2.09 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:10:21.169306: step 40670, loss = 2.19 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:22.330987: step 40680, loss = 2.06 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:23.512064: step 40690, loss = 1.92 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:24.670750: step 40700, loss = 2.07 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:25.832522: step 40710, loss = 2.11 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:27.003124: step 40720, loss = 2.06 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:28.161885: step 40730, loss = 1.85 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:29.328621: step 40740, loss = 2.19 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:30.493927: step 40750, loss = 2.02 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:31.659825: step 40760, loss = 2.26 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:32.809115: step 40770, loss = 2.07 (1113.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:10:33.990516: step 40780, loss = 2.00 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:35.177085: step 40790, loss = 2.03 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:10:36.348562: step 40800, loss = 1.93 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:37.507515: step 40810, loss = 2.14 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:38.663091: step 40820, loss = 2.02 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:39.834703: step 40830, loss = 2.00 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:41.003888: step 40840, loss = 2.14 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:42.171189: step 40850, loss = 1.99 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:43.349212: step 40860, loss = 2.10 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:44.538537: step 40870, loss = 1.95 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:10:45.706432: step 40880, loss = 2.03 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:46.894755: step 40890, loss = 2.13 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:10:48.070521: step 40900, loss = 2.16 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:49.231406: step 40910, loss = 2.08 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:50.408873: step 40920, loss = 2.00 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:51.572352: step 40930, loss = 1.88 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:52.732341: step 40940, loss = 2.05 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:53.883297: step 40950, loss = 1.98 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:10:55.068033: step 40960, loss = 1.90 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:56.241361: step 40970, loss = 1.91 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:10:57.422737: step 40980, loss = 2.14 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:10:58.586969: step 40990, loss = 2.06 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:10:59.777884: step 41000, loss = 1.98 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:11:00.942534: step 41010, loss = 2.06 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:02.093330: step 41020, loss = 2.01 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:11:03.284993: step 41030, loss = 1.95 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:11:04.448341: step 41040, loss = 1.98 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:05.601252: step 41050, loss = 1.93 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:11:06.764595: step 41060, loss = 2.06 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:07.913253: step 41070, loss = 1.91 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:11:09.077829: step 41080, loss = 2.06 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:10.242179: step 41090, loss = 2.02 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:11.415023: step 41100, loss = 2.05 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:12.608852: step 41110, loss = 1.87 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:11:13.767966: step 41120, loss = 2.05 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:14.949101: step 41130, loss = 2.15 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:11:16.106161: step 41140, loss = 1.93 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:17.271331: step 41150, loss = 1.91 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:18.425978: step 41160, loss = 2.06 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:11:19.609165: step 41170, loss = 1.94 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:11:20.768751: step 41180, loss = 2.02 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:21.933321: step 41190, loss = 2.16 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:23.087294: step 41200, loss = 2.01 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-04 23:11:24.257185: step 41210, loss = 1.79 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:25.424782: step 41220, loss = 2.05 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:26.610987: step 41230, loss = 2.09 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:11:27.784211: step 41240, loss = 2.07 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:28.968613: step 41250, loss = 2.01 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:11:30.116440: step 41260, loss = 1.98 (1115.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:11:31.276480: step 41270, loss = 1.88 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:32.448050: step 41280, loss = 1.95 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:33.610505: step 41290, loss = 1.93 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:34.776631: step 41300, loss = 2.07 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:35.960657: step 41310, loss = 1.99 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:11:37.118607: step 41320, loss = 2.14 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:38.291198: step 41330, loss = 2.00 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:39.456311: step 41340, loss = 2.09 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:40.627198: step 41350, loss = 2.10 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:41.789538: step 41360, loss = 2.03 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:42.980462: step 41370, loss = 2.05 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:11:44.169891: step 41380, loss = 1.89 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:11:45.343343: step 41390, loss = 2.04 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:46.499782: step 41400, loss = 2.05 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:47.675644: step 41410, loss = 2.06 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:11:48.842511: step 41420, loss = 1.93 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:49.995429: step 41430, loss = 2.08 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-04 23:11:51.175878: step 41440, loss = 2.04 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:11:52.344664: step 41450, loss = 2.21 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:53.516572: step 41460, loss = 2.05 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:11:54.672524: step 41470, loss = 1.86 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:55.820028: step 41480, loss = 2.07 (1115.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:11:57.007019: step 41490, loss = 1.99 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:11:58.170126: step 41500, loss = 1.88 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:11:59.355128: step 41510, loss = 2.13 (1080.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:12:00.531048: step 41520E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 433 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
, loss = 2.08 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:12:01.690421: step 41530, loss = 1.98 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:12:02.867366: step 41540, loss = 1.86 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:12:04.038369: step 41550, loss = 2.00 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:12:05.199833: step 41560, loss = 1.97 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:12:06.362021: step 41570, loss = 2.17 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:12:07.616810: step 41580, loss = 1.92 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-04 23:12:08.703531: step 41590, loss = 1.98 (1177.8 examples/sec; 0.109 sec/batch)
2017-05-04 23:12:09.877350: step 41600, loss = 1.84 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:12:11.051111: step 41610, loss = 2.06 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:12:12.225764: step 41620, loss = 1.79 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:12:13.389814: step 41630, loss = 1.93 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:12:14.606681: step 41640, loss = 1.98 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:15.823958: step 41650, loss = 2.13 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:17.038722: step 41660, loss = 2.04 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:18.257604: step 41670, loss = 1.94 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:19.461525: step 41680, loss = 2.07 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:12:20.695175: step 41690, loss = 1.98 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:12:21.932926: step 41700, loss = 2.02 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:12:23.131522: step 41710, loss = 2.08 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:12:24.342097: step 41720, loss = 2.17 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:25.568356: step 41730, loss = 1.94 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:12:26.768451: step 41740, loss = 2.00 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:12:27.990566: step 41750, loss = 1.96 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:29.198293: step 41760, loss = 2.15 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:30.432897: step 41770, loss = 2.12 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:12:31.720871: step 41780, loss = 1.91 (993.8 examples/sec; 0.129 sec/batch)
2017-05-04 23:12:32.853037: step 41790, loss = 2.03 (1130.6 examples/sec; 0.113 sec/batch)
2017-05-04 23:12:34.053772: step 41800, loss = 1.92 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:12:35.259477: step 41810, loss = 2.01 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:36.480502: step 41820, loss = 1.87 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:37.703095: step 41830, loss = 2.07 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:38.912469: step 41840, loss = 2.03 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:40.112702: step 41850, loss = 2.21 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:12:41.323770: step 41860, loss = 2.00 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:42.545116: step 41870, loss = 2.16 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:43.768091: step 41880, loss = 2.05 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:44.990711: step 41890, loss = 2.09 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:46.201715: step 41900, loss = 2.01 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:47.420384: step 41910, loss = 2.04 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:48.630207: step 41920, loss = 2.01 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:49.841965: step 41930, loss = 1.94 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:51.051615: step 41940, loss = 2.11 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:52.268220: step 41950, loss = 1.99 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:53.484995: step 41960, loss = 1.90 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:54.734366: step 41970, loss = 1.93 (1024.5 examples/sec; 0.125 sec/batch)
2017-05-04 23:12:55.892911: step 41980, loss = 1.84 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:12:57.116464: step 41990, loss = 1.99 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:12:58.330525: step 42000, loss = 1.98 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:12:59.552121: step 42010, loss = 2.01 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:13:00.761961: step 42020, loss = 1.93 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:13:01.979779: step 42030, loss = 2.07 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:13:03.201512: step 42040, loss = 1.99 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:13:04.402353: step 42050, loss = 1.94 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:13:05.618422: step 42060, loss = 1.96 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:13:06.817137: step 42070, loss = 2.03 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:13:08.026301: step 42080, loss = 1.87 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:13:09.231548: step 42090, loss = 1.82 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:13:10.405893: step 42100, loss = 1.92 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:11.621074: step 42110, loss = 1.94 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:13:12.834692: step 42120, loss = 2.08 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:13:14.031044: step 42130, loss = 1.84 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:13:15.241367: step 42140, loss = 2.00 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:13:16.440382: step 42150, loss = 1.90 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:13:17.625604: step 42160, loss = 2.00 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:13:18.810773: step 42170, loss = 2.06 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:13:19.983815: step 42180, loss = 1.97 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:21.148043: step 42190, loss = 2.04 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:13:22.317839: step 42200, loss = 2.03 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:23.489548: step 42210, loss = 2.01 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:24.658073: step 42220, loss = 2.14 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:25.813085: step 42230, loss = 2.05 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:13:26.989929: step 42240, loss = 1.95 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:13:28.162214: step 42250, loss = 1.91 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:29.347960: step 42260, loss = 2.05 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:13:30.511323: step 42270, loss = 2.08 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:13:31.671803: step 42280, loss = 2.00 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:13:32.827532: step 42290, loss = 1.93 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:13:34.002838: step 42300, loss = 1.93 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:13:35.168740: step 42310, loss = 1.99 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:36.343276: step 42320, loss = 2.08 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:37.529809: step 42330, loss = 1.96 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:13:38.674051: step 42340, loss = 2.11 (1118.6 examples/sec; 0.114 sec/batch)
2017-05-04 23:13:39.832025: step 42350, loss = 2.09 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:13:40.988333: step 42360, loss = 1.93 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:13:42.149628: step 42370, loss = 1.98 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:13:43.335216: step 42380, loss = 2.01 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:13:44.536437: step 42390, loss = 1.94 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:13:45.704498: step 42400, loss = 1.99 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:46.907676: step 42410, loss = 1.92 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:13:48.105642: step 42420, lE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 444 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
oss = 1.98 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:13:49.304481: step 42430, loss = 2.06 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:13:50.469504: step 42440, loss = 1.90 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:51.636504: step 42450, loss = 2.10 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:52.796687: step 42460, loss = 1.88 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:13:53.967790: step 42470, loss = 1.87 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:55.151331: step 42480, loss = 1.90 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:13:56.328672: step 42490, loss = 1.94 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:13:57.496112: step 42500, loss = 2.08 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:13:58.686979: step 42510, loss = 2.05 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:13:59.852576: step 42520, loss = 2.02 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:01.044573: step 42530, loss = 1.96 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:14:02.214770: step 42540, loss = 2.04 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:03.387433: step 42550, loss = 2.04 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:04.563671: step 42560, loss = 2.00 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:05.823205: step 42570, loss = 1.90 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-04 23:14:06.886284: step 42580, loss = 1.87 (1204.1 examples/sec; 0.106 sec/batch)
2017-05-04 23:14:08.054735: step 42590, loss = 1.88 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:09.221927: step 42600, loss = 2.12 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:10.378499: step 42610, loss = 1.99 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:11.551615: step 42620, loss = 1.91 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:12.719618: step 42630, loss = 2.07 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:13.862491: step 42640, loss = 2.06 (1120.0 examples/sec; 0.114 sec/batch)
2017-05-04 23:14:15.033466: step 42650, loss = 2.06 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:16.202651: step 42660, loss = 2.17 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:17.354857: step 42670, loss = 2.20 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:14:18.534652: step 42680, loss = 2.07 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:19.702983: step 42690, loss = 1.96 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:20.867677: step 42700, loss = 1.99 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:22.012568: step 42710, loss = 1.90 (1118.0 examples/sec; 0.114 sec/batch)
2017-05-04 23:14:23.180001: step 42720, loss = 2.03 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:24.344053: step 42730, loss = 1.82 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:25.504516: step 42740, loss = 2.00 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:26.670137: step 42750, loss = 1.96 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:27.852326: step 42760, loss = 2.13 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:29.032661: step 42770, loss = 1.91 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:30.157016: step 42780, loss = 2.29 (1138.4 examples/sec; 0.112 sec/batch)
2017-05-04 23:14:31.317716: step 42790, loss = 2.00 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:32.494027: step 42800, loss = 2.10 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:33.652728: step 42810, loss = 2.10 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:34.825187: step 42820, loss = 2.07 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:35.990206: step 42830, loss = 1.80 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:37.155342: step 42840, loss = 2.02 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:38.339636: step 42850, loss = 2.13 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:39.515524: step 42860, loss = 2.09 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:40.694674: step 42870, loss = 2.24 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:41.873422: step 42880, loss = 2.02 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:14:43.085807: step 42890, loss = 1.88 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:14:44.277647: step 42900, loss = 2.04 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:14:45.447085: step 42910, loss = 2.03 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:46.611724: step 42920, loss = 2.19 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:14:47.810323: step 42930, loss = 2.07 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:14:49.018523: step 42940, loss = 2.28 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:14:50.187772: step 42950, loss = 2.04 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:51.360505: step 42960, loss = 2.10 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:14:52.547373: step 42970, loss = 2.00 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:14:53.735718: step 42980, loss = 1.88 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:14:54.927109: step 42990, loss = 1.87 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:14:56.125401: step 43000, loss = 2.04 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:14:57.347126: step 43010, loss = 2.07 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:14:58.544321: step 43020, loss = 2.03 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:14:59.757938: step 43030, loss = 2.08 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:15:00.963766: step 43040, loss = 2.15 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:15:02.162929: step 43050, loss = 2.11 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:15:03.376885: step 43060, loss = 2.02 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:15:04.579912: step 43070, loss = 2.07 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:15:05.766487: step 43080, loss = 2.02 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:15:06.976387: step 43090, loss = 2.00 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:15:08.195330: step 43100, loss = 2.07 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:15:09.398786: step 43110, loss = 1.82 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:15:10.622464: step 43120, loss = 1.89 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:15:11.840653: step 43130, loss = 1.99 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:15:13.067370: step 43140, loss = 2.08 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:15:14.273618: step 43150, loss = 1.87 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:15:15.487517: step 43160, loss = 1.97 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:15:16.668982: step 43170, loss = 1.96 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:17.843899: step 43180, loss = 1.98 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:19.034276: step 43190, loss = 2.16 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:15:20.227247: step 43200, loss = 2.05 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:15:21.412808: step 43210, loss = 2.07 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:15:22.605367: step 43220, loss = 2.06 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:15:23.778042: step 43230, loss = 1.96 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:24.970657: step 43240, loss = 2.04 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:15:26.138720: step 43250, loss = 1.97 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:27.319553: step 43260, loss = 1.99 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:28.491271: step 43270, loss = 1.98 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:29.656964: step 43280, loss = 2.03 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:30.834726: step 43290, loss = 1.93 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:31.998021: step 43300, loss = 2.03 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:15:33.172007: step 43310, loss = 2.00 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:34.329586: step 43320, loss = 2.19 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:15:35.511966: step 43330, loss = 2.08 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:36.673734: step 43340, loss = 1.93 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:15:37.847193: step 43350, loss = 1.99 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:39.014505: step 43360, loss = 1.88 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:40.198500: step 43370, loss = 2.12 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:41.366090: step 43380, loss = 1.89 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:42.532811: step 43390, loss = 2.02 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:43.691386: step 43400, loss = 2.04 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:15:44.867788: step 43410, loss = 2.22 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:46.030793: step 43420, loss = 2.04 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:15:47.225204: step 43430, loss = 1.92 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:15:48.382671: step 43440, loss = 1.92 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:15:49.530865: step 43450, loss = 2.18 (1114.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:15:50.690304: step 43460, loss = 2.00 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:15:51.856909: step 43470, loss = 1.90 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:53.012953: step 43480, loss = 2.15 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:15:54.196543: step 43490, loss = 1.92 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:15:55.369498: step 43500, loss = 1.94 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:56.536037: step 43510, loss = 2.02 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:15:57.694903: step 43520, loss = 2.12 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:15:58.882136: step 43530, loss = 1.98 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:16:00.055293: step 43540, loss = 2.00 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:01.214378: step 43550, loss = 2.16 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:02.466793: step 43560, loss = 1.92 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-04 23:16:03.532384: step 43570, loss = 1.97 (1201.2 examples/sec; 0.107 sec/batch)
2017-05-04 23:16:04.699703: step 43580, loss = 2.26 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:05.853134: step 43590, loss = 1.91 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:16:07.014444: step 43600, loss = 2.05 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:08.189383: step 43610, loss = 1.95 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:09.360527: step 43620, loss = 1.93 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:10.509918: step 43630, loss = 1.99 (1113.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:16:11.681858: step 43640, loss = 2.10 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:12.859899: step 43650, loss = 2.09 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:16:14.020310: step 43660, loss = 2.00 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:15.173571: step 43670, loss = 1.87 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:16:16.351333: step 43680, loss = 2.14 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:16:17.521014: step 43690, loss = 1.95 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:18.671696: step 43700, loss = 2.03 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:16:19.825780: step 43710, loss = 1.97 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:16:21.003586: step 43720, loss = 2.05 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:16:22.165186: step 43730, loss = 2.16 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:23.356636: step 43740, loss = 2.00 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:16:24.505070: step 43750, loss = 2.06 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:16:25.690996: step 43760, loss = 2.08 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:16:26.856900: step 43770, lossE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 455 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
 = 2.05 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:28.022455: step 43780, loss = 2.07 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:29.194533: step 43790, loss = 2.06 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:30.376171: step 43800, loss = 1.86 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:16:31.570983: step 43810, loss = 1.91 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:16:32.729146: step 43820, loss = 2.21 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:33.893266: step 43830, loss = 2.10 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:35.065593: step 43840, loss = 1.92 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:36.234009: step 43850, loss = 2.11 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:37.401576: step 43860, loss = 2.02 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:38.563740: step 43870, loss = 1.98 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:39.723757: step 43880, loss = 2.03 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:40.881157: step 43890, loss = 1.94 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:42.036587: step 43900, loss = 1.98 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:43.234320: step 43910, loss = 1.95 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:16:44.408277: step 43920, loss = 1.99 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:45.554369: step 43930, loss = 1.97 (1116.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:16:46.736996: step 43940, loss = 1.93 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:16:47.908980: step 43950, loss = 1.99 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:49.075332: step 43960, loss = 1.95 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:50.241612: step 43970, loss = 2.17 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:51.401723: step 43980, loss = 1.99 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:52.575161: step 43990, loss = 2.09 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:53.731769: step 44000, loss = 1.89 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:16:54.910403: step 44010, loss = 2.09 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:16:56.091832: step 44020, loss = 2.02 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:16:57.279356: step 44030, loss = 2.00 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:16:58.447300: step 44040, loss = 2.10 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:16:59.618550: step 44050, loss = 2.03 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:00.774655: step 44060, loss = 2.01 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:17:01.932716: step 44070, loss = 1.84 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:17:03.088934: step 44080, loss = 2.09 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:17:04.257857: step 44090, loss = 2.03 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:05.418168: step 44100, loss = 2.01 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:17:06.584760: step 44110, loss = 1.95 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:07.761423: step 44120, loss = 2.11 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:08.924969: step 44130, loss = 1.86 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:17:10.086082: step 44140, loss = 2.11 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:17:11.257945: step 44150, loss = 2.02 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:12.428441: step 44160, loss = 1.91 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:13.601711: step 44170, loss = 1.95 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:14.751182: step 44180, loss = 2.03 (1113.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:17:15.909305: step 44190, loss = 2.10 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:17:17.092692: step 44200, loss = 1.90 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:18.250881: step 44210, loss = 2.00 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:17:19.432453: step 44220, loss = 2.09 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:20.613112: step 44230, loss = 2.00 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:21.792188: step 44240, loss = 2.05 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:22.982430: step 44250, loss = 2.12 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:17:24.172005: step 44260, loss = 1.91 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:17:25.373864: step 44270, loss = 2.10 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:17:26.552501: step 44280, loss = 2.16 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:27.746683: step 44290, loss = 2.07 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:17:28.931987: step 44300, loss = 2.05 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:17:30.101597: step 44310, loss = 2.16 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:17:31.280163: step 44320, loss = 1.96 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:32.458600: step 44330, loss = 1.95 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:33.617856: step 44340, loss = 1.92 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:17:34.794429: step 44350, loss = 2.11 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:35.986016: step 44360, loss = 1.96 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:17:37.164531: step 44370, loss = 1.95 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:38.322011: step 44380, loss = 1.99 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:17:39.497870: step 44390, loss = 1.95 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:17:40.683795: step 44400, loss = 2.08 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:17:41.879386: step 44410, loss = 1.94 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:17:43.071784: step 44420, loss = 2.14 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:17:44.286625: step 44430, loss = 1.92 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:17:45.495912: step 44440, loss = 2.10 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:17:46.701534: step 44450, loss = 1.94 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:17:47.932661: step 44460, loss = 2.09 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:17:49.162806: step 44470, loss = 2.10 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:17:50.361970: step 44480, loss = 2.04 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:17:51.573419: step 44490, loss = 2.03 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:17:52.805770: step 44500, loss = 2.11 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:17:54.023037: step 44510, loss = 2.24 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:17:55.239036: step 44520, loss = 2.11 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:17:56.469726: step 44530, loss = 1.99 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:17:57.680519: step 44540, loss = 1.95 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:17:58.985924: step 44550, loss = 1.94 (980.5 examples/sec; 0.131 sec/batch)
2017-05-04 23:18:00.112458: step 44560, loss = 2.05 (1136.2 examples/sec; 0.113 sec/batch)
2017-05-04 23:18:01.335129: step 44570, loss = 1.89 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:02.541572: step 44580, loss = 1.87 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:03.757204: step 44590, loss = 1.91 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:04.974004: step 44600, loss = 1.96 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:06.188060: step 44610, loss = 1.97 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:07.420870: step 44620, loss = 2.00 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:18:08.634399: step 44630, loss = 2.00 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:09.852135: step 44640, loss = 2.03 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:11.091255: step 44650, loss = 1.93 (1033.0 examples/sec; 0.124 sec/batch)
2017-05-04 23:18:12.338032: step 44660, loss = 2.08 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-04 23:18:13.544293: step 44670, loss = E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 467 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
1.98 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:14.761269: step 44680, loss = 2.15 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:15.970912: step 44690, loss = 2.03 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:17.183888: step 44700, loss = 2.14 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:18.398894: step 44710, loss = 1.88 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:19.614112: step 44720, loss = 2.02 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:20.819915: step 44730, loss = 1.97 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:22.015102: step 44740, loss = 2.03 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:18:23.205759: step 44750, loss = 1.98 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:18:24.417833: step 44760, loss = 1.90 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:25.616515: step 44770, loss = 1.81 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:18:26.798409: step 44780, loss = 1.94 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:18:27.996535: step 44790, loss = 1.85 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:18:29.211784: step 44800, loss = 2.02 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:30.405193: step 44810, loss = 1.96 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:18:31.608706: step 44820, loss = 1.97 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:18:32.806337: step 44830, loss = 2.02 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:18:33.995171: step 44840, loss = 2.10 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:18:35.243285: step 44850, loss = 1.96 (1025.5 examples/sec; 0.125 sec/batch)
2017-05-04 23:18:36.463032: step 44860, loss = 1.92 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:37.663332: step 44870, loss = 1.95 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:18:38.899150: step 44880, loss = 2.17 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-04 23:18:40.121155: step 44890, loss = 1.96 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:41.352380: step 44900, loss = 1.90 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:18:42.583526: step 44910, loss = 1.90 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:18:43.794367: step 44920, loss = 2.11 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:45.009619: step 44930, loss = 1.91 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:46.214825: step 44940, loss = 2.03 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:47.391823: step 44950, loss = 2.06 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:18:48.609701: step 44960, loss = 1.86 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:49.820041: step 44970, loss = 1.90 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:51.036504: step 44980, loss = 2.00 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:52.245544: step 44990, loss = 1.96 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:18:53.461380: step 45000, loss = 1.93 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:18:54.620331: step 45010, loss = 2.07 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:18:55.815542: step 45020, loss = 1.90 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:18:56.983042: step 45030, loss = 2.07 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:18:58.164276: step 45040, loss = 1.82 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:18:59.335170: step 45050, loss = 2.09 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:00.537255: step 45060, loss = 2.05 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:19:01.709455: step 45070, loss = 2.05 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:02.898669: step 45080, loss = 1.84 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:04.087190: step 45090, loss = 2.06 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:05.262909: step 45100, loss = 1.97 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:06.426715: step 45110, loss = 2.02 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:19:07.603882: step 45120, loss = 1.90 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:08.782103: step 45130, loss = 1.95 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:09.955456: step 45140, loss = 2.06 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:11.157785: step 45150, loss = 1.94 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:19:12.332101: step 45160, loss = 2.03 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:13.504581: step 45170, loss = 2.01 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:14.650719: step 45180, loss = 2.15 (1116.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:19:15.819449: step 45190, loss = 2.11 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:16.992271: step 45200, loss = 2.03 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:18.153633: step 45210, loss = 2.00 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:19:19.321115: step 45220, loss = 1.88 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:20.491979: step 45230, loss = 1.89 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:21.676540: step 45240, loss = 1.95 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:22.825439: step 45250, loss = 2.09 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:19:24.007407: step 45260, loss = 2.06 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:25.167104: step 45270, loss = 1.95 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:19:26.317434: step 45280, loss = 1.97 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:19:27.483309: step 45290, loss = 1.87 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:28.648118: step 45300, loss = 1.92 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:19:29.802809: step 45310, loss = 2.21 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:19:30.971930: step 45320, loss = 2.05 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:32.132841: step 45330, loss = 1.89 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:19:33.294306: step 45340, loss = 1.95 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:19:34.472937: step 45350, loss = 1.98 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:35.637879: step 45360, loss = 2.12 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:19:36.828602: step 45370, loss = 1.92 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:37.997495: step 45380, loss = 2.06 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:39.196993: step 45390, loss = 1.97 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:19:40.384517: step 45400, loss = 2.17 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:41.551982: step 45410, loss = 2.10 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:42.730853: step 45420, loss = 2.18 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:43.901585: step 45430, loss = 2.02 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:45.071491: step 45440, loss = 2.00 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:46.245692: step 45450, loss = 1.90 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:19:47.422323: step 45460, loss = 1.89 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:48.611692: step 45470, loss = 2.04 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:49.771767: step 45480, loss = 1.86 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:19:50.956335: step 45490, loss = 1.96 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:52.132599: step 45500, loss = 1.92 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:53.311167: step 45510, loss = 2.11 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:19:54.502820: step 45520, loss = 2.18 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:55.688608: step 45530, loss = 2.09 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:19:56.947476: step 45540, loss = 2.02 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-04 23:19:57.998803: step 45550, loss = 1.83 (1217.5 examples/sec; 0.105 sec/batch)
2017-05-04 23:19:59.180511: step 45560, loss = 2.21 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:00.334576: step 45570, loss = 2.01 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:20:01.490033: step 45580, loss = 1.89 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:02.647115: step 45590, loss = 2.01 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:03.816466: step 45600, loss = 2.11 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:04.996433: step 45610, loss = 1.97 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:06.155944: step 45620, loss = 1.96 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:07.325849: step 45630, loss = 2.00 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:08.491317: step 45640, loss = 1.86 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:09.634717: step 45650, loss = 1.96 (1119.5 examples/sec; 0.114 sec/batch)
2017-05-04 23:20:10.802708: step 45660, loss = 2.03 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:11.957690: step 45670, loss = 1.81 (1108.2 examples/sec; 0.115 sec/batch)
2017-05-04 23:20:13.109451: step 45680, loss = 2.04 (1111.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:20:14.290340: step 45690, loss = 2.02 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:15.453562: step 45700, loss = 1.90 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:16.632457: step 45710, loss = 2.13 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:17.791050: step 45720, loss = 2.00 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:18.950532: step 45730, loss = 2.03 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:20.132156: step 45740, loss = 1.92 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:21.276401: step 45750, loss = 1.97 (1118.6 examples/sec; 0.114 sec/batch)
2017-05-04 23:20:22.447676: step 45760, loss = 1.98 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:23.599490: step 45770, loss = 1.96 (1111.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:20:24.765954: step 45780, loss = 1.97 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:25.935023: step 45790, loss = 2.05 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:27.094962: step 45800, loss = 2.18 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:28.268613: step 45810, loss = 2.06 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:29.420340: step 45820, loss = 2.02 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:20:30.573108: step 45830, loss = 2.04 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:20:31.746158: step 45840, loss = 1.90 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:32.933775: step 45850, loss = 1.96 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:20:34.110619: step 45860, loss = 1.93 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:35.289464: step 45870, loss = 1.81 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:36.457547: step 45880, loss = 2.05 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:37.617368: step 45890, loss = 2.04 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:38.784563: step 45900, loss = 2.02 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:39.947976: step 45910, loss = 1.88 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:41.104870: step 45920, loss = 2.07 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:42.268676: step 45930, loss = 1.94 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:20:43.447207: step 45940, loss = 2.05 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:44.625393: step 45950, loss = 2.07 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:20:45.777589: step 45960, loss = 2.09 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:20:46.969023: step 45970, loss = 2.00 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:20:48.136907: step 45980, loss = 1.89 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:49.303446: step 45990, loss = 2.04 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:50.478070: step 46000, loss = 2.06 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:20:51.693451: step 46010, loss = 2.16 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:20:52.899782: step 46020, loss = 1.8E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 478 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
2 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:20:54.113949: step 46030, loss = 2.02 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:20:55.330170: step 46040, loss = 2.09 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:20:56.533876: step 46050, loss = 1.96 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:20:57.739085: step 46060, loss = 2.15 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:20:58.944007: step 46070, loss = 2.03 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:21:00.166699: step 46080, loss = 1.85 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:21:01.370385: step 46090, loss = 2.10 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:21:02.553422: step 46100, loss = 2.11 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:03.739783: step 46110, loss = 1.98 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:21:04.943017: step 46120, loss = 2.04 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:21:06.119178: step 46130, loss = 1.95 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:07.323958: step 46140, loss = 2.04 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:21:08.504971: step 46150, loss = 1.87 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:09.667965: step 46160, loss = 2.21 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:10.851196: step 46170, loss = 1.84 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:12.018974: step 46180, loss = 1.99 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:21:13.199268: step 46190, loss = 2.11 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:14.374402: step 46200, loss = 2.02 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:15.533562: step 46210, loss = 2.03 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:16.693905: step 46220, loss = 1.98 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:17.849500: step 46230, loss = 1.94 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:19.030082: step 46240, loss = 1.90 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:20.191971: step 46250, loss = 2.00 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:21.354340: step 46260, loss = 2.11 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:22.519085: step 46270, loss = 2.03 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:23.670705: step 46280, loss = 2.08 (1111.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:21:24.841422: step 46290, loss = 1.97 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:21:25.994985: step 46300, loss = 2.11 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:21:27.155424: step 46310, loss = 1.93 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:28.326893: step 46320, loss = 1.96 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:21:29.502910: step 46330, loss = 2.10 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:30.685633: step 46340, loss = 2.00 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:31.855675: step 46350, loss = 2.10 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:21:33.036114: step 46360, loss = 2.00 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:34.177942: step 46370, loss = 1.99 (1121.0 examples/sec; 0.114 sec/batch)
2017-05-04 23:21:35.353994: step 46380, loss = 1.95 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:36.520707: step 46390, loss = 1.90 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:21:37.684848: step 46400, loss = 1.88 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:38.847526: step 46410, loss = 2.04 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:40.003883: step 46420, loss = 2.13 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:21:41.170411: step 46430, loss = 2.04 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:21:42.351471: step 46440, loss = 2.02 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:43.518288: step 46450, loss = 1.96 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:21:44.698127: step 46460, loss = 1.94 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:21:45.897808: step 46470, loss = 2.02 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:21:47.084471: step 46480, loss = 2.10 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:21:48.282181: step 46490, loss = 1.96 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:21:49.475917: step 46500, loss = 2.11 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:21:50.650216: step 46510, loss = 1.97 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:21:51.847998: step 46520, loss = 1.98 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:21:53.163436: step 46530, loss = 2.09 (973.1 examples/sec; 0.132 sec/batch)
2017-05-04 23:21:54.224975: step 46540, loss = 2.18 (1205.8 examples/sec; 0.106 sec/batch)
2017-05-04 23:21:55.440915: step 46550, loss = 2.00 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:21:56.648934: step 46560, loss = 1.89 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:21:57.846294: step 46570, loss = 1.89 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:21:59.054450: step 46580, loss = 2.06 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:22:00.241688: step 46590, loss = 2.09 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:22:01.436595: step 46600, loss = 2.08 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:22:02.638161: step 46610, loss = 1.99 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:22:03.833878: step 46620, loss = 2.08 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:22:05.007160: step 46630, loss = 1.97 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:06.166528: step 46640, loss = 2.15 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:07.369106: step 46650, loss = 1.92 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:22:08.531462: step 46660, loss = 2.02 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:09.692727: step 46670, loss = 1.95 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:10.893034: step 46680, loss = 2.00 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:22:12.077658: step 46690, loss = 2.02 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:13.261192: step 46700, loss = 1.97 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:14.450593: step 46710, loss = 1.98 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:22:15.631009: step 46720, loss = 2.02 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:16.801983: step 46730, loss = 2.04 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:17.946540: step 46740, loss = 1.99 (1118.3 examples/sec; 0.114 sec/batch)
2017-05-04 23:22:19.120680: step 46750, loss = 1.89 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:20.297196: step 46760, loss = 1.91 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:21.466700: step 46770, loss = 1.98 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:22.647438: step 46780, loss = 2.22 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:23.821765: step 46790, loss = 2.01 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:24.987940: step 46800, loss = 1.94 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:26.138623: step 46810, loss = 2.04 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:22:27.302149: step 46820, loss = 1.96 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:28.477134: step 46830, loss = 2.02 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:29.640345: step 46840, loss = 1.81 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:30.818797: step 46850, loss = 2.03 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:31.970029: step 46860, loss = 2.05 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:22:33.142770: step 46870, loss = 1.96 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:34.298178: step 46880, loss = 2.07 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:35.471479: step 46890, loss = 1.97 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:36.657001: step 46900, loss = 2.08 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:22:37.819710: step 46910, loss = 2.02 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:38.967977: step 46920, loss = 1.95 (E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 489 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
1114.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:22:40.171530: step 46930, loss = 2.18 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:22:41.342105: step 46940, loss = 1.93 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:42.507272: step 46950, loss = 1.91 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:43.668933: step 46960, loss = 2.04 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:44.850734: step 46970, loss = 1.97 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:46.018719: step 46980, loss = 2.11 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:47.178687: step 46990, loss = 2.01 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:48.339707: step 47000, loss = 1.87 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:49.509999: step 47010, loss = 1.93 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:50.671524: step 47020, loss = 1.98 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:51.828123: step 47030, loss = 1.87 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:22:52.996240: step 47040, loss = 1.94 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:54.163031: step 47050, loss = 1.86 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:22:55.340527: step 47060, loss = 1.95 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:22:56.525644: step 47070, loss = 2.18 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:22:57.671493: step 47080, loss = 2.01 (1117.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:22:58.824845: step 47090, loss = 2.02 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:22:59.993439: step 47100, loss = 2.02 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:01.163108: step 47110, loss = 1.99 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:02.312509: step 47120, loss = 1.86 (1113.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:23:03.495999: step 47130, loss = 1.90 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:23:04.650912: step 47140, loss = 1.92 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:23:05.793992: step 47150, loss = 2.02 (1119.8 examples/sec; 0.114 sec/batch)
2017-05-04 23:23:06.953981: step 47160, loss = 2.02 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:08.124514: step 47170, loss = 1.95 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:09.300755: step 47180, loss = 1.98 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:23:10.457406: step 47190, loss = 2.20 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:11.626686: step 47200, loss = 2.09 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:12.821605: step 47210, loss = 1.99 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:23:13.987912: step 47220, loss = 2.11 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:15.163394: step 47230, loss = 2.01 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:23:16.316406: step 47240, loss = 2.12 (1110.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:23:17.478899: step 47250, loss = 1.99 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:18.638964: step 47260, loss = 1.79 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:19.786628: step 47270, loss = 1.98 (1115.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:23:20.955427: step 47280, loss = 1.93 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:22.106495: step 47290, loss = 2.26 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-04 23:23:23.262181: step 47300, loss = 2.04 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:24.431921: step 47310, loss = 1.91 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:25.588712: step 47320, loss = 2.01 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:26.747849: step 47330, loss = 2.11 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:27.914472: step 47340, loss = 1.92 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:29.091595: step 47350, loss = 1.83 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:23:30.274034: step 47360, loss = 1.95 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:23:31.437574: step 47370, loss = 1.88 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:32.607739: step 47380, loss = 1.84 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:33.773013: step 47390, loss = 1.99 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:34.950199: step 47400, loss = 2.07 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:23:36.113699: step 47410, loss = 1.88 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:37.286697: step 47420, loss = 1.99 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:38.462479: step 47430, loss = 1.92 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:23:39.636710: step 47440, loss = 2.13 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:40.840482: step 47450, loss = 1.98 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:23:41.980002: step 47460, loss = 1.98 (1123.3 examples/sec; 0.114 sec/batch)
2017-05-04 23:23:43.169773: step 47470, loss = 2.01 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:23:44.336656: step 47480, loss = 2.02 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:45.523197: step 47490, loss = 2.01 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:23:46.697120: step 47500, loss = 1.98 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:47.860079: step 47510, loss = 2.02 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:49.145115: step 47520, loss = 2.06 (996.1 examples/sec; 0.129 sec/batch)
2017-05-04 23:23:50.228677: step 47530, loss = 2.04 (1181.3 examples/sec; 0.108 sec/batch)
2017-05-04 23:23:51.396724: step 47540, loss = 2.01 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:52.579866: step 47550, loss = 1.92 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:23:53.747367: step 47560, loss = 2.17 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:54.913746: step 47570, loss = 2.06 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:56.079949: step 47580, loss = 1.86 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:57.253115: step 47590, loss = 2.07 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:23:58.417015: step 47600, loss = 2.07 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:23:59.613448: step 47610, loss = 1.90 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:24:00.758288: step 47620, loss = 2.01 (1118.1 examples/sec; 0.114 sec/batch)
2017-05-04 23:24:01.920884: step 47630, loss = 2.01 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:24:03.090071: step 47640, loss = 2.04 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:04.272995: step 47650, loss = 1.87 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:05.449958: step 47660, loss = 2.01 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:06.609129: step 47670, loss = 2.02 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:24:07.776698: step 47680, loss = 2.00 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:08.958933: step 47690, loss = 2.00 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:10.136757: step 47700, loss = 1.94 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:11.326181: step 47710, loss = 2.17 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:12.542962: step 47720, loss = 2.10 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:24:13.675102: step 47730, loss = 1.99 (1130.6 examples/sec; 0.113 sec/batch)
2017-05-04 23:24:14.838101: step 47740, loss = 2.08 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:24:16.022072: step 47750, loss = 1.91 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:17.221543: step 47760, loss = 2.01 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:24:18.390828: step 47770, loss = 2.07 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:19.583491: step 47780, loss = 1.91 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:20.766846: step 47790, loss = 1.96 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:21.948844: step 47800, loss = 1.95 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:23.157015: step 47810, loss = 2.05 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:24:24.335347: step 47820, loss = 1.94 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:25.500699: step 47830, loss = 2.17 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:26.679008: step 47840, loss = 1.98 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:27.853533: step 47850, loss = 2.02 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:29.039567: step 47860, loss = 1.94 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:30.222667: step 47870, loss = 1.91 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:31.403543: step 47880, loss = 1.99 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:32.606678: step 47890, loss = 1.95 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:24:33.787677: step 47900, loss = 2.12 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:34.955286: step 47910, loss = 2.09 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:36.141847: step 47920, loss = 1.88 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:37.347176: step 47930, loss = 2.12 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:24:38.537149: step 47940, loss = 1.86 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:39.730130: step 47950, loss = 2.16 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:40.907695: step 47960, loss = 1.85 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:42.075799: step 47970, loss = 1.99 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:43.242930: step 47980, loss = 2.00 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:44.417689: step 47990, loss = 1.89 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:24:45.574753: step 48000, loss = 2.06 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:24:46.761344: step 48010, loss = 2.04 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:47.938092: step 48020, loss = 1.93 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:49.121470: step 48030, loss = 2.14 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:50.282742: step 48040, loss = 2.22 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:24:51.471264: step 48050, loss = 2.00 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:52.676720: step 48060, loss = 1.94 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:24:53.851745: step 48070, loss = 2.13 (1089.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:55.041181: step 48080, loss = 1.95 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:24:56.222031: step 48090, loss = 1.99 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:57.406127: step 48100, loss = 2.05 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:58.587988: step 48110, loss = 1.99 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:24:59.760931: step 48120, loss = 1.90 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:00.958426: step 48130, loss = 1.91 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:25:02.152426: step 48140, loss = 1.90 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:03.328192: step 48150, loss = 1.98 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:25:04.518403: step 48160, loss = 2.05 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:05.696165: step 48170, loss = 2.07 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:25:06.881384: step 48180, loss = 2.20 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:08.084176: step 48190, loss = 1.95 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:25:09.265015: step 48200, loss = 1.93 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:25:10.429059: step 48210, loss = 1.92 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:11.602333: step 48220, loss = 2.20 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:12.781647: step 48230, loss = 2.13 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:25:13.946591: step 48240, loss = 2.10 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:15.103827: step 48250, loss = 1.90 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:16.270176: step 48260, loss = 1.92 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:17.440827: step 48270, loss = 2.15 (1093E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 500 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:18.614602: step 48280, loss = 2.07 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:19.765894: step 48290, loss = 1.80 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:25:20.928641: step 48300, loss = 1.96 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:22.094387: step 48310, loss = 2.00 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:23.288381: step 48320, loss = 1.98 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:24.461453: step 48330, loss = 2.14 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:25.614214: step 48340, loss = 2.04 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:25:26.786177: step 48350, loss = 2.11 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:27.966784: step 48360, loss = 1.84 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:25:29.126342: step 48370, loss = 2.14 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:30.282084: step 48380, loss = 2.04 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:31.449572: step 48390, loss = 1.79 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:32.620706: step 48400, loss = 1.97 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:33.769409: step 48410, loss = 1.98 (1114.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:25:34.930860: step 48420, loss = 2.03 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:36.101837: step 48430, loss = 2.08 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:37.276136: step 48440, loss = 2.04 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:38.408289: step 48450, loss = 2.01 (1130.6 examples/sec; 0.113 sec/batch)
2017-05-04 23:25:39.593470: step 48460, loss = 1.82 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:40.755924: step 48470, loss = 1.97 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:41.922045: step 48480, loss = 1.97 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:43.085384: step 48490, loss = 2.18 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:25:44.276916: step 48500, loss = 1.98 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:45.555800: step 48510, loss = 2.01 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-04 23:25:46.634397: step 48520, loss = 1.93 (1186.7 examples/sec; 0.108 sec/batch)
2017-05-04 23:25:47.858120: step 48530, loss = 1.96 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:25:49.065063: step 48540, loss = 1.87 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:25:50.259430: step 48550, loss = 2.17 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:51.445924: step 48560, loss = 1.89 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:52.652555: step 48570, loss = 2.03 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:25:53.839395: step 48580, loss = 1.94 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:55.032447: step 48590, loss = 1.93 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:56.221764: step 48600, loss = 2.07 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:25:57.406534: step 48610, loss = 2.21 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:25:58.577105: step 48620, loss = 1.97 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:25:59.747668: step 48630, loss = 2.14 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:00.914040: step 48640, loss = 2.10 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:02.062435: step 48650, loss = 1.99 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:26:03.216352: step 48660, loss = 2.05 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:26:04.382151: step 48670, loss = 1.85 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:05.548458: step 48680, loss = 1.95 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:06.707558: step 48690, loss = 1.96 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:07.880618: step 48700, loss = 1.89 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:09.059622: step 48710, loss = 1.98 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:26:10.209765: step 48720, loss = 1.87 (1112.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:26:11.394916: step 48730, loss = 1.96 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:26:12.550399: step 48740, loss = 2.14 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:13.719503: step 48750, loss = 2.07 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:14.875308: step 48760, loss = 1.97 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:16.046745: step 48770, loss = 1.90 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:17.208536: step 48780, loss = 1.98 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:18.365294: step 48790, loss = 1.98 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:19.539157: step 48800, loss = 1.92 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:20.705615: step 48810, loss = 1.91 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:21.845982: step 48820, loss = 1.85 (1122.4 examples/sec; 0.114 sec/batch)
2017-05-04 23:26:23.007347: step 48830, loss = 2.04 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:24.159300: step 48840, loss = 1.90 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-04 23:26:25.320523: step 48850, loss = 2.09 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:26.477306: step 48860, loss = 1.98 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:27.666411: step 48870, loss = 2.20 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:26:28.850711: step 48880, loss = 2.14 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:26:30.012723: step 48890, loss = 2.04 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:31.174626: step 48900, loss = 1.98 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:32.344647: step 48910, loss = 1.99 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:33.504435: step 48920, loss = 1.94 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:34.666996: step 48930, loss = 1.85 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:35.828335: step 48940, loss = 1.96 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:37.029216: step 48950, loss = 1.99 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:26:38.165458: step 48960, loss = 2.14 (1126.5 examples/sec; 0.114 sec/batch)
2017-05-04 23:26:39.376022: step 48970, loss = 1.93 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:26:40.539083: step 48980, loss = 2.01 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:41.703762: step 48990, loss = 2.05 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:42.870645: step 49000, loss = 2.03 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:44.042524: step 49010, loss = 1.95 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:45.229144: step 49020, loss = 1.88 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:26:46.387857: step 49030, loss = 1.97 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:26:47.582517: step 49040, loss = 2.00 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:26:48.763389: step 49050, loss = 2.03 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:26:49.940506: step 49060, loss = 2.07 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:26:51.116709: step 49070, loss = 2.02 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:26:52.285077: step 49080, loss = 1.88 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:53.458562: step 49090, loss = 2.23 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:54.644341: step 49100, loss = 1.93 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:26:55.814317: step 49110, loss = 1.97 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:26:56.961460: step 49120, loss = 1.98 (1115.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:26:58.139477: step 49130, loss = 2.15 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:26:59.312680: step 49140, loss = 1.93 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:00.473717: step 49150, loss = 1.74 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:27:01.645150: step 49160, loss = 2.12 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:02.823114: step 49170, loss = 1.87 (1086.6E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 511 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:04.000962: step 49180, loss = 2.07 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:05.168469: step 49190, loss = 1.91 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:06.337906: step 49200, loss = 2.01 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:07.508827: step 49210, loss = 2.03 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:08.675966: step 49220, loss = 2.01 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:09.836832: step 49230, loss = 2.00 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:27:10.994467: step 49240, loss = 1.89 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:27:12.170505: step 49250, loss = 2.03 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:13.347553: step 49260, loss = 1.97 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:14.509918: step 49270, loss = 1.93 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:27:15.693824: step 49280, loss = 2.04 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:16.881042: step 49290, loss = 2.11 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:27:18.049079: step 49300, loss = 2.02 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:19.224934: step 49310, loss = 1.88 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:20.413852: step 49320, loss = 2.08 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:27:21.586644: step 49330, loss = 1.92 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:22.785484: step 49340, loss = 1.91 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:23.954311: step 49350, loss = 1.94 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:25.136591: step 49360, loss = 2.10 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:26.309711: step 49370, loss = 1.89 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:27.507964: step 49380, loss = 2.04 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:28.712528: step 49390, loss = 2.12 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:29.881280: step 49400, loss = 2.22 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:27:31.066813: step 49410, loss = 1.92 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:27:32.257365: step 49420, loss = 2.03 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:27:33.441631: step 49430, loss = 1.91 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:34.634642: step 49440, loss = 1.99 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:27:35.833486: step 49450, loss = 2.07 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:37.052386: step 49460, loss = 1.97 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:27:38.267265: step 49470, loss = 1.95 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:27:39.469136: step 49480, loss = 1.97 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:40.684840: step 49490, loss = 2.14 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:27:41.996350: step 49500, loss = 1.91 (976.0 examples/sec; 0.131 sec/batch)
2017-05-04 23:27:43.138173: step 49510, loss = 1.92 (1121.0 examples/sec; 0.114 sec/batch)
2017-05-04 23:27:44.361433: step 49520, loss = 2.05 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:27:45.571155: step 49530, loss = 2.07 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:27:46.770343: step 49540, loss = 1.84 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:47.971688: step 49550, loss = 1.98 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:49.181954: step 49560, loss = 1.87 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:27:50.392781: step 49570, loss = 2.03 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:27:51.607478: step 49580, loss = 1.97 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:27:52.807612: step 49590, loss = 2.00 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:54.000906: step 49600, loss = 1.96 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:27:55.200004: step 49610, loss = 2.09 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:27:56.410051: step 49620, loss = 2.16 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:27:57.588550: step 49630, loss = 1.93 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:27:58.796760: step 49640, loss = 1.92 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:27:59.986675: step 49650, loss = 2.25 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:28:01.201217: step 49660, loss = 2.03 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:28:02.407766: step 49670, loss = 2.13 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:28:03.597095: step 49680, loss = 1.89 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:28:04.781319: step 49690, loss = 1.97 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:05.947323: step 49700, loss = 2.02 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:07.105544: step 49710, loss = 1.88 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:08.267651: step 49720, loss = 1.86 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:09.442294: step 49730, loss = 1.97 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:10.598077: step 49740, loss = 1.95 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:11.759591: step 49750, loss = 1.91 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:12.907530: step 49760, loss = 1.90 (1115.0 examples/sec; 0.115 sec/batch)
2017-05-04 23:28:14.062769: step 49770, loss = 2.11 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:15.224743: step 49780, loss = 1.95 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:16.396170: step 49790, loss = 2.04 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:17.539342: step 49800, loss = 2.07 (1119.7 examples/sec; 0.114 sec/batch)
2017-05-04 23:28:18.703386: step 49810, loss = 2.17 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:19.881978: step 49820, loss = 1.95 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:21.073350: step 49830, loss = 1.96 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:28:22.216851: step 49840, loss = 2.08 (1119.4 examples/sec; 0.114 sec/batch)
2017-05-04 23:28:23.407074: step 49850, loss = 2.07 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:28:24.583963: step 49860, loss = 2.08 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:25.730333: step 49870, loss = 1.92 (1116.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:28:26.901329: step 49880, loss = 1.96 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:28.069149: step 49890, loss = 1.95 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:29.245980: step 49900, loss = 2.13 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:30.401584: step 49910, loss = 1.82 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:31.579312: step 49920, loss = 1.94 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:32.761017: step 49930, loss = 1.93 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:33.922689: step 49940, loss = 2.13 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:35.132379: step 49950, loss = 2.09 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:28:36.333184: step 49960, loss = 1.95 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:28:37.483480: step 49970, loss = 1.97 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:28:38.640029: step 49980, loss = 1.86 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:39.807473: step 49990, loss = 1.95 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:40.976876: step 50000, loss = 1.90 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:42.117190: step 50010, loss = 1.92 (1122.5 examples/sec; 0.114 sec/batch)
2017-05-04 23:28:43.296125: step 50020, loss = 1.97 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:44.453518: step 50030, loss = 1.95 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:45.612578: step 50040, loss = 2.07 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:28:46.786575: step 50050, loss = 2.14 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:47.963411: step 50060, loss = 1.94 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:49.146165: step 50070, loss = 1.96 (1082.2 exE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 523 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
amples/sec; 0.118 sec/batch)
2017-05-04 23:28:50.342803: step 50080, loss = 2.07 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:28:51.520177: step 50090, loss = 2.04 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:52.687279: step 50100, loss = 1.98 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:53.829878: step 50110, loss = 1.89 (1120.2 examples/sec; 0.114 sec/batch)
2017-05-04 23:28:55.001524: step 50120, loss = 2.06 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:56.168601: step 50130, loss = 2.06 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:57.337875: step 50140, loss = 2.09 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:28:58.514339: step 50150, loss = 1.83 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:28:59.699152: step 50160, loss = 1.97 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:00.865795: step 50170, loss = 1.92 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:02.035126: step 50180, loss = 2.02 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:03.199652: step 50190, loss = 1.92 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:04.376090: step 50200, loss = 1.85 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:05.555934: step 50210, loss = 2.01 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:06.714413: step 50220, loss = 1.96 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:07.890015: step 50230, loss = 2.01 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:09.068651: step 50240, loss = 1.93 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:10.215292: step 50250, loss = 1.78 (1116.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:29:11.383403: step 50260, loss = 2.04 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:12.556955: step 50270, loss = 2.11 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:13.744296: step 50280, loss = 1.97 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:29:14.885363: step 50290, loss = 2.08 (1121.7 examples/sec; 0.114 sec/batch)
2017-05-04 23:29:16.031027: step 50300, loss = 2.15 (1117.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:29:17.214351: step 50310, loss = 2.16 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:18.369685: step 50320, loss = 2.06 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:19.538428: step 50330, loss = 2.00 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:20.681855: step 50340, loss = 2.07 (1119.4 examples/sec; 0.114 sec/batch)
2017-05-04 23:29:21.842151: step 50350, loss = 2.26 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:23.026041: step 50360, loss = 2.07 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:24.223661: step 50370, loss = 2.17 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:29:25.395896: step 50380, loss = 2.06 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:26.568876: step 50390, loss = 2.00 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:27.747343: step 50400, loss = 1.87 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:28.929703: step 50410, loss = 1.95 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:30.079315: step 50420, loss = 1.95 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:29:31.245357: step 50430, loss = 2.20 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:32.439491: step 50440, loss = 1.97 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:29:33.592196: step 50450, loss = 1.89 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:29:34.767856: step 50460, loss = 1.99 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:35.921621: step 50470, loss = 2.22 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:29:37.105794: step 50480, loss = 1.96 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:38.363026: step 50490, loss = 1.91 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-04 23:29:39.435548: step 50500, loss = 2.20 (1193.4 examples/sec; 0.107 sec/batch)
2017-05-04 23:29:40.627646: step 50510, loss = 2.11 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:29:41.797495: step 50520, loss = 2.03 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:42.961913: step 50530, loss = 1.93 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:44.130283: step 50540, loss = 2.01 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:45.307771: step 50550, loss = 1.97 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:46.464848: step 50560, loss = 1.99 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:47.633452: step 50570, loss = 2.02 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:48.792020: step 50580, loss = 2.06 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:49.949148: step 50590, loss = 2.02 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:51.133057: step 50600, loss = 1.86 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:29:52.304012: step 50610, loss = 1.99 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:53.473943: step 50620, loss = 1.94 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:54.619342: step 50630, loss = 1.90 (1117.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:29:55.783902: step 50640, loss = 2.08 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:29:56.954210: step 50650, loss = 2.03 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:58.123842: step 50660, loss = 1.88 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:29:59.305954: step 50670, loss = 1.90 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:00.488361: step 50680, loss = 2.13 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:01.649352: step 50690, loss = 2.15 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:02.824435: step 50700, loss = 1.86 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:03.987270: step 50710, loss = 2.02 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:05.152095: step 50720, loss = 1.91 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:06.304974: step 50730, loss = 1.85 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:30:07.474557: step 50740, loss = 1.97 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:08.654958: step 50750, loss = 1.99 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:09.815503: step 50760, loss = 1.97 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:10.993126: step 50770, loss = 1.92 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:12.173288: step 50780, loss = 2.19 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:13.349132: step 50790, loss = 2.11 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:14.506877: step 50800, loss = 2.05 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:15.682114: step 50810, loss = 2.09 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:16.848088: step 50820, loss = 2.00 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:18.023099: step 50830, loss = 2.03 (1089.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:19.183576: step 50840, loss = 2.10 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:20.337103: step 50850, loss = 1.99 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:30:21.512118: step 50860, loss = 2.09 (1089.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:22.679746: step 50870, loss = 1.86 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:23.846844: step 50880, loss = 1.85 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:25.010523: step 50890, loss = 2.14 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:26.165987: step 50900, loss = 2.01 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:27.344070: step 50910, loss = 1.93 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:28.513086: step 50920, loss = 2.20 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:29.666552: step 50930, loss = 1.98 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:30:30.833034: step 50940, loss = 2.01 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:31.999018: step 50950, loss = 2.04 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:33.178966: step 50960, loss = 1.94 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:34.339164: step 50970, loss = 1.87 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:30:35.522838: step 50980, loss = 2.03 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:36.693731: step 50990, loss = 2.04 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:37.870354: step 51000, loss = 1.88 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:39.043893: step 51010, loss = 1.97 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:30:40.226984: step 51020, loss = 1.91 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:30:41.415985: step 51030, loss = 1.87 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:30:42.630878: step 51040, loss = 2.11 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:30:43.838832: step 51050, loss = 1.89 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:30:45.069900: step 51060, loss = 2.08 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:30:46.286670: step 51070, loss = 1.99 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:30:47.509068: step 51080, loss = 1.99 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:30:48.724830: step 51090, loss = 1.87 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:30:49.945925: step 51100, loss = 1.98 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:30:51.192852: step 51110, loss = 2.10 (1026.5 examples/sec; 0.125 sec/batch)
2017-05-04 23:30:52.415937: step 51120, loss = 2.06 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:30:53.649775: step 51130, loss = 1.96 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:30:54.874127: step 51140, loss = 2.06 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:30:56.105838: step 51150, loss = 1.87 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:30:57.319960: step 51160, loss = 1.93 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:30:58.526580: step 51170, loss = 1.83 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:30:59.752909: step 51180, loss = 2.05 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:31:01.005961: step 51190, loss = 1.99 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-04 23:31:02.227808: step 51200, loss = 1.95 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:03.450755: step 51210, loss = 1.92 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:04.667018: step 51220, loss = 2.02 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:05.873626: step 51230, loss = 2.00 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:31:07.091979: step 51240, loss = 1.95 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:08.296741: step 51250, loss = 2.04 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:09.521632: step 51260, loss = 1.95 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:10.755750: step 51270, loss = 1.96 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:31:11.985000: step 51280, loss = 2.06 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:31:13.175868: step 51290, loss = 1.98 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:31:14.385652: step 51300, loss = 1.96 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:31:15.607404: step 51310, loss = 2.03 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:16.856791: step 51320, loss = 1.99 (1024.5 examples/sec; 0.125 sec/batch)
2017-05-04 23:31:18.080457: step 51330, loss = 1.94 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:19.305800: step 51340, loss = 2.04 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:31:20.526666: step 51350, loss = 1.81 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:21.739611: step 51360, loss = 1.96 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:31:22.965829: step 51370, loss = 2.09 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:31:24.181610: step 51380, loss = 1.97 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:25.402543: step 51390, loss = 2.08 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:26.609855: step 51400, loss = 1.85 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:31:27.812359: step 51410, loss = 2.03 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:29.034764: step 51420, loss = 1.86 (1047.1 exampE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 534 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
les/sec; 0.122 sec/batch)
2017-05-04 23:31:30.242006: step 51430, loss = 1.82 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:31:31.460157: step 51440, loss = 1.87 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:32.689831: step 51450, loss = 2.24 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:31:33.908950: step 51460, loss = 1.89 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:35.119517: step 51470, loss = 2.04 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:31:36.425406: step 51480, loss = 1.80 (980.2 examples/sec; 0.131 sec/batch)
2017-05-04 23:31:37.536193: step 51490, loss = 1.95 (1152.3 examples/sec; 0.111 sec/batch)
2017-05-04 23:31:38.734706: step 51500, loss = 1.90 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:39.944557: step 51510, loss = 2.19 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:31:41.171558: step 51520, loss = 2.01 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:31:42.375044: step 51530, loss = 2.11 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:43.567066: step 51540, loss = 2.07 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:31:44.781560: step 51550, loss = 1.96 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:31:45.975465: step 51560, loss = 1.97 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:31:47.178520: step 51570, loss = 2.02 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:48.367019: step 51580, loss = 1.92 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:31:49.553517: step 51590, loss = 1.99 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:31:50.747362: step 51600, loss = 1.97 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:31:51.945058: step 51610, loss = 2.09 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:53.140836: step 51620, loss = 1.94 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:54.326723: step 51630, loss = 2.06 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:31:55.527928: step 51640, loss = 2.03 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:56.722975: step 51650, loss = 2.00 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:31:57.943007: step 51660, loss = 2.01 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:31:59.158095: step 51670, loss = 1.93 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:32:00.348208: step 51680, loss = 2.00 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:01.540122: step 51690, loss = 2.13 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:02.738127: step 51700, loss = 2.00 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:32:03.924385: step 51710, loss = 1.90 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:05.119768: step 51720, loss = 2.04 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:32:06.308698: step 51730, loss = 1.85 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:07.487650: step 51740, loss = 1.80 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:32:08.673195: step 51750, loss = 2.11 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:09.842276: step 51760, loss = 2.03 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:11.014748: step 51770, loss = 2.00 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:12.208923: step 51780, loss = 2.01 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:13.403207: step 51790, loss = 1.96 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:14.597770: step 51800, loss = 1.99 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:15.813260: step 51810, loss = 2.05 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:32:17.028954: step 51820, loss = 2.03 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:32:18.227045: step 51830, loss = 1.94 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:32:19.420858: step 51840, loss = 2.05 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:20.611995: step 51850, loss = 2.19 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:21.792592: step 51860, loss = 2.17 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:32:22.983199: step 51870, loss = 1.95 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:24.176081: step 51880, loss = 2.07 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:25.348516: step 51890, loss = 1.90 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:26.517105: step 51900, loss = 2.15 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:27.674843: step 51910, loss = 2.01 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:32:28.831723: step 51920, loss = 1.92 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:32:29.977147: step 51930, loss = 1.78 (1117.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:32:31.147775: step 51940, loss = 1.82 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:32.321169: step 51950, loss = 2.01 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:33.496740: step 51960, loss = 1.94 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:32:34.690097: step 51970, loss = 1.92 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:32:35.887567: step 51980, loss = 1.96 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:32:37.047086: step 51990, loss = 1.85 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:32:38.206392: step 52000, loss = 1.99 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:32:39.380154: step 52010, loss = 2.25 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:40.542866: step 52020, loss = 1.83 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:32:41.716551: step 52030, loss = 2.09 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:42.876616: step 52040, loss = 1.87 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:32:44.037210: step 52050, loss = 1.91 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:32:45.195687: step 52060, loss = 2.16 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:32:46.363551: step 52070, loss = 1.98 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:47.534763: step 52080, loss = 2.00 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:48.679043: step 52090, loss = 1.86 (1118.6 examples/sec; 0.114 sec/batch)
2017-05-04 23:32:49.854098: step 52100, loss = 2.13 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:32:51.025059: step 52110, loss = 2.17 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:32:52.188744: step 52120, loss = 1.96 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:32:53.371105: step 52130, loss = 1.94 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:32:54.533296: step 52140, loss = 1.96 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:32:55.713357: step 52150, loss = 2.07 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:32:56.869333: step 52160, loss = 1.72 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:32:58.023413: step 52170, loss = 2.08 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:32:59.210830: step 52180, loss = 2.06 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:33:00.391027: step 52190, loss = 2.00 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:33:01.555723: step 52200, loss = 2.08 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:33:02.723333: step 52210, loss = 1.90 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:33:03.892603: step 52220, loss = 2.10 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:33:05.074861: step 52230, loss = 1.87 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:33:06.255558: step 52240, loss = 1.89 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:33:07.442494: step 52250, loss = 1.99 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:33:08.621516: step 52260, loss = 1.83 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:33:09.802748: step 52270, loss = 1.81 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:33:11.002335: step 52280, loss = 2.15 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:33:12.188802: step 52290, loss = 2.14 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:33:13.383510: step 52300, loss = 1.99 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:33:14.555393: step 52310, loss = 2.09 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:33:15.747320: step 52320, loss = 1.87 (1073.9 examplesE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 545 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
/sec; 0.119 sec/batch)
2017-05-04 23:33:16.920023: step 52330, loss = 2.09 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:33:18.091762: step 52340, loss = 1.99 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:33:19.297559: step 52350, loss = 1.93 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:33:20.495461: step 52360, loss = 2.07 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:33:21.695582: step 52370, loss = 2.14 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:33:22.913112: step 52380, loss = 2.06 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:24.130520: step 52390, loss = 2.03 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:25.341210: step 52400, loss = 1.86 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:33:26.550614: step 52410, loss = 2.01 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:33:27.766852: step 52420, loss = 2.02 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:28.989150: step 52430, loss = 2.15 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:30.203657: step 52440, loss = 2.02 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:33:31.429991: step 52450, loss = 1.98 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:33:32.656728: step 52460, loss = 1.99 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:33:33.954718: step 52470, loss = 1.95 (986.1 examples/sec; 0.130 sec/batch)
2017-05-04 23:33:35.073211: step 52480, loss = 2.00 (1144.4 examples/sec; 0.112 sec/batch)
2017-05-04 23:33:36.293849: step 52490, loss = 2.06 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:37.518057: step 52500, loss = 1.94 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:38.744200: step 52510, loss = 2.01 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:33:39.973194: step 52520, loss = 2.00 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:33:41.194776: step 52530, loss = 1.93 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:42.395143: step 52540, loss = 2.17 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:33:43.608726: step 52550, loss = 2.10 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:33:44.817511: step 52560, loss = 1.99 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:33:46.018519: step 52570, loss = 1.84 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:33:47.238198: step 52580, loss = 2.00 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:48.437590: step 52590, loss = 2.18 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:33:49.642674: step 52600, loss = 1.97 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:33:50.875752: step 52610, loss = 1.97 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:33:52.075664: step 52620, loss = 2.06 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:33:53.273615: step 52630, loss = 1.95 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:33:54.476860: step 52640, loss = 1.82 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:33:55.680636: step 52650, loss = 1.78 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:33:56.900767: step 52660, loss = 1.91 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:33:58.109712: step 52670, loss = 1.94 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:33:59.333876: step 52680, loss = 2.06 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:00.545938: step 52690, loss = 2.02 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:01.768609: step 52700, loss = 2.13 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:02.994491: step 52710, loss = 2.03 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:04.224660: step 52720, loss = 1.97 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:05.454665: step 52730, loss = 2.07 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:06.669689: step 52740, loss = 1.93 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:07.891852: step 52750, loss = 1.96 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:09.122554: step 52760, loss = 1.92 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:10.348499: step 52770, loss = 2.03 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:11.560967: step 52780, loss = 2.12 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:12.775823: step 52790, loss = 1.97 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:13.995046: step 52800, loss = 2.08 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:15.211548: step 52810, loss = 1.90 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:16.417534: step 52820, loss = 2.01 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:17.640124: step 52830, loss = 2.01 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:18.861532: step 52840, loss = 2.16 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:20.053322: step 52850, loss = 1.84 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:34:21.268567: step 52860, loss = 1.99 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:22.465386: step 52870, loss = 2.02 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:34:23.681561: step 52880, loss = 2.21 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:24.893742: step 52890, loss = 1.95 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:26.113493: step 52900, loss = 2.11 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:27.337459: step 52910, loss = 1.88 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:28.557269: step 52920, loss = 1.97 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:29.773888: step 52930, loss = 1.98 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:31.001526: step 52940, loss = 2.04 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:32.220712: step 52950, loss = 2.00 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:33.440667: step 52960, loss = 1.98 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:34.652043: step 52970, loss = 2.02 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:35.870384: step 52980, loss = 1.97 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:37.097997: step 52990, loss = 1.91 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:38.314951: step 53000, loss = 2.21 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:39.516905: step 53010, loss = 1.96 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:34:40.725021: step 53020, loss = 2.03 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:41.932506: step 53030, loss = 1.97 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:43.146452: step 53040, loss = 1.95 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:44.370414: step 53050, loss = 1.85 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:45.573135: step 53060, loss = 1.93 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:34:46.758662: step 53070, loss = 1.92 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:34:47.986008: step 53080, loss = 1.98 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:49.207886: step 53090, loss = 2.14 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:50.422893: step 53100, loss = 2.13 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:51.649211: step 53110, loss = 1.98 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:52.868998: step 53120, loss = 1.98 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:34:54.078058: step 53130, loss = 2.08 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:34:55.304756: step 53140, loss = 1.93 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:56.534930: step 53150, loss = 1.92 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:34:57.715399: step 53160, loss = 2.12 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:34:58.933445: step 53170, loss = 1.92 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:00.155529: step 53180, loss = 1.98 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:01.367476: step 53190, loss = 2.03 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:35:02.599860: step 53200, loss = 1.94 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:03.816707: step 53210, loss = 1.98 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:05.047915: step 53220, loss = 2.13 (1039.6 examples/seE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 555 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
c; 0.123 sec/batch)
2017-05-04 23:35:06.251444: step 53230, loss = 1.97 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:07.488936: step 53240, loss = 1.94 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-04 23:35:08.711130: step 53250, loss = 2.04 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:09.913311: step 53260, loss = 2.01 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:11.108663: step 53270, loss = 1.97 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:12.330542: step 53280, loss = 1.96 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:13.530219: step 53290, loss = 1.99 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:14.760929: step 53300, loss = 1.92 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:15.977570: step 53310, loss = 2.03 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:17.207385: step 53320, loss = 1.90 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:18.404675: step 53330, loss = 2.02 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:19.616634: step 53340, loss = 1.93 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:35:20.836602: step 53350, loss = 1.95 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:22.038836: step 53360, loss = 2.00 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:23.268361: step 53370, loss = 2.00 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:24.490070: step 53380, loss = 2.06 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:25.718538: step 53390, loss = 2.07 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:26.938287: step 53400, loss = 1.91 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:28.164632: step 53410, loss = 1.85 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:29.372481: step 53420, loss = 2.05 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:35:30.576891: step 53430, loss = 1.89 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:31.792168: step 53440, loss = 2.04 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:33.001071: step 53450, loss = 2.11 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:35:34.303524: step 53460, loss = 2.01 (982.8 examples/sec; 0.130 sec/batch)
2017-05-04 23:35:35.420100: step 53470, loss = 2.03 (1146.4 examples/sec; 0.112 sec/batch)
2017-05-04 23:35:36.641671: step 53480, loss = 2.02 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:37.846391: step 53490, loss = 2.05 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:39.072605: step 53500, loss = 1.80 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:40.283584: step 53510, loss = 2.10 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:35:41.504947: step 53520, loss = 1.90 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:42.713212: step 53530, loss = 2.07 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:35:43.934154: step 53540, loss = 1.91 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:45.153081: step 53550, loss = 2.03 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:46.367124: step 53560, loss = 1.93 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:35:47.579599: step 53570, loss = 1.94 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:35:48.801900: step 53580, loss = 1.93 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:50.025205: step 53590, loss = 1.91 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:51.241255: step 53600, loss = 1.94 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:35:52.468383: step 53610, loss = 1.99 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:53.669775: step 53620, loss = 2.16 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:54.895047: step 53630, loss = 1.94 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:35:56.083675: step 53640, loss = 1.96 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:35:57.284900: step 53650, loss = 2.01 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:35:58.451786: step 53660, loss = 1.88 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:35:59.612025: step 53670, loss = 1.88 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:36:00.786092: step 53680, loss = 1.94 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:01.951885: step 53690, loss = 1.91 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:03.112857: step 53700, loss = 2.24 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:36:04.269075: step 53710, loss = 1.99 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:36:05.437354: step 53720, loss = 1.83 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:06.577017: step 53730, loss = 1.91 (1123.1 examples/sec; 0.114 sec/batch)
2017-05-04 23:36:07.768509: step 53740, loss = 2.00 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:36:08.922577: step 53750, loss = 1.98 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:36:10.074681: step 53760, loss = 1.79 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-04 23:36:11.262121: step 53770, loss = 2.05 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:36:12.449136: step 53780, loss = 1.90 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:36:13.617079: step 53790, loss = 1.95 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:14.786433: step 53800, loss = 2.12 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:15.974339: step 53810, loss = 1.93 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:36:17.140027: step 53820, loss = 1.94 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:18.308222: step 53830, loss = 2.09 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:19.479790: step 53840, loss = 2.03 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:20.647198: step 53850, loss = 2.21 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:21.796989: step 53860, loss = 1.88 (1113.2 examples/sec; 0.115 sec/batch)
2017-05-04 23:36:22.954490: step 53870, loss = 1.90 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:36:24.137294: step 53880, loss = 1.95 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:25.313063: step 53890, loss = 1.74 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:26.460639: step 53900, loss = 2.04 (1115.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:36:27.636089: step 53910, loss = 1.91 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:28.793073: step 53920, loss = 1.98 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:36:29.949177: step 53930, loss = 1.98 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:36:31.112660: step 53940, loss = 1.95 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:36:32.282833: step 53950, loss = 1.85 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:33.467424: step 53960, loss = 2.10 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:34.650043: step 53970, loss = 1.95 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:35.840177: step 53980, loss = 2.08 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:36:37.007542: step 53990, loss = 2.02 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:38.193612: step 54000, loss = 2.01 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:36:39.385754: step 54010, loss = 2.01 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:36:40.571297: step 54020, loss = 2.06 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:36:41.746496: step 54030, loss = 2.03 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:42.935991: step 54040, loss = 2.00 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:36:44.112329: step 54050, loss = 1.96 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:45.294145: step 54060, loss = 2.13 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:46.468103: step 54070, loss = 1.86 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:47.628662: step 54080, loss = 1.93 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:36:48.795671: step 54090, loss = 2.06 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:49.951385: step 54100, loss = 2.11 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:36:51.107047: step 54110, loss = 2.01 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:36:52.281787: step 54120, loss = 2.03 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:53.463715: step 54130, loss = 2.05 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:54.645600: step 54140, loss = 2.04 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:36:55.844191: step 54150, loss = 2.03 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:36:57.036005: step 54160, loss = 1.86 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:36:58.206331: step 54170, loss = 2.00 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:36:59.385741: step 54180, loss = 2.08 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:00.559266: step 54190, loss = 2.02 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:01.732489: step 54200, loss = 1.94 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:02.889531: step 54210, loss = 1.99 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:37:04.060636: step 54220, loss = 1.93 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:05.213770: step 54230, loss = 2.11 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-04 23:37:06.385361: step 54240, loss = 1.87 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:07.561891: step 54250, loss = 1.79 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:08.715495: step 54260, loss = 2.08 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:37:09.869584: step 54270, loss = 1.97 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:37:11.050089: step 54280, loss = 1.82 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:12.212127: step 54290, loss = 2.16 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:37:13.364319: step 54300, loss = 2.03 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:37:14.525638: step 54310, loss = 2.05 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:37:15.707306: step 54320, loss = 2.01 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:16.895257: step 54330, loss = 1.94 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:37:18.085806: step 54340, loss = 2.17 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:37:19.268558: step 54350, loss = 1.99 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:20.447537: step 54360, loss = 1.94 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:21.615592: step 54370, loss = 1.94 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:22.783309: step 54380, loss = 2.14 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:23.976431: step 54390, loss = 1.84 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:37:25.191157: step 54400, loss = 2.03 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:37:26.389027: step 54410, loss = 1.96 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:37:27.608076: step 54420, loss = 2.07 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:37:28.828357: step 54430, loss = 1.87 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:37:30.030101: step 54440, loss = 2.00 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:37:31.336116: step 54450, loss = 1.91 (980.1 examples/sec; 0.131 sec/batch)
2017-05-04 23:37:32.428035: step 54460, loss = 2.08 (1172.3 examples/sec; 0.109 sec/batch)
2017-05-04 23:37:33.612903: step 54470, loss = 2.02 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:34.801487: step 54480, loss = 2.05 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:37:36.012344: step 54490, loss = 1.96 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:37:37.191699: step 54500, loss = 1.91 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:38.367910: step 54510, loss = 1.85 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:39.536730: step 54520, loss = 1.93 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:40.726213: step 54530, loss = 2.06 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:37:41.896506: step 54540, loss = 2.03 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:43.082367: step 54550, loss = 1.93 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:37:44.251501: step 54560, loss = 2.15 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:45.433082: step 54570, loss = 2.05 (1083.3 examples/sec; 0.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 566 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
118 sec/batch)
2017-05-04 23:37:46.604428: step 54580, loss = 1.99 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:37:47.792909: step 54590, loss = 1.84 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:37:48.988022: step 54600, loss = 1.87 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:37:50.195438: step 54610, loss = 2.00 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:37:51.376110: step 54620, loss = 2.00 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:52.558848: step 54630, loss = 2.07 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:37:53.722215: step 54640, loss = 2.05 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:37:54.913767: step 54650, loss = 2.05 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:37:56.068933: step 54660, loss = 2.18 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:37:57.256155: step 54670, loss = 2.03 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:37:58.445416: step 54680, loss = 1.92 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:37:59.653091: step 54690, loss = 2.01 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:38:00.854300: step 54700, loss = 1.86 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:38:02.059119: step 54710, loss = 2.06 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:38:03.262044: step 54720, loss = 1.88 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:38:04.441574: step 54730, loss = 1.89 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:05.602548: step 54740, loss = 1.98 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:06.762049: step 54750, loss = 2.00 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:07.944489: step 54760, loss = 1.97 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:09.115083: step 54770, loss = 2.11 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:10.291002: step 54780, loss = 1.88 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:11.471373: step 54790, loss = 1.86 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:12.633337: step 54800, loss = 2.03 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:13.787789: step 54810, loss = 1.90 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:38:14.960154: step 54820, loss = 2.00 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:16.112816: step 54830, loss = 2.06 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:38:17.290498: step 54840, loss = 1.96 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:18.451795: step 54850, loss = 1.91 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:19.631488: step 54860, loss = 1.85 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:20.812309: step 54870, loss = 1.95 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:21.953724: step 54880, loss = 2.07 (1121.4 examples/sec; 0.114 sec/batch)
2017-05-04 23:38:23.126683: step 54890, loss = 1.86 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:24.280188: step 54900, loss = 2.02 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:38:25.440811: step 54910, loss = 1.94 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:26.608912: step 54920, loss = 2.10 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:27.772018: step 54930, loss = 1.92 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:28.953192: step 54940, loss = 2.04 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:30.100877: step 54950, loss = 1.93 (1115.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:38:31.268300: step 54960, loss = 2.16 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:32.448685: step 54970, loss = 1.81 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:33.606126: step 54980, loss = 1.98 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:34.779810: step 54990, loss = 2.04 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:35.973559: step 55000, loss = 2.10 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:38:37.140333: step 55010, loss = 1.95 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:38.309618: step 55020, loss = 1.99 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:39.478474: step 55030, loss = 2.06 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:40.652537: step 55040, loss = 1.88 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:41.818898: step 55050, loss = 2.21 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:42.989641: step 55060, loss = 1.99 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:44.164434: step 55070, loss = 2.00 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:45.321278: step 55080, loss = 1.97 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:46.480471: step 55090, loss = 2.11 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:47.647543: step 55100, loss = 1.88 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:48.804114: step 55110, loss = 1.80 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:38:49.979620: step 55120, loss = 1.89 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:51.144972: step 55130, loss = 2.06 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:52.311501: step 55140, loss = 2.04 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:53.480516: step 55150, loss = 1.92 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:54.663834: step 55160, loss = 1.94 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:55.846905: step 55170, loss = 1.91 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:57.030396: step 55180, loss = 1.96 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:38:58.200854: step 55190, loss = 2.03 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:38:59.368515: step 55200, loss = 2.07 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:39:00.561379: step 55210, loss = 1.86 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:39:01.727859: step 55220, loss = 2.28 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:39:02.885781: step 55230, loss = 2.07 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:39:04.049220: step 55240, loss = 2.08 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:39:05.217224: step 55250, loss = 1.92 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:39:06.383466: step 55260, loss = 2.01 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:39:07.559872: step 55270, loss = 2.18 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:39:08.751755: step 55280, loss = 2.04 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:39:09.943759: step 55290, loss = 2.06 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:39:11.161173: step 55300, loss = 2.04 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:39:12.335442: step 55310, loss = 2.01 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:39:13.513132: step 55320, loss = 2.17 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:39:14.680927: step 55330, loss = 1.76 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:39:15.878273: step 55340, loss = 2.14 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:17.073836: step 55350, loss = 1.99 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:18.252784: step 55360, loss = 1.84 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:39:19.452656: step 55370, loss = 1.98 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:20.659177: step 55380, loss = 2.17 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:39:21.838138: step 55390, loss = 2.04 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:39:23.042410: step 55400, loss = 2.02 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:24.257857: step 55410, loss = 2.00 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:39:25.456055: step 55420, loss = 2.09 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:26.645627: step 55430, loss = 1.82 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:39:27.932631: step 55440, loss = 1.99 (994.6 examples/sec; 0.129 sec/batch)
2017-05-04 23:39:29.056447: step 55450, loss = 1.94 (1139.0 examples/sec; 0.112 sec/batch)
2017-05-04 23:39:30.260058: step 55460, loss = 1.97 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:31.501352: step 55470, loss = 2.24 (1031.2 examples/sec; 0.124E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 578 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
 sec/batch)
2017-05-04 23:39:32.706716: step 55480, loss = 2.00 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:39:33.914606: step 55490, loss = 2.03 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:39:35.119188: step 55500, loss = 1.95 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:36.338193: step 55510, loss = 1.95 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:39:37.551312: step 55520, loss = 2.00 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:39:38.738556: step 55530, loss = 1.84 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:39:39.940054: step 55540, loss = 1.98 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:41.155948: step 55550, loss = 2.02 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:39:42.383599: step 55560, loss = 1.91 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:39:43.597482: step 55570, loss = 1.85 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:39:44.819155: step 55580, loss = 1.90 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:39:46.015028: step 55590, loss = 2.08 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:47.217520: step 55600, loss = 1.98 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:48.442529: step 55610, loss = 2.13 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:39:49.626672: step 55620, loss = 1.89 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:39:50.832483: step 55630, loss = 2.03 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:39:52.094339: step 55640, loss = 1.97 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-04 23:39:53.239988: step 55650, loss = 2.08 (1117.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:39:54.429074: step 55660, loss = 1.92 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:39:55.618946: step 55670, loss = 2.04 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:39:56.844816: step 55680, loss = 2.06 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:39:58.047388: step 55690, loss = 1.95 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:39:59.281947: step 55700, loss = 1.97 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:40:00.490378: step 55710, loss = 1.95 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:01.691554: step 55720, loss = 2.01 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:02.921383: step 55730, loss = 1.95 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:40:04.157733: step 55740, loss = 1.95 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-04 23:40:05.392637: step 55750, loss = 1.93 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:40:06.593300: step 55760, loss = 1.96 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:07.839167: step 55770, loss = 2.07 (1027.4 examples/sec; 0.125 sec/batch)
2017-05-04 23:40:09.066278: step 55780, loss = 2.01 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:40:10.279826: step 55790, loss = 1.88 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:11.476938: step 55800, loss = 1.90 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:12.708564: step 55810, loss = 2.09 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:40:13.912729: step 55820, loss = 1.86 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:15.134760: step 55830, loss = 1.96 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:16.337870: step 55840, loss = 1.95 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:17.548060: step 55850, loss = 1.93 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:18.757417: step 55860, loss = 1.90 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:19.989469: step 55870, loss = 2.01 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:40:21.209213: step 55880, loss = 2.00 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:22.412975: step 55890, loss = 1.93 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:23.626931: step 55900, loss = 1.87 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:24.846978: step 55910, loss = 2.09 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:26.053496: step 55920, loss = 2.05 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:27.283989: step 55930, loss = 2.04 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:40:28.491013: step 55940, loss = 1.93 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:29.689434: step 55950, loss = 2.09 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:30.904760: step 55960, loss = 2.01 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:32.132088: step 55970, loss = 2.12 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:40:33.333408: step 55980, loss = 2.03 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:34.563612: step 55990, loss = 2.24 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:40:35.779588: step 56000, loss = 2.00 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:36.991729: step 56010, loss = 2.06 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:38.196213: step 56020, loss = 2.08 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:39.400578: step 56030, loss = 1.87 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:40.609745: step 56040, loss = 1.78 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:41.832036: step 56050, loss = 2.00 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:43.030851: step 56060, loss = 2.08 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:44.257825: step 56070, loss = 1.94 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:40:45.485497: step 56080, loss = 1.91 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:40:46.716764: step 56090, loss = 2.04 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:40:47.909163: step 56100, loss = 2.02 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:40:49.139829: step 56110, loss = 2.01 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:40:50.341846: step 56120, loss = 2.09 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:51.553935: step 56130, loss = 1.97 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:52.773830: step 56140, loss = 1.97 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:53.989850: step 56150, loss = 2.10 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:40:55.188931: step 56160, loss = 1.93 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:40:56.402503: step 56170, loss = 2.01 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:57.609428: step 56180, loss = 1.99 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:40:58.819720: step 56190, loss = 1.86 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:00.040340: step 56200, loss = 2.02 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:01.259534: step 56210, loss = 2.12 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:02.480951: step 56220, loss = 1.85 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:03.695453: step 56230, loss = 2.07 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:04.892120: step 56240, loss = 2.06 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:41:06.121345: step 56250, loss = 2.06 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:41:07.333568: step 56260, loss = 2.07 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:08.561467: step 56270, loss = 2.08 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:41:09.784826: step 56280, loss = 1.83 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:11.005921: step 56290, loss = 1.94 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:12.254444: step 56300, loss = 1.92 (1025.2 examples/sec; 0.125 sec/batch)
2017-05-04 23:41:13.457784: step 56310, loss = 1.93 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:41:14.671537: step 56320, loss = 1.98 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:15.890551: step 56330, loss = 1.99 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:17.105425: step 56340, loss = 2.03 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:18.300874: step 56350, loss = 2.13 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:41:19.500390: step 56360, loss = 1.91 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:41:20.710944: step 56370, loss = 2.00 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:21.919408: step 56380, loss = 2.18 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:23.144715: step 56390, loss = 2.04 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:41:24.351459: step 56400, loss = 2.10 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:25.581757: step 56410, loss = 1.88 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:41:26.779659: step 56420, loss = 2.04 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:41:28.141423: step 56430, loss = 1.80 (940.0 examples/sec; 0.136 sec/batch)
2017-05-04 23:41:29.214754: step 56440, loss = 1.90 (1192.5 examples/sec; 0.107 sec/batch)
2017-05-04 23:41:30.413261: step 56450, loss = 2.11 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:41:31.636469: step 56460, loss = 2.04 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:32.863739: step 56470, loss = 2.03 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:41:34.074027: step 56480, loss = 2.06 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:35.309208: step 56490, loss = 1.95 (1036.3 examples/sec; 0.124 sec/batch)
2017-05-04 23:41:36.529409: step 56500, loss = 1.97 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:37.725799: step 56510, loss = 1.88 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:41:38.945419: step 56520, loss = 1.85 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:40.182197: step 56530, loss = 2.19 (1034.9 examples/sec; 0.124 sec/batch)
2017-05-04 23:41:41.386638: step 56540, loss = 1.90 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:41:42.594631: step 56550, loss = 1.97 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:43.813299: step 56560, loss = 2.02 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:45.015839: step 56570, loss = 1.87 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:41:46.239837: step 56580, loss = 1.97 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:47.445656: step 56590, loss = 2.02 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:48.653421: step 56600, loss = 2.07 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:49.860859: step 56610, loss = 2.09 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:51.077328: step 56620, loss = 1.78 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:41:52.387032: step 56630, loss = 1.87 (977.3 examples/sec; 0.131 sec/batch)
2017-05-04 23:41:53.505264: step 56640, loss = 2.11 (1144.7 examples/sec; 0.112 sec/batch)
2017-05-04 23:41:54.709947: step 56650, loss = 2.08 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:41:55.921851: step 56660, loss = 2.03 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:57.164847: step 56670, loss = 2.06 (1029.8 examples/sec; 0.124 sec/batch)
2017-05-04 23:41:58.372102: step 56680, loss = 1.88 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:41:59.601349: step 56690, loss = 2.07 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:00.819308: step 56700, loss = 1.94 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:02.042876: step 56710, loss = 2.02 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:03.263404: step 56720, loss = 2.00 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:04.495689: step 56730, loss = 1.90 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:05.707970: step 56740, loss = 1.84 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:06.940318: step 56750, loss = 1.84 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:08.127116: step 56760, loss = 1.97 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:42:09.359899: step 56770, loss = 1.97 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:10.563480: step 56780, loss = 1.88 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:42:11.813316: step 56790, loss = 1.93 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-04 23:42:13.010337: step 56800, loss = 2.02 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:42:14.220382: step 56810, loss = 1.99 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:15.441616: step 56820, loss = 1.85 (1048.1 examples/sec; 0.122 sec/E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 588 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
batch)
2017-05-04 23:42:16.635588: step 56830, loss = 1.99 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:42:17.861928: step 56840, loss = 2.09 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:19.107678: step 56850, loss = 1.96 (1027.5 examples/sec; 0.125 sec/batch)
2017-05-04 23:42:20.307269: step 56860, loss = 1.97 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:42:21.526763: step 56870, loss = 1.92 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:22.726799: step 56880, loss = 2.11 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:42:23.962270: step 56890, loss = 1.94 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-04 23:42:25.163963: step 56900, loss = 1.90 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:42:26.381858: step 56910, loss = 2.16 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:27.596555: step 56920, loss = 1.80 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:28.835215: step 56930, loss = 2.04 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-04 23:42:30.025731: step 56940, loss = 1.96 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:42:31.258470: step 56950, loss = 1.81 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:32.470305: step 56960, loss = 1.98 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:33.681497: step 56970, loss = 1.94 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:34.881555: step 56980, loss = 1.92 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:42:36.104143: step 56990, loss = 2.06 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:37.326355: step 57000, loss = 1.94 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:38.551989: step 57010, loss = 1.95 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:39.764838: step 57020, loss = 1.91 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:40.952839: step 57030, loss = 1.80 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:42:42.169585: step 57040, loss = 1.93 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:43.397023: step 57050, loss = 2.03 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:44.599821: step 57060, loss = 2.03 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:42:45.813772: step 57070, loss = 1.97 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:47.047508: step 57080, loss = 2.02 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:48.273758: step 57090, loss = 1.86 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:42:49.491599: step 57100, loss = 2.00 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:42:50.728876: step 57110, loss = 1.96 (1034.5 examples/sec; 0.124 sec/batch)
2017-05-04 23:42:51.942181: step 57120, loss = 1.84 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:42:53.145423: step 57130, loss = 1.88 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:42:54.306854: step 57140, loss = 2.02 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:42:55.497161: step 57150, loss = 2.01 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:42:56.681169: step 57160, loss = 1.87 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:42:57.839478: step 57170, loss = 2.12 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:42:59.023787: step 57180, loss = 1.88 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:43:00.214278: step 57190, loss = 1.99 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:43:01.389711: step 57200, loss = 1.90 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:43:02.562559: step 57210, loss = 1.80 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:03.736602: step 57220, loss = 2.07 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:04.924408: step 57230, loss = 2.00 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:43:06.084555: step 57240, loss = 1.93 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:43:07.270716: step 57250, loss = 1.84 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:43:08.468592: step 57260, loss = 2.01 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:43:09.627449: step 57270, loss = 2.02 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:43:10.797324: step 57280, loss = 1.98 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:11.967322: step 57290, loss = 2.11 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:13.131435: step 57300, loss = 1.88 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:43:14.313706: step 57310, loss = 2.03 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:43:15.487330: step 57320, loss = 1.97 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:16.656421: step 57330, loss = 2.02 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:17.814773: step 57340, loss = 1.97 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:43:18.990041: step 57350, loss = 1.79 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:43:20.155771: step 57360, loss = 1.94 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:21.330567: step 57370, loss = 2.04 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:22.507458: step 57380, loss = 1.97 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:43:23.677642: step 57390, loss = 1.75 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:43:24.859952: step 57400, loss = 2.02 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:43:26.056689: step 57410, loss = 1.81 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:43:27.356261: step 57420, loss = 1.96 (985.0 examples/sec; 0.130 sec/batch)
2017-05-04 23:43:28.447013: step 57430, loss = 2.00 (1173.5 examples/sec; 0.109 sec/batch)
2017-05-04 23:43:29.638101: step 57440, loss = 2.09 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:43:30.831502: step 57450, loss = 1.98 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:43:32.020840: step 57460, loss = 1.93 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:43:33.245425: step 57470, loss = 1.96 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:43:34.458948: step 57480, loss = 1.97 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:43:35.670980: step 57490, loss = 2.01 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:43:36.901362: step 57500, loss = 1.85 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:43:38.105983: step 57510, loss = 2.14 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:43:39.324219: step 57520, loss = 1.95 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:43:40.531281: step 57530, loss = 1.98 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:43:41.741687: step 57540, loss = 2.05 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:43:42.967011: step 57550, loss = 2.09 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:43:44.193490: step 57560, loss = 1.89 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:43:45.412222: step 57570, loss = 2.03 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:43:46.629138: step 57580, loss = 1.99 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:43:47.844387: step 57590, loss = 1.94 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:43:49.067341: step 57600, loss = 2.03 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:43:50.275953: step 57610, loss = 2.04 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:43:51.479002: step 57620, loss = 2.07 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:43:52.680066: step 57630, loss = 2.10 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:43:53.884681: step 57640, loss = 1.97 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:43:55.108036: step 57650, loss = 2.30 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:43:56.335620: step 57660, loss = 2.06 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:43:57.563208: step 57670, loss = 2.00 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:43:58.783365: step 57680, loss = 1.90 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:43:59.995139: step 57690, loss = 2.01 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:01.209957: step 57700, loss = 2.05 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:02.405847: step 57710, loss = 1.92 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:03.612551: step 57720, loss = 2.04 (1060.7 examples/sec; 0.121 sec/batE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 599 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
ch)
2017-05-04 23:44:04.832026: step 57730, loss = 1.90 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:06.042880: step 57740, loss = 1.97 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:07.266124: step 57750, loss = 2.04 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:08.466128: step 57760, loss = 2.03 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:09.675142: step 57770, loss = 2.06 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:10.884759: step 57780, loss = 2.06 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:12.098619: step 57790, loss = 2.15 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:13.302667: step 57800, loss = 2.17 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:14.505274: step 57810, loss = 1.93 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:15.707368: step 57820, loss = 1.82 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:16.910657: step 57830, loss = 2.07 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:18.132298: step 57840, loss = 1.97 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:19.350299: step 57850, loss = 2.00 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:20.567110: step 57860, loss = 2.02 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:21.792839: step 57870, loss = 2.04 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:44:23.000278: step 57880, loss = 1.85 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:24.217894: step 57890, loss = 2.01 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:25.423819: step 57900, loss = 1.99 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:26.636863: step 57910, loss = 1.95 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:27.852649: step 57920, loss = 2.14 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:29.086348: step 57930, loss = 1.95 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:44:30.282723: step 57940, loss = 1.99 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:31.501380: step 57950, loss = 2.10 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:32.741486: step 57960, loss = 1.92 (1032.2 examples/sec; 0.124 sec/batch)
2017-05-04 23:44:33.937861: step 57970, loss = 2.21 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:35.165347: step 57980, loss = 1.89 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:44:36.392130: step 57990, loss = 1.91 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:44:37.599685: step 58000, loss = 2.00 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:38.820731: step 58010, loss = 2.00 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:40.022856: step 58020, loss = 2.07 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:41.243051: step 58030, loss = 1.94 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:42.447751: step 58040, loss = 1.99 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:43.662544: step 58050, loss = 2.04 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:44.881261: step 58060, loss = 1.85 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:46.082411: step 58070, loss = 2.32 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:47.282059: step 58080, loss = 2.10 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:48.491296: step 58090, loss = 1.99 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:49.713741: step 58100, loss = 1.87 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:50.913993: step 58110, loss = 2.00 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:44:52.133947: step 58120, loss = 2.08 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:53.341190: step 58130, loss = 1.96 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:54.557337: step 58140, loss = 1.81 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:44:55.769599: step 58150, loss = 1.88 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:56.975987: step 58160, loss = 1.89 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:44:58.161521: step 58170, loss = 1.95 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:44:59.361327: step 58180, loss = 2.04 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:45:00.548678: step 58190, loss = 1.98 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:45:01.729830: step 58200, loss = 2.00 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:02.908518: step 58210, loss = 1.95 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:04.085555: step 58220, loss = 2.09 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:05.263872: step 58230, loss = 1.91 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:06.422859: step 58240, loss = 1.88 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:07.580776: step 58250, loss = 1.93 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:08.741011: step 58260, loss = 1.96 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:09.928431: step 58270, loss = 1.87 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:45:11.092462: step 58280, loss = 1.87 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:12.277544: step 58290, loss = 1.87 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:45:13.447312: step 58300, loss = 1.84 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:14.624274: step 58310, loss = 2.03 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:15.774952: step 58320, loss = 1.92 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:45:16.952665: step 58330, loss = 2.01 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:18.111510: step 58340, loss = 2.00 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:19.288773: step 58350, loss = 1.96 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:20.454147: step 58360, loss = 1.95 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:21.610733: step 58370, loss = 2.07 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:22.754336: step 58380, loss = 2.04 (1119.3 examples/sec; 0.114 sec/batch)
2017-05-04 23:45:23.934425: step 58390, loss = 1.88 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:25.103703: step 58400, loss = 1.88 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:26.366354: step 58410, loss = 2.06 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-04 23:45:27.453955: step 58420, loss = 1.95 (1176.9 examples/sec; 0.109 sec/batch)
2017-05-04 23:45:28.633596: step 58430, loss = 2.16 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:29.794005: step 58440, loss = 2.02 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:30.971960: step 58450, loss = 2.03 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:32.129050: step 58460, loss = 2.03 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:33.300776: step 58470, loss = 2.00 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:34.462967: step 58480, loss = 2.01 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:35.640983: step 58490, loss = 1.98 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:36.808986: step 58500, loss = 1.88 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:37.985069: step 58510, loss = 1.94 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:39.163570: step 58520, loss = 2.00 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:40.327754: step 58530, loss = 2.01 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:41.496178: step 58540, loss = 2.02 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:42.671565: step 58550, loss = 1.90 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:43.850386: step 58560, loss = 1.91 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:45.004324: step 58570, loss = 1.92 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-04 23:45:46.157934: step 58580, loss = 2.03 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-04 23:45:47.333907: step 58590, loss = 1.86 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:48.518812: step 58600, loss = 2.05 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:49.687601: step 58610, loss = 1.90 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:50.855202: step 58620, loss = 1.96 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:52.021889: step 58630, loss = 2.10 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:45:53.201087: step 58640, loss = 1.93 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:54.360299: step 58650, loss = 1.91 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:45:55.539806: step 58660, loss = 1.95 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:56.719595: step 58670, loss = 2.09 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:57.897485: step 58680, loss = 2.02 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:45:59.077188: step 58690, loss = 2.19 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:00.261532: step 58700, loss = 1.95 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:01.443103: step 58710, loss = 1.98 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:02.632017: step 58720, loss = 1.99 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:46:03.793482: step 58730, loss = 2.02 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:04.962440: step 58740, loss = 2.00 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:06.129376: step 58750, loss = 2.00 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:07.308741: step 58760, loss = 2.16 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:08.493151: step 58770, loss = 1.89 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:09.671422: step 58780, loss = 1.85 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:10.834678: step 58790, loss = 2.27 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:12.012167: step 58800, loss = 1.92 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:13.187434: step 58810, loss = 1.86 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:14.345382: step 58820, loss = 1.92 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:15.519688: step 58830, loss = 2.02 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:16.695309: step 58840, loss = 1.97 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:17.868869: step 58850, loss = 2.15 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:19.034906: step 58860, loss = 1.94 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:20.205211: step 58870, loss = 1.90 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:21.381438: step 58880, loss = 1.97 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:22.548051: step 58890, loss = 2.03 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:23.719073: step 58900, loss = 1.97 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:24.883589: step 58910, loss = 2.05 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:26.034751: step 58920, loss = 1.95 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:46:27.205080: step 58930, loss = 1.98 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:28.372978: step 58940, loss = 1.96 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:29.527342: step 58950, loss = 1.91 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:46:30.695504: step 58960, loss = 2.07 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:31.862738: step 58970, loss = 1.90 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:33.028598: step 58980, loss = 1.95 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:34.179174: step 58990, loss = 1.99 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:46:35.345174: step 59000, loss = 2.02 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:36.517605: step 59010, loss = 2.08 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:37.682422: step 59020, loss = 1.91 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:38.869543: step 59030, loss = 1.81 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:46:40.033643: step 59040, loss = 1.93 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:41.211108: step 59050, loss = 1.99 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:42.374510: step 59060, loss = 2.02 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:43.545637: step 59070, loss = 1.81 (1093.0 examples/sec; 0.117 sec/batch)E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 611 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU

2017-05-04 23:46:44.706531: step 59080, loss = 1.92 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:45.865898: step 59090, loss = 1.83 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:47.064324: step 59100, loss = 2.04 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:46:48.255459: step 59110, loss = 2.00 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:46:49.426331: step 59120, loss = 1.95 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:50.592452: step 59130, loss = 2.02 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:51.759115: step 59140, loss = 2.08 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:52.929735: step 59150, loss = 1.96 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:54.072547: step 59160, loss = 1.98 (1120.0 examples/sec; 0.114 sec/batch)
2017-05-04 23:46:55.253374: step 59170, loss = 2.02 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:46:56.410964: step 59180, loss = 1.83 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:46:57.580146: step 59190, loss = 2.17 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:46:58.768556: step 59200, loss = 2.00 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:46:59.951539: step 59210, loss = 2.07 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:47:01.174748: step 59220, loss = 1.85 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:02.369778: step 59230, loss = 1.92 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:47:03.574264: step 59240, loss = 1.92 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:47:04.792833: step 59250, loss = 2.00 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:05.963990: step 59260, loss = 1.95 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:47:07.149190: step 59270, loss = 2.00 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:47:08.331804: step 59280, loss = 1.90 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:47:09.506903: step 59290, loss = 2.08 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:47:10.713881: step 59300, loss = 2.00 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:11.915024: step 59310, loss = 2.05 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:47:13.101193: step 59320, loss = 1.99 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:47:14.297790: step 59330, loss = 1.95 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:47:15.514266: step 59340, loss = 2.06 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:16.766273: step 59350, loss = 1.91 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-04 23:47:17.993193: step 59360, loss = 1.97 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:47:19.226277: step 59370, loss = 2.02 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:47:20.438449: step 59380, loss = 1.86 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:21.643904: step 59390, loss = 1.91 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:22.996129: step 59400, loss = 2.09 (946.6 examples/sec; 0.135 sec/batch)
2017-05-04 23:47:24.089353: step 59410, loss = 1.92 (1170.9 examples/sec; 0.109 sec/batch)
2017-05-04 23:47:25.308309: step 59420, loss = 1.95 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:26.527804: step 59430, loss = 2.07 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:27.732002: step 59440, loss = 1.97 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:47:28.963514: step 59450, loss = 1.76 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:47:30.174619: step 59460, loss = 1.86 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:31.386336: step 59470, loss = 1.90 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:32.600637: step 59480, loss = 1.99 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:33.820440: step 59490, loss = 2.02 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:35.026684: step 59500, loss = 2.08 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:36.243147: step 59510, loss = 2.01 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:37.471710: step 59520, loss = 1.92 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:47:38.692888: step 59530, loss = 1.84 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:39.906458: step 59540, loss = 1.95 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:41.154033: step 59550, loss = 2.02 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-04 23:47:42.351020: step 59560, loss = 1.81 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:47:43.575907: step 59570, loss = 1.94 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:44.801525: step 59580, loss = 1.90 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:47:46.015826: step 59590, loss = 1.79 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:47.328156: step 59600, loss = 1.92 (975.4 examples/sec; 0.131 sec/batch)
2017-05-04 23:47:48.445441: step 59610, loss = 2.07 (1145.6 examples/sec; 0.112 sec/batch)
2017-05-04 23:47:49.645596: step 59620, loss = 1.95 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:47:50.872853: step 59630, loss = 2.04 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:47:52.111299: step 59640, loss = 1.95 (1033.6 examples/sec; 0.124 sec/batch)
2017-05-04 23:47:53.319595: step 59650, loss = 1.99 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:47:54.539742: step 59660, loss = 1.97 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:55.759306: step 59670, loss = 2.09 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:56.977741: step 59680, loss = 1.99 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:47:58.164124: step 59690, loss = 1.90 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:47:59.363799: step 59700, loss = 1.96 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:48:00.585599: step 59710, loss = 2.07 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:01.766736: step 59720, loss = 2.21 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:48:02.964081: step 59730, loss = 1.95 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:48:04.167644: step 59740, loss = 1.97 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:48:05.367937: step 59750, loss = 1.97 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:48:06.563932: step 59760, loss = 1.93 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:48:07.764042: step 59770, loss = 1.93 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:48:08.941039: step 59780, loss = 2.04 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:48:10.122295: step 59790, loss = 1.87 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:48:11.317764: step 59800, loss = 1.86 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:48:12.528665: step 59810, loss = 1.97 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:13.732136: step 59820, loss = 2.01 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:48:14.965116: step 59830, loss = 1.91 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:48:16.183761: step 59840, loss = 1.98 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:17.425557: step 59850, loss = 1.99 (1030.8 examples/sec; 0.124 sec/batch)
2017-05-04 23:48:18.637772: step 59860, loss = 1.97 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:19.856732: step 59870, loss = 2.00 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:21.081216: step 59880, loss = 1.91 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:22.291215: step 59890, loss = 1.92 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:23.526342: step 59900, loss = 2.07 (1036.3 examples/sec; 0.124 sec/batch)
2017-05-04 23:48:24.729070: step 59910, loss = 1.90 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:48:25.925522: step 59920, loss = 1.88 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:48:27.151535: step 59930, loss = 1.91 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:48:28.374427: step 59940, loss = 1.91 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:29.613363: step 59950, loss = 2.18 (1033.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:48:30.803365: step 59960, loss = 1.99 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:48:32.024601: step 59970, loss = 1.96 (1048.1 examples/sec; 0.122 sec/batch)
201E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 622 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
7-05-04 23:48:33.254476: step 59980, loss = 1.97 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:48:34.466936: step 59990, loss = 1.93 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:35.665978: step 60000, loss = 1.92 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:48:36.882642: step 60010, loss = 2.05 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:38.090807: step 60020, loss = 1.96 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:39.320155: step 60030, loss = 2.00 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:48:40.536608: step 60040, loss = 1.92 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:41.761508: step 60050, loss = 2.02 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:42.956821: step 60060, loss = 2.15 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:48:44.192158: step 60070, loss = 1.93 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:48:45.386624: step 60080, loss = 1.99 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:48:46.608846: step 60090, loss = 1.85 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:47.832529: step 60100, loss = 1.96 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:49.053169: step 60110, loss = 1.93 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:50.278263: step 60120, loss = 1.94 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:48:51.510495: step 60130, loss = 1.79 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-04 23:48:52.728042: step 60140, loss = 1.98 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:48:53.972445: step 60150, loss = 1.84 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-04 23:48:55.198034: step 60160, loss = 1.85 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:48:56.408795: step 60170, loss = 2.11 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:48:57.634697: step 60180, loss = 1.98 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:48:58.850493: step 60190, loss = 2.00 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:00.022948: step 60200, loss = 2.07 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:49:01.248066: step 60210, loss = 2.14 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-04 23:49:02.450689: step 60220, loss = 2.04 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:49:03.681934: step 60230, loss = 1.90 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:49:04.895144: step 60240, loss = 1.91 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:06.096906: step 60250, loss = 2.02 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:49:07.301646: step 60260, loss = 2.02 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:49:08.522625: step 60270, loss = 2.01 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:09.736388: step 60280, loss = 1.95 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:10.948144: step 60290, loss = 1.81 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:12.187015: step 60300, loss = 2.09 (1033.2 examples/sec; 0.124 sec/batch)
2017-05-04 23:49:13.405012: step 60310, loss = 2.10 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:14.614956: step 60320, loss = 2.37 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:15.830993: step 60330, loss = 2.06 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:17.037709: step 60340, loss = 1.97 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:18.266888: step 60350, loss = 1.86 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:49:19.482164: step 60360, loss = 1.83 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:20.694385: step 60370, loss = 1.91 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:21.891951: step 60380, loss = 1.76 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:49:23.226905: step 60390, loss = 1.83 (958.8 examples/sec; 0.133 sec/batch)
2017-05-04 23:49:24.296422: step 60400, loss = 1.98 (1196.8 examples/sec; 0.107 sec/batch)
2017-05-04 23:49:25.510057: step 60410, loss = 1.98 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:26.724867: step 60420, loss = 1.88 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:27.949671: step 60430, loss = 1.89 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:29.167580: step 60440, loss = 2.13 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:30.387969: step 60450, loss = 1.99 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:31.579397: step 60460, loss = 1.87 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:49:32.816853: step 60470, loss = 2.02 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-04 23:49:33.992180: step 60480, loss = 2.12 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:49:35.242265: step 60490, loss = 1.92 (1023.9 examples/sec; 0.125 sec/batch)
2017-05-04 23:49:36.470597: step 60500, loss = 1.81 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:49:37.675734: step 60510, loss = 2.00 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:38.899212: step 60520, loss = 1.98 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:40.141854: step 60530, loss = 1.82 (1030.1 examples/sec; 0.124 sec/batch)
2017-05-04 23:49:41.342533: step 60540, loss = 2.06 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:49:42.564553: step 60550, loss = 1.86 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:43.773176: step 60560, loss = 2.02 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:44.979274: step 60570, loss = 2.26 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:46.176726: step 60580, loss = 1.94 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:49:47.501188: step 60590, loss = 1.97 (966.4 examples/sec; 0.132 sec/batch)
2017-05-04 23:49:48.610328: step 60600, loss = 1.98 (1154.0 examples/sec; 0.111 sec/batch)
2017-05-04 23:49:49.831033: step 60610, loss = 2.11 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:49:51.037499: step 60620, loss = 2.01 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:52.246098: step 60630, loss = 1.86 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:49:53.437098: step 60640, loss = 1.87 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:49:54.615052: step 60650, loss = 1.76 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:49:55.796469: step 60660, loss = 1.95 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:49:56.978108: step 60670, loss = 2.12 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:49:58.140042: step 60680, loss = 1.85 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:49:59.323208: step 60690, loss = 1.90 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:50:00.508538: step 60700, loss = 1.94 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:50:01.685077: step 60710, loss = 1.97 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:50:02.891430: step 60720, loss = 2.05 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:04.080003: step 60730, loss = 2.15 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:50:05.283655: step 60740, loss = 1.91 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:06.464990: step 60750, loss = 1.90 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:50:07.647772: step 60760, loss = 1.85 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:50:08.855168: step 60770, loss = 1.83 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:10.031651: step 60780, loss = 2.02 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:50:11.227437: step 60790, loss = 2.07 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:12.397969: step 60800, loss = 1.94 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:50:13.559212: step 60810, loss = 1.96 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:50:14.735942: step 60820, loss = 2.06 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:50:15.921320: step 60830, loss = 1.91 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:50:17.117015: step 60840, loss = 2.01 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:18.317116: step 60850, loss = 1.87 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:19.529683: step 60860, loss = 2.01 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:20.725994: step 60870, loss = 1.99 (1070.0 examples/sec; 0.120 sec/batch)
2017-05E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 633 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
-04 23:50:21.912904: step 60880, loss = 1.94 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:50:23.123882: step 60890, loss = 2.12 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:24.347283: step 60900, loss = 2.14 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:50:25.532762: step 60910, loss = 2.02 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:50:26.732051: step 60920, loss = 1.89 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:27.942363: step 60930, loss = 2.15 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:29.137260: step 60940, loss = 1.91 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:50:30.326610: step 60950, loss = 2.16 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:50:31.523566: step 60960, loss = 2.04 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:32.731437: step 60970, loss = 1.84 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:33.941903: step 60980, loss = 2.01 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:35.150831: step 60990, loss = 1.96 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:36.360530: step 61000, loss = 1.99 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:37.582797: step 61010, loss = 1.92 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:50:38.823386: step 61020, loss = 1.90 (1031.8 examples/sec; 0.124 sec/batch)
2017-05-04 23:50:40.059550: step 61030, loss = 1.93 (1035.5 examples/sec; 0.124 sec/batch)
2017-05-04 23:50:41.259334: step 61040, loss = 2.05 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:42.464530: step 61050, loss = 1.97 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:43.685070: step 61060, loss = 1.97 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:50:44.889101: step 61070, loss = 1.99 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:46.082395: step 61080, loss = 2.12 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:50:47.315582: step 61090, loss = 1.89 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:50:48.540032: step 61100, loss = 1.90 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:50:49.736696: step 61110, loss = 1.97 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:50.961726: step 61120, loss = 2.04 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:50:52.179845: step 61130, loss = 1.85 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:50:53.386497: step 61140, loss = 2.02 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:54.592756: step 61150, loss = 1.97 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:50:55.784336: step 61160, loss = 2.12 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:50:57.016003: step 61170, loss = 2.03 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:50:58.212039: step 61180, loss = 1.84 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:50:59.434480: step 61190, loss = 1.94 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:00.648505: step 61200, loss = 1.76 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:01.866769: step 61210, loss = 1.90 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:03.083553: step 61220, loss = 1.91 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:04.305384: step 61230, loss = 1.85 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:05.499324: step 61240, loss = 1.94 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:51:06.728811: step 61250, loss = 1.95 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:51:07.964142: step 61260, loss = 1.98 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-04 23:51:09.160866: step 61270, loss = 1.96 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:51:10.360619: step 61280, loss = 1.94 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:51:11.592215: step 61290, loss = 2.03 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:51:12.815128: step 61300, loss = 2.10 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:14.055722: step 61310, loss = 2.26 (1031.8 examples/sec; 0.124 sec/batch)
2017-05-04 23:51:15.243054: step 61320, loss = 1.84 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:51:16.462377: step 61330, loss = 1.94 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:17.679830: step 61340, loss = 1.96 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:18.902650: step 61350, loss = 2.04 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:20.087864: step 61360, loss = 2.06 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:51:21.316044: step 61370, loss = 2.02 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:51:22.627139: step 61380, loss = 1.93 (976.3 examples/sec; 0.131 sec/batch)
2017-05-04 23:51:23.702605: step 61390, loss = 1.92 (1190.2 examples/sec; 0.108 sec/batch)
2017-05-04 23:51:24.912172: step 61400, loss = 2.14 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:26.143603: step 61410, loss = 1.99 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:51:27.348529: step 61420, loss = 2.09 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:51:28.565985: step 61430, loss = 2.08 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:29.768962: step 61440, loss = 2.16 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:51:31.015486: step 61450, loss = 1.96 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-04 23:51:32.223033: step 61460, loss = 1.81 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:33.447333: step 61470, loss = 2.09 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:34.661406: step 61480, loss = 1.91 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:35.880935: step 61490, loss = 2.12 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:37.078841: step 61500, loss = 1.90 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:51:38.307926: step 61510, loss = 2.11 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:51:39.514141: step 61520, loss = 1.92 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:40.738717: step 61530, loss = 2.08 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:41.944218: step 61540, loss = 2.12 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:43.163296: step 61550, loss = 1.92 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:44.355532: step 61560, loss = 2.01 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:51:45.584782: step 61570, loss = 1.88 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:51:46.883481: step 61580, loss = 1.88 (985.6 examples/sec; 0.130 sec/batch)
2017-05-04 23:51:47.978645: step 61590, loss = 2.21 (1168.8 examples/sec; 0.110 sec/batch)
2017-05-04 23:51:49.199836: step 61600, loss = 1.90 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-04 23:51:50.403902: step 61610, loss = 1.90 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:51:51.637753: step 61620, loss = 2.02 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:51:52.880550: step 61630, loss = 1.99 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-04 23:51:54.087695: step 61640, loss = 1.98 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:55.297520: step 61650, loss = 1.99 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:56.525488: step 61660, loss = 1.97 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-04 23:51:57.738656: step 61670, loss = 2.02 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:51:58.951088: step 61680, loss = 2.10 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:00.160479: step 61690, loss = 2.08 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:01.377652: step 61700, loss = 1.79 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:02.579406: step 61710, loss = 1.98 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:52:03.809877: step 61720, loss = 1.95 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:05.033653: step 61730, loss = 2.06 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:06.227475: step 61740, loss = 1.92 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:52:07.451871: step 61750, loss = 1.96 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:08.650921: step 61760, loss = 2.09 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:52:09.873491: step 61770, loss = 2.02 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-04 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 643 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
23:52:11.061688: step 61780, loss = 1.94 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:52:12.277178: step 61790, loss = 1.95 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:13.467930: step 61800, loss = 1.91 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:52:14.720291: step 61810, loss = 1.84 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-04 23:52:15.919751: step 61820, loss = 2.12 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-04 23:52:17.159003: step 61830, loss = 2.08 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-04 23:52:18.366718: step 61840, loss = 2.07 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:19.609401: step 61850, loss = 1.81 (1030.0 examples/sec; 0.124 sec/batch)
2017-05-04 23:52:20.822650: step 61860, loss = 2.10 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:22.042334: step 61870, loss = 1.98 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:23.266890: step 61880, loss = 1.89 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:24.499881: step 61890, loss = 1.90 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:25.695787: step 61900, loss = 1.89 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:52:26.904855: step 61910, loss = 2.04 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:28.133475: step 61920, loss = 1.85 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:29.350116: step 61930, loss = 1.88 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:30.567612: step 61940, loss = 1.78 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:31.779192: step 61950, loss = 2.02 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:32.981400: step 61960, loss = 1.94 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:52:34.195092: step 61970, loss = 2.06 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:35.388543: step 61980, loss = 1.94 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:52:36.616202: step 61990, loss = 1.97 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:37.841191: step 62000, loss = 2.03 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:39.076493: step 62010, loss = 1.98 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-04 23:52:40.299671: step 62020, loss = 2.02 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:41.525740: step 62030, loss = 1.95 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:42.718481: step 62040, loss = 2.03 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:52:43.957780: step 62050, loss = 2.24 (1032.8 examples/sec; 0.124 sec/batch)
2017-05-04 23:52:45.189013: step 62060, loss = 1.84 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:46.374087: step 62070, loss = 1.89 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:52:47.603090: step 62080, loss = 2.00 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-04 23:52:48.822519: step 62090, loss = 1.93 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-04 23:52:50.030502: step 62100, loss = 2.11 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:51.237806: step 62110, loss = 2.01 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:52.440070: step 62120, loss = 1.97 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:52:53.648579: step 62130, loss = 1.95 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:54.861889: step 62140, loss = 1.96 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:56.099556: step 62150, loss = 2.02 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-04 23:52:57.308961: step 62160, loss = 2.08 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:58.519952: step 62170, loss = 1.87 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:52:59.705980: step 62180, loss = 1.96 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:53:00.906022: step 62190, loss = 1.89 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:53:02.138806: step 62200, loss = 2.08 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-04 23:53:03.384551: step 62210, loss = 2.31 (1027.5 examples/sec; 0.125 sec/batch)
2017-05-04 23:53:04.602283: step 62220, loss = 1.86 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:53:05.818874: step 62230, loss = 1.95 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:53:07.011576: step 62240, loss = 1.88 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:53:08.238992: step 62250, loss = 1.85 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-04 23:53:09.459653: step 62260, loss = 2.07 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:53:10.673079: step 62270, loss = 2.04 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:11.873615: step 62280, loss = 1.87 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:53:13.086686: step 62290, loss = 1.91 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:14.300382: step 62300, loss = 1.86 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:15.512757: step 62310, loss = 1.81 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:16.720433: step 62320, loss = 2.03 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:17.939563: step 62330, loss = 2.10 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:53:19.187954: step 62340, loss = 1.94 (1025.3 examples/sec; 0.125 sec/batch)
2017-05-04 23:53:20.383825: step 62350, loss = 1.87 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:53:21.598296: step 62360, loss = 2.06 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:22.900310: step 62370, loss = 2.09 (983.1 examples/sec; 0.130 sec/batch)
2017-05-04 23:53:24.015921: step 62380, loss = 2.02 (1147.3 examples/sec; 0.112 sec/batch)
2017-05-04 23:53:25.237806: step 62390, loss = 2.06 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-04 23:53:26.432393: step 62400, loss = 2.13 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:53:27.622722: step 62410, loss = 1.93 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:53:28.809615: step 62420, loss = 2.01 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:53:29.995704: step 62430, loss = 1.88 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:53:31.210076: step 62440, loss = 2.14 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:32.417645: step 62450, loss = 2.10 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:33.628466: step 62460, loss = 1.94 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:34.841106: step 62470, loss = 2.07 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:36.055479: step 62480, loss = 1.96 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:37.256372: step 62490, loss = 1.82 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:53:38.464434: step 62500, loss = 1.93 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-04 23:53:39.657346: step 62510, loss = 1.90 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:53:40.857141: step 62520, loss = 1.92 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:53:42.053740: step 62530, loss = 1.93 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-04 23:53:43.237564: step 62540, loss = 2.08 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:53:44.411540: step 62550, loss = 1.96 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:53:45.585353: step 62560, loss = 2.03 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:53:46.763569: step 62570, loss = 1.88 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:53:47.922399: step 62580, loss = 1.94 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:53:49.104995: step 62590, loss = 1.96 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:53:50.269593: step 62600, loss = 1.96 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:53:51.438938: step 62610, loss = 1.84 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:53:52.609987: step 62620, loss = 1.88 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:53:53.750015: step 62630, loss = 2.00 (1122.8 examples/sec; 0.114 sec/batch)
2017-05-04 23:53:54.914537: step 62640, loss = 1.96 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:53:56.082436: step 62650, loss = 1.91 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:53:57.252401: step 62660, loss = 1.95 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:53:58.415260: step 62670, loss = 2.01 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:53:59.586439: step 62680, loss = 2.02 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:00.761803: step 62690, loss = 1.98 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:01.917413: step 62700, loss = 1.96 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:54:03.084203: step 62710, loss = 2.08 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:04.256632: step 62720, loss = 1.86 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:05.434085: step 62730, loss = 1.95 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:06.593950: step 62740, loss = 1.96 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:54:07.744036: step 62750, loss = 2.08 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-04 23:54:08.908852: step 62760, loss = 1.91 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:54:10.065988: step 62770, loss = 1.87 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:54:11.239140: step 62780, loss = 2.00 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:12.405991: step 62790, loss = 1.88 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:13.576398: step 62800, loss = 1.94 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:14.743344: step 62810, loss = 2.04 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:15.929329: step 62820, loss = 2.00 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:54:17.101931: step 62830, loss = 2.01 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:18.283173: step 62840, loss = 1.97 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:19.470879: step 62850, loss = 1.83 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:54:20.663061: step 62860, loss = 1.99 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:54:21.841740: step 62870, loss = 1.93 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:23.044539: step 62880, loss = 1.96 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-04 23:54:24.254147: step 62890, loss = 1.80 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-04 23:54:25.451670: step 62900, loss = 2.03 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:54:26.659382: step 62910, loss = 2.16 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-04 23:54:27.882618: step 62920, loss = 1.84 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-04 23:54:29.101573: step 62930, loss = 1.88 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-04 23:54:30.301880: step 62940, loss = 1.85 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:54:31.502608: step 62950, loss = 1.91 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:54:32.757329: step 62960, loss = 1.98 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-04 23:54:33.937197: step 62970, loss = 2.05 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:35.156204: step 62980, loss = 1.97 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-04 23:54:36.376518: step 62990, loss = 1.98 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-04 23:54:37.575012: step 63000, loss = 1.96 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:54:38.777085: step 63010, loss = 2.08 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:54:39.970729: step 63020, loss = 2.09 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:54:41.151904: step 63030, loss = 2.00 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:42.322322: step 63040, loss = 1.90 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:43.519330: step 63050, loss = 2.01 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-04 23:54:44.693390: step 63060, loss = 2.19 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:45.877833: step 63070, loss = 2.02 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:47.056759: step 63080, loss = 2.06 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:48.215967: step 63090, loss = 1.91 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:54:49.404866: step 63100, loss = 1.91 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:54:50.581849: step 63110, loss = 2.00 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:51.753625: step 63120, loss = 1.91 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:5E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 654 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
4:52.947397: step 63130, loss = 1.90 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-04 23:54:54.106722: step 63140, loss = 2.02 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:54:55.289870: step 63150, loss = 1.86 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:56.465948: step 63160, loss = 1.92 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:54:57.639467: step 63170, loss = 2.04 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:54:58.829441: step 63180, loss = 1.99 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:54:59.994836: step 63190, loss = 2.12 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:01.163064: step 63200, loss = 2.02 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:02.354348: step 63210, loss = 1.90 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-04 23:55:03.544083: step 63220, loss = 1.94 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:55:04.691512: step 63230, loss = 1.95 (1115.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:55:05.860301: step 63240, loss = 2.02 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:07.035871: step 63250, loss = 2.08 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:55:08.216075: step 63260, loss = 1.89 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:55:09.391869: step 63270, loss = 1.88 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:55:10.551929: step 63280, loss = 1.90 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:11.724806: step 63290, loss = 1.86 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:12.894731: step 63300, loss = 1.96 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:14.058679: step 63310, loss = 1.96 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:15.229518: step 63320, loss = 1.95 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:16.383967: step 63330, loss = 1.78 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:55:17.554014: step 63340, loss = 1.91 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:18.722734: step 63350, loss = 2.00 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:19.975644: step 63360, loss = 1.92 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-04 23:55:21.033911: step 63370, loss = 1.98 (1209.5 examples/sec; 0.106 sec/batch)
2017-05-04 23:55:22.205125: step 63380, loss = 1.97 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:23.385335: step 63390, loss = 1.87 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:55:24.559522: step 63400, loss = 2.00 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:25.709420: step 63410, loss = 1.86 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-04 23:55:26.872571: step 63420, loss = 2.11 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:28.044027: step 63430, loss = 1.92 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:29.214858: step 63440, loss = 1.98 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:30.359949: step 63450, loss = 1.99 (1117.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:55:31.543128: step 63460, loss = 1.96 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:55:32.712297: step 63470, loss = 1.88 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:33.865170: step 63480, loss = 1.96 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:55:35.032416: step 63490, loss = 2.18 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:36.193818: step 63500, loss = 1.96 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:37.329988: step 63510, loss = 2.01 (1126.6 examples/sec; 0.114 sec/batch)
2017-05-04 23:55:38.485509: step 63520, loss = 1.88 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:39.671901: step 63530, loss = 1.86 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-04 23:55:40.849601: step 63540, loss = 2.10 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:55:41.993581: step 63550, loss = 1.93 (1118.9 examples/sec; 0.114 sec/batch)
2017-05-04 23:55:43.176469: step 63560, loss = 2.16 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:55:44.322639: step 63570, loss = 2.15 (1116.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:55:45.489358: step 63580, loss = 1.93 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:46.658960: step 63590, loss = 1.98 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:47.829134: step 63600, loss = 2.01 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:48.996028: step 63610, loss = 2.02 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:50.160696: step 63620, loss = 1.88 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:51.326479: step 63630, loss = 1.82 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:52.478587: step 63640, loss = 2.10 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-04 23:55:53.646449: step 63650, loss = 2.00 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:54.805873: step 63660, loss = 1.89 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:55.966133: step 63670, loss = 1.88 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:57.138120: step 63680, loss = 1.96 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:55:58.301513: step 63690, loss = 1.98 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:55:59.448015: step 63700, loss = 2.02 (1116.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:56:00.630690: step 63710, loss = 1.98 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:01.802928: step 63720, loss = 1.93 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:02.984894: step 63730, loss = 2.07 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:04.144902: step 63740, loss = 1.80 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:05.313607: step 63750, loss = 2.02 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:06.468537: step 63760, loss = 1.93 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:56:07.632085: step 63770, loss = 2.05 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:08.802360: step 63780, loss = 2.03 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:09.962140: step 63790, loss = 1.87 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:11.139682: step 63800, loss = 2.17 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:12.328933: step 63810, loss = 1.95 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:56:13.494064: step 63820, loss = 1.90 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:14.679095: step 63830, loss = 2.01 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:56:15.857881: step 63840, loss = 1.97 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:17.017450: step 63850, loss = 1.98 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:18.167179: step 63860, loss = 1.96 (1113.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:56:19.337864: step 63870, loss = 1.82 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:20.507161: step 63880, loss = 1.83 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:21.666164: step 63890, loss = 1.80 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:22.841409: step 63900, loss = 2.11 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:24.025491: step 63910, loss = 1.91 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:25.177823: step 63920, loss = 2.04 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:56:26.333615: step 63930, loss = 1.99 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:27.509112: step 63940, loss = 1.94 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:28.691015: step 63950, loss = 1.94 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:29.854890: step 63960, loss = 1.86 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:31.048459: step 63970, loss = 2.05 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:56:32.220026: step 63980, loss = 2.00 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:33.393600: step 63990, loss = 2.03 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:34.547612: step 64000, loss = 1.93 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-04 23:56:35.707133: step 64010, loss = 1.88 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:36.881462: step 64020, loss = 2.01 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 665 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
38.033858: step 64030, loss = 2.04 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:56:39.218712: step 64040, loss = 1.98 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:40.403501: step 64050, loss = 1.95 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:41.587413: step 64060, loss = 1.92 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:42.747749: step 64070, loss = 2.04 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:43.918725: step 64080, loss = 1.97 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:45.092372: step 64090, loss = 2.06 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:46.246141: step 64100, loss = 1.87 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:56:47.426644: step 64110, loss = 1.97 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:56:48.590021: step 64120, loss = 1.86 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:49.752039: step 64130, loss = 2.00 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:50.925643: step 64140, loss = 2.00 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:52.088592: step 64150, loss = 2.04 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:53.261969: step 64160, loss = 1.89 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:54.416521: step 64170, loss = 1.84 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:56:55.582296: step 64180, loss = 1.87 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:56:56.741837: step 64190, loss = 2.04 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:57.898364: step 64200, loss = 1.95 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:56:59.069846: step 64210, loss = 2.07 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:00.246668: step 64220, loss = 1.90 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:01.420077: step 64230, loss = 1.99 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:02.578931: step 64240, loss = 1.96 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:57:03.774607: step 64250, loss = 1.87 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:57:04.949295: step 64260, loss = 2.15 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:06.110429: step 64270, loss = 1.88 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:57:07.286138: step 64280, loss = 1.95 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:08.457492: step 64290, loss = 1.96 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:09.625009: step 64300, loss = 1.91 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:10.791223: step 64310, loss = 2.05 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:11.964352: step 64320, loss = 1.94 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:13.162392: step 64330, loss = 2.11 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:57:14.344953: step 64340, loss = 2.04 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:15.653268: step 64350, loss = 1.95 (978.4 examples/sec; 0.131 sec/batch)
2017-05-04 23:57:16.737869: step 64360, loss = 1.91 (1180.1 examples/sec; 0.108 sec/batch)
2017-05-04 23:57:17.911958: step 64370, loss = 2.01 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:19.088778: step 64380, loss = 2.00 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:20.284115: step 64390, loss = 2.12 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-04 23:57:21.471207: step 64400, loss = 2.05 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:57:22.667010: step 64410, loss = 2.03 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-04 23:57:23.873692: step 64420, loss = 1.84 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-04 23:57:25.068203: step 64430, loss = 1.92 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:57:26.270025: step 64440, loss = 1.97 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-04 23:57:27.443268: step 64450, loss = 1.96 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:28.630010: step 64460, loss = 2.05 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:57:29.819491: step 64470, loss = 1.95 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:57:31.017489: step 64480, loss = 1.90 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-04 23:57:32.209784: step 64490, loss = 2.02 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:57:33.409513: step 64500, loss = 2.08 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-04 23:57:34.584812: step 64510, loss = 1.97 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:35.767111: step 64520, loss = 2.04 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:36.951873: step 64530, loss = 2.00 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:38.121000: step 64540, loss = 2.09 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:39.329400: step 64550, loss = 1.88 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-04 23:57:40.476506: step 64560, loss = 1.94 (1115.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:57:41.642880: step 64570, loss = 1.78 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:42.804281: step 64580, loss = 2.16 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:57:43.971661: step 64590, loss = 1.94 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:45.157886: step 64600, loss = 1.94 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:57:46.321998: step 64610, loss = 1.89 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:57:47.512403: step 64620, loss = 2.07 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-04 23:57:48.671578: step 64630, loss = 2.02 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:57:49.829025: step 64640, loss = 1.99 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:57:50.996611: step 64650, loss = 1.81 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:52.159273: step 64660, loss = 1.94 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:57:53.337315: step 64670, loss = 1.88 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:54.490702: step 64680, loss = 2.00 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:57:55.684931: step 64690, loss = 1.85 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-04 23:57:56.866664: step 64700, loss = 1.99 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:57:58.037036: step 64710, loss = 1.96 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:57:59.215485: step 64720, loss = 2.00 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:00.388544: step 64730, loss = 1.94 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:01.546108: step 64740, loss = 1.89 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:02.739659: step 64750, loss = 2.20 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:58:03.919050: step 64760, loss = 1.85 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:05.073564: step 64770, loss = 1.91 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:58:06.246019: step 64780, loss = 2.05 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:07.415764: step 64790, loss = 2.18 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:08.572556: step 64800, loss = 2.19 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:09.721330: step 64810, loss = 1.96 (1114.2 examples/sec; 0.115 sec/batch)
2017-05-04 23:58:10.903545: step 64820, loss = 1.99 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:12.072861: step 64830, loss = 2.02 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:13.236112: step 64840, loss = 2.01 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:14.385193: step 64850, loss = 2.03 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:58:15.545768: step 64860, loss = 1.95 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:16.707584: step 64870, loss = 2.05 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:17.875948: step 64880, loss = 1.93 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:19.059200: step 64890, loss = 1.83 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:20.216442: step 64900, loss = 1.91 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:21.352352: step 64910, loss = 1.90 (1126.8 examples/sec; 0.114 sec/batch)
2017-05-04 23:58:22.512193: step 64920, loss = 1.93 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:23.689047: step 64930, loss = 1.97 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:24.862372: step 64940, loss = 2.13 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:26.002255: step 64950, loss = 1.87 (1122.9 examples/sec; 0.114 sec/batch)
2017-05-04 23:58:27.177501: step 64960, loss = 2.04 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:28.346574: step 64970, loss = 1.95 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:29.523113: step 64980, loss = 1.86 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:30.692642: step 64990, loss = 2.00 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:31.853349: step 65000, loss = 1.87 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:33.014879: step 65010, loss = 2.02 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:34.179068: step 65020, loss = 1.90 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:35.384362: step 65030, loss = 1.78 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-04 23:58:36.560171: step 65040, loss = 1.86 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:37.717638: step 65050, loss = 1.91 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:38.885287: step 65060, loss = 1.76 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:40.050368: step 65070, loss = 2.06 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:41.217625: step 65080, loss = 1.82 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:42.372085: step 65090, loss = 2.14 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-04 23:58:43.550913: step 65100, loss = 2.12 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:44.725248: step 65110, loss = 1.90 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:45.877473: step 65120, loss = 1.81 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-04 23:58:47.055297: step 65130, loss = 1.91 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:48.226884: step 65140, loss = 2.28 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:49.386615: step 65150, loss = 1.97 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:50.558489: step 65160, loss = 1.79 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:51.722900: step 65170, loss = 1.88 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:58:52.888262: step 65180, loss = 2.14 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:54.056295: step 65190, loss = 2.03 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:55.229015: step 65200, loss = 2.02 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:56.384007: step 65210, loss = 1.96 (1108.2 examples/sec; 0.115 sec/batch)
2017-05-04 23:58:57.552048: step 65220, loss = 1.97 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-04 23:58:58.730068: step 65230, loss = 2.02 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-04 23:58:59.924821: step 65240, loss = 1.89 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:59:01.073994: step 65250, loss = 1.88 (1113.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:59:02.262370: step 65260, loss = 1.96 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-04 23:59:03.442789: step 65270, loss = 2.05 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:04.625660: step 65280, loss = 2.00 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:05.776452: step 65290, loss = 1.98 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-04 23:59:06.945596: step 65300, loss = 2.09 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:08.110956: step 65310, loss = 1.96 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:09.281734: step 65320, loss = 2.10 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:10.439016: step 65330, loss = 1.95 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:11.699520: step 65340, loss = 1.92 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-04 23:59:12.770894: step 65350, loss = 2.03 (1194.7 examples/sec; 0.107 sec/batch)
2017-05-04 23:59:13.943828: step 65360, loss = 2.13 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:15.101028: step 65370, loss = 2.08 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:16.2E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 677 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
53779: step 65380, loss = 1.88 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-04 23:59:17.442594: step 65390, loss = 2.00 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-04 23:59:18.600681: step 65400, loss = 1.89 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:19.773600: step 65410, loss = 2.02 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:20.958831: step 65420, loss = 2.01 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:59:22.101894: step 65430, loss = 1.99 (1119.8 examples/sec; 0.114 sec/batch)
2017-05-04 23:59:23.274468: step 65440, loss = 1.86 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:24.440557: step 65450, loss = 1.99 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:25.591877: step 65460, loss = 2.00 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-04 23:59:26.754972: step 65470, loss = 2.02 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:27.939020: step 65480, loss = 2.02 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:29.116777: step 65490, loss = 1.96 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:30.289568: step 65500, loss = 2.00 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:31.452865: step 65510, loss = 2.00 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:32.605543: step 65520, loss = 1.93 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-04 23:59:33.769787: step 65530, loss = 1.94 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:34.970955: step 65540, loss = 1.81 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-04 23:59:36.152998: step 65550, loss = 1.96 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:37.337820: step 65560, loss = 1.87 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:38.521544: step 65570, loss = 2.01 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:39.712653: step 65580, loss = 1.86 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-04 23:59:40.896536: step 65590, loss = 1.90 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:42.071584: step 65600, loss = 1.99 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:43.243260: step 65610, loss = 1.96 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:44.429066: step 65620, loss = 1.76 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-04 23:59:45.591972: step 65630, loss = 1.98 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:46.764866: step 65640, loss = 1.87 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:47.950104: step 65650, loss = 1.99 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:59:49.138566: step 65660, loss = 2.07 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-04 23:59:50.288631: step 65670, loss = 1.97 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-04 23:59:51.455485: step 65680, loss = 1.97 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:52.625210: step 65690, loss = 1.96 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:53.794094: step 65700, loss = 1.84 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-04 23:59:54.959085: step 65710, loss = 1.94 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:56.119358: step 65720, loss = 1.96 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-04 23:59:57.296462: step 65730, loss = 1.87 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:58.471606: step 65740, loss = 1.98 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-04 23:59:59.661528: step 65750, loss = 2.01 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:00:00.839596: step 65760, loss = 1.86 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:02.017770: step 65770, loss = 1.90 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:03.222325: step 65780, loss = 1.99 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:00:04.403648: step 65790, loss = 1.85 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:05.583298: step 65800, loss = 1.94 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:06.768715: step 65810, loss = 1.95 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:00:07.949917: step 65820, loss = 1.92 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:09.127863: step 65830, loss = 1.92 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:10.316906: step 65840, loss = 1.95 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:00:11.512439: step 65850, loss = 2.12 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:00:12.694165: step 65860, loss = 1.93 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:13.841280: step 65870, loss = 1.92 (1115.8 examples/sec; 0.115 sec/batch)
2017-05-05 00:00:14.992567: step 65880, loss = 2.03 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-05 00:00:16.145622: step 65890, loss = 2.03 (1110.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:00:17.324454: step 65900, loss = 2.10 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:18.491159: step 65910, loss = 1.93 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:19.654812: step 65920, loss = 2.07 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:20.847511: step 65930, loss = 1.84 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:00:22.024298: step 65940, loss = 1.92 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:23.165491: step 65950, loss = 2.05 (1121.6 examples/sec; 0.114 sec/batch)
2017-05-05 00:00:24.319131: step 65960, loss = 1.98 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:00:25.487012: step 65970, loss = 2.07 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:26.646290: step 65980, loss = 2.02 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:27.822724: step 65990, loss = 1.93 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:28.989352: step 66000, loss = 1.98 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:30.170671: step 66010, loss = 1.90 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:31.331780: step 66020, loss = 1.90 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:32.506651: step 66030, loss = 1.91 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:33.669025: step 66040, loss = 2.13 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:34.831884: step 66050, loss = 1.93 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:36.026074: step 66060, loss = 2.01 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:00:37.181874: step 66070, loss = 2.01 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:38.336507: step 66080, loss = 1.85 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:00:39.509322: step 66090, loss = 2.06 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:40.684936: step 66100, loss = 1.90 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:41.832418: step 66110, loss = 1.92 (1115.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:00:42.996052: step 66120, loss = 2.04 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:44.175920: step 66130, loss = 1.90 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:45.360647: step 66140, loss = 2.01 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:46.534502: step 66150, loss = 1.92 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:47.695152: step 66160, loss = 1.92 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:48.860224: step 66170, loss = 1.95 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:50.022531: step 66180, loss = 1.99 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:00:51.174050: step 66190, loss = 2.02 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:00:52.344560: step 66200, loss = 1.79 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:53.519592: step 66210, loss = 1.99 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:00:54.685021: step 66220, loss = 2.15 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:55.851132: step 66230, loss = 1.95 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:56.992281: step 66240, loss = 2.07 (1121.7 examples/sec; 0.114 sec/batch)
2017-05-05 00:00:58.158346: step 66250, loss = 2.02 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:00:59.319640: step 66260, loss = 1.94 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:00.502659: step 66270, loss = 1.91 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:01.683E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 688 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
648: step 66280, loss = 1.89 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:02.845515: step 66290, loss = 2.01 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:03.997504: step 66300, loss = 2.07 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:01:05.157299: step 66310, loss = 1.96 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:06.336183: step 66320, loss = 2.06 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:07.604464: step 66330, loss = 2.15 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-05 00:01:08.694327: step 66340, loss = 1.96 (1174.5 examples/sec; 0.109 sec/batch)
2017-05-05 00:01:09.867338: step 66350, loss = 1.88 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:11.043682: step 66360, loss = 2.08 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:12.223992: step 66370, loss = 2.21 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:13.395487: step 66380, loss = 2.07 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:14.557604: step 66390, loss = 2.04 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:15.736112: step 66400, loss = 1.89 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:16.897951: step 66410, loss = 1.96 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:18.048925: step 66420, loss = 1.97 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:01:19.215332: step 66430, loss = 2.08 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:20.383566: step 66440, loss = 1.96 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:21.541410: step 66450, loss = 1.86 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:22.722867: step 66460, loss = 2.04 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:23.906680: step 66470, loss = 2.06 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:25.064353: step 66480, loss = 1.97 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:26.226936: step 66490, loss = 1.93 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:27.419497: step 66500, loss = 2.12 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:01:28.587781: step 66510, loss = 1.96 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:29.765863: step 66520, loss = 1.91 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:30.949000: step 66530, loss = 1.81 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:32.133930: step 66540, loss = 1.87 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:33.310154: step 66550, loss = 1.89 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:34.480953: step 66560, loss = 1.98 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:35.641916: step 66570, loss = 1.98 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:36.823087: step 66580, loss = 1.90 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:38.000563: step 66590, loss = 2.03 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:39.183707: step 66600, loss = 2.04 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:40.354435: step 66610, loss = 1.98 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:41.515415: step 66620, loss = 1.97 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:42.685089: step 66630, loss = 2.25 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:43.869696: step 66640, loss = 1.96 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:45.041301: step 66650, loss = 1.96 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:46.201597: step 66660, loss = 2.13 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:47.372878: step 66670, loss = 1.96 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:48.552295: step 66680, loss = 1.87 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:49.713649: step 66690, loss = 1.99 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:50.875922: step 66700, loss = 1.96 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:01:52.049787: step 66710, loss = 1.91 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:01:53.244896: step 66720, loss = 1.96 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:01:54.424046: step 66730, loss = 2.05 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:01:55.612283: step 66740, loss = 2.06 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:01:56.811718: step 66750, loss = 2.04 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:01:57.998475: step 66760, loss = 1.91 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:01:59.176994: step 66770, loss = 1.99 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:02:00.344046: step 66780, loss = 1.81 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:02:01.520928: step 66790, loss = 1.95 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:02:02.687270: step 66800, loss = 1.95 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:02:03.898295: step 66810, loss = 2.02 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:02:05.102365: step 66820, loss = 2.08 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:06.299511: step 66830, loss = 1.94 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:07.510987: step 66840, loss = 1.87 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:02:08.706460: step 66850, loss = 1.99 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:09.887139: step 66860, loss = 2.01 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:02:11.067128: step 66870, loss = 1.90 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:02:12.241176: step 66880, loss = 2.15 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:02:13.424766: step 66890, loss = 2.02 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:02:14.601064: step 66900, loss = 1.87 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:02:15.802356: step 66910, loss = 2.08 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:17.009454: step 66920, loss = 2.05 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:02:18.231891: step 66930, loss = 1.94 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:19.446954: step 66940, loss = 1.99 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:20.664052: step 66950, loss = 1.92 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:21.866377: step 66960, loss = 1.92 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:23.110307: step 66970, loss = 1.92 (1029.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:02:24.334102: step 66980, loss = 1.91 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:25.550063: step 66990, loss = 1.94 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:26.771501: step 67000, loss = 2.00 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:28.003811: step 67010, loss = 1.94 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:02:29.223630: step 67020, loss = 1.93 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:30.419603: step 67030, loss = 1.95 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:31.629385: step 67040, loss = 1.98 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:02:32.838584: step 67050, loss = 1.85 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:02:34.054053: step 67060, loss = 2.05 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:35.281904: step 67070, loss = 1.91 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:02:36.497572: step 67080, loss = 2.05 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:37.716462: step 67090, loss = 1.82 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:38.942393: step 67100, loss = 2.22 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:02:40.146008: step 67110, loss = 2.00 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:41.385543: step 67120, loss = 1.96 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:02:42.594339: step 67130, loss = 2.18 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:02:43.812899: step 67140, loss = 2.04 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:45.049851: step 67150, loss = 1.88 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:02:46.248151: step 67160, loss = 2.07 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:47.461794: step 67170, loss = 2.01 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:02:48.68649E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 698 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
8: step 67180, loss = 2.03 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:49.910951: step 67190, loss = 2.25 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:51.135367: step 67200, loss = 2.02 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:52.352544: step 67210, loss = 1.90 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:53.563133: step 67220, loss = 1.98 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:02:54.788018: step 67230, loss = 1.94 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:56.004921: step 67240, loss = 1.83 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:57.225655: step 67250, loss = 1.95 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:02:58.421411: step 67260, loss = 1.81 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:02:59.628198: step 67270, loss = 1.99 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:00.827519: step 67280, loss = 2.04 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:03:02.037330: step 67290, loss = 1.96 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:03.254447: step 67300, loss = 1.84 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:04.490090: step 67310, loss = 1.88 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-05 00:03:05.775857: step 67320, loss = 2.04 (995.5 examples/sec; 0.129 sec/batch)
2017-05-05 00:03:06.891012: step 67330, loss = 1.90 (1147.8 examples/sec; 0.112 sec/batch)
2017-05-05 00:03:08.100410: step 67340, loss = 1.98 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:09.345288: step 67350, loss = 1.96 (1028.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:03:10.539191: step 67360, loss = 1.98 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:03:11.756671: step 67370, loss = 1.87 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:12.974840: step 67380, loss = 1.86 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:14.197119: step 67390, loss = 1.97 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:15.426634: step 67400, loss = 1.84 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:03:16.621569: step 67410, loss = 2.00 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:03:17.815574: step 67420, loss = 1.87 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:03:19.032792: step 67430, loss = 1.93 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:20.244182: step 67440, loss = 1.84 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:21.452444: step 67450, loss = 1.93 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:22.673137: step 67460, loss = 1.96 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:23.895459: step 67470, loss = 1.90 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:25.122473: step 67480, loss = 1.83 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:03:26.309190: step 67490, loss = 1.98 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:03:27.521774: step 67500, loss = 2.00 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:28.739575: step 67510, loss = 2.06 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:29.915024: step 67520, loss = 1.94 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:03:31.128708: step 67530, loss = 2.02 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:32.328820: step 67540, loss = 1.98 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:03:33.517092: step 67550, loss = 1.83 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:03:34.720918: step 67560, loss = 1.99 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:03:35.922358: step 67570, loss = 1.89 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:03:37.117587: step 67580, loss = 1.97 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:03:38.288507: step 67590, loss = 2.01 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:03:39.495357: step 67600, loss = 2.06 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:40.683753: step 67610, loss = 1.86 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:03:41.870631: step 67620, loss = 1.84 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:03:43.067910: step 67630, loss = 1.96 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:03:44.259273: step 67640, loss = 2.06 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:03:45.458190: step 67650, loss = 1.94 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:03:46.674181: step 67660, loss = 1.94 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:47.902034: step 67670, loss = 1.97 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:03:49.118746: step 67680, loss = 2.11 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:50.334972: step 67690, loss = 1.88 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:51.556760: step 67700, loss = 1.98 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:03:52.762056: step 67710, loss = 1.95 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:03:53.940649: step 67720, loss = 1.95 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:03:55.126744: step 67730, loss = 2.10 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:03:56.309300: step 67740, loss = 1.91 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:03:57.472067: step 67750, loss = 1.90 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:03:58.644825: step 67760, loss = 1.96 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:03:59.796689: step 67770, loss = 1.81 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:04:00.967071: step 67780, loss = 1.95 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:02.134066: step 67790, loss = 1.94 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:03.282915: step 67800, loss = 2.24 (1114.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:04:04.453293: step 67810, loss = 1.91 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:05.618690: step 67820, loss = 1.88 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:06.778949: step 67830, loss = 1.92 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:07.935465: step 67840, loss = 1.88 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:09.091159: step 67850, loss = 1.84 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:10.259421: step 67860, loss = 1.90 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:11.437437: step 67870, loss = 2.12 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:04:12.605517: step 67880, loss = 1.93 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:13.755829: step 67890, loss = 1.99 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:04:14.910904: step 67900, loss = 1.92 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:16.060955: step 67910, loss = 1.93 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:04:17.233678: step 67920, loss = 1.94 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:18.391118: step 67930, loss = 2.04 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:19.561313: step 67940, loss = 1.98 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:20.724717: step 67950, loss = 1.83 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:21.900876: step 67960, loss = 1.98 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:04:23.084933: step 67970, loss = 2.05 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:04:24.244156: step 67980, loss = 2.14 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:25.407672: step 67990, loss = 1.86 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:26.569397: step 68000, loss = 2.07 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:27.752903: step 68010, loss = 2.04 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:04:28.920746: step 68020, loss = 2.02 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:30.067545: step 68030, loss = 2.11 (1116.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:04:31.235888: step 68040, loss = 1.89 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:32.405690: step 68050, loss = 1.96 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:33.551970: step 68060, loss = 2.09 (1116.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:04:34.720497: step 68070, loss = 1.95 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:04:35.884778: step 68080, loss = 2.03 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:37.074322: step 68090, loss = 1.83 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:04:38.252283: step 68100, loss = 1.88 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:04:39.459985: step 68110, loss = 1.85 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:04:40.636379: step 68120, loss = 1.92 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:04:41.792575: step 68130, loss = 1.97 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:04:42.976170: step 68140, loss = 1.93 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:04:44.164223: step 68150, loss = 2.15 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:04:45.359556: step 68160, loss = 2.10 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:04:46.550825: step 68170, loss = 1.88 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:04:47.756719: step 68180, loss = 1.98 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:04:48.964202: step 68190, loss = 1.96 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:04:50.156735: step 68200, loss = 1.92 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:04:51.359292: step 68210, loss = 1.86 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:04:52.568984: step 68220, loss = 1.99 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:04:53.759182: step 68230, loss = 1.96 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:04:54.981714: step 68240, loss = 1.95 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:04:56.192574: step 68250, loss = 1.92 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:04:57.392332: step 68260, loss = 1.84 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:04:58.611963: step 68270, loss = 2.03 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:04:59.825399: step 68280, loss = 1.89 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:01.049379: step 68290, loss = 1.83 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:02.277946: step 68300, loss = 1.94 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:05:03.596434: step 68310, loss = 2.05 (970.8 examples/sec; 0.132 sec/batch)
2017-05-05 00:05:04.709368: step 68320, loss = 1.91 (1150.1 examples/sec; 0.111 sec/batch)
2017-05-05 00:05:05.921554: step 68330, loss = 2.00 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:07.157110: step 68340, loss = 1.89 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:05:08.371514: step 68350, loss = 1.76 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:09.580002: step 68360, loss = 2.08 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:10.801008: step 68370, loss = 2.13 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:12.035566: step 68380, loss = 2.06 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:05:13.259793: step 68390, loss = 2.19 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:14.472987: step 68400, loss = 2.00 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:15.691106: step 68410, loss = 1.87 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:16.925594: step 68420, loss = 1.97 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:05:18.132172: step 68430, loss = 1.77 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:19.351466: step 68440, loss = 1.83 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:20.576822: step 68450, loss = 2.07 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:05:21.804106: step 68460, loss = 1.99 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:05:23.012883: step 68470, loss = 1.89 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:24.232538: step 68480, loss = 1.91 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:25.449291: step 68490, loss = 2.05 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:26.658611: step 68500, loss = 2.04 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:27.846347: step 68510, loss = 1.85 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:05:29.080574: step 68520, loss = 2.06 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:05:30.303611: stE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 709 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
ep 68530, loss = 2.08 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:31.528943: step 68540, loss = 1.96 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:05:32.749010: step 68550, loss = 1.93 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:33.949486: step 68560, loss = 1.97 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:05:35.180952: step 68570, loss = 2.13 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:05:36.418219: step 68580, loss = 1.97 (1034.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:05:37.629174: step 68590, loss = 1.94 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:38.851122: step 68600, loss = 1.90 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:40.070968: step 68610, loss = 1.82 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:41.284866: step 68620, loss = 1.83 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:42.503678: step 68630, loss = 2.16 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:43.725808: step 68640, loss = 1.90 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:44.942358: step 68650, loss = 1.99 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:46.150292: step 68660, loss = 1.88 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:47.367594: step 68670, loss = 1.99 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:05:48.578100: step 68680, loss = 1.91 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:49.782320: step 68690, loss = 1.95 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:05:50.993456: step 68700, loss = 1.98 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:52.228620: step 68710, loss = 1.93 (1036.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:05:53.439663: step 68720, loss = 2.17 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:54.644841: step 68730, loss = 1.98 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:55.851889: step 68740, loss = 2.06 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:57.062068: step 68750, loss = 1.86 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:58.267644: step 68760, loss = 1.85 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:05:59.493750: step 68770, loss = 1.80 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:06:00.723859: step 68780, loss = 2.03 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:06:01.959441: step 68790, loss = 1.91 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:06:03.190091: step 68800, loss = 2.07 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:06:04.419580: step 68810, loss = 1.79 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:06:05.639466: step 68820, loss = 2.12 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:06:06.871017: step 68830, loss = 2.05 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:06:08.088337: step 68840, loss = 2.17 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:06:09.320642: step 68850, loss = 2.01 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:06:10.525050: step 68860, loss = 2.02 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:06:11.753319: step 68870, loss = 1.99 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:06:12.969178: step 68880, loss = 2.03 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:06:14.172736: step 68890, loss = 1.88 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:06:15.397630: step 68900, loss = 1.85 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:06:16.587868: step 68910, loss = 1.96 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:06:17.802759: step 68920, loss = 1.88 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:06:19.002474: step 68930, loss = 1.91 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:06:20.207659: step 68940, loss = 1.80 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:06:21.401607: step 68950, loss = 1.99 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:06:22.612762: step 68960, loss = 2.07 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:06:23.824089: step 68970, loss = 1.92 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:06:25.031404: step 68980, loss = 2.08 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:06:26.217524: step 68990, loss = 1.94 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:06:27.428142: step 69000, loss = 1.94 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:06:28.633610: step 69010, loss = 2.00 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:06:29.809894: step 69020, loss = 1.77 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:06:31.000939: step 69030, loss = 1.96 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:06:32.172561: step 69040, loss = 1.92 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:06:33.342019: step 69050, loss = 2.19 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:06:34.506021: step 69060, loss = 2.11 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:06:35.682768: step 69070, loss = 1.90 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:06:36.851389: step 69080, loss = 1.89 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:06:38.009918: step 69090, loss = 1.92 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:06:39.183656: step 69100, loss = 2.19 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:06:40.376570: step 69110, loss = 1.97 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:06:41.555834: step 69120, loss = 2.01 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:06:42.739958: step 69130, loss = 2.00 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:06:43.933723: step 69140, loss = 1.99 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:06:45.119338: step 69150, loss = 1.90 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:06:46.275341: step 69160, loss = 2.08 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:06:47.457562: step 69170, loss = 2.02 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:06:48.627120: step 69180, loss = 1.82 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:06:49.787738: step 69190, loss = 1.93 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:06:50.974972: step 69200, loss = 1.91 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:06:52.138495: step 69210, loss = 1.85 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:06:53.309303: step 69220, loss = 2.02 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:06:54.471860: step 69230, loss = 1.89 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:06:55.635438: step 69240, loss = 1.90 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:06:56.771874: step 69250, loss = 1.94 (1126.3 examples/sec; 0.114 sec/batch)
2017-05-05 00:06:57.946791: step 69260, loss = 2.04 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:06:59.116531: step 69270, loss = 1.96 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:07:00.297217: step 69280, loss = 1.95 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:07:01.487947: step 69290, loss = 1.89 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:07:02.771551: step 69300, loss = 1.89 (997.2 examples/sec; 0.128 sec/batch)
2017-05-05 00:07:03.880779: step 69310, loss = 1.99 (1153.9 examples/sec; 0.111 sec/batch)
2017-05-05 00:07:05.081312: step 69320, loss = 2.13 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:07:06.272008: step 69330, loss = 1.96 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:07:07.460157: step 69340, loss = 1.95 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:07:08.631214: step 69350, loss = 1.89 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:07:09.806603: step 69360, loss = 1.96 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:07:10.996401: step 69370, loss = 1.87 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:07:12.190777: step 69380, loss = 1.77 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:07:13.387688: step 69390, loss = 2.13 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:07:14.571471: step 69400, loss = 1.94 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:07:15.755073: step 69410, loss = 1.89 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:07:16.939855: step 69420, loss = 2.03 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:07:18.110383: step E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 720 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
69430, loss = 1.92 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:07:19.294524: step 69440, loss = 1.97 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:07:20.474613: step 69450, loss = 1.81 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:07:21.660750: step 69460, loss = 1.90 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:07:22.864176: step 69470, loss = 2.06 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:07:24.080821: step 69480, loss = 1.98 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:25.304202: step 69490, loss = 1.89 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:26.505944: step 69500, loss = 1.91 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:07:27.727574: step 69510, loss = 1.89 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:28.961572: step 69520, loss = 1.92 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:07:30.169064: step 69530, loss = 2.07 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:07:31.381156: step 69540, loss = 2.02 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:07:32.601213: step 69550, loss = 2.16 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:33.800936: step 69560, loss = 1.89 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:07:35.032024: step 69570, loss = 1.91 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:07:36.249058: step 69580, loss = 1.92 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:37.455725: step 69590, loss = 1.93 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:07:38.663590: step 69600, loss = 1.90 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:07:39.887870: step 69610, loss = 1.82 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:41.121065: step 69620, loss = 2.13 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:07:42.357119: step 69630, loss = 2.04 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:07:43.564963: step 69640, loss = 2.05 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:07:44.803462: step 69650, loss = 2.01 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:07:46.023128: step 69660, loss = 1.82 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:47.246012: step 69670, loss = 1.97 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:48.470300: step 69680, loss = 2.06 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:49.683363: step 69690, loss = 1.83 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:07:50.864141: step 69700, loss = 1.99 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:07:52.089135: step 69710, loss = 2.01 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:53.326564: step 69720, loss = 1.87 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:07:54.540174: step 69730, loss = 2.05 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:07:55.758093: step 69740, loss = 2.03 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:56.980876: step 69750, loss = 2.02 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:07:58.194382: step 69760, loss = 1.90 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:07:59.422417: step 69770, loss = 2.03 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:00.630158: step 69780, loss = 2.02 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:08:01.886126: step 69790, loss = 1.89 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-05 00:08:03.108555: step 69800, loss = 1.90 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:04.333717: step 69810, loss = 1.97 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:05.534591: step 69820, loss = 1.95 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:08:06.751280: step 69830, loss = 2.03 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:07.979087: step 69840, loss = 2.12 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:09.205173: step 69850, loss = 2.15 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:10.436671: step 69860, loss = 1.92 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:11.675872: step 69870, loss = 1.91 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-05 00:08:12.885562: step 69880, loss = 1.91 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:08:14.102835: step 69890, loss = 2.00 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:15.307937: step 69900, loss = 1.92 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:08:16.541835: step 69910, loss = 1.98 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:17.741896: step 69920, loss = 2.08 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:08:18.969111: step 69930, loss = 1.92 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:20.188995: step 69940, loss = 1.88 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:21.402963: step 69950, loss = 1.95 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:08:22.613173: step 69960, loss = 1.90 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:08:23.839027: step 69970, loss = 2.05 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:25.069077: step 69980, loss = 1.97 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:26.293085: step 69990, loss = 1.84 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:27.507922: step 70000, loss = 1.97 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:08:28.731313: step 70010, loss = 2.00 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:29.952382: step 70020, loss = 2.01 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:31.181669: step 70030, loss = 2.10 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:32.383814: step 70040, loss = 1.77 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:08:33.594507: step 70050, loss = 1.75 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:08:34.834141: step 70060, loss = 1.89 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:08:36.071913: step 70070, loss = 2.08 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-05 00:08:37.291676: step 70080, loss = 1.76 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:38.517039: step 70090, loss = 2.10 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:39.707861: step 70100, loss = 1.91 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:08:40.925651: step 70110, loss = 1.91 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:42.155491: step 70120, loss = 2.09 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:43.391193: step 70130, loss = 2.02 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:08:44.606619: step 70140, loss = 2.01 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:45.807422: step 70150, loss = 1.94 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:08:47.048588: step 70160, loss = 1.97 (1031.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:08:48.250436: step 70170, loss = 1.84 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:08:49.475159: step 70180, loss = 1.94 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:50.699916: step 70190, loss = 2.16 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:51.927149: step 70200, loss = 1.97 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:53.159585: step 70210, loss = 1.84 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:54.361397: step 70220, loss = 1.93 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:08:55.581484: step 70230, loss = 1.90 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:56.812851: step 70240, loss = 1.92 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:08:58.030471: step 70250, loss = 1.97 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:08:59.240273: step 70260, loss = 1.94 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:00.463128: step 70270, loss = 1.96 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:01.671545: step 70280, loss = 1.93 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:02.975362: step 70290, loss = 1.88 (981.7 examples/sec; 0.130 sec/batch)
2017-05-05 00:09:04.086314: step 70300, loss = 1.93 (1152.2 examples/sec; 0.111 sec/batch)
2017-05-05 00:09:05.305528: step 70310, loss = 1.91 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:06.522439: step 70320, loss = 1.87 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:07.757455: step 70330, loss = 2.09 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:09:08.980568: step 70340, loss = 2.09 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:10.196042: step 70350, loss = 1.96 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:11.421295: step 70360, loss = 2.02 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:12.640579: step 70370, loss = 2.13 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:13.846526: step 70380, loss = 2.00 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:15.069665: step 70390, loss = 1.81 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:16.270094: step 70400, loss = 2.09 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:09:17.497543: step 70410, loss = 1.84 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:18.725742: step 70420, loss = 1.88 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:19.969097: step 70430, loss = 1.84 (1029.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:09:21.201732: step 70440, loss = 1.95 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:22.406854: step 70450, loss = 1.83 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:23.628061: step 70460, loss = 1.88 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:24.862358: step 70470, loss = 2.20 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:26.074533: step 70480, loss = 1.95 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:27.256172: step 70490, loss = 1.94 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:09:28.483961: step 70500, loss = 1.94 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:29.695413: step 70510, loss = 2.07 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:30.916599: step 70520, loss = 1.98 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:32.155147: step 70530, loss = 2.14 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:09:33.381890: step 70540, loss = 1.87 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:34.593727: step 70550, loss = 1.87 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:35.814167: step 70560, loss = 1.94 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:37.043182: step 70570, loss = 2.13 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:38.260620: step 70580, loss = 1.94 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:39.494373: step 70590, loss = 2.01 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:40.716704: step 70600, loss = 1.86 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:41.933490: step 70610, loss = 1.82 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:43.150616: step 70620, loss = 1.99 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:44.361550: step 70630, loss = 2.03 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:45.568574: step 70640, loss = 1.99 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:46.785917: step 70650, loss = 1.85 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:48.007688: step 70660, loss = 1.98 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:49.234422: step 70670, loss = 2.10 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:50.446313: step 70680, loss = 1.94 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:51.649068: step 70690, loss = 2.01 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:09:52.867119: step 70700, loss = 2.05 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:54.091620: step 70710, loss = 1.96 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:55.302487: step 70720, loss = 2.01 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:09:56.531594: step 70730, loss = 2.01 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:09:57.748851: step 70740, loss = 1.95 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:09:58.974716: step 70750, loss = 2.04 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:00.187382: step 70760, loss = 1.88 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:01.399990: step 70770, loss = 1.97 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:02.627991: step 7078E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 731 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
0, loss = 1.74 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:03.855663: step 70790, loss = 1.99 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:05.083604: step 70800, loss = 1.88 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:06.319660: step 70810, loss = 2.01 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:10:07.530828: step 70820, loss = 1.94 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:08.763112: step 70830, loss = 2.03 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:09.974301: step 70840, loss = 1.92 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:11.208841: step 70850, loss = 2.06 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:12.431636: step 70860, loss = 2.02 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:13.645074: step 70870, loss = 1.98 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:14.884738: step 70880, loss = 1.97 (1032.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:10:16.071093: step 70890, loss = 2.05 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:10:17.259192: step 70900, loss = 1.93 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:10:18.484445: step 70910, loss = 1.89 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:19.717890: step 70920, loss = 1.84 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:20.935449: step 70930, loss = 1.70 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:22.153054: step 70940, loss = 2.02 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:23.368117: step 70950, loss = 1.96 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:24.597518: step 70960, loss = 1.86 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:25.799116: step 70970, loss = 2.10 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:10:27.016026: step 70980, loss = 1.96 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:28.228472: step 70990, loss = 1.91 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:29.437605: step 71000, loss = 1.93 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:30.659493: step 71010, loss = 1.98 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:31.884593: step 71020, loss = 1.99 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:33.112620: step 71030, loss = 1.88 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:34.344768: step 71040, loss = 2.09 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:35.574607: step 71050, loss = 1.83 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:36.795539: step 71060, loss = 1.89 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:38.001495: step 71070, loss = 1.98 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:39.234379: step 71080, loss = 1.97 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:40.421796: step 71090, loss = 2.04 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:10:41.631359: step 71100, loss = 1.97 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:42.859792: step 71110, loss = 2.00 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:44.098554: step 71120, loss = 1.91 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:10:45.303090: step 71130, loss = 1.89 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:10:46.507504: step 71140, loss = 1.99 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:10:47.741114: step 71150, loss = 1.93 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:48.965531: step 71160, loss = 2.03 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:50.187755: step 71170, loss = 1.96 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:51.419142: step 71180, loss = 1.92 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:52.629813: step 71190, loss = 2.07 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:10:53.851665: step 71200, loss = 1.95 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:55.087367: step 71210, loss = 2.14 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:10:56.287812: step 71220, loss = 2.02 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:10:57.511758: step 71230, loss = 1.96 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:10:58.739270: step 71240, loss = 1.81 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:10:59.963271: step 71250, loss = 2.12 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:01.189657: step 71260, loss = 1.83 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:11:02.409508: step 71270, loss = 1.97 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:03.706641: step 71280, loss = 1.83 (986.8 examples/sec; 0.130 sec/batch)
2017-05-05 00:11:04.824518: step 71290, loss = 2.13 (1145.0 examples/sec; 0.112 sec/batch)
2017-05-05 00:11:06.037065: step 71300, loss = 1.99 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:07.258440: step 71310, loss = 1.82 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:08.466722: step 71320, loss = 1.86 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:09.669341: step 71330, loss = 1.92 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:11:10.881490: step 71340, loss = 1.94 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:12.098643: step 71350, loss = 1.78 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:13.334591: step 71360, loss = 1.97 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:11:14.536184: step 71370, loss = 1.99 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:11:15.761156: step 71380, loss = 2.12 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:16.977316: step 71390, loss = 1.85 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:18.186063: step 71400, loss = 1.94 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:19.393725: step 71410, loss = 2.05 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:20.619608: step 71420, loss = 1.95 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:11:21.852933: step 71430, loss = 1.92 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:11:23.072691: step 71440, loss = 1.86 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:24.302266: step 71450, loss = 1.89 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:11:25.509451: step 71460, loss = 1.82 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:26.714782: step 71470, loss = 1.95 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:27.903455: step 71480, loss = 1.89 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:11:29.142522: step 71490, loss = 1.85 (1033.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:11:30.356952: step 71500, loss = 2.05 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:31.588700: step 71510, loss = 1.90 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:11:32.810478: step 71520, loss = 1.99 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:34.022989: step 71530, loss = 2.02 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:35.254706: step 71540, loss = 1.96 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:11:36.505126: step 71550, loss = 1.93 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-05 00:11:37.711194: step 71560, loss = 1.95 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:38.941496: step 71570, loss = 2.00 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:11:40.171718: step 71580, loss = 2.11 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:11:41.391969: step 71590, loss = 2.05 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:42.623653: step 71600, loss = 2.04 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:11:43.860109: step 71610, loss = 1.99 (1035.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:11:45.093433: step 71620, loss = 1.95 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:11:46.305747: step 71630, loss = 2.02 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:47.513647: step 71640, loss = 1.92 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:48.748504: step 71650, loss = 1.83 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:11:49.967629: step 71660, loss = 2.17 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:51.171806: step 71670, loss = 1.91 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:11:52.347159: step 71680, E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 741 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
loss = 1.92 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:11:53.567025: step 71690, loss = 1.96 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:11:54.781024: step 71700, loss = 2.08 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:55.995583: step 71710, loss = 1.83 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:57.191907: step 71720, loss = 2.00 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:11:58.401948: step 71730, loss = 1.95 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:11:59.637813: step 71740, loss = 1.95 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:12:00.856700: step 71750, loss = 1.86 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:02.077196: step 71760, loss = 1.88 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:03.297800: step 71770, loss = 2.02 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:04.533331: step 71780, loss = 1.90 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:12:05.739337: step 71790, loss = 1.84 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:06.965696: step 71800, loss = 2.03 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:08.188917: step 71810, loss = 1.92 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:09.408507: step 71820, loss = 1.95 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:10.605129: step 71830, loss = 2.05 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:12:11.816048: step 71840, loss = 1.96 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:13.051068: step 71850, loss = 1.97 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:12:14.259225: step 71860, loss = 1.87 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:15.477967: step 71870, loss = 1.94 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:16.711358: step 71880, loss = 2.00 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:17.923065: step 71890, loss = 1.86 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:19.155466: step 71900, loss = 1.84 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:20.388025: step 71910, loss = 1.89 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:21.623323: step 71920, loss = 2.10 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:12:22.842347: step 71930, loss = 1.87 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:24.073733: step 71940, loss = 1.92 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:25.286867: step 71950, loss = 1.87 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:26.514268: step 71960, loss = 1.81 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:27.738161: step 71970, loss = 1.98 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:28.948544: step 71980, loss = 1.92 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:30.155986: step 71990, loss = 1.84 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:31.388054: step 72000, loss = 1.89 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:32.622497: step 72010, loss = 1.91 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:33.834710: step 72020, loss = 1.92 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:35.070496: step 72030, loss = 2.12 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:12:36.277800: step 72040, loss = 2.06 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:37.480470: step 72050, loss = 2.00 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:12:38.709690: step 72060, loss = 2.03 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:39.940441: step 72070, loss = 1.97 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:41.122692: step 72080, loss = 2.00 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:12:42.338902: step 72090, loss = 2.02 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:43.568594: step 72100, loss = 1.97 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:44.795172: step 72110, loss = 1.89 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:46.020863: step 72120, loss = 1.86 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:47.250342: step 72130, loss = 2.04 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:48.472015: step 72140, loss = 1.90 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:49.703785: step 72150, loss = 1.87 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:50.923488: step 72160, loss = 2.06 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:52.141466: step 72170, loss = 2.00 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:53.351086: step 72180, loss = 1.89 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:54.565617: step 72190, loss = 2.00 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:12:55.792901: step 72200, loss = 2.06 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:12:57.015711: step 72210, loss = 1.96 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:58.231491: step 72220, loss = 1.96 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:12:59.449971: step 72230, loss = 1.87 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:00.668440: step 72240, loss = 1.88 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:01.871010: step 72250, loss = 1.88 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:13:03.081837: step 72260, loss = 1.84 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:13:04.390313: step 72270, loss = 2.10 (978.2 examples/sec; 0.131 sec/batch)
2017-05-05 00:13:05.488304: step 72280, loss = 1.95 (1165.8 examples/sec; 0.110 sec/batch)
2017-05-05 00:13:06.707641: step 72290, loss = 1.99 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:07.928832: step 72300, loss = 1.87 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:09.164129: step 72310, loss = 1.75 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:13:10.374677: step 72320, loss = 1.84 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:13:11.589808: step 72330, loss = 2.01 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:12.813377: step 72340, loss = 1.85 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:14.038553: step 72350, loss = 1.91 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:13:15.239122: step 72360, loss = 1.96 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:13:16.466332: step 72370, loss = 2.03 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:13:17.679408: step 72380, loss = 1.84 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:13:18.913628: step 72390, loss = 2.02 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:13:20.151315: step 72400, loss = 1.98 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:13:21.355038: step 72410, loss = 1.85 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:13:22.585899: step 72420, loss = 1.89 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:13:23.818446: step 72430, loss = 1.99 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:13:25.039887: step 72440, loss = 2.03 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:26.249606: step 72450, loss = 1.98 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:13:27.468663: step 72460, loss = 1.83 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:28.666852: step 72470, loss = 2.09 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:13:29.875049: step 72480, loss = 2.00 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:13:31.107581: step 72490, loss = 2.19 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:13:32.327107: step 72500, loss = 2.08 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:33.549287: step 72510, loss = 1.93 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:34.777581: step 72520, loss = 1.98 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:13:35.997219: step 72530, loss = 1.95 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:37.219054: step 72540, loss = 1.96 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:38.438579: step 72550, loss = 2.02 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:39.680828: step 72560, loss = 1.87 (1030.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:13:40.903600: step 72570, loss = 2.00 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:42.124464: step 72580, losE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 751 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
s = 1.86 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:43.364018: step 72590, loss = 2.04 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:13:44.586494: step 72600, loss = 1.96 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:45.791627: step 72610, loss = 2.03 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:13:47.005536: step 72620, loss = 1.88 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:13:48.239223: step 72630, loss = 1.89 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:13:49.458839: step 72640, loss = 1.96 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:50.675577: step 72650, loss = 1.90 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:51.918658: step 72660, loss = 1.88 (1029.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:13:53.085595: step 72670, loss = 1.94 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:13:54.302151: step 72680, loss = 1.82 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:13:55.502942: step 72690, loss = 2.03 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:13:56.703726: step 72700, loss = 1.96 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:13:57.891364: step 72710, loss = 2.16 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:13:59.082940: step 72720, loss = 1.93 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:14:00.274734: step 72730, loss = 2.04 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:14:01.465030: step 72740, loss = 1.77 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:14:02.664275: step 72750, loss = 1.80 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:14:03.885179: step 72760, loss = 1.98 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:05.111440: step 72770, loss = 1.96 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:06.321403: step 72780, loss = 1.97 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:07.553992: step 72790, loss = 1.95 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:08.783093: step 72800, loss = 1.95 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:09.994053: step 72810, loss = 1.78 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:11.209877: step 72820, loss = 1.93 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:12.442220: step 72830, loss = 2.04 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:13.651808: step 72840, loss = 2.05 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:14.886756: step 72850, loss = 1.93 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:16.112522: step 72860, loss = 2.06 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:17.311034: step 72870, loss = 1.95 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:14:18.535825: step 72880, loss = 1.87 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:19.766857: step 72890, loss = 2.05 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:20.984313: step 72900, loss = 1.95 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:22.198105: step 72910, loss = 2.04 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:23.430639: step 72920, loss = 1.94 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:24.646902: step 72930, loss = 1.83 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:25.849278: step 72940, loss = 2.01 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:14:27.063390: step 72950, loss = 2.05 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:28.293347: step 72960, loss = 1.89 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:29.496168: step 72970, loss = 1.90 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:14:30.705655: step 72980, loss = 1.96 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:31.930308: step 72990, loss = 1.86 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:33.160126: step 73000, loss = 1.93 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:34.379003: step 73010, loss = 1.95 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:35.607679: step 73020, loss = 1.91 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:36.819500: step 73030, loss = 2.03 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:38.037586: step 73040, loss = 1.97 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:39.255845: step 73050, loss = 1.98 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:40.470824: step 73060, loss = 1.98 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:41.660152: step 73070, loss = 2.01 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:14:42.888649: step 73080, loss = 2.03 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:44.108746: step 73090, loss = 1.85 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:45.326403: step 73100, loss = 2.04 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:46.539056: step 73110, loss = 2.02 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:47.754085: step 73120, loss = 1.86 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:14:48.984196: step 73130, loss = 1.84 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:50.185835: step 73140, loss = 1.96 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:14:51.416180: step 73150, loss = 1.87 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:52.642273: step 73160, loss = 1.80 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:53.855819: step 73170, loss = 2.19 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:55.082651: step 73180, loss = 1.85 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:56.325547: step 73190, loss = 1.93 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-05 00:14:57.539603: step 73200, loss = 1.79 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:14:58.774151: step 73210, loss = 2.05 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:14:59.986741: step 73220, loss = 1.95 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:01.204310: step 73230, loss = 1.95 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:02.426053: step 73240, loss = 2.03 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:03.649460: step 73250, loss = 1.91 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:04.953347: step 73260, loss = 1.84 (981.7 examples/sec; 0.130 sec/batch)
2017-05-05 00:15:06.045161: step 73270, loss = 2.00 (1172.4 examples/sec; 0.109 sec/batch)
2017-05-05 00:15:07.271216: step 73280, loss = 2.00 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:15:08.506031: step 73290, loss = 1.92 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:15:09.728574: step 73300, loss = 1.93 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:10.940132: step 73310, loss = 2.01 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:12.152769: step 73320, loss = 1.93 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:13.387351: step 73330, loss = 1.90 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:15:14.629330: step 73340, loss = 1.80 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:15:15.871457: step 73350, loss = 1.88 (1030.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:15:17.094900: step 73360, loss = 1.93 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:18.310202: step 73370, loss = 2.10 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:19.539519: step 73380, loss = 1.87 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:15:20.751901: step 73390, loss = 2.03 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:21.962860: step 73400, loss = 2.03 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:23.179766: step 73410, loss = 1.95 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:24.404210: step 73420, loss = 1.86 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:25.630557: step 73430, loss = 1.76 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:15:26.866078: step 73440, loss = 1.90 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:15:28.081214: step 73450, loss = 1.91 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:29.255517: step 73460, loss = 1.96 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:15:30.452514: step 73470, loss = 2.06 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:15:31.661424: step 73480, loss =E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 761 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
 2.03 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:32.869902: step 73490, loss = 1.95 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:34.067607: step 73500, loss = 1.84 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:15:35.282774: step 73510, loss = 1.88 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:36.484262: step 73520, loss = 2.09 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:15:37.701405: step 73530, loss = 2.00 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:38.917375: step 73540, loss = 1.96 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:40.123279: step 73550, loss = 2.01 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:41.311841: step 73560, loss = 2.11 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:15:42.481114: step 73570, loss = 2.13 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:15:43.686005: step 73580, loss = 1.80 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:15:44.892345: step 73590, loss = 2.04 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:46.063383: step 73600, loss = 1.87 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:15:47.260693: step 73610, loss = 1.86 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:15:48.430450: step 73620, loss = 1.94 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:15:49.622553: step 73630, loss = 1.80 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:15:50.817609: step 73640, loss = 1.89 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:15:52.018550: step 73650, loss = 1.99 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:15:53.230576: step 73660, loss = 1.89 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:15:54.451967: step 73670, loss = 1.88 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:55.676886: step 73680, loss = 2.07 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:56.894297: step 73690, loss = 1.94 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:58.118835: step 73700, loss = 1.83 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:15:59.335743: step 73710, loss = 1.77 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:00.562622: step 73720, loss = 1.88 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:16:01.778929: step 73730, loss = 1.87 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:02.992111: step 73740, loss = 2.03 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:04.225395: step 73750, loss = 2.00 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:16:05.463788: step 73760, loss = 2.04 (1033.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:16:06.663827: step 73770, loss = 1.94 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:16:07.882362: step 73780, loss = 1.95 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:09.094444: step 73790, loss = 1.95 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:10.317785: step 73800, loss = 1.93 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:11.541612: step 73810, loss = 2.01 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:12.768355: step 73820, loss = 1.88 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:16:13.987490: step 73830, loss = 1.98 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:15.222886: step 73840, loss = 1.84 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-05 00:16:16.413451: step 73850, loss = 2.00 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:16:17.606566: step 73860, loss = 1.84 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:16:18.825554: step 73870, loss = 2.20 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:20.043175: step 73880, loss = 1.93 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:21.265154: step 73890, loss = 1.90 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:22.469781: step 73900, loss = 2.12 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:16:23.674943: step 73910, loss = 1.75 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:24.904564: step 73920, loss = 1.75 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:16:26.117586: step 73930, loss = 1.73 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:27.328626: step 73940, loss = 1.82 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:28.549777: step 73950, loss = 1.97 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:29.765843: step 73960, loss = 1.96 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:30.981978: step 73970, loss = 1.76 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:32.193467: step 73980, loss = 1.76 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:33.406189: step 73990, loss = 2.21 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:34.614547: step 74000, loss = 2.05 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:35.842837: step 74010, loss = 1.92 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:16:37.075795: step 74020, loss = 1.99 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:16:38.278665: step 74030, loss = 1.99 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:16:39.508082: step 74040, loss = 1.90 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:16:40.733667: step 74050, loss = 1.87 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:16:41.905862: step 74060, loss = 1.95 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:16:43.122584: step 74070, loss = 1.97 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:44.331798: step 74080, loss = 1.89 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:45.549046: step 74090, loss = 1.97 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:46.770929: step 74100, loss = 2.12 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:47.967631: step 74110, loss = 1.98 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:16:49.189459: step 74120, loss = 1.85 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:50.407951: step 74130, loss = 2.07 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:51.624296: step 74140, loss = 1.96 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:16:52.858056: step 74150, loss = 2.01 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:16:54.063927: step 74160, loss = 1.93 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:55.269701: step 74170, loss = 1.90 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:16:56.462057: step 74180, loss = 2.02 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:16:57.660564: step 74190, loss = 1.91 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:16:58.870806: step 74200, loss = 2.12 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:00.084021: step 74210, loss = 1.85 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:01.309141: step 74220, loss = 1.93 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:02.534278: step 74230, loss = 1.87 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:03.766425: step 74240, loss = 1.78 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:05.067478: step 74250, loss = 1.90 (983.8 examples/sec; 0.130 sec/batch)
2017-05-05 00:17:06.205333: step 74260, loss = 1.99 (1124.9 examples/sec; 0.114 sec/batch)
2017-05-05 00:17:07.421108: step 74270, loss = 1.85 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:08.645395: step 74280, loss = 1.92 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:09.863983: step 74290, loss = 1.87 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:11.084918: step 74300, loss = 2.14 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:12.306871: step 74310, loss = 2.07 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:13.516544: step 74320, loss = 2.02 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:14.711172: step 74330, loss = 1.91 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:17:15.941993: step 74340, loss = 2.00 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:17.160734: step 74350, loss = 1.86 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:18.368015: step 74360, loss = 1.86 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:19.587270: step 74370, loss = 1.97 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:20.803421: step 74380, loss = 2.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 771 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
03 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:22.004103: step 74390, loss = 2.05 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:17:23.228448: step 74400, loss = 1.92 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:24.442831: step 74410, loss = 2.08 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:25.645539: step 74420, loss = 1.88 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:17:26.876913: step 74430, loss = 2.06 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:28.110418: step 74440, loss = 2.03 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:29.310865: step 74450, loss = 1.84 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:17:30.518437: step 74460, loss = 1.98 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:31.731474: step 74470, loss = 1.90 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:32.948203: step 74480, loss = 1.94 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:34.158422: step 74490, loss = 2.00 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:35.369741: step 74500, loss = 1.79 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:36.584915: step 74510, loss = 1.82 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:37.789096: step 74520, loss = 2.07 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:17:39.017203: step 74530, loss = 2.09 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:40.247429: step 74540, loss = 2.00 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:41.457371: step 74550, loss = 1.86 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:42.672130: step 74560, loss = 2.04 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:43.891674: step 74570, loss = 1.90 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:45.116926: step 74580, loss = 2.19 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:46.325540: step 74590, loss = 2.00 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:17:47.547399: step 74600, loss = 1.90 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:48.767654: step 74610, loss = 1.96 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:50.001645: step 74620, loss = 2.10 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:51.243468: step 74630, loss = 2.01 (1030.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:17:52.471078: step 74640, loss = 2.03 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:53.641794: step 74650, loss = 1.93 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:17:54.861620: step 74660, loss = 1.89 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:56.091170: step 74670, loss = 2.02 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:57.323515: step 74680, loss = 1.78 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:17:58.541755: step 74690, loss = 1.99 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:17:59.760471: step 74700, loss = 1.79 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:00.978919: step 74710, loss = 1.99 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:02.205470: step 74720, loss = 2.14 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:18:03.435218: step 74730, loss = 1.95 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:18:04.658126: step 74740, loss = 2.09 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:05.890293: step 74750, loss = 1.99 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:18:07.095013: step 74760, loss = 1.88 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:18:08.315563: step 74770, loss = 1.85 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:09.528898: step 74780, loss = 2.06 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:10.746395: step 74790, loss = 2.04 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:11.987647: step 74800, loss = 1.82 (1031.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:18:13.195799: step 74810, loss = 1.84 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:14.420356: step 74820, loss = 1.95 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:15.656794: step 74830, loss = 1.94 (1035.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:18:16.880201: step 74840, loss = 2.04 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:18.082581: step 74850, loss = 2.17 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:18:19.294784: step 74860, loss = 2.01 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:20.534786: step 74870, loss = 1.87 (1032.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:18:21.757811: step 74880, loss = 1.96 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:23.000626: step 74890, loss = 2.09 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-05 00:18:24.215645: step 74900, loss = 2.02 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:25.421748: step 74910, loss = 1.89 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:26.641141: step 74920, loss = 2.11 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:27.876211: step 74930, loss = 1.82 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:18:29.091080: step 74940, loss = 1.84 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:30.307970: step 74950, loss = 1.92 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:31.519215: step 74960, loss = 1.97 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:32.751485: step 74970, loss = 1.80 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:18:33.962135: step 74980, loss = 1.89 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:35.173874: step 74990, loss = 1.94 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:36.428224: step 75000, loss = 1.86 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-05 00:18:37.642127: step 75010, loss = 1.97 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:38.853940: step 75020, loss = 1.84 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:40.079409: step 75030, loss = 2.02 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:18:41.284353: step 75040, loss = 1.97 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:18:42.477988: step 75050, loss = 1.78 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:18:43.716619: step 75060, loss = 2.20 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:18:44.934538: step 75070, loss = 1.92 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:46.147945: step 75080, loss = 1.94 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:47.364298: step 75090, loss = 1.93 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:48.590064: step 75100, loss = 1.85 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:18:49.804864: step 75110, loss = 1.81 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:51.012493: step 75120, loss = 1.84 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:52.238123: step 75130, loss = 1.99 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:18:53.441677: step 75140, loss = 1.91 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:18:54.660433: step 75150, loss = 1.96 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:55.872406: step 75160, loss = 1.97 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:18:57.089991: step 75170, loss = 2.01 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:58.307454: step 75180, loss = 1.96 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:18:59.533645: step 75190, loss = 1.97 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:00.762392: step 75200, loss = 2.04 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:01.971394: step 75210, loss = 1.90 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:03.200944: step 75220, loss = 1.94 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:04.434457: step 75230, loss = 1.99 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:05.741168: step 75240, loss = 1.99 (979.6 examples/sec; 0.131 sec/batch)
2017-05-05 00:19:06.849987: step 75250, loss = 2.04 (1154.4 examples/sec; 0.111 sec/batch)
2017-05-05 00:19:08.077138: step 75260, loss = 2.00 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:09.294374: step 75270, loss = 1.91 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:10.503216: step 75280, loss = 1.85 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:11.718807: step 75290, loss = 1.77 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:12.945990: step 75300, loss = 1.99 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:14.155762: step 75310, loss = 2.08 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:15.374637: step 75320, loss = 1.95 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:16.606115: step 75330, loss = 1.91 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:17.823218: step 75340, loss = 2.10 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:19.055090: step 75350, loss = 2.14 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:20.288970: step 75360, loss = 1.96 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:21.496367: step 75370, loss = 1.83 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:22.717516: step 75380, loss = 1.93 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:23.944925: step 75390, loss = 1.91 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:25.171181: step 75400, loss = 1.92 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:26.370483: step 75410, loss = 1.97 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:19:27.597591: step 75420, loss = 1.84 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:28.832646: step 75430, loss = 2.12 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:19:30.010287: step 75440, loss = 1.94 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:19:31.245174: step 75450, loss = 1.92 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:32.466895: step 75460, loss = 1.91 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:33.674110: step 75470, loss = 1.97 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:34.889992: step 75480, loss = 2.09 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:36.104809: step 75490, loss = 1.87 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:37.322502: step 75500, loss = 1.96 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:38.536759: step 75510, loss = 2.03 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:39.756778: step 75520, loss = 2.00 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:40.991256: step 75530, loss = 1.93 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:42.194000: step 75540, loss = 2.00 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:19:43.420630: step 75550, loss = 2.12 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:44.621009: step 75560, loss = 1.91 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:19:45.850894: step 75570, loss = 1.89 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:47.078628: step 75580, loss = 1.81 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:48.303394: step 75590, loss = 2.08 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:49.527684: step 75600, loss = 2.02 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:50.753872: step 75610, loss = 1.96 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:51.980394: step 75620, loss = 2.01 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:53.206638: step 75630, loss = 1.91 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:54.373382: step 75640, loss = 1.96 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:19:55.597164: step 75650, loss = 1.88 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:19:56.827355: step 75660, loss = 1.86 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:19:58.033960: step 75670, loss = 2.01 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:19:59.261336: step 75680, loss = 1.96 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:00.483254: step 75690, loss = 1.83 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:01.687848: step 75700, loss = 2.05 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:20:02.923471: step 75710, loss = 1.87 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-05 00:20:04.153125: step 75720, loss = 1.94 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:05.358901: step 75730, loss = 2.03 (E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 782 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:06.578996: step 75740, loss = 2.08 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:07.792947: step 75750, loss = 1.86 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:09.029681: step 75760, loss = 1.91 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:20:10.232730: step 75770, loss = 1.98 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:20:11.465682: step 75780, loss = 1.98 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:12.683077: step 75790, loss = 2.13 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:13.900372: step 75800, loss = 2.14 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:15.129072: step 75810, loss = 1.84 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:16.351514: step 75820, loss = 1.88 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:17.569551: step 75830, loss = 1.90 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:18.782705: step 75840, loss = 1.89 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:20.013704: step 75850, loss = 1.88 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:21.241964: step 75860, loss = 1.78 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:22.444677: step 75870, loss = 1.96 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:20:23.680962: step 75880, loss = 1.97 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:20:24.890109: step 75890, loss = 1.75 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:26.098004: step 75900, loss = 2.03 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:27.321965: step 75910, loss = 1.85 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:28.548199: step 75920, loss = 1.92 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:29.747619: step 75930, loss = 1.85 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:20:30.972665: step 75940, loss = 1.94 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:32.203979: step 75950, loss = 1.90 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:33.411617: step 75960, loss = 1.85 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:34.631290: step 75970, loss = 2.05 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:35.848603: step 75980, loss = 1.96 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:37.068226: step 75990, loss = 1.99 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:38.283600: step 76000, loss = 1.98 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:39.503270: step 76010, loss = 1.83 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:40.704347: step 76020, loss = 1.99 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:20:41.904083: step 76030, loss = 1.96 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:20:43.105765: step 76040, loss = 1.83 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:20:44.331386: step 76050, loss = 1.95 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:45.544728: step 76060, loss = 1.88 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:46.759100: step 76070, loss = 1.96 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:47.967546: step 76080, loss = 1.82 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:49.189996: step 76090, loss = 2.02 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:20:50.389144: step 76100, loss = 1.95 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:20:51.633553: step 76110, loss = 1.93 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:20:52.867836: step 76120, loss = 1.90 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:54.077958: step 76130, loss = 1.98 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:55.289458: step 76140, loss = 2.06 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:56.518907: step 76150, loss = 1.88 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:20:57.726135: step 76160, loss = 2.07 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:20:58.948305: step 76170, loss = 1.95 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:00.182229: step 76180, loss = 1.92 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:01.405858: step 76190, loss = 2.00 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:02.623421: step 76200, loss = 1.93 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:03.860530: step 76210, loss = 2.02 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:21:05.095063: step 76220, loss = 1.99 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:06.393337: step 76230, loss = 1.88 (985.9 examples/sec; 0.130 sec/batch)
2017-05-05 00:21:07.484958: step 76240, loss = 1.84 (1172.6 examples/sec; 0.109 sec/batch)
2017-05-05 00:21:08.705345: step 76250, loss = 1.99 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:09.928322: step 76260, loss = 2.07 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:11.162931: step 76270, loss = 1.89 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:12.371850: step 76280, loss = 1.94 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:13.583197: step 76290, loss = 1.88 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:14.797819: step 76300, loss = 1.82 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:16.022878: step 76310, loss = 1.97 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:17.250919: step 76320, loss = 2.04 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:18.462132: step 76330, loss = 1.76 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:19.681420: step 76340, loss = 1.94 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:20.883646: step 76350, loss = 1.91 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:21:22.094315: step 76360, loss = 1.97 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:23.338565: step 76370, loss = 1.93 (1028.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:21:24.548145: step 76380, loss = 1.89 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:25.759704: step 76390, loss = 1.92 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:26.972086: step 76400, loss = 1.87 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:28.180209: step 76410, loss = 2.13 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:29.383890: step 76420, loss = 2.00 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:21:30.577567: step 76430, loss = 1.91 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:21:31.807465: step 76440, loss = 1.97 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:33.023048: step 76450, loss = 1.92 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:34.230483: step 76460, loss = 2.10 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:35.449663: step 76470, loss = 1.83 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:36.660677: step 76480, loss = 2.00 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:37.870834: step 76490, loss = 1.86 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:39.103596: step 76500, loss = 1.95 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:40.321532: step 76510, loss = 1.98 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:41.525113: step 76520, loss = 1.95 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:21:42.741950: step 76530, loss = 1.97 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:43.964714: step 76540, loss = 1.99 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:45.204684: step 76550, loss = 1.94 (1032.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:21:46.418288: step 76560, loss = 1.79 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:47.653046: step 76570, loss = 1.95 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:48.877208: step 76580, loss = 1.86 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:21:50.090019: step 76590, loss = 1.83 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:51.320199: step 76600, loss = 1.95 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:52.523602: step 76610, loss = 1.85 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:21:53.726177: step 76620, loss = 2.10 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:21:54.941136: step 76630, loss = 1.90 (105E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 792 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
3.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:56.174174: step 76640, loss = 1.99 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:57.401938: step 76650, loss = 1.87 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:21:58.615068: step 76660, loss = 1.84 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:21:59.841409: step 76670, loss = 1.87 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:01.071470: step 76680, loss = 2.06 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:02.307541: step 76690, loss = 2.01 (1035.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:22:03.517437: step 76700, loss = 2.00 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:04.738466: step 76710, loss = 2.01 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:05.936961: step 76720, loss = 1.97 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:22:07.177777: step 76730, loss = 2.03 (1031.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:22:08.393403: step 76740, loss = 1.98 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:09.595466: step 76750, loss = 1.94 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:22:10.814929: step 76760, loss = 1.98 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:12.038095: step 76770, loss = 1.82 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:13.257569: step 76780, loss = 1.93 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:14.462576: step 76790, loss = 1.96 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:15.702004: step 76800, loss = 1.94 (1032.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:22:16.931843: step 76810, loss = 1.92 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:18.153115: step 76820, loss = 1.86 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:19.344122: step 76830, loss = 1.92 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:22:20.554749: step 76840, loss = 1.86 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:21.766622: step 76850, loss = 1.92 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:22.997117: step 76860, loss = 2.02 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:24.227021: step 76870, loss = 1.97 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:25.449272: step 76880, loss = 1.91 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:26.686899: step 76890, loss = 1.88 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:22:27.921392: step 76900, loss = 1.89 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:29.146677: step 76910, loss = 2.06 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:30.367069: step 76920, loss = 1.97 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:31.576555: step 76930, loss = 1.85 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:32.792481: step 76940, loss = 1.95 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:34.016514: step 76950, loss = 1.98 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:35.234439: step 76960, loss = 1.95 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:36.451066: step 76970, loss = 1.82 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:37.676506: step 76980, loss = 1.97 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:38.883331: step 76990, loss = 1.91 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:40.104453: step 77000, loss = 1.87 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:41.327050: step 77010, loss = 2.02 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:42.524684: step 77020, loss = 1.88 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:22:43.715567: step 77030, loss = 1.88 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:22:44.938777: step 77040, loss = 1.90 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:46.147862: step 77050, loss = 1.97 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:47.383733: step 77060, loss = 1.98 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:22:48.600500: step 77070, loss = 1.94 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:49.823522: step 77080, loss = 1.85 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:51.047579: step 77090, loss = 1.86 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:52.274592: step 77100, loss = 1.93 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:53.477775: step 77110, loss = 2.04 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:22:54.709082: step 77120, loss = 1.82 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:55.943314: step 77130, loss = 1.88 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:22:57.167970: step 77140, loss = 1.96 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:22:58.381017: step 77150, loss = 2.14 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:22:59.607503: step 77160, loss = 1.95 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:23:00.829368: step 77170, loss = 1.96 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:02.041286: step 77180, loss = 1.91 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:03.264527: step 77190, loss = 1.90 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:04.482465: step 77200, loss = 2.17 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:05.699523: step 77210, loss = 1.85 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:07.009937: step 77220, loss = 1.95 (976.8 examples/sec; 0.131 sec/batch)
2017-05-05 00:23:08.093347: step 77230, loss = 1.89 (1181.5 examples/sec; 0.108 sec/batch)
2017-05-05 00:23:09.302492: step 77240, loss = 1.78 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:10.519423: step 77250, loss = 2.00 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:11.722661: step 77260, loss = 1.97 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:23:12.936409: step 77270, loss = 1.87 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:14.132780: step 77280, loss = 1.86 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:23:15.362968: step 77290, loss = 2.09 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:23:16.573881: step 77300, loss = 2.21 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:17.791296: step 77310, loss = 2.02 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:19.009386: step 77320, loss = 1.96 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:20.247100: step 77330, loss = 1.94 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:23:21.468545: step 77340, loss = 1.86 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:22.672396: step 77350, loss = 1.82 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:23:23.895403: step 77360, loss = 1.92 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:25.114578: step 77370, loss = 1.96 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:26.331989: step 77380, loss = 1.91 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:27.552648: step 77390, loss = 1.86 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:28.782720: step 77400, loss = 1.99 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:23:30.000525: step 77410, loss = 1.91 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:31.196548: step 77420, loss = 1.89 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:23:32.414317: step 77430, loss = 1.87 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:33.610319: step 77440, loss = 1.99 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:23:34.854131: step 77450, loss = 1.96 (1029.1 examples/sec; 0.124 sec/batch)
2017-05-05 00:23:36.063156: step 77460, loss = 1.86 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:37.300063: step 77470, loss = 2.08 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:23:38.518730: step 77480, loss = 1.93 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:39.735091: step 77490, loss = 2.07 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:40.982589: step 77500, loss = 2.13 (1026.1 examples/sec; 0.125 sec/batch)
2017-05-05 00:23:42.191882: step 77510, loss = 1.88 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:43.400295: step 77520, loss = 2.00 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:44.633378: step 77530, loss = 1.91 (1038.1E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 802 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
 examples/sec; 0.123 sec/batch)
2017-05-05 00:23:45.850061: step 77540, loss = 2.00 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:47.081838: step 77550, loss = 1.97 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:23:48.303281: step 77560, loss = 2.05 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:49.525645: step 77570, loss = 1.98 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:50.766588: step 77580, loss = 1.98 (1031.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:23:51.986627: step 77590, loss = 1.99 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:53.219025: step 77600, loss = 1.94 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:23:54.424110: step 77610, loss = 2.03 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:55.632195: step 77620, loss = 2.10 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:23:56.856744: step 77630, loss = 1.97 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:23:58.086830: step 77640, loss = 1.76 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:23:59.306708: step 77650, loss = 1.75 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:00.525736: step 77660, loss = 2.02 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:01.730541: step 77670, loss = 2.00 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:24:02.947890: step 77680, loss = 2.13 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:04.172603: step 77690, loss = 1.97 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:05.386415: step 77700, loss = 1.93 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:06.611871: step 77710, loss = 2.10 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:07.838145: step 77720, loss = 1.84 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:09.058144: step 77730, loss = 2.06 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:10.272565: step 77740, loss = 2.20 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:11.505718: step 77750, loss = 1.84 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:12.742310: step 77760, loss = 1.98 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-05 00:24:13.955124: step 77770, loss = 1.92 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:15.174846: step 77780, loss = 2.09 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:16.409906: step 77790, loss = 1.80 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:24:17.628364: step 77800, loss = 1.92 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:18.852757: step 77810, loss = 1.83 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:20.034427: step 77820, loss = 2.03 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:24:21.237636: step 77830, loss = 1.82 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:24:22.447020: step 77840, loss = 1.96 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:23.666928: step 77850, loss = 1.85 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:24.886465: step 77860, loss = 1.88 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:26.093922: step 77870, loss = 2.07 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:27.318858: step 77880, loss = 2.08 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:28.539424: step 77890, loss = 1.99 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:29.740431: step 77900, loss = 1.83 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:24:30.980811: step 77910, loss = 2.00 (1031.9 examples/sec; 0.124 sec/batch)
2017-05-05 00:24:32.209601: step 77920, loss = 2.02 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:33.433553: step 77930, loss = 1.92 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:34.641490: step 77940, loss = 1.88 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:35.874397: step 77950, loss = 1.90 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:37.096147: step 77960, loss = 2.20 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:38.316383: step 77970, loss = 2.15 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:39.537881: step 77980, loss = 1.92 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:40.751903: step 77990, loss = 2.07 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:41.963932: step 78000, loss = 1.87 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:43.211608: step 78010, loss = 2.03 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-05 00:24:44.400683: step 78020, loss = 2.08 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:24:45.635325: step 78030, loss = 2.07 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:46.853434: step 78040, loss = 2.00 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:48.077662: step 78050, loss = 1.85 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:49.302829: step 78060, loss = 1.99 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:24:50.509921: step 78070, loss = 2.05 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:51.733965: step 78080, loss = 1.95 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:52.954849: step 78090, loss = 1.95 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:54.165044: step 78100, loss = 1.89 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:55.379381: step 78110, loss = 2.10 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:24:56.604355: step 78120, loss = 2.00 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:57.821690: step 78130, loss = 1.91 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:24:59.028764: step 78140, loss = 1.90 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:00.233744: step 78150, loss = 1.94 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:25:01.445851: step 78160, loss = 2.02 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:02.693059: step 78170, loss = 1.96 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-05 00:25:03.910217: step 78180, loss = 2.10 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:25:05.124091: step 78190, loss = 1.98 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:06.323652: step 78200, loss = 1.96 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:25:07.626699: step 78210, loss = 1.99 (982.3 examples/sec; 0.130 sec/batch)
2017-05-05 00:25:08.751940: step 78220, loss = 1.96 (1137.5 examples/sec; 0.113 sec/batch)
2017-05-05 00:25:09.954409: step 78230, loss = 1.99 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:25:11.163770: step 78240, loss = 1.97 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:12.389067: step 78250, loss = 1.84 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:25:13.605262: step 78260, loss = 1.87 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:25:14.825819: step 78270, loss = 1.94 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:25:16.044642: step 78280, loss = 1.89 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:25:17.261543: step 78290, loss = 1.81 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:25:18.478308: step 78300, loss = 1.80 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:25:19.697844: step 78310, loss = 2.04 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:25:20.928099: step 78320, loss = 1.97 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:25:22.134858: step 78330, loss = 1.96 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:23.355341: step 78340, loss = 1.94 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:25:24.574406: step 78350, loss = 1.93 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:25:25.785555: step 78360, loss = 1.94 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:26.997011: step 78370, loss = 1.80 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:28.219471: step 78380, loss = 1.95 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:25:29.420404: step 78390, loss = 2.10 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:25:30.626933: step 78400, loss = 1.88 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:25:31.821049: step 78410, loss = 1.96 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:25:33.011954: step 78420, loss = 1.94 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:25:34.202688: step 78430, loss = 1.95 (1075.0 exE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 813 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
amples/sec; 0.119 sec/batch)
2017-05-05 00:25:35.386093: step 78440, loss = 1.94 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:25:36.583700: step 78450, loss = 1.98 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:25:37.752209: step 78460, loss = 1.95 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:25:38.933122: step 78470, loss = 2.00 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:25:40.099047: step 78480, loss = 1.98 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:25:41.258479: step 78490, loss = 1.96 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:25:42.419595: step 78500, loss = 1.95 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:25:43.596750: step 78510, loss = 1.96 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:25:44.783931: step 78520, loss = 1.86 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:25:45.945068: step 78530, loss = 1.99 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:25:47.108187: step 78540, loss = 1.83 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:25:48.279933: step 78550, loss = 1.87 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:25:49.440994: step 78560, loss = 1.93 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:25:50.603074: step 78570, loss = 1.85 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:25:51.784547: step 78580, loss = 1.79 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:25:52.969818: step 78590, loss = 1.93 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:25:54.129095: step 78600, loss = 1.99 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:25:55.323242: step 78610, loss = 1.99 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:25:56.516613: step 78620, loss = 2.08 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:25:57.685382: step 78630, loss = 1.89 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:25:58.872158: step 78640, loss = 1.91 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:26:00.035843: step 78650, loss = 1.99 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:01.194680: step 78660, loss = 1.92 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:02.361605: step 78670, loss = 1.87 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:03.533651: step 78680, loss = 2.14 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:04.703648: step 78690, loss = 1.92 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:05.843980: step 78700, loss = 2.02 (1122.5 examples/sec; 0.114 sec/batch)
2017-05-05 00:26:07.011515: step 78710, loss = 1.96 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:08.205582: step 78720, loss = 1.96 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:26:09.377498: step 78730, loss = 1.94 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:10.533741: step 78740, loss = 2.20 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:11.703409: step 78750, loss = 1.89 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:12.854578: step 78760, loss = 2.01 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:26:14.016346: step 78770, loss = 2.10 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:15.176425: step 78780, loss = 2.03 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:16.353883: step 78790, loss = 1.98 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:26:17.517264: step 78800, loss = 2.00 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:18.670393: step 78810, loss = 1.96 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:26:19.819516: step 78820, loss = 1.89 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:26:20.980886: step 78830, loss = 1.83 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:22.143992: step 78840, loss = 1.98 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:23.303465: step 78850, loss = 1.96 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:24.468063: step 78860, loss = 1.96 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:25.628375: step 78870, loss = 1.94 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:26.805741: step 78880, loss = 1.89 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:26:27.978829: step 78890, loss = 1.85 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:29.176350: step 78900, loss = 1.97 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:26:30.334984: step 78910, loss = 1.93 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:31.498786: step 78920, loss = 1.94 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:32.677628: step 78930, loss = 1.86 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:26:33.833412: step 78940, loss = 1.97 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:35.003113: step 78950, loss = 1.81 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:36.201525: step 78960, loss = 1.94 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:26:37.369699: step 78970, loss = 2.00 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:38.543580: step 78980, loss = 1.92 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:39.722939: step 78990, loss = 1.96 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:26:40.880975: step 79000, loss = 1.81 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:42.042991: step 79010, loss = 2.09 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:43.221926: step 79020, loss = 1.96 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:26:44.385867: step 79030, loss = 1.98 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:45.544373: step 79040, loss = 1.88 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:46.724980: step 79050, loss = 1.89 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:26:47.895898: step 79060, loss = 1.98 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:49.093338: step 79070, loss = 2.00 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:26:50.248722: step 79080, loss = 1.81 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:51.400692: step 79090, loss = 1.92 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:26:52.568196: step 79100, loss = 1.98 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:53.724767: step 79110, loss = 2.02 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:54.882958: step 79120, loss = 1.85 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:26:56.049090: step 79130, loss = 1.91 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:57.218386: step 79140, loss = 1.93 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:26:58.365923: step 79150, loss = 2.00 (1115.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:26:59.541648: step 79160, loss = 1.85 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:27:00.707852: step 79170, loss = 2.01 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:01.863969: step 79180, loss = 1.85 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:03.033603: step 79190, loss = 1.94 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:04.314092: step 79200, loss = 1.91 (999.6 examples/sec; 0.128 sec/batch)
2017-05-05 00:27:05.374101: step 79210, loss = 2.01 (1207.5 examples/sec; 0.106 sec/batch)
2017-05-05 00:27:06.541586: step 79220, loss = 1.85 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:07.706537: step 79230, loss = 1.90 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:08.860292: step 79240, loss = 1.90 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:27:10.017899: step 79250, loss = 1.86 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:11.171329: step 79260, loss = 1.80 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:27:12.353319: step 79270, loss = 2.00 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:27:13.506764: step 79280, loss = 1.90 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:27:14.678453: step 79290, loss = 1.86 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:15.837854: step 79300, loss = 1.89 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:17.032108: step 79310, loss = 2.09 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:27:18.207104: step 79320, loss = 1.94 (1089.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:27:19.389659: step 79330, loss = 1.97 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:27:20.564153: step 79340, loss = 1.95 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:21.732132: step 79350, loss = 1.92 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:22.889278: step 79360, loss = 2.07 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:24.058027: step 79370, loss = 1.82 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:25.213700: step 79380, loss = 1.99 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:26.367408: step 79390, loss = 1.90 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:27:27.541316: step 79400, loss = 1.93 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:28.713642: step 79410, loss = 2.00 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:29.879816: step 79420, loss = 2.09 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:31.067434: step 79430, loss = 1.84 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:27:32.244588: step 79440, loss = 1.84 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:27:33.415387: step 79450, loss = 1.83 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:34.588060: step 79460, loss = 1.82 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:35.767314: step 79470, loss = 1.97 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:27:36.938876: step 79480, loss = 1.92 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:38.100937: step 79490, loss = 1.91 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:39.269430: step 79500, loss = 1.97 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:40.422619: step 79510, loss = 2.01 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:27:41.586835: step 79520, loss = 2.02 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:42.747207: step 79530, loss = 1.95 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:43.927905: step 79540, loss = 2.00 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:27:45.111013: step 79550, loss = 1.83 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:27:46.249789: step 79560, loss = 1.93 (1124.0 examples/sec; 0.114 sec/batch)
2017-05-05 00:27:47.445334: step 79570, loss = 1.94 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:27:48.619661: step 79580, loss = 1.86 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:49.778976: step 79590, loss = 2.12 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:50.938434: step 79600, loss = 1.86 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:52.100177: step 79610, loss = 2.13 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:27:53.273453: step 79620, loss = 1.97 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:54.442504: step 79630, loss = 1.93 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:27:55.593205: step 79640, loss = 1.99 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:27:56.747110: step 79650, loss = 2.02 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:27:57.893697: step 79660, loss = 1.76 (1116.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:27:59.062840: step 79670, loss = 1.91 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:00.225518: step 79680, loss = 1.88 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:01.402285: step 79690, loss = 1.83 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:02.581617: step 79700, loss = 1.81 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:03.766074: step 79710, loss = 1.95 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:04.937421: step 79720, loss = 1.89 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:06.076851: step 79730, loss = 1.98 (1123.4 examples/sec; 0.114 sec/batch)
2017-05-05 00:28:07.264115: step 79740, loss = 2.00 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:28:08.439410: step 79750, loss = 1.85 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:09.621908: step 79760, loss = 2.06 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:10.787033: step 79770, loss = 1.90 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:11.963075: step 79780, loss = 1.83 (1088.4 examplE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 824 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
es/sec; 0.118 sec/batch)
2017-05-05 00:28:13.142619: step 79790, loss = 1.94 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:14.308271: step 79800, loss = 1.87 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:15.484699: step 79810, loss = 1.89 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:16.644273: step 79820, loss = 1.81 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:17.793085: step 79830, loss = 2.09 (1114.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:28:18.966157: step 79840, loss = 2.00 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:20.135106: step 79850, loss = 2.08 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:21.283988: step 79860, loss = 1.90 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:28:22.434539: step 79870, loss = 1.91 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:28:23.605705: step 79880, loss = 1.86 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:24.765177: step 79890, loss = 1.93 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:25.932388: step 79900, loss = 2.00 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:27.097922: step 79910, loss = 2.01 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:28.276271: step 79920, loss = 2.02 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:29.449552: step 79930, loss = 2.08 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:30.630114: step 79940, loss = 1.88 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:31.783651: step 79950, loss = 2.05 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:28:32.950068: step 79960, loss = 1.89 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:34.116907: step 79970, loss = 1.85 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:35.393421: step 79980, loss = 1.89 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-05 00:28:36.516129: step 79990, loss = 1.99 (1140.1 examples/sec; 0.112 sec/batch)
2017-05-05 00:28:37.684515: step 80000, loss = 2.01 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:38.849953: step 80010, loss = 2.05 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:40.002694: step 80020, loss = 1.92 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:28:41.181998: step 80030, loss = 2.14 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:42.335657: step 80040, loss = 1.98 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:28:43.530465: step 80050, loss = 1.90 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:28:44.688316: step 80060, loss = 1.85 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:45.842161: step 80070, loss = 1.98 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:28:47.008622: step 80080, loss = 2.08 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:48.188841: step 80090, loss = 1.99 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:49.343965: step 80100, loss = 1.84 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:50.518218: step 80110, loss = 1.98 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:51.686276: step 80120, loss = 1.93 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:28:52.846588: step 80130, loss = 1.83 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:53.982801: step 80140, loss = 1.82 (1126.5 examples/sec; 0.114 sec/batch)
2017-05-05 00:28:55.161476: step 80150, loss = 1.93 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:56.320114: step 80160, loss = 2.00 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:57.495723: step 80170, loss = 1.84 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:28:58.651445: step 80180, loss = 1.85 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:28:59.923341: step 80190, loss = 1.95 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-05 00:29:01.000496: step 80200, loss = 2.00 (1188.3 examples/sec; 0.108 sec/batch)
2017-05-05 00:29:02.145366: step 80210, loss = 1.96 (1118.0 examples/sec; 0.114 sec/batch)
2017-05-05 00:29:03.305216: step 80220, loss = 2.01 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:29:04.465336: step 80230, loss = 1.98 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:29:05.615783: step 80240, loss = 1.96 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:29:06.769389: step 80250, loss = 1.89 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:29:07.953016: step 80260, loss = 1.90 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:09.112955: step 80270, loss = 1.85 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:29:10.271884: step 80280, loss = 1.98 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:29:11.451298: step 80290, loss = 1.82 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:12.641439: step 80300, loss = 1.96 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:13.799144: step 80310, loss = 1.85 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:29:14.968002: step 80320, loss = 1.96 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:29:16.133041: step 80330, loss = 1.86 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:29:17.305520: step 80340, loss = 1.94 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:29:18.475759: step 80350, loss = 1.90 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:29:19.652387: step 80360, loss = 1.89 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:20.826951: step 80370, loss = 1.92 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:29:22.007962: step 80380, loss = 1.98 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:23.269437: step 80390, loss = 1.95 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-05 00:29:24.391397: step 80400, loss = 2.09 (1140.9 examples/sec; 0.112 sec/batch)
2017-05-05 00:29:25.577084: step 80410, loss = 1.88 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:26.752978: step 80420, loss = 1.86 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:27.961169: step 80430, loss = 2.11 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:29:29.168524: step 80440, loss = 1.84 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:29:30.357579: step 80450, loss = 1.86 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:31.561715: step 80460, loss = 1.81 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:29:32.754694: step 80470, loss = 1.99 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:33.944745: step 80480, loss = 2.03 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:35.143948: step 80490, loss = 1.93 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:29:36.334938: step 80500, loss = 2.03 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:37.514107: step 80510, loss = 2.01 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:38.690900: step 80520, loss = 1.99 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:39.863018: step 80530, loss = 1.95 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:29:41.077017: step 80540, loss = 2.05 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:29:42.241591: step 80550, loss = 1.98 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:29:43.421999: step 80560, loss = 1.96 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:44.626984: step 80570, loss = 2.00 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:29:45.821121: step 80580, loss = 1.87 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:46.996780: step 80590, loss = 1.95 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:48.196912: step 80600, loss = 1.96 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:29:49.386448: step 80610, loss = 2.10 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:50.592056: step 80620, loss = 1.96 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:29:51.773968: step 80630, loss = 1.98 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:52.960687: step 80640, loss = 2.02 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:54.128265: step 80650, loss = 1.85 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:29:55.314152: step 80660, loss = 1.95 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:29:56.511881: step 80670, loss = 2.07 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:29:57.694578: step 80680, loss = 1.93 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:29:58.915214: step 80690, loss = 1.97 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:30:00.124384: step 80700, loss = 2.08 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:30:01.333284: step 80710, loss = 1.78 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:30:02.512444: step 80720, loss = 1.84 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:03.712923: step 80730, loss = 1.94 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:30:04.887332: step 80740, loss = 1.99 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:06.057548: step 80750, loss = 2.07 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:07.219555: step 80760, loss = 1.88 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:30:08.404388: step 80770, loss = 1.99 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:09.569192: step 80780, loss = 2.05 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:30:10.737165: step 80790, loss = 1.98 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:11.934208: step 80800, loss = 1.80 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:30:13.108508: step 80810, loss = 1.93 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:14.262389: step 80820, loss = 2.07 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:30:15.420089: step 80830, loss = 1.94 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:30:16.581616: step 80840, loss = 1.86 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:30:17.750626: step 80850, loss = 2.04 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:18.940133: step 80860, loss = 1.82 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:30:20.106340: step 80870, loss = 1.97 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:21.255049: step 80880, loss = 1.94 (1114.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:30:22.416199: step 80890, loss = 1.99 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:30:23.579523: step 80900, loss = 1.93 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:30:24.746291: step 80910, loss = 2.04 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:25.915773: step 80920, loss = 2.07 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:27.089715: step 80930, loss = 2.09 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:28.273295: step 80940, loss = 2.18 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:29.435822: step 80950, loss = 1.90 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:30:30.602443: step 80960, loss = 1.96 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:31.770942: step 80970, loss = 1.99 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:32.962437: step 80980, loss = 1.95 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:30:34.148434: step 80990, loss = 1.97 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:30:35.344817: step 81000, loss = 1.90 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:30:36.555288: step 81010, loss = 2.12 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:30:37.761981: step 81020, loss = 1.95 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:30:38.977618: step 81030, loss = 2.02 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:30:40.164566: step 81040, loss = 2.03 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:30:41.381410: step 81050, loss = 1.85 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:30:42.571353: step 81060, loss = 2.05 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:30:43.738087: step 81070, loss = 2.04 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:30:44.929160: step 81080, loss = 1.98 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:30:46.116738: step 81090, loss = 1.67 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:30:47.299421: step 81100, loss = 1.95 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:48.477961: step 81110, loss = 1.92 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:49.660738: step 81120, loss = 1.91 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:50.837642: step 81130, loss = 2.11 (1087.6 examples/E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 837 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
sec; 0.118 sec/batch)
2017-05-05 00:30:52.021616: step 81140, loss = 2.03 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:53.201159: step 81150, loss = 1.88 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:30:54.359627: step 81160, loss = 1.83 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:30:55.513927: step 81170, loss = 1.79 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:30:56.788657: step 81180, loss = 1.98 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-05 00:30:57.846770: step 81190, loss = 2.08 (1209.7 examples/sec; 0.106 sec/batch)
2017-05-05 00:30:59.036831: step 81200, loss = 2.04 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:31:00.210697: step 81210, loss = 1.95 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:01.376305: step 81220, loss = 2.04 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:02.556039: step 81230, loss = 1.90 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:03.714721: step 81240, loss = 2.03 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:04.902130: step 81250, loss = 1.92 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:31:06.059315: step 81260, loss = 1.91 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:07.257052: step 81270, loss = 2.11 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:31:08.454677: step 81280, loss = 1.92 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:31:09.613862: step 81290, loss = 1.89 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:10.784326: step 81300, loss = 2.05 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:11.964284: step 81310, loss = 1.81 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:13.133476: step 81320, loss = 1.91 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:14.304576: step 81330, loss = 2.01 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:15.483122: step 81340, loss = 1.95 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:16.647014: step 81350, loss = 2.25 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:17.823735: step 81360, loss = 1.86 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:18.985401: step 81370, loss = 2.01 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:20.153980: step 81380, loss = 1.93 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:21.322674: step 81390, loss = 2.04 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:22.489308: step 81400, loss = 1.85 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:23.658558: step 81410, loss = 1.94 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:24.823784: step 81420, loss = 2.00 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:25.972716: step 81430, loss = 2.05 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:31:27.132117: step 81440, loss = 1.88 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:28.307264: step 81450, loss = 1.91 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:29.477579: step 81460, loss = 2.06 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:30.634124: step 81470, loss = 1.85 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:31.792222: step 81480, loss = 1.91 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:32.965365: step 81490, loss = 1.99 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:34.111489: step 81500, loss = 1.87 (1116.8 examples/sec; 0.115 sec/batch)
2017-05-05 00:31:35.286353: step 81510, loss = 1.83 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:36.444364: step 81520, loss = 1.92 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:37.619803: step 81530, loss = 2.09 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:38.778193: step 81540, loss = 1.88 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:39.937803: step 81550, loss = 2.05 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:41.128381: step 81560, loss = 1.86 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:31:42.293685: step 81570, loss = 1.91 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:43.484976: step 81580, loss = 1.93 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:31:44.653937: step 81590, loss = 1.92 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:31:45.816646: step 81600, loss = 2.01 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:46.979189: step 81610, loss = 2.02 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:48.157157: step 81620, loss = 2.00 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:49.310924: step 81630, loss = 2.01 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:31:50.469021: step 81640, loss = 1.98 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:51.650016: step 81650, loss = 1.86 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:52.835905: step 81660, loss = 1.84 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:31:53.994504: step 81670, loss = 1.98 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:31:55.171375: step 81680, loss = 1.91 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:56.362382: step 81690, loss = 1.91 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:31:57.547814: step 81700, loss = 1.95 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:31:58.730193: step 81710, loss = 1.89 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:31:59.920358: step 81720, loss = 2.04 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:01.105676: step 81730, loss = 2.04 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:02.270485: step 81740, loss = 1.94 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:03.457891: step 81750, loss = 2.10 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:04.646053: step 81760, loss = 1.83 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:05.812378: step 81770, loss = 1.81 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:07.015337: step 81780, loss = 1.93 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:32:08.231904: step 81790, loss = 1.86 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:32:09.424049: step 81800, loss = 1.92 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:10.614616: step 81810, loss = 1.88 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:11.800728: step 81820, loss = 1.99 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:12.963540: step 81830, loss = 1.78 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:14.134778: step 81840, loss = 1.86 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:15.366978: step 81850, loss = 1.91 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:32:16.560937: step 81860, loss = 2.02 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:17.732010: step 81870, loss = 2.02 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:18.908748: step 81880, loss = 2.00 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:20.091772: step 81890, loss = 1.99 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:21.279553: step 81900, loss = 1.94 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:22.455963: step 81910, loss = 1.97 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:23.606612: step 81920, loss = 1.94 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:32:24.786715: step 81930, loss = 1.96 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:25.959843: step 81940, loss = 1.99 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:27.126194: step 81950, loss = 1.82 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:28.311915: step 81960, loss = 1.92 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:29.486203: step 81970, loss = 1.96 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:30.677442: step 81980, loss = 1.95 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:31.851424: step 81990, loss = 1.89 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:33.052640: step 82000, loss = 2.07 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:32:34.242646: step 82010, loss = 1.87 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:35.427432: step 82020, loss = 1.75 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:36.615690: step 82030, loss = 2.01 (1077.2 examples/seE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 849 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
c; 0.119 sec/batch)
2017-05-05 00:32:37.784192: step 82040, loss = 1.85 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:38.961515: step 82050, loss = 1.93 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:40.116764: step 82060, loss = 1.88 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:41.279881: step 82070, loss = 2.05 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:42.447843: step 82080, loss = 1.96 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:32:43.612818: step 82090, loss = 1.90 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:44.793159: step 82100, loss = 1.97 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:45.943793: step 82110, loss = 2.03 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:32:47.136594: step 82120, loss = 1.94 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:32:48.315615: step 82130, loss = 1.84 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:49.491106: step 82140, loss = 1.95 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:50.648920: step 82150, loss = 2.00 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:51.810101: step 82160, loss = 2.03 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:53.069098: step 82170, loss = 1.94 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-05 00:32:54.140364: step 82180, loss = 2.02 (1194.8 examples/sec; 0.107 sec/batch)
2017-05-05 00:32:55.316560: step 82190, loss = 1.96 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:56.478279: step 82200, loss = 1.90 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:57.638914: step 82210, loss = 1.93 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:32:58.814977: step 82220, loss = 1.93 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:32:59.977163: step 82230, loss = 1.92 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:01.142799: step 82240, loss = 1.74 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:02.314619: step 82250, loss = 1.82 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:03.468159: step 82260, loss = 1.95 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:33:04.652877: step 82270, loss = 1.81 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:33:05.820031: step 82280, loss = 2.29 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:06.987630: step 82290, loss = 2.03 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:08.153935: step 82300, loss = 1.80 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:09.341281: step 82310, loss = 1.83 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:33:10.498198: step 82320, loss = 2.09 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:11.657709: step 82330, loss = 1.88 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:12.818836: step 82340, loss = 2.07 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:13.981170: step 82350, loss = 1.89 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:15.151880: step 82360, loss = 2.22 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:16.320710: step 82370, loss = 1.97 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:17.472683: step 82380, loss = 1.76 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:33:18.642885: step 82390, loss = 1.88 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:19.800803: step 82400, loss = 1.95 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:20.962600: step 82410, loss = 1.89 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:22.117563: step 82420, loss = 1.95 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:33:23.297457: step 82430, loss = 1.88 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:33:24.473694: step 82440, loss = 1.87 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:33:25.653860: step 82450, loss = 2.00 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:33:26.810446: step 82460, loss = 2.08 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:27.973940: step 82470, loss = 1.93 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:29.136452: step 82480, loss = 1.94 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:30.279690: step 82490, loss = 1.92 (1119.6 examples/sec; 0.114 sec/batch)
2017-05-05 00:33:31.440881: step 82500, loss = 2.01 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:32.613441: step 82510, loss = 2.07 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:33.770425: step 82520, loss = 1.97 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:34.939599: step 82530, loss = 2.00 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:36.103639: step 82540, loss = 1.74 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:37.269182: step 82550, loss = 2.01 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:33:38.423626: step 82560, loss = 2.09 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 00:33:39.604156: step 82570, loss = 2.05 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:33:40.784414: step 82580, loss = 1.85 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:33:41.944645: step 82590, loss = 1.99 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:33:43.128276: step 82600, loss = 1.94 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:33:44.331270: step 82610, loss = 1.83 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:33:45.523700: step 82620, loss = 2.07 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:33:46.720311: step 82630, loss = 1.92 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:33:47.927479: step 82640, loss = 1.96 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:33:49.126734: step 82650, loss = 1.81 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:33:50.343967: step 82660, loss = 1.95 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:33:51.560337: step 82670, loss = 1.97 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:33:52.788859: step 82680, loss = 2.01 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:33:53.995038: step 82690, loss = 1.90 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:33:55.203760: step 82700, loss = 1.96 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:33:56.448182: step 82710, loss = 1.94 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:33:57.640846: step 82720, loss = 1.95 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:33:58.866450: step 82730, loss = 1.94 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:00.071540: step 82740, loss = 1.84 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:01.311897: step 82750, loss = 2.00 (1032.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:34:02.497429: step 82760, loss = 2.09 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:34:03.704643: step 82770, loss = 1.97 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:04.926263: step 82780, loss = 1.88 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:06.137531: step 82790, loss = 1.98 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:07.349471: step 82800, loss = 2.01 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:08.589750: step 82810, loss = 2.01 (1032.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:34:09.775832: step 82820, loss = 1.68 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:34:10.996344: step 82830, loss = 1.96 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:12.230277: step 82840, loss = 1.91 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:13.467733: step 82850, loss = 2.13 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:34:14.653639: step 82860, loss = 1.88 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:34:15.885679: step 82870, loss = 1.98 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:17.097418: step 82880, loss = 1.89 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:18.326137: step 82890, loss = 1.89 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:19.539692: step 82900, loss = 1.96 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:20.757893: step 82910, loss = 2.06 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:21.950920: step 82920, loss = 1.92 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:34:23.185906: step 82930, loss = 2.03 (1036.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:24.415799: step 82940, loss = 1.86 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:25.612515: step 82950, loss = 1.82 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:34:26.821495: step 82960, loss = 1.93 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:28.036212: step 82970, loss = 1.96 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:29.252878: step 82980, loss = 2.02 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:30.457612: step 82990, loss = 1.92 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:34:31.701535: step 83000, loss = 1.91 (1029.0 examples/sec; 0.124 sec/batch)
2017-05-05 00:34:32.906744: step 83010, loss = 1.84 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:34.128489: step 83020, loss = 1.99 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:35.357765: step 83030, loss = 1.96 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:36.562049: step 83040, loss = 1.81 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:34:37.758959: step 83050, loss = 2.23 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:34:38.981148: step 83060, loss = 2.01 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:40.207908: step 83070, loss = 1.87 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:41.417614: step 83080, loss = 2.14 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:42.651025: step 83090, loss = 1.82 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:43.865935: step 83100, loss = 2.06 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:45.101044: step 83110, loss = 1.93 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:34:46.300675: step 83120, loss = 1.88 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:34:47.524889: step 83130, loss = 1.88 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:34:48.725598: step 83140, loss = 1.92 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:34:49.954310: step 83150, loss = 1.90 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:51.240431: step 83160, loss = 1.89 (995.2 examples/sec; 0.129 sec/batch)
2017-05-05 00:34:52.354607: step 83170, loss = 1.96 (1148.8 examples/sec; 0.111 sec/batch)
2017-05-05 00:34:53.554026: step 83180, loss = 1.99 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:34:54.779366: step 83190, loss = 2.10 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:55.982536: step 83200, loss = 1.85 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:34:57.207545: step 83210, loss = 1.89 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:34:58.421032: step 83220, loss = 1.88 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:34:59.656809: step 83230, loss = 1.90 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:35:00.853697: step 83240, loss = 1.94 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:35:02.060070: step 83250, loss = 1.97 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:03.267116: step 83260, loss = 1.76 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:04.486437: step 83270, loss = 1.90 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:05.693862: step 83280, loss = 1.92 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:06.910829: step 83290, loss = 2.08 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:08.140655: step 83300, loss = 1.83 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:09.343023: step 83310, loss = 1.93 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:35:10.562731: step 83320, loss = 1.99 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:11.800831: step 83330, loss = 2.02 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:35:13.010653: step 83340, loss = 1.92 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:14.222342: step 83350, loss = 1.94 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:15.515578: step 83360, loss = 1.83 (989.8 examples/sec; 0.129 sec/batch)
2017-05-05 00:35:16.654630: step 83370, loss = 2.07 (1123.7 examples/sec; 0.114 sec/batch)
2017-05-05 00:35:17.864888: step 83380, loss = 1.86 (1057.6 examples/sec; 0.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 860 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
121 sec/batch)
2017-05-05 00:35:19.099672: step 83390, loss = 2.03 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:20.302457: step 83400, loss = 2.04 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:35:21.505790: step 83410, loss = 1.81 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:35:22.733520: step 83420, loss = 1.89 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:23.940070: step 83430, loss = 1.93 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:25.170015: step 83440, loss = 2.00 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:26.389122: step 83450, loss = 1.81 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:27.616791: step 83460, loss = 1.94 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:28.848588: step 83470, loss = 2.01 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:30.046589: step 83480, loss = 1.90 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:35:31.274381: step 83490, loss = 1.88 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:32.500556: step 83500, loss = 1.99 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:33.688100: step 83510, loss = 1.91 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:35:34.919076: step 83520, loss = 2.03 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:36.140430: step 83530, loss = 2.01 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:37.373786: step 83540, loss = 1.92 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:38.598253: step 83550, loss = 1.81 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:39.781060: step 83560, loss = 1.95 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:35:41.011118: step 83570, loss = 1.92 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:42.210333: step 83580, loss = 2.11 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:35:43.444597: step 83590, loss = 1.99 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:44.649640: step 83600, loss = 1.83 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:45.865115: step 83610, loss = 1.87 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:47.083594: step 83620, loss = 1.97 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:48.314714: step 83630, loss = 2.05 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:35:49.510876: step 83640, loss = 1.97 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:35:50.722298: step 83650, loss = 1.91 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:35:51.964642: step 83660, loss = 1.86 (1030.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:35:53.189420: step 83670, loss = 1.92 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:54.410582: step 83680, loss = 1.77 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:55.635171: step 83690, loss = 1.78 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:56.850223: step 83700, loss = 1.88 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:35:58.052078: step 83710, loss = 1.97 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:35:59.286673: step 83720, loss = 1.80 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:36:00.512912: step 83730, loss = 2.08 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:36:01.712616: step 83740, loss = 1.97 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:36:02.945629: step 83750, loss = 2.11 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:36:04.126864: step 83760, loss = 1.90 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:36:05.355908: step 83770, loss = 2.08 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:36:06.547128: step 83780, loss = 1.89 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:36:07.771419: step 83790, loss = 1.92 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:09.003982: step 83800, loss = 1.84 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:36:10.203532: step 83810, loss = 2.08 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:36:11.419093: step 83820, loss = 1.96 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:12.647280: step 83830, loss = 2.21 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:36:13.856311: step 83840, loss = 1.89 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:15.082403: step 83850, loss = 2.03 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:36:16.269482: step 83860, loss = 2.01 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:36:17.489517: step 83870, loss = 2.08 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:18.702605: step 83880, loss = 1.90 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:19.913403: step 83890, loss = 2.00 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:21.125993: step 83900, loss = 1.94 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:22.345342: step 83910, loss = 1.98 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:23.557412: step 83920, loss = 1.76 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:24.797347: step 83930, loss = 1.91 (1032.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:36:26.001396: step 83940, loss = 1.78 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:36:27.242120: step 83950, loss = 1.86 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:36:28.400196: step 83960, loss = 1.93 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:36:29.621550: step 83970, loss = 1.97 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:30.838770: step 83980, loss = 1.84 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:32.054036: step 83990, loss = 1.91 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:33.249347: step 84000, loss = 1.82 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:36:34.448827: step 84010, loss = 2.02 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:36:35.651320: step 84020, loss = 1.77 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:36:36.853714: step 84030, loss = 1.95 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:36:38.055959: step 84040, loss = 1.88 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:36:39.250546: step 84050, loss = 1.91 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:36:40.457563: step 84060, loss = 1.94 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:41.646898: step 84070, loss = 1.90 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:36:42.863077: step 84080, loss = 2.02 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:44.095948: step 84090, loss = 1.85 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:36:45.313536: step 84100, loss = 2.01 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:46.526730: step 84110, loss = 1.97 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:36:47.744691: step 84120, loss = 1.78 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:48.968704: step 84130, loss = 1.90 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:50.188957: step 84140, loss = 1.85 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:51.533206: step 84150, loss = 1.87 (952.2 examples/sec; 0.134 sec/batch)
2017-05-05 00:36:52.614557: step 84160, loss = 1.99 (1183.7 examples/sec; 0.108 sec/batch)
2017-05-05 00:36:53.811527: step 84170, loss = 1.98 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:36:55.036683: step 84180, loss = 1.82 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:36:56.294454: step 84190, loss = 2.08 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-05 00:36:57.495252: step 84200, loss = 1.87 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:36:58.717193: step 84210, loss = 1.95 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:36:59.926381: step 84220, loss = 1.97 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:01.138833: step 84230, loss = 1.77 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:02.347948: step 84240, loss = 1.97 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:03.584302: step 84250, loss = 1.97 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:37:04.779964: step 84260, loss = 2.00 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:37:05.999286: step 84270, loss = 1.93 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:37:07.218557: step 84280, loss = 1.91 (1049.8 examples/sec; 0.122E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 870 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
 sec/batch)
2017-05-05 00:37:08.424760: step 84290, loss = 1.91 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:09.638406: step 84300, loss = 1.98 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:10.879346: step 84310, loss = 1.91 (1031.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:37:12.093885: step 84320, loss = 1.93 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:13.326326: step 84330, loss = 1.82 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:37:14.526244: step 84340, loss = 2.13 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:37:15.813039: step 84350, loss = 1.78 (994.7 examples/sec; 0.129 sec/batch)
2017-05-05 00:37:16.908952: step 84360, loss = 2.06 (1168.0 examples/sec; 0.110 sec/batch)
2017-05-05 00:37:18.133162: step 84370, loss = 1.93 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:37:19.362864: step 84380, loss = 1.97 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:37:20.566633: step 84390, loss = 1.97 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:37:21.766838: step 84400, loss = 1.90 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:37:22.992023: step 84410, loss = 1.89 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:37:24.202118: step 84420, loss = 2.08 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:25.420287: step 84430, loss = 1.97 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:37:26.604770: step 84440, loss = 1.89 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:37:27.851626: step 84450, loss = 1.81 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-05 00:37:29.063999: step 84460, loss = 2.04 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:30.285277: step 84470, loss = 1.82 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:37:31.497706: step 84480, loss = 1.88 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:32.741059: step 84490, loss = 2.01 (1029.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:37:33.924101: step 84500, loss = 1.95 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:37:35.178402: step 84510, loss = 1.99 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-05 00:37:36.401957: step 84520, loss = 2.04 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:37:37.612067: step 84530, loss = 1.92 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:38.818965: step 84540, loss = 1.99 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:39.994515: step 84550, loss = 1.99 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:37:41.223065: step 84560, loss = 2.07 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:37:42.434998: step 84570, loss = 1.83 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:43.643885: step 84580, loss = 1.98 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:44.839018: step 84590, loss = 1.92 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:37:46.022110: step 84600, loss = 2.09 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:37:47.233437: step 84610, loss = 1.97 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:37:48.424597: step 84620, loss = 1.91 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:37:49.589183: step 84630, loss = 2.09 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:37:50.762106: step 84640, loss = 1.84 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:37:51.928149: step 84650, loss = 1.99 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:37:53.094743: step 84660, loss = 1.99 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:37:54.259366: step 84670, loss = 2.02 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:37:55.410589: step 84680, loss = 2.02 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-05 00:37:56.596944: step 84690, loss = 1.78 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:37:57.739823: step 84700, loss = 1.95 (1120.0 examples/sec; 0.114 sec/batch)
2017-05-05 00:37:58.915847: step 84710, loss = 1.96 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:00.098551: step 84720, loss = 1.76 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:01.280110: step 84730, loss = 1.96 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:02.453038: step 84740, loss = 1.91 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:03.613745: step 84750, loss = 1.98 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:38:04.774497: step 84760, loss = 1.89 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:38:05.935750: step 84770, loss = 1.83 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:38:07.120327: step 84780, loss = 1.97 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:08.280805: step 84790, loss = 2.06 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:38:09.439271: step 84800, loss = 1.93 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:38:10.608347: step 84810, loss = 1.88 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:11.788188: step 84820, loss = 1.87 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:12.960640: step 84830, loss = 1.98 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:14.135453: step 84840, loss = 1.83 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:15.306882: step 84850, loss = 1.88 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:16.492587: step 84860, loss = 2.09 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:38:17.654172: step 84870, loss = 2.02 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:38:18.826767: step 84880, loss = 2.00 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:19.998358: step 84890, loss = 1.94 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:21.191784: step 84900, loss = 1.96 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:38:22.349244: step 84910, loss = 1.96 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:38:23.538812: step 84920, loss = 1.99 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:38:24.730101: step 84930, loss = 1.82 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:38:25.903522: step 84940, loss = 1.91 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:27.085407: step 84950, loss = 1.91 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:28.267761: step 84960, loss = 1.95 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:29.451787: step 84970, loss = 1.85 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:30.623907: step 84980, loss = 1.95 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:31.785015: step 84990, loss = 2.08 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:38:32.940204: step 85000, loss = 1.74 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:38:34.094039: step 85010, loss = 2.03 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:38:35.268875: step 85020, loss = 1.83 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:36.458522: step 85030, loss = 1.89 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:38:37.607434: step 85040, loss = 2.05 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:38:38.796073: step 85050, loss = 1.91 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:38:39.974207: step 85060, loss = 1.73 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:41.158944: step 85070, loss = 1.88 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:42.321590: step 85080, loss = 2.06 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:38:43.490923: step 85090, loss = 1.90 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:44.675205: step 85100, loss = 1.91 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:45.851418: step 85110, loss = 1.87 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:47.027272: step 85120, loss = 1.84 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:48.222032: step 85130, loss = 1.99 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:38:49.508513: step 85140, loss = 2.02 (995.0 examples/sec; 0.129 sec/batch)
2017-05-05 00:38:50.575268: step 85150, loss = 1.93 (1199.9 examples/sec; 0.107 sec/batch)
2017-05-05 00:38:51.748362: step 85160, loss = 1.94 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:38:52.939045: step 85170, loss = 1.82 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:38:54.119427: step 85180, loss = 1.83 (1084.4 examples/sec; 0.118 secE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 882 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
/batch)
2017-05-05 00:38:55.333014: step 85190, loss = 1.90 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:38:56.509339: step 85200, loss = 1.99 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:38:57.717604: step 85210, loss = 2.03 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:38:58.899353: step 85220, loss = 1.89 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:00.098345: step 85230, loss = 1.87 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:39:01.302485: step 85240, loss = 2.00 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:39:02.474600: step 85250, loss = 1.95 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:03.687560: step 85260, loss = 1.89 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:39:04.896925: step 85270, loss = 1.87 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:39:06.079728: step 85280, loss = 1.84 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:07.282995: step 85290, loss = 1.97 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:39:08.477698: step 85300, loss = 1.91 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:09.651781: step 85310, loss = 2.03 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:10.862063: step 85320, loss = 1.87 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:39:12.067678: step 85330, loss = 1.87 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:39:13.298709: step 85340, loss = 1.99 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:39:14.449116: step 85350, loss = 1.97 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:39:15.629848: step 85360, loss = 1.96 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:16.810191: step 85370, loss = 2.01 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:17.987481: step 85380, loss = 1.95 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:19.173108: step 85390, loss = 1.82 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:20.337009: step 85400, loss = 1.91 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:39:21.489710: step 85410, loss = 2.15 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:39:22.661097: step 85420, loss = 1.90 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:23.846285: step 85430, loss = 1.93 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:25.037777: step 85440, loss = 1.87 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:26.195795: step 85450, loss = 1.92 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:39:27.367478: step 85460, loss = 2.01 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:28.528008: step 85470, loss = 1.86 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:39:29.694022: step 85480, loss = 1.92 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:30.854616: step 85490, loss = 1.94 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:39:32.021987: step 85500, loss = 1.86 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:33.187972: step 85510, loss = 2.04 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:34.373855: step 85520, loss = 2.00 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:35.538076: step 85530, loss = 2.13 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:39:36.701352: step 85540, loss = 1.88 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:39:37.846316: step 85550, loss = 2.13 (1117.9 examples/sec; 0.114 sec/batch)
2017-05-05 00:39:39.027709: step 85560, loss = 1.76 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:40.188942: step 85570, loss = 1.87 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:39:41.350341: step 85580, loss = 1.87 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:39:42.536213: step 85590, loss = 1.89 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:43.711564: step 85600, loss = 1.81 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:44.890558: step 85610, loss = 1.91 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:46.079756: step 85620, loss = 1.95 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:47.260618: step 85630, loss = 1.93 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:48.450401: step 85640, loss = 1.82 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:49.632938: step 85650, loss = 1.88 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:50.825189: step 85660, loss = 1.94 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:51.991260: step 85670, loss = 1.93 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:39:53.173189: step 85680, loss = 1.93 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:54.351903: step 85690, loss = 1.92 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:55.529673: step 85700, loss = 1.97 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:39:56.717749: step 85710, loss = 1.80 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:39:57.867065: step 85720, loss = 1.86 (1113.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:39:59.036350: step 85730, loss = 1.96 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:00.205109: step 85740, loss = 2.02 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:01.361899: step 85750, loss = 2.07 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:02.540814: step 85760, loss = 1.93 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:03.696736: step 85770, loss = 2.13 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:04.875725: step 85780, loss = 2.01 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:06.026392: step 85790, loss = 1.85 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:40:07.192745: step 85800, loss = 2.05 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:08.357141: step 85810, loss = 2.07 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:09.542562: step 85820, loss = 1.80 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:40:10.699049: step 85830, loss = 2.24 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:11.877377: step 85840, loss = 1.93 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:13.045439: step 85850, loss = 1.83 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:14.196037: step 85860, loss = 1.95 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:40:15.353322: step 85870, loss = 1.85 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:16.523026: step 85880, loss = 1.99 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:17.682278: step 85890, loss = 1.88 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:18.864389: step 85900, loss = 1.94 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:20.020225: step 85910, loss = 1.96 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:21.180890: step 85920, loss = 1.92 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:22.342064: step 85930, loss = 1.93 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:23.504468: step 85940, loss = 2.04 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:24.677681: step 85950, loss = 1.86 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:25.830426: step 85960, loss = 1.79 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:40:27.009263: step 85970, loss = 1.97 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:28.183588: step 85980, loss = 1.91 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:29.346511: step 85990, loss = 2.04 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:30.529071: step 86000, loss = 1.95 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:31.689375: step 86010, loss = 1.93 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:32.846673: step 86020, loss = 2.01 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:34.010786: step 86030, loss = 1.93 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:35.195870: step 86040, loss = 2.04 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:40:36.366705: step 86050, loss = 1.88 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:37.521001: step 86060, loss = 2.15 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:40:38.676000: step 86070, loss = 2.01 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:40:39.854072: step 86080, loss = 1.99 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:41.005080: step 86090, loss = 1.96 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:40:42.155486: step 86100, loss = 2.04 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:40:43.301721: step 86110, loss = 2.07 (1116.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:40:44.492651: step 86120, loss = 2.01 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:40:45.740109: step 86130, loss = 1.96 (1026.1 examples/sec; 0.125 sec/batch)
2017-05-05 00:40:46.828429: step 86140, loss = 2.00 (1176.1 examples/sec; 0.109 sec/batch)
2017-05-05 00:40:47.999094: step 86150, loss = 2.14 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:40:49.147092: step 86160, loss = 1.96 (1115.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:40:50.293043: step 86170, loss = 1.79 (1117.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:40:51.470593: step 86180, loss = 1.92 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:52.655003: step 86190, loss = 1.91 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:53.839524: step 86200, loss = 1.78 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:55.030294: step 86210, loss = 2.00 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:40:56.238347: step 86220, loss = 1.95 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:40:57.413381: step 86230, loss = 1.85 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:40:58.600181: step 86240, loss = 2.07 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:40:59.802630: step 86250, loss = 2.01 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:41:00.977474: step 86260, loss = 1.90 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:02.151307: step 86270, loss = 2.07 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:03.337397: step 86280, loss = 2.03 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:41:04.513364: step 86290, loss = 1.79 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:05.675392: step 86300, loss = 2.03 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:06.869834: step 86310, loss = 1.88 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:41:08.023917: step 86320, loss = 2.03 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:41:09.205925: step 86330, loss = 1.88 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:10.378329: step 86340, loss = 2.03 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:11.566114: step 86350, loss = 1.87 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:41:12.750731: step 86360, loss = 2.00 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:13.907146: step 86370, loss = 2.03 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:15.074278: step 86380, loss = 1.91 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:16.246704: step 86390, loss = 1.91 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:17.407307: step 86400, loss = 2.10 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:18.559377: step 86410, loss = 1.94 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:41:19.725015: step 86420, loss = 1.86 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:20.868897: step 86430, loss = 1.87 (1119.0 examples/sec; 0.114 sec/batch)
2017-05-05 00:41:22.028737: step 86440, loss = 2.12 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:23.197872: step 86450, loss = 1.93 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:24.373069: step 86460, loss = 1.81 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:25.534979: step 86470, loss = 1.95 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:26.710380: step 86480, loss = 1.98 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:27.898550: step 86490, loss = 1.84 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:41:29.053405: step 86500, loss = 1.80 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:41:30.228397: step 86510, loss = 2.06 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:31.390653: step 86520, loss = 2.01 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:32.554783: step 86530, loss = 1.79 (1099.5 examples/sec; 0.116 sec/baE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 893 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
tch)
2017-05-05 00:41:33.710528: step 86540, loss = 2.09 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:34.864839: step 86550, loss = 2.01 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:41:36.027066: step 86560, loss = 1.94 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:37.203858: step 86570, loss = 1.97 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:38.361661: step 86580, loss = 1.95 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:39.535900: step 86590, loss = 1.79 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:40.705539: step 86600, loss = 2.08 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:41.896736: step 86610, loss = 1.86 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:41:43.075861: step 86620, loss = 1.95 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:44.261388: step 86630, loss = 1.92 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:41:45.421200: step 86640, loss = 2.01 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:46.570835: step 86650, loss = 1.95 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:41:47.739905: step 86660, loss = 1.95 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:48.915527: step 86670, loss = 1.93 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:41:50.075680: step 86680, loss = 1.86 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:51.269318: step 86690, loss = 1.99 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:41:52.461518: step 86700, loss = 2.00 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:41:53.620522: step 86710, loss = 2.02 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:54.787934: step 86720, loss = 1.89 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:55.948054: step 86730, loss = 1.89 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:57.116300: step 86740, loss = 1.95 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:41:58.273370: step 86750, loss = 1.90 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:41:59.439684: step 86760, loss = 1.94 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:00.624523: step 86770, loss = 1.74 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:01.760370: step 86780, loss = 1.86 (1126.9 examples/sec; 0.114 sec/batch)
2017-05-05 00:42:02.924700: step 86790, loss = 1.93 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:04.079839: step 86800, loss = 1.84 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:05.241903: step 86810, loss = 2.01 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:06.398327: step 86820, loss = 1.95 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:07.558373: step 86830, loss = 2.02 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:08.733760: step 86840, loss = 1.90 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:09.873336: step 86850, loss = 1.85 (1123.2 examples/sec; 0.114 sec/batch)
2017-05-05 00:42:11.032816: step 86860, loss = 1.96 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:12.197766: step 86870, loss = 1.86 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:13.361767: step 86880, loss = 1.83 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:14.549578: step 86890, loss = 1.95 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:42:15.719484: step 86900, loss = 2.05 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:16.899498: step 86910, loss = 1.94 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:18.054425: step 86920, loss = 2.08 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:42:19.229522: step 86930, loss = 1.80 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:20.400349: step 86940, loss = 1.90 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:21.575525: step 86950, loss = 1.90 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:22.728940: step 86960, loss = 1.94 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:42:23.892267: step 86970, loss = 1.84 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:25.063050: step 86980, loss = 1.79 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:26.247094: step 86990, loss = 1.97 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:27.417878: step 87000, loss = 1.91 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:28.581111: step 87010, loss = 1.90 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:29.730159: step 87020, loss = 1.99 (1114.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:42:30.898628: step 87030, loss = 2.05 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:32.065416: step 87040, loss = 1.87 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:33.247289: step 87050, loss = 1.89 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:34.389871: step 87060, loss = 1.91 (1120.3 examples/sec; 0.114 sec/batch)
2017-05-05 00:42:35.557439: step 87070, loss = 2.03 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:36.725429: step 87080, loss = 1.90 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:37.892225: step 87090, loss = 2.03 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:39.079188: step 87100, loss = 1.91 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:42:40.243854: step 87110, loss = 1.88 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:41.513425: step 87120, loss = 1.99 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-05 00:42:42.575026: step 87130, loss = 2.06 (1205.7 examples/sec; 0.106 sec/batch)
2017-05-05 00:42:43.725678: step 87140, loss = 1.94 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:42:44.895400: step 87150, loss = 1.91 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:46.046055: step 87160, loss = 2.00 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:42:47.225111: step 87170, loss = 1.88 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:48.387165: step 87180, loss = 2.11 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:49.555408: step 87190, loss = 1.95 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:50.732887: step 87200, loss = 1.92 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:51.908014: step 87210, loss = 1.87 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:53.083940: step 87220, loss = 2.08 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:54.244110: step 87230, loss = 1.97 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:42:55.425721: step 87240, loss = 1.84 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:56.602194: step 87250, loss = 1.83 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:42:57.776659: step 87260, loss = 2.11 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:42:58.948457: step 87270, loss = 1.89 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:00.133952: step 87280, loss = 1.88 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:43:01.296969: step 87290, loss = 1.96 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:02.482392: step 87300, loss = 1.91 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:43:03.640222: step 87310, loss = 1.84 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:04.799147: step 87320, loss = 1.87 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:05.955685: step 87330, loss = 1.80 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:07.116727: step 87340, loss = 1.84 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:08.306606: step 87350, loss = 2.06 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:43:09.476443: step 87360, loss = 1.86 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:10.641207: step 87370, loss = 1.95 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:11.810168: step 87380, loss = 1.92 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:12.992754: step 87390, loss = 1.88 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:14.144647: step 87400, loss = 1.92 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:43:15.333742: step 87410, loss = 2.06 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:43:16.509080: step 87420, loss = 1.84 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:17.692879: step 87430, loss = 2.00 (1081.3 examples/sec; 0.118 sec/batcE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 904 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
h)
2017-05-05 00:43:18.890067: step 87440, loss = 1.90 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:43:20.065624: step 87450, loss = 1.94 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:21.240404: step 87460, loss = 1.80 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:22.393090: step 87470, loss = 2.03 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:43:23.568065: step 87480, loss = 2.01 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:24.733458: step 87490, loss = 1.92 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:25.892393: step 87500, loss = 2.03 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:27.059153: step 87510, loss = 2.06 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:28.241427: step 87520, loss = 2.04 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:29.422902: step 87530, loss = 1.87 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:30.586513: step 87540, loss = 2.00 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:31.757133: step 87550, loss = 1.99 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:32.934976: step 87560, loss = 2.02 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:34.094362: step 87570, loss = 1.97 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:35.262397: step 87580, loss = 2.03 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:36.428730: step 87590, loss = 1.79 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:37.591871: step 87600, loss = 1.97 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:38.765541: step 87610, loss = 1.90 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:39.923783: step 87620, loss = 1.88 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:41.096349: step 87630, loss = 1.97 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:42.244100: step 87640, loss = 1.81 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:43:43.411121: step 87650, loss = 1.85 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:44.580646: step 87660, loss = 2.02 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:45.757959: step 87670, loss = 1.87 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:46.919780: step 87680, loss = 2.03 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:48.101336: step 87690, loss = 1.87 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:49.271264: step 87700, loss = 1.99 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:50.416699: step 87710, loss = 1.95 (1117.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:43:51.589583: step 87720, loss = 2.02 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:43:52.735138: step 87730, loss = 1.76 (1117.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:43:53.893502: step 87740, loss = 2.02 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:55.057378: step 87750, loss = 2.06 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:56.239388: step 87760, loss = 1.86 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:43:57.397614: step 87770, loss = 2.00 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:43:58.550111: step 87780, loss = 1.85 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:43:59.710465: step 87790, loss = 1.86 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:00.877029: step 87800, loss = 1.90 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:02.041008: step 87810, loss = 2.01 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:03.213929: step 87820, loss = 1.87 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:04.394800: step 87830, loss = 1.97 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:44:05.560466: step 87840, loss = 2.01 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:06.712968: step 87850, loss = 2.18 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:44:07.872621: step 87860, loss = 1.97 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:09.045802: step 87870, loss = 1.84 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:10.227898: step 87880, loss = 1.99 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:44:11.390714: step 87890, loss = 2.09 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:12.561552: step 87900, loss = 1.90 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:13.730632: step 87910, loss = 1.95 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:14.889452: step 87920, loss = 2.09 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:16.071416: step 87930, loss = 1.88 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:44:17.237539: step 87940, loss = 2.00 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:18.394223: step 87950, loss = 1.87 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:19.533879: step 87960, loss = 2.04 (1123.1 examples/sec; 0.114 sec/batch)
2017-05-05 00:44:20.687190: step 87970, loss = 1.97 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 00:44:21.848705: step 87980, loss = 1.92 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:23.034156: step 87990, loss = 2.07 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:44:24.191352: step 88000, loss = 1.96 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:25.364316: step 88010, loss = 1.97 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:26.513807: step 88020, loss = 1.69 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:44:27.699934: step 88030, loss = 2.13 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:44:28.870830: step 88040, loss = 1.88 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:30.031389: step 88050, loss = 2.03 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:31.198173: step 88060, loss = 1.89 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:32.372953: step 88070, loss = 2.04 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:33.538201: step 88080, loss = 2.01 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:34.698045: step 88090, loss = 1.98 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:35.860752: step 88100, loss = 2.03 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:37.146481: step 88110, loss = 1.92 (995.5 examples/sec; 0.129 sec/batch)
2017-05-05 00:44:38.208295: step 88120, loss = 2.03 (1205.5 examples/sec; 0.106 sec/batch)
2017-05-05 00:44:39.376764: step 88130, loss = 1.88 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:40.562561: step 88140, loss = 1.94 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:44:41.742978: step 88150, loss = 1.99 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:44:42.952585: step 88160, loss = 1.98 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:44:44.151533: step 88170, loss = 1.85 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:44:45.355702: step 88180, loss = 1.87 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:44:46.527670: step 88190, loss = 1.86 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:44:47.734191: step 88200, loss = 1.87 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:44:48.922871: step 88210, loss = 1.93 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:44:50.125429: step 88220, loss = 2.04 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:44:51.311546: step 88230, loss = 1.91 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:44:52.506281: step 88240, loss = 1.92 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:44:53.689682: step 88250, loss = 1.84 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:44:54.884336: step 88260, loss = 2.00 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:44:56.083032: step 88270, loss = 1.99 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:44:57.280168: step 88280, loss = 1.88 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:44:58.441212: step 88290, loss = 2.05 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:44:59.617989: step 88300, loss = 1.76 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:00.859391: step 88310, loss = 1.91 (1031.1 examples/sec; 0.124 sec/batch)
2017-05-05 00:45:01.999170: step 88320, loss = 2.06 (1123.0 examples/sec; 0.114 sec/batch)
2017-05-05 00:45:03.192586: step 88330, loss = 2.15 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:45:04.379630: step 88340, loss = 1.85 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:45:05.570020: step 88350, loss = 2.07 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:45:06.763453: step 88360, loss = 1.99 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:45:07.952431: step 88370, loss = 1.94 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:45:09.134557: step 88380, loss = 1.78 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:10.318749: step 88390, loss = 1.79 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:11.488737: step 88400, loss = 2.06 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:12.668261: step 88410, loss = 1.90 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:13.850134: step 88420, loss = 1.88 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:15.021150: step 88430, loss = 1.99 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:16.198038: step 88440, loss = 1.91 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:17.359569: step 88450, loss = 1.95 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:18.518646: step 88460, loss = 1.98 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:19.708965: step 88470, loss = 1.87 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:45:20.914353: step 88480, loss = 2.09 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:45:22.077871: step 88490, loss = 1.95 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:23.270367: step 88500, loss = 1.74 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:45:24.433684: step 88510, loss = 1.95 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:25.609572: step 88520, loss = 1.87 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:26.772148: step 88530, loss = 1.98 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:27.946037: step 88540, loss = 2.03 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:29.116680: step 88550, loss = 1.94 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:30.273763: step 88560, loss = 1.86 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:31.428878: step 88570, loss = 1.88 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:32.582971: step 88580, loss = 1.85 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:45:33.739771: step 88590, loss = 2.00 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:34.902429: step 88600, loss = 1.86 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:36.078886: step 88610, loss = 1.90 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:37.266772: step 88620, loss = 1.83 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:45:38.433682: step 88630, loss = 2.04 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:39.627305: step 88640, loss = 1.98 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:45:40.817668: step 88650, loss = 1.89 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:45:41.982632: step 88660, loss = 1.84 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:43.167381: step 88670, loss = 1.92 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:44.338615: step 88680, loss = 1.84 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:45.514417: step 88690, loss = 1.94 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:46.690611: step 88700, loss = 1.96 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:47.870258: step 88710, loss = 1.95 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:45:49.038144: step 88720, loss = 1.84 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:50.191576: step 88730, loss = 1.87 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:45:51.351547: step 88740, loss = 1.87 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:52.509840: step 88750, loss = 2.01 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:53.681605: step 88760, loss = 2.06 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:54.856023: step 88770, loss = 1.77 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:45:56.022808: step 88780, loss = 1.97 (1097.0 examples/sec; 0.117 sec/batch)
2E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 915 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
017-05-05 00:45:57.180594: step 88790, loss = 2.03 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:45:58.369118: step 88800, loss = 1.95 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:45:59.521320: step 88810, loss = 2.02 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:46:00.679164: step 88820, loss = 1.71 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:01.837712: step 88830, loss = 1.99 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:03.005831: step 88840, loss = 1.89 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:04.181541: step 88850, loss = 1.87 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:46:05.372983: step 88860, loss = 1.92 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:46:06.543558: step 88870, loss = 1.88 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:07.708924: step 88880, loss = 2.00 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:08.869750: step 88890, loss = 1.91 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:10.031344: step 88900, loss = 1.98 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:11.193830: step 88910, loss = 1.99 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:12.363276: step 88920, loss = 1.97 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:13.521821: step 88930, loss = 1.75 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:14.693545: step 88940, loss = 1.81 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:15.861495: step 88950, loss = 2.08 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:17.039162: step 88960, loss = 1.98 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:46:18.198812: step 88970, loss = 1.80 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:19.362515: step 88980, loss = 1.86 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:20.545173: step 88990, loss = 1.95 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:46:21.684552: step 89000, loss = 1.91 (1123.4 examples/sec; 0.114 sec/batch)
2017-05-05 00:46:22.847753: step 89010, loss = 2.02 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:24.007499: step 89020, loss = 1.79 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:25.175198: step 89030, loss = 1.83 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:26.331409: step 89040, loss = 1.91 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:27.501622: step 89050, loss = 1.93 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:28.687848: step 89060, loss = 1.98 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:46:29.877549: step 89070, loss = 1.93 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:46:31.058534: step 89080, loss = 1.93 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:46:32.267959: step 89090, loss = 2.01 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:46:33.547539: step 89100, loss = 1.79 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-05 00:46:34.634621: step 89110, loss = 2.01 (1177.5 examples/sec; 0.109 sec/batch)
2017-05-05 00:46:35.833791: step 89120, loss = 1.93 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:46:37.039727: step 89130, loss = 1.85 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:46:38.236793: step 89140, loss = 1.81 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:46:39.437781: step 89150, loss = 1.88 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:46:40.631980: step 89160, loss = 2.06 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:46:41.803005: step 89170, loss = 1.99 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:42.977809: step 89180, loss = 1.86 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:44.162593: step 89190, loss = 2.00 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:46:45.357025: step 89200, loss = 1.88 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:46:46.554831: step 89210, loss = 1.91 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:46:47.729084: step 89220, loss = 1.82 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:48.902220: step 89230, loss = 2.05 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:50.072493: step 89240, loss = 1.88 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:51.235350: step 89250, loss = 1.95 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:52.399606: step 89260, loss = 1.95 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:46:53.567510: step 89270, loss = 2.09 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:54.736407: step 89280, loss = 1.83 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:46:55.921781: step 89290, loss = 2.06 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:46:57.141957: step 89300, loss = 2.09 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:46:58.277221: step 89310, loss = 1.87 (1127.5 examples/sec; 0.114 sec/batch)
2017-05-05 00:46:59.447855: step 89320, loss = 1.93 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:00.644644: step 89330, loss = 1.90 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:47:01.832944: step 89340, loss = 1.91 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:47:03.017301: step 89350, loss = 1.88 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:47:04.200005: step 89360, loss = 1.99 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:47:05.346494: step 89370, loss = 1.96 (1116.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:47:06.502900: step 89380, loss = 2.15 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:07.680283: step 89390, loss = 1.99 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:47:08.841887: step 89400, loss = 1.83 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:10.004090: step 89410, loss = 1.86 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:11.159864: step 89420, loss = 1.86 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:12.333976: step 89430, loss = 1.81 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:13.505666: step 89440, loss = 1.81 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:14.662762: step 89450, loss = 2.05 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:15.832136: step 89460, loss = 1.89 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:17.016925: step 89470, loss = 1.95 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:47:18.159120: step 89480, loss = 1.81 (1120.6 examples/sec; 0.114 sec/batch)
2017-05-05 00:47:19.328496: step 89490, loss = 1.92 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:20.510637: step 89500, loss = 1.94 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:47:21.670272: step 89510, loss = 1.89 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:22.832511: step 89520, loss = 1.95 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:24.001424: step 89530, loss = 1.88 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:25.163963: step 89540, loss = 1.83 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:26.326013: step 89550, loss = 1.98 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:27.486969: step 89560, loss = 1.93 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:28.639560: step 89570, loss = 1.89 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-05 00:47:29.789587: step 89580, loss = 2.05 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:47:30.969630: step 89590, loss = 1.94 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:47:32.158772: step 89600, loss = 1.86 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:47:33.295839: step 89610, loss = 1.94 (1125.7 examples/sec; 0.114 sec/batch)
2017-05-05 00:47:34.464144: step 89620, loss = 1.94 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:35.647858: step 89630, loss = 1.95 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:47:36.800557: step 89640, loss = 1.97 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 00:47:37.962377: step 89650, loss = 2.03 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:39.134953: step 89660, loss = 1.80 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:40.311437: step 89670, loss = 2.02 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:47:41.484387: step 89680, loss = 1.95 (1091.3 examples/sec; 0.117 sec/batch)
201E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 926 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
7-05-05 00:47:42.658854: step 89690, loss = 1.87 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:43.817013: step 89700, loss = 2.08 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:44.985216: step 89710, loss = 1.91 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:46.138313: step 89720, loss = 1.91 (1110.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:47:47.312544: step 89730, loss = 1.86 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:48.474535: step 89740, loss = 1.84 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:49.632024: step 89750, loss = 1.86 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:50.798436: step 89760, loss = 2.08 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:51.957665: step 89770, loss = 1.97 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:53.143609: step 89780, loss = 1.85 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:47:54.309662: step 89790, loss = 1.98 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:47:55.471536: step 89800, loss = 1.85 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:56.615819: step 89810, loss = 2.04 (1118.6 examples/sec; 0.114 sec/batch)
2017-05-05 00:47:57.778288: step 89820, loss = 1.91 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:47:58.945306: step 89830, loss = 1.98 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:00.115553: step 89840, loss = 1.94 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:01.268390: step 89850, loss = 1.90 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:48:02.428149: step 89860, loss = 1.89 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:48:03.591473: step 89870, loss = 1.84 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:48:04.773432: step 89880, loss = 1.84 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:05.946350: step 89890, loss = 1.96 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:07.124520: step 89900, loss = 2.00 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:08.312076: step 89910, loss = 1.93 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:48:09.476687: step 89920, loss = 1.85 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:48:10.644853: step 89930, loss = 1.99 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:11.817981: step 89940, loss = 1.99 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:12.989186: step 89950, loss = 1.79 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:14.158422: step 89960, loss = 1.81 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:15.344530: step 89970, loss = 2.07 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:48:16.518462: step 89980, loss = 2.01 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:17.680749: step 89990, loss = 2.14 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:48:18.864856: step 90000, loss = 1.98 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:20.059384: step 90010, loss = 1.97 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:48:21.253742: step 90020, loss = 1.90 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:48:22.418708: step 90030, loss = 2.06 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:48:23.592836: step 90040, loss = 1.89 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:24.786107: step 90050, loss = 2.21 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:48:25.997170: step 90060, loss = 1.93 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:48:27.203797: step 90070, loss = 2.04 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:48:28.399128: step 90080, loss = 2.05 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:48:29.689532: step 90090, loss = 1.90 (991.9 examples/sec; 0.129 sec/batch)
2017-05-05 00:48:30.798440: step 90100, loss = 1.81 (1154.3 examples/sec; 0.111 sec/batch)
2017-05-05 00:48:31.980107: step 90110, loss = 1.94 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:33.183496: step 90120, loss = 2.06 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:48:34.359758: step 90130, loss = 2.04 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:35.642347: step 90140, loss = 2.10 (998.0 examples/sec; 0.128 sec/batch)
2017-05-05 00:48:36.782109: step 90150, loss = 1.85 (1123.0 examples/sec; 0.114 sec/batch)
2017-05-05 00:48:37.915107: step 90160, loss = 2.08 (1129.7 examples/sec; 0.113 sec/batch)
2017-05-05 00:48:39.080009: step 90170, loss = 2.07 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:48:40.249309: step 90180, loss = 1.97 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:41.385562: step 90190, loss = 1.99 (1126.5 examples/sec; 0.114 sec/batch)
2017-05-05 00:48:42.542194: step 90200, loss = 1.82 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:48:43.719842: step 90210, loss = 1.88 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:44.889482: step 90220, loss = 1.83 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:46.049028: step 90230, loss = 1.94 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:48:47.230322: step 90240, loss = 1.89 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:48.393167: step 90250, loss = 1.99 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:48:49.558553: step 90260, loss = 1.92 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:50.739095: step 90270, loss = 1.87 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:48:51.910552: step 90280, loss = 1.87 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:48:53.099437: step 90290, loss = 1.78 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:48:54.310519: step 90300, loss = 1.98 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:48:55.452342: step 90310, loss = 2.02 (1121.0 examples/sec; 0.114 sec/batch)
2017-05-05 00:48:56.677391: step 90320, loss = 1.86 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:48:57.864921: step 90330, loss = 1.88 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:48:59.059510: step 90340, loss = 1.99 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:49:00.256724: step 90350, loss = 1.89 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:49:01.472607: step 90360, loss = 1.93 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:02.708918: step 90370, loss = 1.82 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-05 00:49:03.921099: step 90380, loss = 1.96 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:05.145734: step 90390, loss = 2.00 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:06.359628: step 90400, loss = 1.88 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:07.566098: step 90410, loss = 1.88 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:08.802022: step 90420, loss = 1.96 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:49:10.020701: step 90430, loss = 1.83 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:11.248383: step 90440, loss = 1.83 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:12.474933: step 90450, loss = 1.86 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:13.667947: step 90460, loss = 1.81 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:49:14.902383: step 90470, loss = 1.86 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:16.116387: step 90480, loss = 1.91 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:17.329697: step 90490, loss = 1.91 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:18.555970: step 90500, loss = 1.98 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:19.758383: step 90510, loss = 1.90 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:49:20.988388: step 90520, loss = 2.01 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:22.202453: step 90530, loss = 1.80 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:23.400225: step 90540, loss = 1.98 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:49:24.629256: step 90550, loss = 1.93 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:25.813129: step 90560, loss = 1.95 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:49:27.054758: step 90570, loss = 1.92 (1030.9 examples/sec; 0.124 sec/batch)
2017-05-05 00:49:28.266445: step 90580, loss = 2.01 (1056.4 examples/sec; 0.121 sec/batch)
2017-05E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 938 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
-05 00:49:29.495861: step 90590, loss = 1.91 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:30.732306: step 90600, loss = 1.83 (1035.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:49:31.936613: step 90610, loss = 1.94 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:49:33.143419: step 90620, loss = 1.87 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:34.363425: step 90630, loss = 1.99 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:35.571575: step 90640, loss = 1.99 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:36.803770: step 90650, loss = 1.83 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:37.985970: step 90660, loss = 1.82 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:49:39.213637: step 90670, loss = 2.16 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:40.420272: step 90680, loss = 1.80 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:41.639051: step 90690, loss = 1.82 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:42.832268: step 90700, loss = 1.88 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:49:44.065553: step 90710, loss = 1.86 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:45.273124: step 90720, loss = 2.03 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:46.486753: step 90730, loss = 1.78 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:47.714891: step 90740, loss = 1.86 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:48.939314: step 90750, loss = 1.91 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:50.144385: step 90760, loss = 1.86 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:51.356538: step 90770, loss = 2.03 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:52.576292: step 90780, loss = 2.06 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:53.807053: step 90790, loss = 1.96 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:55.026877: step 90800, loss = 1.95 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:49:56.257300: step 90810, loss = 1.77 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:49:57.466442: step 90820, loss = 1.91 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:49:58.707696: step 90830, loss = 2.12 (1031.2 examples/sec; 0.124 sec/batch)
2017-05-05 00:49:59.898776: step 90840, loss = 1.98 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:50:01.121165: step 90850, loss = 1.80 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:02.339802: step 90860, loss = 1.95 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:03.535637: step 90870, loss = 1.93 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:50:04.755772: step 90880, loss = 1.90 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:05.961335: step 90890, loss = 1.87 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:07.166735: step 90900, loss = 1.92 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:08.386753: step 90910, loss = 1.90 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:09.586948: step 90920, loss = 1.82 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:50:10.835171: step 90930, loss = 1.98 (1025.4 examples/sec; 0.125 sec/batch)
2017-05-05 00:50:12.051566: step 90940, loss = 1.83 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:13.259125: step 90950, loss = 2.09 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:14.464774: step 90960, loss = 1.96 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:15.698268: step 90970, loss = 2.05 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:50:16.914209: step 90980, loss = 1.85 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:18.134036: step 90990, loss = 1.87 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:19.348313: step 91000, loss = 2.02 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:20.557616: step 91010, loss = 1.91 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:21.762988: step 91020, loss = 1.91 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:22.966721: step 91030, loss = 1.87 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:50:24.170923: step 91040, loss = 1.93 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:50:25.382774: step 91050, loss = 2.09 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:26.601329: step 91060, loss = 1.77 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:27.825310: step 91070, loss = 1.84 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:29.141752: step 91080, loss = 2.02 (972.3 examples/sec; 0.132 sec/batch)
2017-05-05 00:50:30.251345: step 91090, loss = 1.91 (1153.6 examples/sec; 0.111 sec/batch)
2017-05-05 00:50:31.444565: step 91100, loss = 2.30 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:50:32.654029: step 91110, loss = 1.84 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:33.855905: step 91120, loss = 2.01 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:50:35.037076: step 91130, loss = 1.78 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:50:36.242561: step 91140, loss = 1.93 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:37.477274: step 91150, loss = 1.88 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:50:38.659062: step 91160, loss = 2.13 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:50:39.880887: step 91170, loss = 1.91 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:41.115746: step 91180, loss = 1.80 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:50:42.322615: step 91190, loss = 1.98 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:43.544488: step 91200, loss = 1.93 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:44.759726: step 91210, loss = 1.96 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:45.977431: step 91220, loss = 1.82 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:47.204391: step 91230, loss = 1.84 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:50:48.409909: step 91240, loss = 2.04 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:49.613988: step 91250, loss = 2.04 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:50:50.839789: step 91260, loss = 2.02 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:50:52.055145: step 91270, loss = 1.87 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:53.353186: step 91280, loss = 1.90 (986.1 examples/sec; 0.130 sec/batch)
2017-05-05 00:50:54.466290: step 91290, loss = 1.91 (1149.9 examples/sec; 0.111 sec/batch)
2017-05-05 00:50:55.707914: step 91300, loss = 1.86 (1030.9 examples/sec; 0.124 sec/batch)
2017-05-05 00:50:56.916486: step 91310, loss = 2.04 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:50:58.133334: step 91320, loss = 1.93 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:50:59.343933: step 91330, loss = 2.00 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:00.559423: step 91340, loss = 1.83 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:01.767920: step 91350, loss = 1.91 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:02.994712: step 91360, loss = 1.86 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:51:04.212926: step 91370, loss = 2.13 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:05.405670: step 91380, loss = 1.94 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:51:06.619636: step 91390, loss = 1.86 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:07.844488: step 91400, loss = 1.93 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:09.084306: step 91410, loss = 1.78 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:51:10.279762: step 91420, loss = 1.81 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:51:11.509611: step 91430, loss = 1.77 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:51:12.738984: step 91440, loss = 1.83 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:51:13.951607: step 91450, loss = 1.95 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:15.172160: step 91460, loss = 2.05 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:16.390734: step 91470, loss = 1.91 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:17.600761: step 91480, loss = 1.94 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:18.838203: step 91490, loss = 2.04 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:51:20.051321: step 91500, loss = 1.95 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:21.270811: step 91510, loss = 1.87 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:22.465273: step 91520, loss = 1.81 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:51:23.687462: step 91530, loss = 2.02 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:24.908377: step 91540, loss = 2.10 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:26.145762: step 91550, loss = 1.90 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:51:27.395954: step 91560, loss = 1.89 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-05 00:51:28.592213: step 91570, loss = 2.13 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:51:29.785384: step 91580, loss = 1.86 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:51:31.041971: step 91590, loss = 1.88 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-05 00:51:32.262027: step 91600, loss = 1.98 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:33.487629: step 91610, loss = 1.96 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:51:34.688958: step 91620, loss = 1.77 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:51:35.911462: step 91630, loss = 1.89 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:37.133059: step 91640, loss = 1.93 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:38.331815: step 91650, loss = 1.94 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:51:39.549515: step 91660, loss = 1.86 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:40.768491: step 91670, loss = 2.00 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:41.948502: step 91680, loss = 1.87 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:51:43.185615: step 91690, loss = 1.82 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:51:44.396387: step 91700, loss = 1.88 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:45.614970: step 91710, loss = 1.94 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:46.820629: step 91720, loss = 1.79 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:48.059877: step 91730, loss = 1.84 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-05 00:51:49.262602: step 91740, loss = 1.93 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:51:50.471461: step 91750, loss = 1.81 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:51:51.693412: step 91760, loss = 1.93 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:52.919481: step 91770, loss = 1.88 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 00:51:54.138872: step 91780, loss = 1.94 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:55.355160: step 91790, loss = 1.97 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:51:56.556481: step 91800, loss = 1.94 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:51:57.731292: step 91810, loss = 1.92 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:51:58.955222: step 91820, loss = 2.16 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:00.179064: step 91830, loss = 2.03 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:01.361510: step 91840, loss = 2.02 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:52:02.548540: step 91850, loss = 1.86 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:52:03.758315: step 91860, loss = 1.90 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:04.974632: step 91870, loss = 1.95 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:06.158311: step 91880, loss = 1.81 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:52:07.384705: step 91890, loss = 1.86 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:52:08.596209: step 91900, loss = 1.80 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:09.796478: step 91910, loss = 1.82 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:52:11.014247: step 91920, loss = 1.99 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:12.243665: step 91930, loss = 2.02 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-05 0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 948 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
0:52:13.445462: step 91940, loss = 2.11 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:52:14.655534: step 91950, loss = 1.82 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:15.881391: step 91960, loss = 2.15 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:52:17.120710: step 91970, loss = 2.03 (1032.8 examples/sec; 0.124 sec/batch)
2017-05-05 00:52:18.327531: step 91980, loss = 1.90 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:19.557411: step 91990, loss = 1.90 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:52:20.758865: step 92000, loss = 2.03 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:52:21.981818: step 92010, loss = 1.92 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:23.202037: step 92020, loss = 1.93 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:24.422941: step 92030, loss = 1.90 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:25.621500: step 92040, loss = 1.77 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:52:26.847393: step 92050, loss = 1.87 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:52:28.061680: step 92060, loss = 1.90 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:29.400932: step 92070, loss = 1.96 (955.8 examples/sec; 0.134 sec/batch)
2017-05-05 00:52:30.466435: step 92080, loss = 1.72 (1201.3 examples/sec; 0.107 sec/batch)
2017-05-05 00:52:31.706332: step 92090, loss = 1.90 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:52:32.908120: step 92100, loss = 1.86 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:52:34.116781: step 92110, loss = 1.89 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:35.354043: step 92120, loss = 1.92 (1034.5 examples/sec; 0.124 sec/batch)
2017-05-05 00:52:36.569530: step 92130, loss = 1.95 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:37.777746: step 92140, loss = 2.01 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:39.011142: step 92150, loss = 1.89 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:52:40.214135: step 92160, loss = 1.88 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:52:41.425164: step 92170, loss = 2.00 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:52:42.629048: step 92180, loss = 1.87 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:52:43.873337: step 92190, loss = 1.83 (1028.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:52:45.074588: step 92200, loss = 1.89 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:52:46.265136: step 92210, loss = 1.87 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:52:47.490035: step 92220, loss = 1.89 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:48.716819: step 92230, loss = 1.93 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 00:52:49.919974: step 92240, loss = 2.03 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:52:51.144424: step 92250, loss = 1.81 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:52.347124: step 92260, loss = 1.81 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:52:53.649389: step 92270, loss = 1.85 (982.9 examples/sec; 0.130 sec/batch)
2017-05-05 00:52:54.762507: step 92280, loss = 1.93 (1149.9 examples/sec; 0.111 sec/batch)
2017-05-05 00:52:55.986934: step 92290, loss = 1.97 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:57.223223: step 92300, loss = 2.02 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:52:58.444446: step 92310, loss = 2.08 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 00:52:59.618660: step 92320, loss = 1.95 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:53:00.875140: step 92330, loss = 1.84 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-05 00:53:02.067436: step 92340, loss = 1.97 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:53:03.302555: step 92350, loss = 1.78 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:53:04.507146: step 92360, loss = 1.91 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:53:05.721007: step 92370, loss = 1.86 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:06.935032: step 92380, loss = 1.98 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:08.137711: step 92390, loss = 2.02 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:53:09.344739: step 92400, loss = 1.97 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:10.579183: step 92410, loss = 2.00 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:53:11.810550: step 92420, loss = 1.96 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 00:53:13.033269: step 92430, loss = 1.89 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:14.222973: step 92440, loss = 2.01 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:53:15.433012: step 92450, loss = 1.98 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:16.638498: step 92460, loss = 1.94 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:17.812345: step 92470, loss = 1.92 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:53:19.039775: step 92480, loss = 2.03 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:53:20.296515: step 92490, loss = 1.86 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-05 00:53:21.489436: step 92500, loss = 1.88 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:53:22.709903: step 92510, loss = 1.80 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:23.931634: step 92520, loss = 1.98 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:25.137083: step 92530, loss = 1.92 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:26.334551: step 92540, loss = 1.91 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:53:27.574069: step 92550, loss = 1.82 (1032.7 examples/sec; 0.124 sec/batch)
2017-05-05 00:53:28.779934: step 92560, loss = 2.03 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:30.011489: step 92570, loss = 1.86 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:53:31.216034: step 92580, loss = 1.81 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:53:32.438279: step 92590, loss = 1.90 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:33.639629: step 92600, loss = 2.04 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:53:34.838264: step 92610, loss = 1.99 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:53:36.050904: step 92620, loss = 1.98 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:37.274257: step 92630, loss = 1.78 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:38.474272: step 92640, loss = 1.92 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:53:39.710489: step 92650, loss = 1.82 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-05 00:53:40.910440: step 92660, loss = 2.05 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:53:42.124790: step 92670, loss = 1.82 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:43.339311: step 92680, loss = 1.92 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:44.570890: step 92690, loss = 2.04 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:53:45.784168: step 92700, loss = 2.02 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:46.997903: step 92710, loss = 1.95 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:48.227813: step 92720, loss = 2.00 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:53:49.452002: step 92730, loss = 1.96 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:50.647769: step 92740, loss = 1.90 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:53:51.880798: step 92750, loss = 1.92 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-05 00:53:53.082669: step 92760, loss = 1.89 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:53:54.324647: step 92770, loss = 1.90 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-05 00:53:55.526494: step 92780, loss = 2.03 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:53:56.742248: step 92790, loss = 1.92 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:53:57.953838: step 92800, loss = 1.83 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:53:59.181193: step 92810, loss = 1.96 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 00:54:00.379816: step 92820, loss = 1.85 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:54:01.604291: step 92830, loss = 1.94 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:54E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 958 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
:02.835449: step 92840, loss = 1.79 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:54:04.057772: step 92850, loss = 1.88 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:05.292623: step 92860, loss = 2.02 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-05 00:54:06.479896: step 92870, loss = 1.91 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:54:07.695703: step 92880, loss = 2.03 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:08.913384: step 92890, loss = 1.92 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:10.137651: step 92900, loss = 1.90 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:11.371150: step 92910, loss = 1.84 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:54:12.577885: step 92920, loss = 1.97 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:54:13.777408: step 92930, loss = 1.95 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:54:14.994487: step 92940, loss = 1.98 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:16.221414: step 92950, loss = 2.20 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-05 00:54:17.436096: step 92960, loss = 1.90 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:54:18.664820: step 92970, loss = 1.77 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:54:19.876867: step 92980, loss = 1.96 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:54:21.109789: step 92990, loss = 1.86 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-05 00:54:22.317405: step 93000, loss = 1.89 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:54:23.541945: step 93010, loss = 1.76 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:24.764126: step 93020, loss = 1.84 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:25.971487: step 93030, loss = 2.04 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:54:27.187575: step 93040, loss = 2.04 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:28.406573: step 93050, loss = 1.97 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:29.731434: step 93060, loss = 1.93 (966.1 examples/sec; 0.132 sec/batch)
2017-05-05 00:54:30.799497: step 93070, loss = 1.95 (1198.4 examples/sec; 0.107 sec/batch)
2017-05-05 00:54:32.011942: step 93080, loss = 1.98 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:54:33.231523: step 93090, loss = 1.93 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:54:34.423544: step 93100, loss = 2.06 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:54:35.620291: step 93110, loss = 1.88 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:54:36.828028: step 93120, loss = 1.93 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:54:38.012666: step 93130, loss = 1.83 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:54:39.218188: step 93140, loss = 1.86 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:54:40.414370: step 93150, loss = 2.22 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:54:41.593263: step 93160, loss = 1.93 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:54:42.764138: step 93170, loss = 2.01 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:54:43.944913: step 93180, loss = 2.05 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:54:45.129297: step 93190, loss = 1.95 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:54:46.295427: step 93200, loss = 1.94 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:54:47.499702: step 93210, loss = 1.92 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:54:48.693342: step 93220, loss = 1.97 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:54:49.861127: step 93230, loss = 2.04 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:54:51.037592: step 93240, loss = 1.90 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:54:52.194475: step 93250, loss = 2.03 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:54:53.365268: step 93260, loss = 2.03 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:54:54.520690: step 93270, loss = 1.90 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:54:55.674727: step 93280, loss = 1.88 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 00:54:56.846703: step 93290, loss = 1.77 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:54:58.028291: step 93300, loss = 1.91 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:54:59.187658: step 93310, loss = 2.00 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:55:00.365597: step 93320, loss = 1.92 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 00:55:01.523148: step 93330, loss = 1.88 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:55:02.703689: step 93340, loss = 1.77 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:55:03.875411: step 93350, loss = 1.90 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:55:05.044541: step 93360, loss = 1.96 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:55:06.197799: step 93370, loss = 1.77 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:55:07.350171: step 93380, loss = 1.86 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-05 00:55:08.538916: step 93390, loss = 1.87 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 00:55:09.689882: step 93400, loss = 1.92 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:55:10.886996: step 93410, loss = 1.89 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:55:12.073940: step 93420, loss = 1.88 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:55:13.255584: step 93430, loss = 1.96 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:55:14.427157: step 93440, loss = 1.96 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:55:15.611278: step 93450, loss = 2.01 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:55:16.808723: step 93460, loss = 2.07 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:55:17.997633: step 93470, loss = 1.89 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:55:19.202248: step 93480, loss = 1.91 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:55:20.385533: step 93490, loss = 1.91 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:55:21.597306: step 93500, loss = 1.93 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:55:22.795508: step 93510, loss = 1.80 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:55:24.010745: step 93520, loss = 1.93 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:55:25.226484: step 93530, loss = 1.92 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 00:55:26.415071: step 93540, loss = 2.04 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:55:27.624459: step 93550, loss = 1.88 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:55:28.847955: step 93560, loss = 1.89 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:55:30.057779: step 93570, loss = 1.91 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:55:31.274925: step 93580, loss = 1.92 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:55:32.487787: step 93590, loss = 1.87 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:55:33.689781: step 93600, loss = 1.86 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:55:34.901682: step 93610, loss = 2.15 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:55:36.109365: step 93620, loss = 2.05 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:55:37.329991: step 93630, loss = 2.11 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 00:55:38.535500: step 93640, loss = 1.91 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:55:39.717753: step 93650, loss = 1.94 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:55:40.921995: step 93660, loss = 1.94 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:55:42.102578: step 93670, loss = 1.86 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:55:43.277622: step 93680, loss = 1.93 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:55:44.465705: step 93690, loss = 1.84 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:55:45.646650: step 93700, loss = 1.83 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:55:46.842760: step 93710, loss = 1.95 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 00:55:48.040662: step 93720, loss = 1.99 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:55:49.210630: step 93730, loss = 2.26 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:55:50E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 969 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
.380857: step 93740, loss = 1.93 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 00:55:51.549566: step 93750, loss = 2.01 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:55:52.708325: step 93760, loss = 1.86 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:55:53.855362: step 93770, loss = 2.06 (1115.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:55:55.014541: step 93780, loss = 2.02 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:55:56.209956: step 93790, loss = 1.99 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:55:57.387476: step 93800, loss = 1.87 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:55:58.555349: step 93810, loss = 2.16 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:55:59.731282: step 93820, loss = 1.94 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:00.905419: step 93830, loss = 1.88 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:02.069755: step 93840, loss = 1.80 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:56:03.232563: step 93850, loss = 1.78 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:56:04.396206: step 93860, loss = 1.91 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:56:05.568479: step 93870, loss = 1.96 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:06.745221: step 93880, loss = 1.86 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:07.924637: step 93890, loss = 2.05 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:09.087766: step 93900, loss = 1.96 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:56:10.263687: step 93910, loss = 1.97 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:11.431271: step 93920, loss = 1.94 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:12.585557: step 93930, loss = 1.89 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:56:13.741466: step 93940, loss = 1.89 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 00:56:14.915581: step 93950, loss = 1.93 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:16.086418: step 93960, loss = 1.86 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:17.250704: step 93970, loss = 1.93 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:56:18.399870: step 93980, loss = 1.85 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:56:19.567318: step 93990, loss = 1.81 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:20.747020: step 94000, loss = 1.83 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:21.917428: step 94010, loss = 1.87 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:23.097278: step 94020, loss = 2.18 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:24.283451: step 94030, loss = 1.82 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:56:25.461350: step 94040, loss = 1.98 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:26.740380: step 94050, loss = 1.88 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-05 00:56:27.828725: step 94060, loss = 2.00 (1176.1 examples/sec; 0.109 sec/batch)
2017-05-05 00:56:29.026093: step 94070, loss = 1.82 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:56:30.216503: step 94080, loss = 1.95 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 00:56:31.428134: step 94090, loss = 1.98 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:56:32.600345: step 94100, loss = 1.76 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:33.768997: step 94110, loss = 1.85 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:34.938895: step 94120, loss = 1.95 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:36.114297: step 94130, loss = 1.80 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:37.284741: step 94140, loss = 2.05 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:38.449908: step 94150, loss = 1.87 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:56:39.631332: step 94160, loss = 1.93 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:40.825487: step 94170, loss = 1.95 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:56:42.020912: step 94180, loss = 1.86 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:56:43.223316: step 94190, loss = 1.81 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:56:44.404744: step 94200, loss = 1.82 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:45.586456: step 94210, loss = 2.03 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:56:46.791287: step 94220, loss = 1.87 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:56:48.002046: step 94230, loss = 1.79 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:56:49.208633: step 94240, loss = 2.19 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:56:50.419439: step 94250, loss = 2.00 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 00:56:51.641443: step 94260, loss = 1.90 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:56:52.866614: step 94270, loss = 1.88 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 00:56:54.076019: step 94280, loss = 1.90 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 00:56:55.317446: step 94290, loss = 1.97 (1031.1 examples/sec; 0.124 sec/batch)
2017-05-05 00:56:56.540772: step 94300, loss = 2.01 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 00:56:57.754529: step 94310, loss = 2.08 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 00:56:58.966856: step 94320, loss = 1.81 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 00:57:00.170284: step 94330, loss = 1.86 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:01.369867: step 94340, loss = 1.80 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:02.571544: step 94350, loss = 1.98 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:03.754368: step 94360, loss = 2.00 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:04.956595: step 94370, loss = 1.82 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:06.154626: step 94380, loss = 2.03 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:07.375077: step 94390, loss = 1.88 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 00:57:08.590060: step 94400, loss = 2.03 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-05 00:57:09.801810: step 94410, loss = 1.98 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 00:57:11.004126: step 94420, loss = 1.85 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:12.199877: step 94430, loss = 1.89 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:13.405202: step 94440, loss = 1.98 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:57:14.584993: step 94450, loss = 1.89 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:15.771345: step 94460, loss = 1.94 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:16.948920: step 94470, loss = 1.91 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:18.109301: step 94480, loss = 1.82 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:57:19.310495: step 94490, loss = 2.07 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:20.503698: step 94500, loss = 1.91 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:21.704582: step 94510, loss = 1.95 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:22.896500: step 94520, loss = 2.08 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:24.094968: step 94530, loss = 2.04 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:25.275560: step 94540, loss = 2.06 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:26.477055: step 94550, loss = 1.78 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:27.674433: step 94560, loss = 1.85 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:28.867128: step 94570, loss = 1.97 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:30.052801: step 94580, loss = 1.91 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:31.242219: step 94590, loss = 1.92 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:57:32.424921: step 94600, loss = 1.91 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:33.623790: step 94610, loss = 2.01 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:34.839123: step 94620, loss = 1.92 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 00:57:36.041802: step 94630, loss = 1.94 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:37.245972: step 94640, loss = 1.95 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:38.406350: step 94650, loss = 2.02 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:57:39.586197: step 94660, loss = 2.15 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:40.787598: step 94670, loss = 1.85 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:57:41.969277: step 94680, loss = 1.88 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:43.152328: step 94690, loss = 1.89 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:44.317626: step 94700, loss = 1.89 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:57:45.493790: step 94710, loss = 1.85 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:57:46.640425: step 94720, loss = 1.91 (1116.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:57:47.811528: step 94730, loss = 2.02 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:57:48.977161: step 94740, loss = 1.76 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:57:50.134566: step 94750, loss = 1.95 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 00:57:51.294658: step 94760, loss = 2.04 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:57:52.462150: step 94770, loss = 1.94 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:57:53.623428: step 94780, loss = 1.83 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:57:54.775419: step 94790, loss = 2.06 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:57:55.935445: step 94800, loss = 1.90 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:57:57.088604: step 94810, loss = 1.88 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-05 00:57:58.228890: step 94820, loss = 1.83 (1122.5 examples/sec; 0.114 sec/batch)
2017-05-05 00:57:59.405302: step 94830, loss = 1.96 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:00.573512: step 94840, loss = 1.81 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:01.721163: step 94850, loss = 1.89 (1115.3 examples/sec; 0.115 sec/batch)
2017-05-05 00:58:02.920755: step 94860, loss = 1.87 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:58:04.096815: step 94870, loss = 2.01 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:05.258750: step 94880, loss = 2.01 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:06.415334: step 94890, loss = 1.79 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:07.580312: step 94900, loss = 1.90 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:08.753069: step 94910, loss = 1.83 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:09.886522: step 94920, loss = 1.82 (1129.3 examples/sec; 0.113 sec/batch)
2017-05-05 00:58:11.059130: step 94930, loss = 2.05 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:12.226281: step 94940, loss = 2.00 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:13.387360: step 94950, loss = 1.88 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:14.541886: step 94960, loss = 1.92 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 00:58:15.715438: step 94970, loss = 1.95 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:16.886948: step 94980, loss = 2.03 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:18.058711: step 94990, loss = 1.96 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:19.223151: step 95000, loss = 1.88 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:20.412496: step 95010, loss = 2.06 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 00:58:21.570813: step 95020, loss = 1.94 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:22.743462: step 95030, loss = 1.85 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:24.009486: step 95040, loss = 1.95 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-05 00:58:25.074326: step 95050, loss = 1.97 (1202.1 examples/sec; 0.106 sec/batch)
2017-05-05 00:58:26.228976: step 95060, loss = 1.98 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 00:58:27.407475: step 95070, loss = 1.77 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:28.574622: step 95080, loss = 2.06 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:29.73E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 980 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
1352: step 95090, loss = 2.03 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:30.893732: step 95100, loss = 1.89 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:32.073052: step 95110, loss = 1.86 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:33.250442: step 95120, loss = 1.94 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:34.412487: step 95130, loss = 2.06 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:35.663826: step 95140, loss = 1.81 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-05 00:58:36.795316: step 95150, loss = 2.03 (1131.3 examples/sec; 0.113 sec/batch)
2017-05-05 00:58:37.939995: step 95160, loss = 2.08 (1118.2 examples/sec; 0.114 sec/batch)
2017-05-05 00:58:39.114628: step 95170, loss = 1.98 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:40.304536: step 95180, loss = 1.93 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:58:41.465211: step 95190, loss = 2.03 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:42.632341: step 95200, loss = 1.84 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:43.782491: step 95210, loss = 1.80 (1112.9 examples/sec; 0.115 sec/batch)
2017-05-05 00:58:44.947607: step 95220, loss = 1.79 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:46.107360: step 95230, loss = 1.83 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 00:58:47.291014: step 95240, loss = 1.94 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:48.438935: step 95250, loss = 1.94 (1115.1 examples/sec; 0.115 sec/batch)
2017-05-05 00:58:49.625730: step 95260, loss = 2.05 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:58:50.822268: step 95270, loss = 1.99 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 00:58:51.995292: step 95280, loss = 1.84 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:58:53.191949: step 95290, loss = 1.95 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 00:58:54.369532: step 95300, loss = 2.08 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:55.573183: step 95310, loss = 1.87 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:58:56.792778: step 95320, loss = 1.86 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 00:58:57.968410: step 95330, loss = 2.05 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 00:58:59.159659: step 95340, loss = 1.86 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:59:00.338137: step 95350, loss = 1.84 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:59:01.538951: step 95360, loss = 1.80 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 00:59:02.717373: step 95370, loss = 1.87 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 00:59:03.937168: step 95380, loss = 2.09 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:59:05.159196: step 95390, loss = 2.12 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-05 00:59:06.365735: step 95400, loss = 1.98 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 00:59:07.600406: step 95410, loss = 1.90 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-05 00:59:08.799621: step 95420, loss = 1.88 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 00:59:09.998158: step 95430, loss = 1.87 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 00:59:11.186644: step 95440, loss = 2.05 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 00:59:12.354323: step 95450, loss = 2.12 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:13.530840: step 95460, loss = 1.78 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:59:14.697524: step 95470, loss = 1.98 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:15.855341: step 95480, loss = 1.97 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:17.032403: step 95490, loss = 1.90 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 00:59:18.203802: step 95500, loss = 1.93 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:19.372724: step 95510, loss = 1.96 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:20.531549: step 95520, loss = 1.93 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:21.688849: step 95530, loss = 2.03 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:22.851272: step 95540, loss = 2.09 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:24.006756: step 95550, loss = 1.98 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:25.173294: step 95560, loss = 1.99 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:26.337515: step 95570, loss = 1.82 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:27.514312: step 95580, loss = 1.86 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:59:28.673087: step 95590, loss = 1.95 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:29.817314: step 95600, loss = 1.84 (1118.7 examples/sec; 0.114 sec/batch)
2017-05-05 00:59:30.991343: step 95610, loss = 1.88 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:32.156908: step 95620, loss = 2.04 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:33.335924: step 95630, loss = 1.96 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 00:59:34.494261: step 95640, loss = 1.98 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:35.657809: step 95650, loss = 1.90 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:36.827260: step 95660, loss = 1.92 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:37.984075: step 95670, loss = 1.89 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:39.174704: step 95680, loss = 1.92 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 00:59:40.358422: step 95690, loss = 1.95 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 00:59:41.522336: step 95700, loss = 1.97 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 00:59:42.710284: step 95710, loss = 1.97 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 00:59:43.876919: step 95720, loss = 1.94 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:45.054356: step 95730, loss = 1.98 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 00:59:46.220146: step 95740, loss = 1.85 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:47.391607: step 95750, loss = 1.84 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:48.574549: step 95760, loss = 1.94 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 00:59:49.764811: step 95770, loss = 1.94 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 00:59:50.938478: step 95780, loss = 1.96 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 00:59:52.128388: step 95790, loss = 1.97 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 00:59:53.330028: step 95800, loss = 1.96 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 00:59:54.535635: step 95810, loss = 2.07 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 00:59:55.742056: step 95820, loss = 1.86 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 00:59:56.949328: step 95830, loss = 1.90 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 00:59:58.148400: step 95840, loss = 1.84 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 00:59:59.349028: step 95850, loss = 1.99 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:00:00.548073: step 95860, loss = 1.95 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:00:01.744523: step 95870, loss = 1.93 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:00:02.939567: step 95880, loss = 1.95 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:00:04.128759: step 95890, loss = 1.95 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:00:05.321738: step 95900, loss = 1.92 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:00:06.471360: step 95910, loss = 1.88 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:00:07.644657: step 95920, loss = 1.98 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:08.795183: step 95930, loss = 1.92 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:00:09.953574: step 95940, loss = 1.86 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:00:11.133388: step 95950, loss = 1.87 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:00:12.307504: step 95960, loss = 1.80 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:13.452949: step 95970, loss = 2.02 (1117.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:00:14.611968: step 95980, loss = 1.98 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:00:15.7884E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 991 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
11: step 95990, loss = 1.97 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:00:16.964845: step 96000, loss = 1.82 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:00:18.110530: step 96010, loss = 1.98 (1117.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:00:19.282686: step 96020, loss = 1.85 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:20.538273: step 96030, loss = 1.99 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-05 01:00:21.609928: step 96040, loss = 1.95 (1194.4 examples/sec; 0.107 sec/batch)
2017-05-05 01:00:22.795581: step 96050, loss = 2.06 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:00:23.960857: step 96060, loss = 1.96 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:25.128962: step 96070, loss = 1.97 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:26.283088: step 96080, loss = 1.96 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:00:27.430106: step 96090, loss = 1.85 (1115.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:00:28.604445: step 96100, loss = 1.82 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:29.753276: step 96110, loss = 1.87 (1114.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:00:30.925304: step 96120, loss = 1.92 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:32.087201: step 96130, loss = 1.74 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:00:33.250498: step 96140, loss = 2.11 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:00:34.424766: step 96150, loss = 1.77 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:35.593313: step 96160, loss = 1.88 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:36.774461: step 96170, loss = 2.02 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:00:37.921132: step 96180, loss = 1.79 (1116.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:00:39.096072: step 96190, loss = 1.89 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:40.281965: step 96200, loss = 1.90 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:00:41.448949: step 96210, loss = 1.91 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:42.612630: step 96220, loss = 1.96 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:00:43.791666: step 96230, loss = 1.78 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:00:44.946042: step 96240, loss = 1.94 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:00:46.105694: step 96250, loss = 2.05 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:00:47.277789: step 96260, loss = 1.91 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:48.447001: step 96270, loss = 1.99 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:49.631584: step 96280, loss = 1.87 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:00:50.823590: step 96290, loss = 2.06 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:00:51.994043: step 96300, loss = 1.87 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:53.171320: step 96310, loss = 1.93 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:00:54.342457: step 96320, loss = 2.00 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:00:55.541902: step 96330, loss = 1.75 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:00:56.740926: step 96340, loss = 1.76 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:00:57.934141: step 96350, loss = 1.98 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:00:59.140589: step 96360, loss = 1.81 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:01:00.341562: step 96370, loss = 1.92 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:01:01.565624: step 96380, loss = 1.96 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:01:02.765641: step 96390, loss = 2.12 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:01:03.977625: step 96400, loss = 1.93 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:01:05.179432: step 96410, loss = 1.81 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:01:06.373195: step 96420, loss = 1.86 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:01:07.569484: step 96430, loss = 1.92 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:01:08.763718: step 96440, loss = 1.98 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:01:09.958948: step 96450, loss = 1.77 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:01:11.143757: step 96460, loss = 1.95 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:12.331956: step 96470, loss = 2.06 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:01:13.508958: step 96480, loss = 1.96 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:14.697152: step 96490, loss = 1.97 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:01:15.872590: step 96500, loss = 1.76 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:17.033025: step 96510, loss = 1.87 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:18.209977: step 96520, loss = 1.87 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:19.383756: step 96530, loss = 1.77 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:01:20.541985: step 96540, loss = 2.09 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:21.712763: step 96550, loss = 2.01 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:01:22.884186: step 96560, loss = 2.03 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:01:24.040523: step 96570, loss = 1.78 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:25.218377: step 96580, loss = 1.89 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:26.377905: step 96590, loss = 1.96 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:27.543748: step 96600, loss = 1.93 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:01:28.703339: step 96610, loss = 1.92 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:29.854727: step 96620, loss = 1.85 (1111.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:01:31.029295: step 96630, loss = 1.89 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:01:32.215571: step 96640, loss = 1.98 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:01:33.387544: step 96650, loss = 2.13 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:01:34.524651: step 96660, loss = 1.90 (1125.7 examples/sec; 0.114 sec/batch)
2017-05-05 01:01:35.718861: step 96670, loss = 1.98 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:01:36.866288: step 96680, loss = 1.88 (1115.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:01:38.029478: step 96690, loss = 1.75 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:39.223163: step 96700, loss = 2.03 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:01:40.390702: step 96710, loss = 1.91 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:01:41.535040: step 96720, loss = 1.91 (1118.5 examples/sec; 0.114 sec/batch)
2017-05-05 01:01:42.703569: step 96730, loss = 1.83 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:01:43.864349: step 96740, loss = 1.95 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:45.014669: step 96750, loss = 1.92 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:01:46.189642: step 96760, loss = 1.83 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:01:47.376731: step 96770, loss = 1.90 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:01:48.531415: step 96780, loss = 2.07 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:01:49.664064: step 96790, loss = 1.89 (1130.1 examples/sec; 0.113 sec/batch)
2017-05-05 01:01:50.826770: step 96800, loss = 1.85 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:01:52.010590: step 96810, loss = 1.98 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:53.205044: step 96820, loss = 1.88 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:01:54.386534: step 96830, loss = 1.94 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:55.567679: step 96840, loss = 1.94 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:56.756979: step 96850, loss = 1.81 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:01:57.938500: step 96860, loss = 1.94 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:01:59.123481: step 96870, loss = 1.94 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:00.300288: step 96880, loss = 1.99 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:01.459886E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1002 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
: step 96890, loss = 1.94 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:02:02.628793: step 96900, loss = 1.91 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:02:03.804885: step 96910, loss = 1.83 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:04.985522: step 96920, loss = 2.02 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:06.144801: step 96930, loss = 2.00 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:02:07.315747: step 96940, loss = 1.98 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:02:08.504636: step 96950, loss = 1.83 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:09.664649: step 96960, loss = 1.77 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:02:10.833507: step 96970, loss = 1.98 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:02:12.001654: step 96980, loss = 1.78 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:02:13.196591: step 96990, loss = 1.83 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:14.356818: step 97000, loss = 1.90 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:02:15.561146: step 97010, loss = 1.85 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:02:16.868362: step 97020, loss = 2.06 (979.2 examples/sec; 0.131 sec/batch)
2017-05-05 01:02:17.945483: step 97030, loss = 1.83 (1188.4 examples/sec; 0.108 sec/batch)
2017-05-05 01:02:19.131107: step 97040, loss = 1.97 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:20.343186: step 97050, loss = 1.95 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:02:21.520414: step 97060, loss = 2.01 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:22.718971: step 97070, loss = 1.96 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:02:23.902886: step 97080, loss = 2.00 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:25.117746: step 97090, loss = 1.93 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:02:26.317370: step 97100, loss = 2.12 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:02:27.549604: step 97110, loss = 1.92 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:02:28.759322: step 97120, loss = 1.83 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:02:29.949436: step 97130, loss = 2.05 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:31.147732: step 97140, loss = 1.96 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:02:32.367035: step 97150, loss = 1.91 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:02:33.557465: step 97160, loss = 2.01 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:34.744703: step 97170, loss = 1.97 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:35.937539: step 97180, loss = 2.00 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:37.126467: step 97190, loss = 2.05 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:38.298226: step 97200, loss = 2.04 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:02:39.506972: step 97210, loss = 1.88 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:02:40.780690: step 97220, loss = 2.02 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-05 01:02:41.910431: step 97230, loss = 1.85 (1133.0 examples/sec; 0.113 sec/batch)
2017-05-05 01:02:43.088669: step 97240, loss = 1.82 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:44.291157: step 97250, loss = 1.90 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:02:45.504285: step 97260, loss = 1.99 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:02:46.727212: step 97270, loss = 1.88 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:02:47.909682: step 97280, loss = 1.78 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:49.087996: step 97290, loss = 1.82 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:50.252528: step 97300, loss = 1.97 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:02:51.443576: step 97310, loss = 1.94 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:52.638527: step 97320, loss = 1.78 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:53.814443: step 97330, loss = 1.89 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:54.990079: step 97340, loss = 1.88 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:02:56.180541: step 97350, loss = 1.85 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:02:57.378926: step 97360, loss = 1.85 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:02:58.580068: step 97370, loss = 2.02 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:02:59.764994: step 97380, loss = 2.02 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:03:00.979528: step 97390, loss = 1.95 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:03:02.148868: step 97400, loss = 2.05 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:03:03.343051: step 97410, loss = 1.78 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:04.514947: step 97420, loss = 2.09 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:03:05.700764: step 97430, loss = 2.01 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:06.891202: step 97440, loss = 2.11 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:08.072139: step 97450, loss = 1.96 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:03:09.276132: step 97460, loss = 2.05 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:10.450316: step 97470, loss = 1.76 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:03:11.639285: step 97480, loss = 2.07 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:12.818053: step 97490, loss = 1.96 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:03:13.991484: step 97500, loss = 1.97 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:03:15.177880: step 97510, loss = 1.97 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:16.353444: step 97520, loss = 2.04 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:03:17.527857: step 97530, loss = 2.09 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:03:18.704071: step 97540, loss = 1.98 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:03:19.903788: step 97550, loss = 1.94 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:21.090761: step 97560, loss = 1.92 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:22.254381: step 97570, loss = 1.95 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:03:23.434010: step 97580, loss = 1.87 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:03:24.624192: step 97590, loss = 1.85 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:25.818861: step 97600, loss = 2.06 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:27.034967: step 97610, loss = 1.90 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:03:28.224900: step 97620, loss = 1.81 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:29.446595: step 97630, loss = 2.00 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:03:30.645152: step 97640, loss = 2.07 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:31.870658: step 97650, loss = 1.99 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:03:33.080785: step 97660, loss = 2.08 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:03:34.294737: step 97670, loss = 1.92 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:03:35.510271: step 97680, loss = 1.91 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:03:36.706060: step 97690, loss = 1.90 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:37.928417: step 97700, loss = 1.97 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:03:39.123437: step 97710, loss = 1.89 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:40.343656: step 97720, loss = 2.01 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:03:41.546950: step 97730, loss = 1.91 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:42.746065: step 97740, loss = 1.86 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:43.952928: step 97750, loss = 2.00 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:03:45.141991: step 97760, loss = 1.83 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:46.313165: step 97770, loss = 2.01 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:03:47.513295: step 97780, loss = 1.83 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:48.698678: step 97790, loss = 2.00 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:49.889495: step 97800, loss = 1.97 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:51.091563: step 97810, loss = 1.92 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:52.296304: step 97820, loss = 1.87 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:03:53.490057: step 97830, loss = 1.82 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:54.681392: step 97840, loss = 1.98 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:55.896191: step 97850, loss = 1.92 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:03:57.090847: step 97860, loss = 1.95 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:03:58.274448: step 97870, loss = 1.87 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:03:59.453425: step 97880, loss = 1.94 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:04:00.633349: step 97890, loss = 1.79 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:04:01.795209: step 97900, loss = 1.95 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:04:02.979028: step 97910, loss = 1.85 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:04:04.166627: step 97920, loss = 1.86 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:05.338440: step 97930, loss = 1.90 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:04:06.514475: step 97940, loss = 1.97 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:04:07.687409: step 97950, loss = 1.80 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:04:08.863176: step 97960, loss = 2.01 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:04:10.019984: step 97970, loss = 1.89 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:04:11.200014: step 97980, loss = 1.89 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:04:12.392688: step 97990, loss = 1.91 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:13.550649: step 98000, loss = 1.95 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:04:14.796831: step 98010, loss = 2.09 (1027.1 examples/sec; 0.125 sec/batch)
2017-05-05 01:04:15.872722: step 98020, loss = 2.01 (1189.7 examples/sec; 0.108 sec/batch)
2017-05-05 01:04:17.038382: step 98030, loss = 2.03 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:04:18.189879: step 98040, loss = 1.79 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:04:19.342806: step 98050, loss = 1.88 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:04:20.510615: step 98060, loss = 1.90 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:04:21.677090: step 98070, loss = 1.94 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:04:22.854036: step 98080, loss = 1.93 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:04:24.051025: step 98090, loss = 2.16 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:04:25.265594: step 98100, loss = 2.01 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:04:26.451067: step 98110, loss = 1.90 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:27.674337: step 98120, loss = 1.89 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:04:28.899770: step 98130, loss = 2.08 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:04:30.084183: step 98140, loss = 1.79 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:04:31.296860: step 98150, loss = 1.98 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:04:32.484410: step 98160, loss = 1.84 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:33.695905: step 98170, loss = 1.93 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:04:34.889341: step 98180, loss = 2.02 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:36.086552: step 98190, loss = 2.08 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:04:37.274827: step 98200, loss = 1.87 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:38.519269: step 98210, loss = 1.89 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-05 01:04:39.659100: step 98220, loss = 2.05 (1123.0 examples/sec; 0.114 sec/batch)
2017-05-05 01:04:40.861308: step 98230, loss = 1.97 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:04:42.031916: stE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1013 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
ep 98240, loss = 1.92 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:04:43.196882: step 98250, loss = 1.86 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:04:44.370563: step 98260, loss = 2.13 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:04:45.563147: step 98270, loss = 1.87 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:46.751021: step 98280, loss = 1.93 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:47.946094: step 98290, loss = 1.96 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:04:49.148007: step 98300, loss = 1.88 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:04:50.340379: step 98310, loss = 1.76 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:51.543210: step 98320, loss = 1.96 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:04:52.776156: step 98330, loss = 1.75 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:04:53.966173: step 98340, loss = 1.95 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:04:55.191711: step 98350, loss = 1.85 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:04:56.408633: step 98360, loss = 1.91 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:04:57.635343: step 98370, loss = 1.90 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:04:58.871947: step 98380, loss = 1.94 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-05 01:05:00.089360: step 98390, loss = 1.88 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:01.310497: step 98400, loss = 1.93 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:02.496947: step 98410, loss = 1.89 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:05:03.705430: step 98420, loss = 2.00 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:04.942074: step 98430, loss = 1.96 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-05 01:05:06.144669: step 98440, loss = 2.00 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:05:07.380746: step 98450, loss = 1.93 (1035.5 examples/sec; 0.124 sec/batch)
2017-05-05 01:05:08.597096: step 98460, loss = 2.08 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:09.810602: step 98470, loss = 1.86 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:11.033810: step 98480, loss = 1.87 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:12.270382: step 98490, loss = 1.94 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-05 01:05:13.458767: step 98500, loss = 1.87 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:05:14.666126: step 98510, loss = 1.92 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:15.892040: step 98520, loss = 1.85 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:05:17.112394: step 98530, loss = 2.07 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:18.321279: step 98540, loss = 1.94 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:19.539557: step 98550, loss = 1.81 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:20.749539: step 98560, loss = 2.08 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:21.956055: step 98570, loss = 1.89 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:23.172107: step 98580, loss = 1.82 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:24.367587: step 98590, loss = 1.89 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:05:25.570819: step 98600, loss = 2.17 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:05:26.775588: step 98610, loss = 1.88 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:05:27.975382: step 98620, loss = 1.93 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:05:29.207322: step 98630, loss = 2.06 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:05:30.416377: step 98640, loss = 1.84 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:31.643564: step 98650, loss = 2.12 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:05:32.862284: step 98660, loss = 1.96 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:34.100182: step 98670, loss = 2.07 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-05 01:05:35.324901: step 98680, loss = 1.86 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:36.546078: step 98690, loss = 2.00 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:37.731126: step 98700, loss = 1.84 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:05:38.963432: step 98710, loss = 1.88 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:05:40.155673: step 98720, loss = 1.89 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:05:41.386314: step 98730, loss = 1.97 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:05:42.581442: step 98740, loss = 1.88 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:05:43.812062: step 98750, loss = 1.95 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:05:45.021835: step 98760, loss = 2.07 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:46.238856: step 98770, loss = 2.00 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:47.446989: step 98780, loss = 1.90 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:48.706437: step 98790, loss = 1.89 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-05 01:05:49.912991: step 98800, loss = 1.81 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:51.087148: step 98810, loss = 1.80 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:05:52.299515: step 98820, loss = 1.89 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:53.508794: step 98830, loss = 2.08 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:54.739928: step 98840, loss = 1.80 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:05:55.962889: step 98850, loss = 1.96 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:05:57.170565: step 98860, loss = 1.97 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:05:58.368926: step 98870, loss = 2.08 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:05:59.583305: step 98880, loss = 1.93 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:06:00.800354: step 98890, loss = 1.98 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:06:02.000016: step 98900, loss = 2.13 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:03.226563: step 98910, loss = 1.86 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:06:04.444346: step 98920, loss = 1.88 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:06:05.648836: step 98930, loss = 1.93 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:06.882145: step 98940, loss = 1.86 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:06:08.091099: step 98950, loss = 1.82 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:06:09.281978: step 98960, loss = 1.86 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:06:10.477978: step 98970, loss = 1.93 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:11.677830: step 98980, loss = 1.91 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:12.853201: step 98990, loss = 1.78 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:06:14.144334: step 99000, loss = 2.00 (991.4 examples/sec; 0.129 sec/batch)
2017-05-05 01:06:15.232995: step 99010, loss = 1.79 (1175.8 examples/sec; 0.109 sec/batch)
2017-05-05 01:06:16.428167: step 99020, loss = 1.94 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:17.620373: step 99030, loss = 1.80 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:06:18.824652: step 99040, loss = 1.94 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:20.036593: step 99050, loss = 1.86 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:06:21.227763: step 99060, loss = 1.93 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:06:22.429670: step 99070, loss = 2.03 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:23.631425: step 99080, loss = 1.85 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:24.825935: step 99090, loss = 1.84 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:06:26.006992: step 99100, loss = 1.91 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:06:27.218784: step 99110, loss = 2.03 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:06:28.437057: step 99120, loss = 2.00 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:06:29.647646: step 99130, loss = 1.88 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:06:30.851724: step E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1024 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
99140, loss = 1.96 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:32.054157: step 99150, loss = 2.05 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:33.279220: step 99160, loss = 1.99 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:06:34.468916: step 99170, loss = 2.02 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:06:35.653531: step 99180, loss = 2.04 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:06:36.849793: step 99190, loss = 2.13 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:38.093882: step 99200, loss = 1.82 (1028.9 examples/sec; 0.124 sec/batch)
2017-05-05 01:06:39.232644: step 99210, loss = 1.82 (1124.0 examples/sec; 0.114 sec/batch)
2017-05-05 01:06:40.435442: step 99220, loss = 1.96 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:41.641563: step 99230, loss = 1.85 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:06:42.825994: step 99240, loss = 1.83 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:06:44.009885: step 99250, loss = 1.94 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:06:45.201683: step 99260, loss = 1.95 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:06:46.371912: step 99270, loss = 1.82 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:06:47.569712: step 99280, loss = 1.89 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:06:48.747905: step 99290, loss = 1.87 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:06:49.893457: step 99300, loss = 2.04 (1117.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:06:51.063978: step 99310, loss = 2.00 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:06:52.229738: step 99320, loss = 1.90 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:06:53.400743: step 99330, loss = 1.90 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:06:54.570560: step 99340, loss = 2.05 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:06:55.749543: step 99350, loss = 1.99 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:06:56.931372: step 99360, loss = 1.88 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:06:58.083212: step 99370, loss = 2.01 (1111.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:06:59.261762: step 99380, loss = 1.94 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:07:00.431552: step 99390, loss = 2.03 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:07:01.579451: step 99400, loss = 1.89 (1115.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:07:02.758926: step 99410, loss = 1.89 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:07:03.950501: step 99420, loss = 2.01 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:07:05.100068: step 99430, loss = 1.83 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:07:06.272037: step 99440, loss = 2.01 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:07:07.418011: step 99450, loss = 1.93 (1117.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:07:08.593267: step 99460, loss = 1.86 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:07:09.760044: step 99470, loss = 1.99 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:07:10.951912: step 99480, loss = 2.03 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:07:12.122942: step 99490, loss = 1.99 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:07:13.287506: step 99500, loss = 1.91 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:07:14.461079: step 99510, loss = 1.96 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:07:15.628907: step 99520, loss = 1.81 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:07:16.796843: step 99530, loss = 1.88 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:07:17.977270: step 99540, loss = 1.85 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:07:19.158498: step 99550, loss = 1.78 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:07:20.344737: step 99560, loss = 1.99 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:07:21.512187: step 99570, loss = 1.88 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:07:22.686921: step 99580, loss = 2.05 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:07:23.866114: step 99590, loss = 1.79 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:07:25.047841: step 99600, loss = 1.94 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:07:26.266553: step 99610, loss = 2.02 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:27.492367: step 99620, loss = 2.06 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:07:28.714006: step 99630, loss = 1.87 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:29.905396: step 99640, loss = 1.93 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:07:31.121402: step 99650, loss = 1.75 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:32.340476: step 99660, loss = 1.79 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:33.557766: step 99670, loss = 1.84 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:34.758693: step 99680, loss = 1.88 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:07:35.976595: step 99690, loss = 2.14 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:37.181138: step 99700, loss = 2.02 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:07:38.418752: step 99710, loss = 1.98 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-05 01:07:39.622464: step 99720, loss = 1.95 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:07:40.860986: step 99730, loss = 1.90 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-05 01:07:42.077686: step 99740, loss = 1.85 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:43.328090: step 99750, loss = 1.83 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-05 01:07:44.550866: step 99760, loss = 1.93 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:45.756015: step 99770, loss = 1.85 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:07:46.979810: step 99780, loss = 2.07 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:48.188879: step 99790, loss = 1.88 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:07:49.377376: step 99800, loss = 1.85 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:07:50.594557: step 99810, loss = 1.85 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:51.800013: step 99820, loss = 1.91 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:07:53.032790: step 99830, loss = 1.80 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-05 01:07:54.211174: step 99840, loss = 1.82 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:07:55.433953: step 99850, loss = 1.84 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:56.650173: step 99860, loss = 1.86 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:07:57.859439: step 99870, loss = 2.03 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:07:59.096557: step 99880, loss = 1.92 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-05 01:08:00.294912: step 99890, loss = 1.85 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:08:01.504084: step 99900, loss = 1.74 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:02.722920: step 99910, loss = 2.01 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:03.948892: step 99920, loss = 1.90 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:08:05.181377: step 99930, loss = 1.86 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:08:06.343727: step 99940, loss = 2.11 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:08:07.589607: step 99950, loss = 2.04 (1027.4 examples/sec; 0.125 sec/batch)
2017-05-05 01:08:08.789652: step 99960, loss = 1.81 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:08:09.996899: step 99970, loss = 1.95 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:11.192643: step 99980, loss = 1.84 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:08:12.536890: step 99990, loss = 2.03 (952.2 examples/sec; 0.134 sec/batch)
2017-05-05 01:08:13.604432: step 100000, loss = 1.81 (1199.0 examples/sec; 0.107 sec/batch)
2017-05-05 01:08:14.820982: step 100010, loss = 1.82 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:16.031816: step 100020, loss = 1.78 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:17.262757: step 100030, loss = 1.86 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:08:18.466877: stepE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1035 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
 100040, loss = 1.92 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:08:19.689422: step 100050, loss = 1.75 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:20.906946: step 100060, loss = 1.93 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:22.102985: step 100070, loss = 1.88 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:08:23.359050: step 100080, loss = 1.83 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-05 01:08:24.560097: step 100090, loss = 2.02 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:08:25.799372: step 100100, loss = 1.87 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-05 01:08:26.982879: step 100110, loss = 1.85 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:08:28.189373: step 100120, loss = 1.94 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:29.412466: step 100130, loss = 1.85 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:30.616815: step 100140, loss = 2.03 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:08:31.835549: step 100150, loss = 2.00 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:33.025444: step 100160, loss = 1.79 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:08:34.274215: step 100170, loss = 1.97 (1025.0 examples/sec; 0.125 sec/batch)
2017-05-05 01:08:35.476428: step 100180, loss = 1.82 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:08:36.760096: step 100190, loss = 1.95 (997.1 examples/sec; 0.128 sec/batch)
2017-05-05 01:08:37.873629: step 100200, loss = 2.05 (1149.5 examples/sec; 0.111 sec/batch)
2017-05-05 01:08:39.111122: step 100210, loss = 1.90 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-05 01:08:40.316525: step 100220, loss = 1.96 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:41.547546: step 100230, loss = 1.85 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:08:42.762754: step 100240, loss = 1.95 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:43.968913: step 100250, loss = 1.80 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:45.199512: step 100260, loss = 1.91 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:08:46.400941: step 100270, loss = 1.96 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:08:47.607086: step 100280, loss = 1.93 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:48.823159: step 100290, loss = 1.86 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:50.020345: step 100300, loss = 1.89 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:08:51.231572: step 100310, loss = 1.96 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:52.451847: step 100320, loss = 1.87 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:08:53.656082: step 100330, loss = 1.81 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:08:54.856716: step 100340, loss = 1.80 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:08:56.071206: step 100350, loss = 1.83 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:57.299451: step 100360, loss = 2.00 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:08:58.512700: step 100370, loss = 1.88 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:08:59.731506: step 100380, loss = 1.90 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:00.953041: step 100390, loss = 1.96 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:02.144960: step 100400, loss = 1.87 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:09:03.359523: step 100410, loss = 1.68 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:04.555578: step 100420, loss = 2.06 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:05.778383: step 100430, loss = 1.85 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:07.023909: step 100440, loss = 1.87 (1027.7 examples/sec; 0.125 sec/batch)
2017-05-05 01:09:08.239994: step 100450, loss = 1.83 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:09.462285: step 100460, loss = 1.98 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:10.684159: step 100470, loss = 2.04 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:11.901411: step 100480, loss = 1.80 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:13.123835: step 100490, loss = 2.12 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:14.329816: step 100500, loss = 1.98 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:15.557951: step 100510, loss = 1.96 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:09:16.762095: step 100520, loss = 1.90 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:17.953982: step 100530, loss = 1.86 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:09:19.197259: step 100540, loss = 1.76 (1029.6 examples/sec; 0.124 sec/batch)
2017-05-05 01:09:20.406420: step 100550, loss = 1.93 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:21.622145: step 100560, loss = 1.89 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:22.857901: step 100570, loss = 1.87 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-05 01:09:24.045196: step 100580, loss = 1.84 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:09:25.255869: step 100590, loss = 1.90 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:26.455407: step 100600, loss = 1.99 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:27.678824: step 100610, loss = 2.01 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:28.887608: step 100620, loss = 1.92 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:30.101862: step 100630, loss = 1.86 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:31.320262: step 100640, loss = 1.89 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:32.526106: step 100650, loss = 1.82 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:33.735536: step 100660, loss = 1.99 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:34.934937: step 100670, loss = 1.97 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:36.135122: step 100680, loss = 1.96 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:37.345391: step 100690, loss = 1.85 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:38.553585: step 100700, loss = 1.88 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:39.740263: step 100710, loss = 1.99 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:09:40.937712: step 100720, loss = 1.87 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:42.143637: step 100730, loss = 2.11 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:43.351773: step 100740, loss = 2.00 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:44.563388: step 100750, loss = 1.89 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:45.757620: step 100760, loss = 1.81 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:09:46.974524: step 100770, loss = 1.80 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:48.186625: step 100780, loss = 1.91 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:49.404759: step 100790, loss = 1.95 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:09:50.610470: step 100800, loss = 1.92 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:51.858094: step 100810, loss = 2.05 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-05 01:09:53.062874: step 100820, loss = 2.01 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:54.266757: step 100830, loss = 1.97 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:09:55.494414: step 100840, loss = 2.07 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:09:56.705031: step 100850, loss = 1.86 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:09:57.932327: step 100860, loss = 2.01 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:09:59.176939: step 100870, loss = 1.84 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-05 01:10:00.373528: step 100880, loss = 1.78 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:01.593155: step 100890, loss = 1.81 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:02.809119: step 100900, loss = 1.95 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:03.999508: step 100910, loss = 1.88 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:10:05.222757: step 100920, loss = 1.91 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:06.416686: step 100930, loss = 1.90 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:10:07.619011: step 100940, loss = 1.99 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:08.822377: step 100950, loss = 1.95 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:10.033785: step 100960, loss = 2.03 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:11.247427: step 100970, loss = 1.97 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:12.564435: step 100980, loss = 1.98 (971.9 examples/sec; 0.132 sec/batch)
2017-05-05 01:10:13.655667: step 100990, loss = 2.03 (1173.0 examples/sec; 0.109 sec/batch)
2017-05-05 01:10:14.868871: step 101000, loss = 1.98 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:16.093889: step 101010, loss = 1.97 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:10:17.311576: step 101020, loss = 2.02 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:18.507827: step 101030, loss = 1.85 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:19.705039: step 101040, loss = 1.98 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:20.899910: step 101050, loss = 1.88 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:10:22.076977: step 101060, loss = 2.01 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:10:23.277475: step 101070, loss = 1.89 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:24.484301: step 101080, loss = 2.04 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:25.669800: step 101090, loss = 1.88 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:10:26.879027: step 101100, loss = 1.85 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:28.078650: step 101110, loss = 1.92 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:29.283323: step 101120, loss = 2.00 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:30.465313: step 101130, loss = 1.91 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:10:31.664986: step 101140, loss = 1.86 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:32.892302: step 101150, loss = 1.94 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:10:34.092571: step 101160, loss = 1.97 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:35.335682: step 101170, loss = 1.95 (1029.7 examples/sec; 0.124 sec/batch)
2017-05-05 01:10:36.664046: step 101180, loss = 1.81 (963.6 examples/sec; 0.133 sec/batch)
2017-05-05 01:10:37.735917: step 101190, loss = 1.88 (1194.2 examples/sec; 0.107 sec/batch)
2017-05-05 01:10:38.950331: step 101200, loss = 1.97 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:40.186494: step 101210, loss = 1.89 (1035.5 examples/sec; 0.124 sec/batch)
2017-05-05 01:10:41.388422: step 101220, loss = 1.89 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:42.616978: step 101230, loss = 1.90 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:10:43.822639: step 101240, loss = 2.14 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:45.054726: step 101250, loss = 2.01 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:10:46.247344: step 101260, loss = 1.78 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:10:47.469816: step 101270, loss = 1.85 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:48.683383: step 101280, loss = 1.93 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:49.878007: step 101290, loss = 1.93 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:10:51.083187: step 101300, loss = 1.97 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:52.309898: step 101310, loss = 1.90 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:10:53.526859: step 101320, loss = 1.90 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:54.737009: step 101330, loss = 2.03 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:55.944149: step 101340, loss = 1.85 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:10:57.162442: step 101350, loss = 1.81 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:10:58.362466: step 101360, loss = 1.94 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:10:59.601268: step 101370, loss = 1.88 (1033.3 examples/sec; 0.124 sec/baE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1045 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
tch)
2017-05-05 01:11:00.770039: step 101380, loss = 1.85 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:11:01.972342: step 101390, loss = 2.10 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:11:03.197065: step 101400, loss = 1.99 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:04.424484: step 101410, loss = 2.01 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:05.612436: step 101420, loss = 1.88 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:11:06.829323: step 101430, loss = 1.94 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:08.064687: step 101440, loss = 1.96 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-05 01:11:09.292369: step 101450, loss = 2.03 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:10.498945: step 101460, loss = 1.91 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:11.724002: step 101470, loss = 2.00 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:12.950386: step 101480, loss = 1.97 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:14.166683: step 101490, loss = 1.76 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:15.373618: step 101500, loss = 1.79 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:16.613125: step 101510, loss = 1.96 (1032.7 examples/sec; 0.124 sec/batch)
2017-05-05 01:11:17.820046: step 101520, loss = 1.96 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:19.044657: step 101530, loss = 1.72 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:20.269523: step 101540, loss = 1.96 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:21.473495: step 101550, loss = 2.00 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:11:22.672355: step 101560, loss = 1.84 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:11:23.888592: step 101570, loss = 1.84 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:25.098694: step 101580, loss = 1.92 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:26.303090: step 101590, loss = 2.08 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:11:27.511342: step 101600, loss = 1.98 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:28.734084: step 101610, loss = 1.83 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:29.950455: step 101620, loss = 1.92 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:31.177736: step 101630, loss = 1.99 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:32.397689: step 101640, loss = 2.04 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:33.605746: step 101650, loss = 1.96 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:34.835846: step 101660, loss = 1.92 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:36.051725: step 101670, loss = 1.87 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:37.256998: step 101680, loss = 2.09 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:38.462733: step 101690, loss = 1.93 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:39.680335: step 101700, loss = 2.05 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:40.900733: step 101710, loss = 2.01 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:42.110328: step 101720, loss = 1.93 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:43.359952: step 101730, loss = 1.86 (1024.3 examples/sec; 0.125 sec/batch)
2017-05-05 01:11:44.564294: step 101740, loss = 1.89 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:11:45.776525: step 101750, loss = 1.95 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:47.014104: step 101760, loss = 1.98 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-05 01:11:48.225282: step 101770, loss = 1.80 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:49.405396: step 101780, loss = 1.90 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:11:50.635318: step 101790, loss = 1.84 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:51.856023: step 101800, loss = 1.88 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:53.084752: step 101810, loss = 1.88 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:11:54.293141: step 101820, loss = 1.90 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:11:55.515783: step 101830, loss = 1.90 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:11:56.751198: step 101840, loss = 2.05 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-05 01:11:57.944749: step 101850, loss = 2.00 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:11:59.159065: step 101860, loss = 1.90 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:00.413550: step 101870, loss = 1.83 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-05 01:12:01.615588: step 101880, loss = 1.75 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:12:02.820056: step 101890, loss = 2.08 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:12:04.051376: step 101900, loss = 1.84 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:05.275932: step 101910, loss = 2.04 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:06.489280: step 101920, loss = 1.84 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:07.713833: step 101930, loss = 1.90 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:08.910289: step 101940, loss = 1.86 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:12:10.137417: step 101950, loss = 1.92 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:11.366626: step 101960, loss = 2.09 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:12.708048: step 101970, loss = 1.85 (954.2 examples/sec; 0.134 sec/batch)
2017-05-05 01:12:13.740549: step 101980, loss = 1.99 (1239.7 examples/sec; 0.103 sec/batch)
2017-05-05 01:12:14.959133: step 101990, loss = 1.95 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:16.172317: step 102000, loss = 2.04 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:17.395503: step 102010, loss = 1.79 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:18.621271: step 102020, loss = 1.86 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:19.851265: step 102030, loss = 1.77 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:21.078992: step 102040, loss = 2.00 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:22.304008: step 102050, loss = 2.11 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:23.539466: step 102060, loss = 1.96 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-05 01:12:24.748828: step 102070, loss = 2.08 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:25.961705: step 102080, loss = 1.89 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:27.189998: step 102090, loss = 1.96 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:28.368932: step 102100, loss = 1.81 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:12:29.592144: step 102110, loss = 1.82 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:30.823056: step 102120, loss = 1.99 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:32.036976: step 102130, loss = 2.00 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:33.273937: step 102140, loss = 1.88 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-05 01:12:34.501309: step 102150, loss = 2.07 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:35.704081: step 102160, loss = 1.94 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:12:36.996892: step 102170, loss = 2.06 (990.1 examples/sec; 0.129 sec/batch)
2017-05-05 01:12:38.075093: step 102180, loss = 1.93 (1187.2 examples/sec; 0.108 sec/batch)
2017-05-05 01:12:39.289258: step 102190, loss = 1.90 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:40.492772: step 102200, loss = 1.89 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:12:41.717108: step 102210, loss = 1.73 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:42.940858: step 102220, loss = 1.98 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:44.161214: step 102230, loss = 2.01 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:45.388480: step 102240, loss = 1.94 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:46.607829: step 102250, loss = 1.86 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:47.829577: step 102260, loss = 1.97 (1047.7 examples/sec; 0.122 sec/batch)
2E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1055 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
017-05-05 01:12:49.056967: step 102270, loss = 2.00 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:50.254506: step 102280, loss = 1.82 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:12:51.477505: step 102290, loss = 1.96 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:52.696849: step 102300, loss = 1.92 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:53.896557: step 102310, loss = 1.89 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:12:55.101749: step 102320, loss = 1.92 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:12:56.332831: step 102330, loss = 1.74 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:12:57.552965: step 102340, loss = 1.88 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:58.770167: step 102350, loss = 1.90 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:12:59.987774: step 102360, loss = 1.89 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:01.205193: step 102370, loss = 1.95 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:02.389341: step 102380, loss = 1.93 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:13:03.638988: step 102390, loss = 1.80 (1024.3 examples/sec; 0.125 sec/batch)
2017-05-05 01:13:04.835956: step 102400, loss = 1.90 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:13:06.038410: step 102410, loss = 1.97 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:13:07.251260: step 102420, loss = 1.92 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:13:08.489037: step 102430, loss = 1.76 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-05 01:13:09.709488: step 102440, loss = 2.02 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:10.941516: step 102450, loss = 1.93 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:13:12.143349: step 102460, loss = 1.88 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:13:13.361796: step 102470, loss = 1.84 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:14.579451: step 102480, loss = 1.91 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:15.811907: step 102490, loss = 1.92 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:13:17.042852: step 102500, loss = 1.96 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:13:18.257584: step 102510, loss = 2.01 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:13:19.470746: step 102520, loss = 1.87 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:13:20.669845: step 102530, loss = 1.88 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:13:21.882485: step 102540, loss = 1.88 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:13:23.097775: step 102550, loss = 1.93 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:24.300372: step 102560, loss = 1.81 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:13:25.489999: step 102570, loss = 1.93 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:13:26.702821: step 102580, loss = 1.93 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:13:27.921592: step 102590, loss = 1.94 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:29.166644: step 102600, loss = 1.79 (1028.1 examples/sec; 0.125 sec/batch)
2017-05-05 01:13:30.361053: step 102610, loss = 1.88 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:13:31.580625: step 102620, loss = 1.94 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:32.802588: step 102630, loss = 1.89 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:33.985809: step 102640, loss = 1.99 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:13:35.231792: step 102650, loss = 1.91 (1027.3 examples/sec; 0.125 sec/batch)
2017-05-05 01:13:36.451865: step 102660, loss = 2.04 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:37.669326: step 102670, loss = 1.77 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:38.873004: step 102680, loss = 2.06 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:13:40.088308: step 102690, loss = 1.88 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:41.305685: step 102700, loss = 1.91 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:42.520463: step 102710, loss = 1.88 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:13:43.738765: step 102720, loss = 1.96 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:44.970665: step 102730, loss = 1.86 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:13:46.165629: step 102740, loss = 1.86 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:13:47.401571: step 102750, loss = 1.79 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-05 01:13:48.618092: step 102760, loss = 2.03 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:49.807373: step 102770, loss = 2.06 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:13:51.006738: step 102780, loss = 1.96 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:13:52.243967: step 102790, loss = 2.06 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-05 01:13:53.434179: step 102800, loss = 1.92 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:13:54.653691: step 102810, loss = 1.85 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:55.890871: step 102820, loss = 1.98 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-05 01:13:57.114751: step 102830, loss = 2.07 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:58.331072: step 102840, loss = 2.01 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:13:59.549348: step 102850, loss = 1.90 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:14:00.762163: step 102860, loss = 1.91 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:14:01.970159: step 102870, loss = 1.83 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:14:03.197993: step 102880, loss = 1.97 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:14:04.412381: step 102890, loss = 2.02 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:14:05.611739: step 102900, loss = 1.80 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:14:06.822880: step 102910, loss = 1.98 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:14:08.039573: step 102920, loss = 1.97 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:14:09.278478: step 102930, loss = 1.86 (1033.2 examples/sec; 0.124 sec/batch)
2017-05-05 01:14:10.498892: step 102940, loss = 1.88 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:14:11.701645: step 102950, loss = 1.89 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:14:13.001118: step 102960, loss = 2.03 (985.0 examples/sec; 0.130 sec/batch)
2017-05-05 01:14:14.094041: step 102970, loss = 1.89 (1171.2 examples/sec; 0.109 sec/batch)
2017-05-05 01:14:15.311634: step 102980, loss = 2.06 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:14:16.512156: step 102990, loss = 1.92 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:14:17.704791: step 103000, loss = 1.83 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:14:18.899726: step 103010, loss = 1.80 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:14:20.086610: step 103020, loss = 2.06 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:14:21.282181: step 103030, loss = 1.90 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:14:22.462343: step 103040, loss = 1.92 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:23.643523: step 103050, loss = 1.98 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:24.854455: step 103060, loss = 1.89 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:14:26.057498: step 103070, loss = 1.96 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:14:27.265866: step 103080, loss = 1.99 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:14:28.479029: step 103090, loss = 1.95 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:14:29.691917: step 103100, loss = 2.00 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:14:30.925604: step 103110, loss = 1.83 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:14:32.121813: step 103120, loss = 1.95 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:14:33.328631: step 103130, loss = 2.15 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:14:34.517763: step 103140, loss = 1.92 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:14:35.704008: step 103150, loss = 1.83 (1079.0 examples/sec; 0.119 sec/batch)
2017-0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1065 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
5-05 01:14:36.926447: step 103160, loss = 1.84 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:14:38.069528: step 103170, loss = 2.14 (1119.8 examples/sec; 0.114 sec/batch)
2017-05-05 01:14:39.247787: step 103180, loss = 1.91 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:40.447931: step 103190, loss = 2.12 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:14:41.608356: step 103200, loss = 1.97 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:14:42.788518: step 103210, loss = 2.00 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:43.974468: step 103220, loss = 2.02 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:14:45.151210: step 103230, loss = 1.83 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:46.320430: step 103240, loss = 1.88 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:14:47.485225: step 103250, loss = 1.93 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:14:48.658831: step 103260, loss = 1.88 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:14:49.830053: step 103270, loss = 2.00 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:14:51.010361: step 103280, loss = 1.91 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:52.199738: step 103290, loss = 1.99 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:14:53.359905: step 103300, loss = 1.98 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:14:54.529125: step 103310, loss = 1.99 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:14:55.712934: step 103320, loss = 1.73 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:56.890753: step 103330, loss = 1.90 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:58.067659: step 103340, loss = 1.96 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:14:59.268826: step 103350, loss = 1.93 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:15:00.445663: step 103360, loss = 1.87 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:01.618084: step 103370, loss = 1.99 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:02.813878: step 103380, loss = 1.84 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:15:03.992544: step 103390, loss = 1.94 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:05.195726: step 103400, loss = 1.83 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:15:06.368652: step 103410, loss = 1.80 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:07.579084: step 103420, loss = 1.98 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:15:08.777296: step 103430, loss = 1.76 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:15:09.976594: step 103440, loss = 1.91 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:15:11.179176: step 103450, loss = 1.87 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:15:12.365496: step 103460, loss = 1.87 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:15:13.543592: step 103470, loss = 1.94 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:14.728026: step 103480, loss = 1.77 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:15.893193: step 103490, loss = 2.00 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:17.064641: step 103500, loss = 1.92 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:18.230646: step 103510, loss = 1.76 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:19.404926: step 103520, loss = 2.09 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:20.587571: step 103530, loss = 1.81 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:21.791739: step 103540, loss = 1.90 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:15:22.990464: step 103550, loss = 1.88 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:15:24.159432: step 103560, loss = 2.02 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:25.325296: step 103570, loss = 1.87 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:26.507604: step 103580, loss = 2.00 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:27.671775: step 103590, loss = 1.86 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:15:28.830956: step 103600, loss = 1.82 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:15:29.999108: step 103610, loss = 1.95 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:31.170255: step 103620, loss = 1.97 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:32.346091: step 103630, loss = 1.98 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:33.505935: step 103640, loss = 1.87 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:15:34.692299: step 103650, loss = 1.82 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:15:35.875431: step 103660, loss = 1.87 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:37.032762: step 103670, loss = 1.94 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:15:38.180785: step 103680, loss = 2.07 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:15:39.364128: step 103690, loss = 1.87 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:40.528174: step 103700, loss = 1.83 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:15:41.691542: step 103710, loss = 2.10 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:15:42.873676: step 103720, loss = 1.81 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:15:44.047190: step 103730, loss = 1.79 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:45.218109: step 103740, loss = 1.90 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:46.384747: step 103750, loss = 2.01 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:47.540347: step 103760, loss = 1.79 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:15:48.703063: step 103770, loss = 1.88 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:15:49.869406: step 103780, loss = 2.03 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:15:51.064910: step 103790, loss = 1.98 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:15:52.255976: step 103800, loss = 1.84 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:15:53.475886: step 103810, loss = 1.92 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:15:54.683892: step 103820, loss = 1.91 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:15:55.883201: step 103830, loss = 2.00 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:15:57.091865: step 103840, loss = 1.85 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:15:58.303342: step 103850, loss = 1.89 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:15:59.525983: step 103860, loss = 1.82 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:16:00.726588: step 103870, loss = 1.94 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:16:01.925638: step 103880, loss = 1.86 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:16:03.119154: step 103890, loss = 1.97 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:16:04.304696: step 103900, loss = 1.96 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:16:05.490616: step 103910, loss = 2.00 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:16:06.668641: step 103920, loss = 1.88 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:07.848896: step 103930, loss = 2.02 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:09.029144: step 103940, loss = 1.94 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:10.305531: step 103950, loss = 2.01 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-05 01:16:11.357580: step 103960, loss = 2.01 (1216.7 examples/sec; 0.105 sec/batch)
2017-05-05 01:16:12.528183: step 103970, loss = 1.83 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:13.696654: step 103980, loss = 1.92 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:14.856095: step 103990, loss = 1.75 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:16:16.035622: step 104000, loss = 1.82 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:17.217141: step 104010, loss = 1.81 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:18.367347: step 104020, loss = 1.88 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:16:19.535273: step 104030, loss = 1.76 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:20.705350: step 104040, loss = 1.84 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:21.867279: step 104050, loss = 1.80 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:16:23.037966: step 104060, loss = 1.95 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:24.222010: step 104070, loss = 1.82 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:25.367834: step 104080, loss = 1.86 (1117.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:16:26.520026: step 104090, loss = 1.94 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:16:27.701790: step 104100, loss = 1.90 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:28.869054: step 104110, loss = 1.93 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:30.046548: step 104120, loss = 1.89 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:31.221552: step 104130, loss = 1.77 (1089.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:32.404297: step 104140, loss = 2.04 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:33.613674: step 104150, loss = 1.87 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:16:34.774385: step 104160, loss = 1.87 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:16:35.967045: step 104170, loss = 2.04 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:16:37.168313: step 104180, loss = 1.88 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:16:38.336691: step 104190, loss = 2.05 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:39.510205: step 104200, loss = 1.91 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:40.686321: step 104210, loss = 1.87 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:41.849126: step 104220, loss = 1.67 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:16:43.034491: step 104230, loss = 1.90 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:16:44.214501: step 104240, loss = 1.86 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:45.390776: step 104250, loss = 2.12 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:46.546949: step 104260, loss = 1.85 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:16:47.712938: step 104270, loss = 1.97 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:48.895903: step 104280, loss = 1.81 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:50.050660: step 104290, loss = 1.69 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:16:51.204668: step 104300, loss = 1.96 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:16:52.377098: step 104310, loss = 1.90 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:53.536332: step 104320, loss = 1.99 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:16:54.688517: step 104330, loss = 1.96 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:16:55.878732: step 104340, loss = 1.92 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:16:57.057453: step 104350, loss = 1.97 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:16:58.229135: step 104360, loss = 2.03 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:16:59.417297: step 104370, loss = 1.88 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:00.627123: step 104380, loss = 2.09 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:17:01.812736: step 104390, loss = 1.94 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:03.000712: step 104400, loss = 2.26 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:04.201684: step 104410, loss = 1.90 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:17:05.383025: step 104420, loss = 1.96 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:06.559293: step 104430, loss = 1.89 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:07.749615: step 104440, loss = 1.97 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:08.971809: step 104450, loss = 1.93 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:17:10.191684: step 104460, loss = 2.02 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:17:11.393661: step 104470, loss = 1.87 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:17:12.589262: step 104480, loss = 1.87 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:17:13.753500: step 104490, loss = 1.85 (1099E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1076 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:14.940410: step 104500, loss = 1.98 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:16.137887: step 104510, loss = 1.83 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:17:17.338705: step 104520, loss = 1.83 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:17:18.517519: step 104530, loss = 1.92 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:19.704946: step 104540, loss = 1.95 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:20.904401: step 104550, loss = 1.87 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:17:22.078858: step 104560, loss = 1.98 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:23.262939: step 104570, loss = 1.94 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:24.457319: step 104580, loss = 2.00 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:25.626586: step 104590, loss = 1.91 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:26.821166: step 104600, loss = 1.96 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:28.006856: step 104610, loss = 1.84 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:29.178116: step 104620, loss = 1.89 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:30.351867: step 104630, loss = 1.71 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:31.506825: step 104640, loss = 1.88 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:17:32.689859: step 104650, loss = 1.84 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:33.844885: step 104660, loss = 1.86 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:35.008696: step 104670, loss = 2.02 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:36.166515: step 104680, loss = 1.86 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:37.329518: step 104690, loss = 1.93 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:38.484099: step 104700, loss = 1.89 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:17:39.667885: step 104710, loss = 2.05 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:40.841043: step 104720, loss = 1.99 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:42.011699: step 104730, loss = 1.90 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:43.191757: step 104740, loss = 1.79 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:44.358892: step 104750, loss = 1.80 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:45.510085: step 104760, loss = 1.95 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:17:46.687440: step 104770, loss = 1.92 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:47.872554: step 104780, loss = 1.84 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:17:49.039578: step 104790, loss = 1.95 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:50.187934: step 104800, loss = 2.00 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:17:51.369458: step 104810, loss = 1.97 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:52.546741: step 104820, loss = 1.97 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:17:53.704838: step 104830, loss = 1.89 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:54.862386: step 104840, loss = 1.91 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:56.036629: step 104850, loss = 2.06 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:17:57.197222: step 104860, loss = 1.93 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:58.353789: step 104870, loss = 1.91 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:17:59.512433: step 104880, loss = 1.75 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:00.692751: step 104890, loss = 2.09 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:18:01.840874: step 104900, loss = 1.83 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:18:02.998897: step 104910, loss = 1.82 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:04.192366: step 104920, loss = 1.92 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:18:05.362781: step 104930, loss = 2.13 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:06.589953: step 104940, loss = 1.92 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:18:07.693471: step 104950, loss = 1.90 (1159.9 examples/sec; 0.110 sec/batch)
2017-05-05 01:18:08.864906: step 104960, loss = 1.76 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:10.013177: step 104970, loss = 1.87 (1114.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:18:11.181402: step 104980, loss = 2.04 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:12.336189: step 104990, loss = 1.90 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:18:13.490278: step 105000, loss = 1.96 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:18:14.657418: step 105010, loss = 1.84 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:15.831501: step 105020, loss = 1.87 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:17.009039: step 105030, loss = 1.97 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:18:18.173691: step 105040, loss = 1.94 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:19.355463: step 105050, loss = 1.95 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:18:20.514050: step 105060, loss = 1.91 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:21.662585: step 105070, loss = 2.03 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:18:22.826551: step 105080, loss = 1.94 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:24.000642: step 105090, loss = 1.88 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:25.182087: step 105100, loss = 1.97 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:18:26.342528: step 105110, loss = 1.87 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:27.527280: step 105120, loss = 1.92 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:18:28.680777: step 105130, loss = 1.84 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:18:29.850669: step 105140, loss = 1.84 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:31.027335: step 105150, loss = 1.92 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:18:32.169840: step 105160, loss = 1.92 (1120.3 examples/sec; 0.114 sec/batch)
2017-05-05 01:18:33.333855: step 105170, loss = 2.04 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:34.491285: step 105180, loss = 1.96 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:35.654455: step 105190, loss = 1.85 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:36.848112: step 105200, loss = 2.00 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:18:38.009910: step 105210, loss = 1.79 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:39.173137: step 105220, loss = 1.88 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:40.335693: step 105230, loss = 2.04 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:41.484305: step 105240, loss = 1.86 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:18:42.636513: step 105250, loss = 2.01 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:18:43.795695: step 105260, loss = 1.81 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:44.966145: step 105270, loss = 1.91 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:46.111078: step 105280, loss = 2.12 (1118.0 examples/sec; 0.114 sec/batch)
2017-05-05 01:18:47.272728: step 105290, loss = 1.82 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:48.430408: step 105300, loss = 1.76 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:49.591643: step 105310, loss = 1.91 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:50.760827: step 105320, loss = 2.05 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:51.927282: step 105330, loss = 1.76 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:18:53.083125: step 105340, loss = 1.87 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:54.246964: step 105350, loss = 1.89 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:55.411547: step 105360, loss = 1.87 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:18:56.591592: step 105370, loss = 1.83 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:18:57.757377: step 105380, loss = 1.99 (1098.0 eE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1088 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
xamples/sec; 0.117 sec/batch)
2017-05-05 01:18:58.923423: step 105390, loss = 1.73 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:00.091049: step 105400, loss = 1.91 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:01.246617: step 105410, loss = 1.95 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:02.396993: step 105420, loss = 1.89 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:19:03.558614: step 105430, loss = 2.02 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:04.726400: step 105440, loss = 1.73 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:05.896788: step 105450, loss = 1.89 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:07.075216: step 105460, loss = 1.98 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:08.236189: step 105470, loss = 1.87 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:09.409502: step 105480, loss = 2.10 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:10.576187: step 105490, loss = 1.89 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:11.748306: step 105500, loss = 1.84 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:12.905841: step 105510, loss = 2.00 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:14.055396: step 105520, loss = 1.94 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:19:15.202340: step 105530, loss = 2.14 (1116.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:19:16.370501: step 105540, loss = 1.82 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:17.545536: step 105550, loss = 1.91 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:18.737128: step 105560, loss = 1.96 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:19:19.897387: step 105570, loss = 1.85 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:21.067750: step 105580, loss = 1.99 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:22.240401: step 105590, loss = 2.05 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:23.399839: step 105600, loss = 2.03 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:24.572975: step 105610, loss = 2.01 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:25.719994: step 105620, loss = 1.94 (1115.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:19:26.888211: step 105630, loss = 1.86 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:28.050391: step 105640, loss = 1.88 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:29.225745: step 105650, loss = 1.96 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:30.362839: step 105660, loss = 1.95 (1125.7 examples/sec; 0.114 sec/batch)
2017-05-05 01:19:31.542560: step 105670, loss = 1.89 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:32.719264: step 105680, loss = 1.87 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:33.883019: step 105690, loss = 1.92 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:35.047562: step 105700, loss = 1.83 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:36.202861: step 105710, loss = 2.02 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:37.382430: step 105720, loss = 1.99 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:19:38.544067: step 105730, loss = 1.86 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:39.703536: step 105740, loss = 1.89 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:40.870279: step 105750, loss = 1.99 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:42.022630: step 105760, loss = 1.77 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:19:43.179081: step 105770, loss = 1.83 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:44.350476: step 105780, loss = 1.83 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:45.514493: step 105790, loss = 2.07 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:46.661908: step 105800, loss = 2.00 (1115.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:19:47.847097: step 105810, loss = 1.78 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:19:48.993606: step 105820, loss = 2.00 (1116.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:19:50.184055: step 105830, loss = 1.87 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:19:51.328185: step 105840, loss = 2.01 (1118.8 examples/sec; 0.114 sec/batch)
2017-05-05 01:19:52.499548: step 105850, loss = 2.01 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:53.692279: step 105860, loss = 2.01 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:19:54.880929: step 105870, loss = 1.98 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:19:56.052946: step 105880, loss = 2.08 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:57.221921: step 105890, loss = 1.85 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:19:58.384763: step 105900, loss = 1.91 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:19:59.574490: step 105910, loss = 2.01 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:00.752657: step 105920, loss = 1.86 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:02.021003: step 105930, loss = 1.92 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-05 01:20:03.134836: step 105940, loss = 1.99 (1149.2 examples/sec; 0.111 sec/batch)
2017-05-05 01:20:04.319349: step 105950, loss = 1.85 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:05.500700: step 105960, loss = 1.93 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:06.705620: step 105970, loss = 1.84 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:07.893950: step 105980, loss = 1.97 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:09.108833: step 105990, loss = 1.72 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:20:10.322027: step 106000, loss = 1.78 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:20:11.524844: step 106010, loss = 2.00 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:12.722507: step 106020, loss = 1.94 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:13.909128: step 106030, loss = 1.99 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:15.096713: step 106040, loss = 1.80 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:16.289863: step 106050, loss = 1.83 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:17.483115: step 106060, loss = 1.71 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:18.677383: step 106070, loss = 1.98 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:19.859120: step 106080, loss = 1.84 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:21.048282: step 106090, loss = 2.03 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:22.244569: step 106100, loss = 1.96 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:23.450168: step 106110, loss = 1.82 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:20:24.672970: step 106120, loss = 1.85 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:20:25.860442: step 106130, loss = 1.83 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:27.057643: step 106140, loss = 1.88 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:28.266592: step 106150, loss = 1.90 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:20:29.446217: step 106160, loss = 1.90 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:30.631148: step 106170, loss = 1.81 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:31.833673: step 106180, loss = 1.94 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:33.029932: step 106190, loss = 1.87 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:34.211591: step 106200, loss = 1.82 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:35.394123: step 106210, loss = 1.87 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:36.602321: step 106220, loss = 1.75 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:20:37.790330: step 106230, loss = 1.76 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:38.967831: step 106240, loss = 1.82 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:40.141211: step 106250, loss = 1.93 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:20:41.308176: step 106260, loss = 1.84 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:20:42.472165: step 106270, loss = 1.91 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:20:43.667167: step 106280, loss = 1.87 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:20:44.844866: step 106290, loss = 1.87 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:46.000550: step 106300, loss = 2.08 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:20:47.175617: step 106310, loss = 1.94 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:48.352729: step 106320, loss = 1.85 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:49.506736: step 106330, loss = 1.88 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:20:50.657061: step 106340, loss = 2.05 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:20:51.843439: step 106350, loss = 1.79 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:20:53.009087: step 106360, loss = 1.92 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:20:54.172966: step 106370, loss = 2.07 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:20:55.339380: step 106380, loss = 1.83 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:20:56.516270: step 106390, loss = 2.05 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:20:57.663306: step 106400, loss = 1.91 (1115.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:20:58.833492: step 106410, loss = 2.09 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:20:59.996403: step 106420, loss = 1.92 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:21:01.159562: step 106430, loss = 1.91 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:21:02.310180: step 106440, loss = 1.93 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:21:03.496034: step 106450, loss = 1.89 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:21:04.662366: step 106460, loss = 1.92 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:05.840743: step 106470, loss = 1.79 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:07.010835: step 106480, loss = 1.78 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:08.195482: step 106490, loss = 1.74 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:09.379953: step 106500, loss = 1.95 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:10.557149: step 106510, loss = 1.99 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:11.755376: step 106520, loss = 1.79 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:21:12.932614: step 106530, loss = 1.83 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:14.126836: step 106540, loss = 1.85 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:21:15.326320: step 106550, loss = 1.87 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:21:16.537837: step 106560, loss = 1.97 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:21:17.718464: step 106570, loss = 1.82 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:18.911869: step 106580, loss = 1.83 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:21:20.121634: step 106590, loss = 1.96 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:21:21.335057: step 106600, loss = 1.83 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:21:22.534300: step 106610, loss = 1.79 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:21:23.732807: step 106620, loss = 1.97 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:21:24.920751: step 106630, loss = 1.92 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:21:26.106335: step 106640, loss = 1.81 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:21:27.292093: step 106650, loss = 1.85 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:21:28.456870: step 106660, loss = 1.98 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:21:29.627196: step 106670, loss = 1.81 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:30.799563: step 106680, loss = 2.05 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:31.941984: step 106690, loss = 1.89 (1120.4 examples/sec; 0.114 sec/batch)
2017-05-05 01:21:33.130562: step 106700, loss = 2.00 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:21:34.272933: step 106710, loss = 1.82 (1120.5 examples/sec; 0.114 sec/batch)
2017-05-05 01:21:35.46E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1099 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
6413: step 106720, loss = 1.93 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:21:36.638357: step 106730, loss = 1.99 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:37.781466: step 106740, loss = 1.94 (1119.8 examples/sec; 0.114 sec/batch)
2017-05-05 01:21:38.964977: step 106750, loss = 2.03 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:40.118865: step 106760, loss = 1.90 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:21:41.295746: step 106770, loss = 1.90 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:42.447715: step 106780, loss = 1.94 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:21:43.626735: step 106790, loss = 1.93 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:21:44.792259: step 106800, loss = 1.83 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:45.981700: step 106810, loss = 1.89 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:21:47.142356: step 106820, loss = 1.96 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:21:48.309870: step 106830, loss = 1.87 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:49.472816: step 106840, loss = 1.97 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:21:50.639652: step 106850, loss = 1.77 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:51.807733: step 106860, loss = 2.15 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:52.996706: step 106870, loss = 1.82 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:21:54.153145: step 106880, loss = 1.88 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:21:55.353096: step 106890, loss = 1.92 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:21:56.522312: step 106900, loss = 1.80 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:21:57.684289: step 106910, loss = 1.89 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:21:58.946336: step 106920, loss = 1.89 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-05 01:22:00.030306: step 106930, loss = 1.79 (1180.8 examples/sec; 0.108 sec/batch)
2017-05-05 01:22:01.185032: step 106940, loss = 1.79 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:22:02.329330: step 106950, loss = 1.92 (1118.6 examples/sec; 0.114 sec/batch)
2017-05-05 01:22:03.476787: step 106960, loss = 1.96 (1115.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:22:04.651981: step 106970, loss = 1.99 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:05.817465: step 106980, loss = 1.86 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:06.996608: step 106990, loss = 2.10 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:08.166951: step 107000, loss = 1.88 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:09.331364: step 107010, loss = 1.92 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:22:10.474879: step 107020, loss = 1.85 (1119.4 examples/sec; 0.114 sec/batch)
2017-05-05 01:22:11.640603: step 107030, loss = 1.96 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:12.798142: step 107040, loss = 1.88 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:22:13.961299: step 107050, loss = 1.80 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:22:15.136430: step 107060, loss = 1.95 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:16.338604: step 107070, loss = 1.93 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:22:17.522789: step 107080, loss = 1.83 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:18.703011: step 107090, loss = 2.11 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:19.899182: step 107100, loss = 1.93 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:22:21.095596: step 107110, loss = 2.05 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:22:22.343365: step 107120, loss = 1.87 (1025.8 examples/sec; 0.125 sec/batch)
2017-05-05 01:22:23.470189: step 107130, loss = 1.84 (1135.9 examples/sec; 0.113 sec/batch)
2017-05-05 01:22:24.682694: step 107140, loss = 2.02 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:22:25.855418: step 107150, loss = 2.04 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:27.026975: step 107160, loss = 1.88 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:28.211879: step 107170, loss = 1.81 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:29.387867: step 107180, loss = 1.87 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:30.553143: step 107190, loss = 1.85 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:31.732154: step 107200, loss = 1.99 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:32.904636: step 107210, loss = 2.01 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:34.080189: step 107220, loss = 1.87 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:35.242937: step 107230, loss = 1.77 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:22:36.401069: step 107240, loss = 1.93 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:22:37.550342: step 107250, loss = 2.00 (1113.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:22:38.703282: step 107260, loss = 1.88 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:22:39.866059: step 107270, loss = 1.94 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:22:41.038659: step 107280, loss = 2.10 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:42.211842: step 107290, loss = 1.84 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:43.396919: step 107300, loss = 1.93 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:22:44.570033: step 107310, loss = 1.97 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:45.733834: step 107320, loss = 2.16 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:22:46.923138: step 107330, loss = 1.92 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:22:48.122916: step 107340, loss = 2.00 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:22:49.315300: step 107350, loss = 1.99 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:22:50.507171: step 107360, loss = 1.79 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:22:51.720315: step 107370, loss = 1.95 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:22:52.917742: step 107380, loss = 1.92 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:22:54.121040: step 107390, loss = 2.06 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:22:55.338368: step 107400, loss = 1.76 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:22:56.522182: step 107410, loss = 1.77 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:22:57.694537: step 107420, loss = 1.94 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:22:58.884970: step 107430, loss = 1.91 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:23:00.072742: step 107440, loss = 1.94 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:23:01.269595: step 107450, loss = 1.84 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:23:02.444163: step 107460, loss = 1.94 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:03.633739: step 107470, loss = 1.92 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:23:04.827824: step 107480, loss = 1.88 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:23:06.028342: step 107490, loss = 2.00 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:23:07.229039: step 107500, loss = 1.95 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:23:08.408884: step 107510, loss = 2.00 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:09.582551: step 107520, loss = 1.88 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:10.755582: step 107530, loss = 1.89 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:11.928336: step 107540, loss = 1.85 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:13.120008: step 107550, loss = 2.01 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:23:14.285342: step 107560, loss = 1.80 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:15.449981: step 107570, loss = 2.01 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:16.626693: step 107580, loss = 1.90 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:17.784391: step 107590, loss = 1.92 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:18.961801: step 107600, loss = 2.06 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:20.139446E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1110 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
: step 107610, loss = 1.73 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:21.308911: step 107620, loss = 1.90 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:22.485116: step 107630, loss = 1.87 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:23.646327: step 107640, loss = 1.86 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:24.824275: step 107650, loss = 2.03 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:25.985125: step 107660, loss = 1.78 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:27.150798: step 107670, loss = 1.99 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:28.323025: step 107680, loss = 1.86 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:29.486906: step 107690, loss = 1.84 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:30.647197: step 107700, loss = 1.91 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:31.820943: step 107710, loss = 1.87 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:32.993315: step 107720, loss = 1.96 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:34.154147: step 107730, loss = 1.87 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:35.328894: step 107740, loss = 2.00 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:36.478093: step 107750, loss = 1.95 (1113.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:23:37.640846: step 107760, loss = 1.87 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:38.825035: step 107770, loss = 1.90 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:40.004388: step 107780, loss = 1.80 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:41.178712: step 107790, loss = 1.83 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:42.330630: step 107800, loss = 1.92 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:23:43.505123: step 107810, loss = 2.01 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:44.673117: step 107820, loss = 1.74 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:45.828355: step 107830, loss = 1.93 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:46.988168: step 107840, loss = 1.86 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:48.152821: step 107850, loss = 2.08 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:49.323703: step 107860, loss = 1.96 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:50.495027: step 107870, loss = 1.97 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:51.650531: step 107880, loss = 1.93 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:23:52.834634: step 107890, loss = 1.94 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:23:53.970825: step 107900, loss = 1.91 (1126.6 examples/sec; 0.114 sec/batch)
2017-05-05 01:23:55.269394: step 107910, loss = 2.03 (985.7 examples/sec; 0.130 sec/batch)
2017-05-05 01:23:56.346908: step 107920, loss = 1.93 (1187.9 examples/sec; 0.108 sec/batch)
2017-05-05 01:23:57.532992: step 107930, loss = 2.08 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:23:58.700927: step 107940, loss = 1.74 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:23:59.881631: step 107950, loss = 1.90 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:24:01.040504: step 107960, loss = 1.80 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:24:02.204172: step 107970, loss = 1.89 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:24:03.355302: step 107980, loss = 1.86 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:24:04.535365: step 107990, loss = 1.99 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:24:05.685095: step 108000, loss = 1.78 (1113.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:24:06.857978: step 108010, loss = 1.84 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:08.026218: step 108020, loss = 1.74 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:09.176860: step 108030, loss = 1.89 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:24:10.333796: step 108040, loss = 1.89 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:24:11.528039: step 108050, loss = 1.90 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:12.692432: step 108060, loss = 1.96 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:24:13.858741: step 108070, loss = 2.00 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:15.038364: step 108080, loss = 1.83 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:24:16.212309: step 108090, loss = 1.87 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:17.379649: step 108100, loss = 1.87 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:18.556517: step 108110, loss = 1.97 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:24:19.746909: step 108120, loss = 1.83 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:20.931410: step 108130, loss = 1.87 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:24:22.090935: step 108140, loss = 1.93 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:24:23.269695: step 108150, loss = 1.92 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:24:24.444218: step 108160, loss = 1.99 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:24:25.634281: step 108170, loss = 2.02 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:26.813153: step 108180, loss = 1.91 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:24:28.003454: step 108190, loss = 2.06 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:29.178872: step 108200, loss = 1.88 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:24:30.367334: step 108210, loss = 1.91 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:31.554457: step 108220, loss = 1.96 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:32.738619: step 108230, loss = 1.83 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:24:33.927402: step 108240, loss = 1.93 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:35.144208: step 108250, loss = 2.08 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:24:36.362026: step 108260, loss = 1.81 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:24:37.571204: step 108270, loss = 1.98 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:24:38.759986: step 108280, loss = 1.91 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:39.971661: step 108290, loss = 1.96 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:24:41.204861: step 108300, loss = 1.83 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:24:42.397461: step 108310, loss = 1.95 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:43.630910: step 108320, loss = 1.82 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:24:44.842048: step 108330, loss = 1.90 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:24:46.052687: step 108340, loss = 1.98 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:24:47.280480: step 108350, loss = 1.97 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:24:48.508999: step 108360, loss = 1.90 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:24:49.695005: step 108370, loss = 1.91 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:50.903057: step 108380, loss = 1.91 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:24:52.113120: step 108390, loss = 1.69 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:24:53.320032: step 108400, loss = 1.75 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:24:54.509889: step 108410, loss = 1.82 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:24:55.720854: step 108420, loss = 1.96 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:24:56.916501: step 108430, loss = 1.91 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:24:58.115261: step 108440, loss = 1.88 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:24:59.334384: step 108450, loss = 1.79 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:25:00.534396: step 108460, loss = 1.97 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:25:01.777877: step 108470, loss = 1.87 (1029.4 examples/sec; 0.124 sec/batch)
2017-05-05 01:25:02.977379: step 108480, loss = 1.87 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:25:04.168135: step 108490, loss = 1.92 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:25:05.364923: steE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1121 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
p 108500, loss = 1.96 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:25:06.564883: step 108510, loss = 1.94 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:25:07.763619: step 108520, loss = 2.01 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:25:08.967523: step 108530, loss = 2.27 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:25:10.148769: step 108540, loss = 1.88 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:25:11.347670: step 108550, loss = 1.77 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:25:12.533474: step 108560, loss = 1.97 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:25:13.700299: step 108570, loss = 2.04 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:14.851174: step 108580, loss = 1.83 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:25:16.037035: step 108590, loss = 1.85 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:25:17.194987: step 108600, loss = 1.92 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:18.357050: step 108610, loss = 1.99 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:19.519685: step 108620, loss = 1.80 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:20.685333: step 108630, loss = 2.08 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:21.852112: step 108640, loss = 1.90 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:23.024774: step 108650, loss = 1.91 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:24.195804: step 108660, loss = 1.88 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:25.335927: step 108670, loss = 1.90 (1122.7 examples/sec; 0.114 sec/batch)
2017-05-05 01:25:26.503932: step 108680, loss = 1.94 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:27.670031: step 108690, loss = 2.06 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:28.831169: step 108700, loss = 1.87 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:29.997469: step 108710, loss = 1.89 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:31.173089: step 108720, loss = 1.89 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:25:32.368165: step 108730, loss = 1.78 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:25:33.530132: step 108740, loss = 1.82 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:34.702048: step 108750, loss = 2.03 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:35.884626: step 108760, loss = 1.87 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:25:37.052785: step 108770, loss = 1.82 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:38.202736: step 108780, loss = 1.96 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:25:39.384458: step 108790, loss = 1.87 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:25:40.559741: step 108800, loss = 1.93 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:25:41.730770: step 108810, loss = 2.09 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:42.900176: step 108820, loss = 1.96 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:44.076561: step 108830, loss = 1.88 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:25:45.233735: step 108840, loss = 2.04 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:46.389957: step 108850, loss = 1.78 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:47.554728: step 108860, loss = 1.81 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:48.705104: step 108870, loss = 2.02 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:25:49.864772: step 108880, loss = 2.00 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:51.037519: step 108890, loss = 1.94 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:25:52.298571: step 108900, loss = 1.97 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-05 01:25:53.348483: step 108910, loss = 1.87 (1219.2 examples/sec; 0.105 sec/batch)
2017-05-05 01:25:54.502777: step 108920, loss = 1.84 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:25:55.697676: step 108930, loss = 1.93 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:25:56.852101: step 108940, loss = 1.90 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:25:58.016883: step 108950, loss = 1.89 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:25:59.192518: step 108960, loss = 1.93 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:00.371128: step 108970, loss = 1.91 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:01.530340: step 108980, loss = 1.97 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:02.693392: step 108990, loss = 1.98 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:03.862411: step 109000, loss = 1.89 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:05.028487: step 109010, loss = 1.85 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:06.193429: step 109020, loss = 1.74 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:07.351276: step 109030, loss = 1.84 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:08.516001: step 109040, loss = 1.99 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:09.696284: step 109050, loss = 1.91 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:10.868148: step 109060, loss = 1.91 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:12.051081: step 109070, loss = 1.87 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:13.236288: step 109080, loss = 1.81 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:26:14.389716: step 109090, loss = 1.91 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:26:15.573519: step 109100, loss = 1.79 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:16.750404: step 109110, loss = 1.79 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:17.903625: step 109120, loss = 1.96 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:26:19.078496: step 109130, loss = 1.89 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:20.256490: step 109140, loss = 1.80 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:21.407122: step 109150, loss = 1.78 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:26:22.571797: step 109160, loss = 2.04 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:23.728610: step 109170, loss = 1.79 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:24.913176: step 109180, loss = 1.89 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:26.077212: step 109190, loss = 1.92 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:27.240623: step 109200, loss = 2.02 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:28.409564: step 109210, loss = 1.92 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:29.581782: step 109220, loss = 2.02 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:30.758897: step 109230, loss = 1.78 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:31.922039: step 109240, loss = 1.85 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:33.097109: step 109250, loss = 1.79 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:34.269956: step 109260, loss = 1.88 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:35.441884: step 109270, loss = 1.96 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:36.626286: step 109280, loss = 1.99 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:37.779617: step 109290, loss = 1.92 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:26:38.940521: step 109300, loss = 1.88 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:40.109671: step 109310, loss = 1.93 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:41.262991: step 109320, loss = 2.05 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:26:42.441486: step 109330, loss = 1.86 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:26:43.598011: step 109340, loss = 1.78 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:44.756216: step 109350, loss = 2.04 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:45.904798: step 109360, loss = 1.86 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:26:47.070634: step 109370, loss = 1.67 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:48.238814: step 109380, loss = 1.87 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:49.396923: step 109390, loss = 1.89 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:50.563266: step 109400, loss = 1.83 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:51.725326: step 109410, loss = 1.87 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:52.897277: step 109420, loss = 1.82 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:54.053856: step 109430, loss = 2.10 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:55.227975: step 109440, loss = 1.83 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:56.397740: step 109450, loss = 1.97 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:26:57.557791: step 109460, loss = 1.96 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:26:58.752381: step 109470, loss = 1.93 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:26:59.900099: step 109480, loss = 1.92 (1115.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:27:01.076631: step 109490, loss = 1.91 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:02.256470: step 109500, loss = 1.77 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:03.425635: step 109510, loss = 1.78 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:27:04.592389: step 109520, loss = 1.95 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:27:05.748663: step 109530, loss = 1.98 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:06.909907: step 109540, loss = 1.89 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:08.072622: step 109550, loss = 1.88 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:09.236924: step 109560, loss = 1.93 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:10.392565: step 109570, loss = 1.77 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:11.565487: step 109580, loss = 2.07 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:27:12.763468: step 109590, loss = 1.77 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:27:13.945119: step 109600, loss = 1.96 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:15.121656: step 109610, loss = 1.95 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:16.317081: step 109620, loss = 1.85 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:27:17.500786: step 109630, loss = 2.04 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:18.684108: step 109640, loss = 2.04 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:19.883870: step 109650, loss = 1.86 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:27:21.073035: step 109660, loss = 1.87 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:27:22.271492: step 109670, loss = 1.89 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:27:23.477626: step 109680, loss = 1.99 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:27:24.684866: step 109690, loss = 1.91 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:27:25.862670: step 109700, loss = 2.04 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:27.049104: step 109710, loss = 1.93 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:27:28.232868: step 109720, loss = 1.89 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:29.425130: step 109730, loss = 2.01 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:27:30.600498: step 109740, loss = 2.07 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:31.785844: step 109750, loss = 1.93 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:27:32.964652: step 109760, loss = 1.93 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:34.126256: step 109770, loss = 1.84 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:35.283377: step 109780, loss = 1.93 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:36.471267: step 109790, loss = 1.93 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:27:37.655394: step 109800, loss = 1.89 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:38.830962: step 109810, loss = 1.99 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:27:39.990766: step 109820, loss = 2.01 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:41.149324: step 109830, loss = 1.99 (1104.8 examples/sec; 0.116 seE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1132 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
c/batch)
2017-05-05 01:27:42.317104: step 109840, loss = 1.82 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:27:43.480192: step 109850, loss = 2.04 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:44.634797: step 109860, loss = 1.91 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:27:45.783112: step 109870, loss = 1.91 (1114.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:27:46.972463: step 109880, loss = 1.94 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:27:48.215815: step 109890, loss = 1.83 (1029.5 examples/sec; 0.124 sec/batch)
2017-05-05 01:27:49.304601: step 109900, loss = 1.84 (1175.6 examples/sec; 0.109 sec/batch)
2017-05-05 01:27:50.461452: step 109910, loss = 2.06 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:27:51.627586: step 109920, loss = 1.81 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:27:52.797362: step 109930, loss = 1.91 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:27:54.004053: step 109940, loss = 1.90 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:27:55.175785: step 109950, loss = 1.82 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:27:56.369681: step 109960, loss = 1.88 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:27:57.544570: step 109970, loss = 1.92 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:27:58.738558: step 109980, loss = 2.00 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:27:59.919896: step 109990, loss = 1.89 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:01.095182: step 110000, loss = 1.95 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:02.281985: step 110010, loss = 2.09 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:28:03.451660: step 110020, loss = 1.90 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:04.609341: step 110030, loss = 1.81 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:05.764830: step 110040, loss = 1.94 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:06.934485: step 110050, loss = 1.99 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:08.090500: step 110060, loss = 1.90 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:09.252745: step 110070, loss = 1.80 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:10.415768: step 110080, loss = 1.78 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:11.575966: step 110090, loss = 1.93 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:12.738465: step 110100, loss = 1.73 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:13.905133: step 110110, loss = 1.98 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:15.074074: step 110120, loss = 1.82 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:16.243506: step 110130, loss = 2.03 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:17.406909: step 110140, loss = 1.82 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:18.567333: step 110150, loss = 1.82 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:19.732943: step 110160, loss = 2.08 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:20.902527: step 110170, loss = 2.06 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:22.060764: step 110180, loss = 1.81 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:23.234728: step 110190, loss = 1.91 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:24.399005: step 110200, loss = 2.07 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:25.559630: step 110210, loss = 1.91 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:26.721496: step 110220, loss = 1.92 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:27.916453: step 110230, loss = 1.88 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:28:29.099176: step 110240, loss = 1.96 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:30.279294: step 110250, loss = 1.92 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:31.478791: step 110260, loss = 2.12 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:28:32.664738: step 110270, loss = 1.97 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:28:33.834893: step 110280, loss = 1.88 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:35.006385: step 110290, loss = 1.87 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:36.200626: step 110300, loss = 1.92 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:28:37.380900: step 110310, loss = 2.01 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:38.555196: step 110320, loss = 1.95 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:39.743941: step 110330, loss = 1.81 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:28:40.947392: step 110340, loss = 1.76 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:28:42.127994: step 110350, loss = 1.99 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:43.317622: step 110360, loss = 1.95 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:28:44.521699: step 110370, loss = 1.90 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:28:45.737675: step 110380, loss = 1.96 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:28:46.931593: step 110390, loss = 2.07 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:28:48.121010: step 110400, loss = 2.08 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:28:49.309564: step 110410, loss = 1.88 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:28:50.491309: step 110420, loss = 1.76 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:51.701845: step 110430, loss = 1.98 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:28:52.883339: step 110440, loss = 1.97 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:54.066845: step 110450, loss = 1.87 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:28:55.219133: step 110460, loss = 1.91 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:28:56.375213: step 110470, loss = 1.96 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:57.549906: step 110480, loss = 1.95 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:28:58.712056: step 110490, loss = 1.84 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:28:59.863827: step 110500, loss = 1.87 (1111.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:29:01.039733: step 110510, loss = 1.91 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:29:02.203645: step 110520, loss = 1.84 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:29:03.366826: step 110530, loss = 1.82 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:29:04.536142: step 110540, loss = 2.03 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:29:05.702084: step 110550, loss = 1.87 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:29:06.868378: step 110560, loss = 1.95 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:29:08.041739: step 110570, loss = 2.08 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:29:09.200439: step 110580, loss = 1.80 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:29:10.350638: step 110590, loss = 1.94 (1112.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:29:11.523398: step 110600, loss = 1.78 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:29:12.694681: step 110610, loss = 1.77 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:29:13.851377: step 110620, loss = 1.88 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:29:15.018265: step 110630, loss = 1.97 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:29:16.198930: step 110640, loss = 1.83 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:29:17.362678: step 110650, loss = 1.95 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:29:18.533425: step 110660, loss = 1.89 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:29:19.716849: step 110670, loss = 1.98 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:29:20.896994: step 110680, loss = 1.97 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:29:22.062477: step 110690, loss = 1.83 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:29:23.250830: step 110700, loss = 1.86 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:29:24.449125: step 110710, loss = 1.81 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:29:25.628933: step 110720, loss = 1.75 (1084.9 examples/sec; 0.118 sec/baE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1144 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
tch)
2017-05-05 01:29:26.847375: step 110730, loss = 1.84 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:29:28.060415: step 110740, loss = 1.93 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:29:29.267394: step 110750, loss = 1.88 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:29:30.463909: step 110760, loss = 1.87 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:29:31.663198: step 110770, loss = 1.74 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:29:32.858585: step 110780, loss = 1.89 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:29:34.067437: step 110790, loss = 1.86 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:29:35.263220: step 110800, loss = 2.05 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:29:36.463893: step 110810, loss = 1.88 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:29:37.662215: step 110820, loss = 1.90 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:29:38.868572: step 110830, loss = 1.99 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:29:40.094889: step 110840, loss = 1.91 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:29:41.323734: step 110850, loss = 1.83 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:29:42.515038: step 110860, loss = 2.07 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:29:43.748183: step 110870, loss = 2.11 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:29:45.050945: step 110880, loss = 1.91 (982.5 examples/sec; 0.130 sec/batch)
2017-05-05 01:29:46.140621: step 110890, loss = 1.96 (1174.7 examples/sec; 0.109 sec/batch)
2017-05-05 01:29:47.361272: step 110900, loss = 2.08 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:29:48.582218: step 110910, loss = 1.84 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:29:49.785603: step 110920, loss = 1.90 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:29:50.996784: step 110930, loss = 1.86 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:29:52.230608: step 110940, loss = 1.76 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:29:53.452633: step 110950, loss = 2.01 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:29:54.691567: step 110960, loss = 1.86 (1033.1 examples/sec; 0.124 sec/batch)
2017-05-05 01:29:55.938831: step 110970, loss = 1.77 (1026.2 examples/sec; 0.125 sec/batch)
2017-05-05 01:29:57.172842: step 110980, loss = 1.89 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-05 01:29:58.373209: step 110990, loss = 2.01 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:29:59.585227: step 111000, loss = 1.92 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:00.812600: step 111010, loss = 1.85 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:30:02.046794: step 111020, loss = 2.01 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:30:03.267576: step 111030, loss = 1.85 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:04.482369: step 111040, loss = 2.11 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:05.672730: step 111050, loss = 1.75 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:30:06.896446: step 111060, loss = 1.94 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:08.116086: step 111070, loss = 1.86 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:09.294440: step 111080, loss = 1.89 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:30:10.491821: step 111090, loss = 1.98 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:30:11.712589: step 111100, loss = 1.99 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:12.941486: step 111110, loss = 2.10 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:30:14.152371: step 111120, loss = 1.95 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:15.381764: step 111130, loss = 1.90 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:30:16.595846: step 111140, loss = 1.93 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:17.792709: step 111150, loss = 1.98 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:30:19.015997: step 111160, loss = 2.01 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:20.229634: step 111170, loss = 2.12 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:21.448463: step 111180, loss = 1.94 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:22.668044: step 111190, loss = 1.78 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:23.876998: step 111200, loss = 1.93 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:25.091700: step 111210, loss = 1.87 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:26.279982: step 111220, loss = 2.01 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:30:27.501930: step 111230, loss = 1.98 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:28.718302: step 111240, loss = 1.86 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:29.910390: step 111250, loss = 1.93 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:30:31.132513: step 111260, loss = 1.96 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:32.359646: step 111270, loss = 2.00 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:30:33.533994: step 111280, loss = 2.21 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:30:34.727834: step 111290, loss = 1.84 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:30:35.947259: step 111300, loss = 1.88 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:37.161005: step 111310, loss = 1.84 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:38.358085: step 111320, loss = 2.03 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:30:39.589054: step 111330, loss = 1.92 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:30:40.798834: step 111340, loss = 1.91 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:42.012437: step 111350, loss = 2.05 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:43.256720: step 111360, loss = 1.90 (1028.7 examples/sec; 0.124 sec/batch)
2017-05-05 01:30:44.478885: step 111370, loss = 1.91 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:45.692566: step 111380, loss = 2.05 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:46.909714: step 111390, loss = 1.90 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:48.135355: step 111400, loss = 2.06 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:30:49.357071: step 111410, loss = 1.97 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:50.558998: step 111420, loss = 1.99 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:30:51.772878: step 111430, loss = 1.69 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:52.987348: step 111440, loss = 1.95 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:54.207541: step 111450, loss = 1.89 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:30:55.421826: step 111460, loss = 1.92 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:56.634331: step 111470, loss = 1.87 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:30:57.829955: step 111480, loss = 1.90 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:30:59.040338: step 111490, loss = 2.04 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:00.259278: step 111500, loss = 1.93 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:31:01.473913: step 111510, loss = 1.95 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:02.702748: step 111520, loss = 2.14 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:31:03.923845: step 111530, loss = 2.00 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:31:05.143855: step 111540, loss = 2.00 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:31:06.360313: step 111550, loss = 1.94 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:31:07.573110: step 111560, loss = 2.00 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:08.801836: step 111570, loss = 1.89 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:31:10.007623: step 111580, loss = 1.77 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:11.227666: step 111590, loss = 1.77 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:31:12.444139: step 111600, loss = 1.81 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:31:13.647785: step 111610, loss = 1.91 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:31:14.871979: step 111620, loss = 1.94 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:31:16.121777: step 111630, loss = 1.93 (1024.2 examples/sec; 0.125 sec/batch)
2017-05-05 01:31:17.331845: step 111640, loss = 1.96 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:18.541385: step 111650, loss = 1.88 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:19.749037: step 111660, loss = 1.93 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:20.974214: step 111670, loss = 1.98 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:31:22.157035: step 111680, loss = 1.93 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:31:23.373846: step 111690, loss = 1.92 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:31:24.579755: step 111700, loss = 2.10 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:25.786757: step 111710, loss = 1.78 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:27.003985: step 111720, loss = 2.03 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:31:28.217058: step 111730, loss = 1.96 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:31:29.410220: step 111740, loss = 1.92 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:31:30.605547: step 111750, loss = 1.86 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:31:31.797053: step 111760, loss = 1.95 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:31:32.978081: step 111770, loss = 1.96 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:31:34.175652: step 111780, loss = 2.11 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:31:35.391429: step 111790, loss = 1.85 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:31:36.595630: step 111800, loss = 1.99 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:31:37.756355: step 111810, loss = 1.90 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:31:38.929413: step 111820, loss = 1.83 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:31:40.089341: step 111830, loss = 1.81 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:31:41.249872: step 111840, loss = 1.75 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:31:42.405084: step 111850, loss = 1.93 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:31:43.560157: step 111860, loss = 1.87 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:31:44.805797: step 111870, loss = 1.93 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-05 01:31:45.863674: step 111880, loss = 1.91 (1210.0 examples/sec; 0.106 sec/batch)
2017-05-05 01:31:47.052483: step 111890, loss = 1.90 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:31:48.235673: step 111900, loss = 1.82 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:31:49.416937: step 111910, loss = 1.76 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:31:50.576656: step 111920, loss = 1.95 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:31:51.754791: step 111930, loss = 2.07 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:31:52.915865: step 111940, loss = 1.93 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:31:54.078589: step 111950, loss = 1.83 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:31:55.243161: step 111960, loss = 1.89 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:31:56.408263: step 111970, loss = 2.01 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:31:57.561876: step 111980, loss = 1.96 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:31:58.726843: step 111990, loss = 1.93 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:31:59.903472: step 112000, loss = 1.95 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:01.078611: step 112010, loss = 2.00 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:02.246963: step 112020, loss = 1.87 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:03.409921: step 112030, loss = 2.02 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:32:04.564341: step 112040, loss = 2.01 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:32:05.707194: step 112050, loss = 1.91 (1120.0 examples/sec; 0.114 sec/batch)
2017-05-05 01:32:06.889516: step 112060, loss = E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1155 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
1.79 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:08.057644: step 112070, loss = 1.89 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:09.228595: step 112080, loss = 1.84 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:10.374412: step 112090, loss = 1.83 (1117.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:32:11.538968: step 112100, loss = 1.95 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:32:12.696196: step 112110, loss = 1.82 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:32:13.855104: step 112120, loss = 1.89 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:32:15.023173: step 112130, loss = 2.00 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:16.179921: step 112140, loss = 1.83 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:32:17.339549: step 112150, loss = 1.95 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:32:18.494758: step 112160, loss = 2.01 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:32:19.648816: step 112170, loss = 1.83 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:32:20.831323: step 112180, loss = 1.94 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:21.999133: step 112190, loss = 1.76 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:23.147117: step 112200, loss = 1.95 (1115.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:32:24.307793: step 112210, loss = 1.92 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:32:25.459713: step 112220, loss = 1.99 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:32:26.601088: step 112230, loss = 1.90 (1121.5 examples/sec; 0.114 sec/batch)
2017-05-05 01:32:27.756977: step 112240, loss = 1.97 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:32:28.934977: step 112250, loss = 1.87 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:30.104061: step 112260, loss = 1.92 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:31.285880: step 112270, loss = 1.94 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:32.483862: step 112280, loss = 1.86 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:32:33.694292: step 112290, loss = 1.84 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:32:34.903582: step 112300, loss = 1.92 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:32:36.105884: step 112310, loss = 1.97 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:32:37.318784: step 112320, loss = 1.89 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:32:38.507157: step 112330, loss = 1.90 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:32:39.699597: step 112340, loss = 1.90 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:32:40.890551: step 112350, loss = 1.75 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:32:42.067905: step 112360, loss = 1.92 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:43.253603: step 112370, loss = 1.78 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:32:44.424406: step 112380, loss = 2.06 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:45.584037: step 112390, loss = 1.89 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:32:46.759974: step 112400, loss = 2.03 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:47.945107: step 112410, loss = 1.87 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:32:49.116181: step 112420, loss = 2.09 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:50.281855: step 112430, loss = 1.97 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:51.497001: step 112440, loss = 1.85 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:32:52.679461: step 112450, loss = 1.97 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:53.848174: step 112460, loss = 1.87 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:32:55.048047: step 112470, loss = 1.95 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:32:56.250507: step 112480, loss = 1.96 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:32:57.432770: step 112490, loss = 1.87 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:58.615806: step 112500, loss = 1.91 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:32:59.794709: step 112510, loss = 1.82 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:33:01.006611: step 112520, loss = 2.19 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:02.207219: step 112530, loss = 1.82 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:33:03.445937: step 112540, loss = 1.86 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-05 01:33:04.656830: step 112550, loss = 1.90 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:05.858873: step 112560, loss = 2.07 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:33:07.090355: step 112570, loss = 1.89 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:33:08.298277: step 112580, loss = 2.00 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:09.476498: step 112590, loss = 1.85 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:33:10.645206: step 112600, loss = 1.90 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:33:11.810979: step 112610, loss = 1.89 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:33:12.986160: step 112620, loss = 1.84 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:33:14.137702: step 112630, loss = 1.76 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:33:15.315616: step 112640, loss = 1.75 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:33:16.508066: step 112650, loss = 1.94 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:33:17.683437: step 112660, loss = 1.79 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:33:18.879889: step 112670, loss = 2.01 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:33:20.082163: step 112680, loss = 1.95 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:33:21.291990: step 112690, loss = 1.89 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:22.521920: step 112700, loss = 1.93 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:33:23.725451: step 112710, loss = 2.00 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:33:24.932081: step 112720, loss = 1.83 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:26.126889: step 112730, loss = 1.91 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:33:27.350999: step 112740, loss = 1.84 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:28.559593: step 112750, loss = 1.91 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:29.775016: step 112760, loss = 1.87 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:30.982335: step 112770, loss = 1.84 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:32.196181: step 112780, loss = 1.88 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:33.392626: step 112790, loss = 1.84 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:33:34.589604: step 112800, loss = 1.83 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:33:35.788469: step 112810, loss = 1.92 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:33:37.002875: step 112820, loss = 2.03 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:38.210862: step 112830, loss = 2.11 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:39.426046: step 112840, loss = 2.08 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:40.630535: step 112850, loss = 1.89 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:33:41.929106: step 112860, loss = 1.90 (985.7 examples/sec; 0.130 sec/batch)
2017-05-05 01:33:43.030172: step 112870, loss = 1.89 (1162.5 examples/sec; 0.110 sec/batch)
2017-05-05 01:33:44.258976: step 112880, loss = 1.86 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:33:45.465125: step 112890, loss = 2.02 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:46.682904: step 112900, loss = 1.86 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:47.922401: step 112910, loss = 1.89 (1032.7 examples/sec; 0.124 sec/batch)
2017-05-05 01:33:49.101168: step 112920, loss = 1.99 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:33:50.337154: step 112930, loss = 1.93 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-05 01:33:51.561283: step 112940, loss = 1.81 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:52.779788: step 112950, loss = 1.93 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1166 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
(1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:33:53.991486: step 112960, loss = 1.90 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:55.206479: step 112970, loss = 1.91 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:56.416997: step 112980, loss = 1.94 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:57.625682: step 112990, loss = 1.83 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:33:58.845721: step 113000, loss = 1.89 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:34:00.055873: step 113010, loss = 1.97 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:01.270941: step 113020, loss = 1.85 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:34:02.448141: step 113030, loss = 1.96 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:34:03.648592: step 113040, loss = 1.93 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:34:04.860440: step 113050, loss = 1.80 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:06.074874: step 113060, loss = 1.89 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:07.224875: step 113070, loss = 2.05 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:34:08.414998: step 113080, loss = 1.84 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:34:09.592134: step 113090, loss = 1.82 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:34:10.793095: step 113100, loss = 1.86 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:34:11.971938: step 113110, loss = 1.63 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:34:13.187363: step 113120, loss = 1.91 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:34:14.385317: step 113130, loss = 1.87 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:34:15.593428: step 113140, loss = 2.00 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:16.807730: step 113150, loss = 2.03 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:18.008651: step 113160, loss = 2.03 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:34:19.212784: step 113170, loss = 1.97 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:34:20.427879: step 113180, loss = 1.96 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:34:21.635297: step 113190, loss = 1.86 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:22.845036: step 113200, loss = 1.80 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:24.083191: step 113210, loss = 2.00 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-05 01:34:25.289378: step 113220, loss = 2.00 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:26.502528: step 113230, loss = 2.02 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:27.702834: step 113240, loss = 1.88 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:34:28.930457: step 113250, loss = 1.89 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:34:30.115462: step 113260, loss = 2.05 (1080.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:34:31.319400: step 113270, loss = 1.96 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:34:32.503642: step 113280, loss = 1.92 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:34:33.708810: step 113290, loss = 2.07 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:34:34.904750: step 113300, loss = 1.83 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:34:36.089287: step 113310, loss = 1.85 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:34:37.282425: step 113320, loss = 2.00 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:34:38.456569: step 113330, loss = 1.88 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:34:39.656781: step 113340, loss = 1.99 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:34:40.842417: step 113350, loss = 2.17 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:34:42.012872: step 113360, loss = 1.82 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:34:43.165288: step 113370, loss = 1.96 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:34:44.320809: step 113380, loss = 1.88 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:34:45.474081: step 113390, loss = 1.79 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:34:46.626457: step 113400, loss = 2.02 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:34:47.796381: step 113410, loss = 1.89 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:34:48.963181: step 113420, loss = 1.88 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:34:50.132815: step 113430, loss = 1.85 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:34:51.321280: step 113440, loss = 1.98 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:34:52.502117: step 113450, loss = 2.08 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:34:53.663407: step 113460, loss = 1.93 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:34:54.842939: step 113470, loss = 1.77 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:34:56.002542: step 113480, loss = 1.80 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:34:57.180357: step 113490, loss = 1.89 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:34:58.344971: step 113500, loss = 2.02 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:34:59.522805: step 113510, loss = 1.89 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:00.704767: step 113520, loss = 1.86 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:01.869725: step 113530, loss = 1.90 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:03.036594: step 113540, loss = 1.80 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:04.199288: step 113550, loss = 1.89 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:05.368539: step 113560, loss = 1.80 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:06.530390: step 113570, loss = 1.87 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:07.699326: step 113580, loss = 1.88 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:08.875896: step 113590, loss = 1.84 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:10.026200: step 113600, loss = 2.03 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:35:11.196168: step 113610, loss = 1.99 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:12.388612: step 113620, loss = 1.86 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:35:13.555175: step 113630, loss = 1.83 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:14.718762: step 113640, loss = 1.73 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:15.894379: step 113650, loss = 1.86 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:17.053335: step 113660, loss = 1.83 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:18.212986: step 113670, loss = 1.96 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:19.378348: step 113680, loss = 1.93 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:20.556511: step 113690, loss = 1.79 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:21.720979: step 113700, loss = 1.87 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:22.914331: step 113710, loss = 1.87 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:35:24.089126: step 113720, loss = 1.89 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:25.271337: step 113730, loss = 1.89 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:26.469129: step 113740, loss = 1.83 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:35:27.657065: step 113750, loss = 1.79 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:35:28.824762: step 113760, loss = 1.86 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:29.964459: step 113770, loss = 1.95 (1123.1 examples/sec; 0.114 sec/batch)
2017-05-05 01:35:31.129406: step 113780, loss = 1.81 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:32.288913: step 113790, loss = 1.89 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:33.425477: step 113800, loss = 1.85 (1126.2 examples/sec; 0.114 sec/batch)
2017-05-05 01:35:34.596042: step 113810, loss = 1.84 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:35.744586: step 113820, loss = 1.97 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:35:36.914576: step 113830, loss = 1.89 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:38.072954: step 113840, loss = 2.05 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:39.404538: step 113850, loss = 2.06 (961.3 examples/sec; 0.133 sec/batch)
2017-05-05 01:35:40.436905: step 113860, loss = 1.91 (1239.9 examples/sec; 0.103 sec/batch)
2017-05-05 01:35:41.609374: step 113870, loss = 1.87 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:42.772221: step 113880, loss = 1.73 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:43.925210: step 113890, loss = 1.89 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:35:45.098312: step 113900, loss = 1.85 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:46.254509: step 113910, loss = 1.95 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:47.427943: step 113920, loss = 1.75 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:48.599774: step 113930, loss = 1.97 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:49.774162: step 113940, loss = 1.83 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:50.947168: step 113950, loss = 1.92 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:52.127444: step 113960, loss = 2.05 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:53.311473: step 113970, loss = 2.03 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:54.469266: step 113980, loss = 1.97 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:35:55.656087: step 113990, loss = 1.74 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:35:56.840546: step 114000, loss = 1.95 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:35:58.007956: step 114010, loss = 1.82 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:35:59.206647: step 114020, loss = 1.79 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:36:00.367818: step 114030, loss = 1.85 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:01.535791: step 114040, loss = 1.86 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:02.695731: step 114050, loss = 1.77 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:03.877111: step 114060, loss = 1.83 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:36:05.029170: step 114070, loss = 1.99 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:36:06.183511: step 114080, loss = 1.84 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:36:07.349514: step 114090, loss = 2.03 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:08.514158: step 114100, loss = 1.92 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:09.669624: step 114110, loss = 1.95 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:10.836884: step 114120, loss = 1.96 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:12.035565: step 114130, loss = 2.00 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:36:13.193970: step 114140, loss = 2.02 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:14.361604: step 114150, loss = 1.96 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:15.530057: step 114160, loss = 1.82 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:16.690523: step 114170, loss = 1.83 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:17.833538: step 114180, loss = 2.07 (1119.9 examples/sec; 0.114 sec/batch)
2017-05-05 01:36:19.003838: step 114190, loss = 1.99 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:20.178286: step 114200, loss = 1.90 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:21.332517: step 114210, loss = 1.82 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:36:22.485377: step 114220, loss = 1.94 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:36:23.634734: step 114230, loss = 1.96 (1113.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:36:24.795159: step 114240, loss = 1.78 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:25.955476: step 114250, loss = 1.95 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:27.132330: step 114260, loss = 2.00 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:36:28.326216: step 114270, loss = 2.02 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:36:29.493427: step 114280, loss = 1.85 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1177 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
36:30.661026: step 114290, loss = 1.84 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:31.819102: step 114300, loss = 1.90 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:32.994060: step 114310, loss = 2.11 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:34.146900: step 114320, loss = 1.92 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:36:35.320326: step 114330, loss = 1.78 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:36.509050: step 114340, loss = 2.02 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:36:37.666927: step 114350, loss = 2.04 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:38.840996: step 114360, loss = 1.81 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:40.003663: step 114370, loss = 1.81 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:41.174007: step 114380, loss = 1.84 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:36:42.337917: step 114390, loss = 2.07 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:43.529625: step 114400, loss = 2.06 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:36:44.710281: step 114410, loss = 1.86 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:36:45.872475: step 114420, loss = 1.96 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:36:47.053022: step 114430, loss = 1.92 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:36:48.235951: step 114440, loss = 1.81 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:36:49.422711: step 114450, loss = 1.86 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:36:50.614174: step 114460, loss = 1.90 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:36:51.813585: step 114470, loss = 1.85 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:36:53.015644: step 114480, loss = 1.75 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:36:54.207177: step 114490, loss = 1.71 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:36:55.422593: step 114500, loss = 1.86 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:36:56.665468: step 114510, loss = 2.03 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-05 01:36:57.875205: step 114520, loss = 1.92 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:36:59.076867: step 114530, loss = 1.83 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:37:00.272909: step 114540, loss = 1.91 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:37:01.484703: step 114550, loss = 2.06 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:37:02.674788: step 114560, loss = 1.85 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:37:03.858397: step 114570, loss = 1.91 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:05.047483: step 114580, loss = 1.81 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:37:06.210764: step 114590, loss = 1.85 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:37:07.390364: step 114600, loss = 1.89 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:08.562762: step 114610, loss = 1.79 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:09.720925: step 114620, loss = 1.95 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:37:10.886518: step 114630, loss = 1.86 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:12.038951: step 114640, loss = 1.97 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:37:13.209332: step 114650, loss = 1.82 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:14.360138: step 114660, loss = 1.93 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:37:15.557792: step 114670, loss = 1.96 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:37:16.701901: step 114680, loss = 1.86 (1118.8 examples/sec; 0.114 sec/batch)
2017-05-05 01:37:17.871956: step 114690, loss = 1.86 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:19.043754: step 114700, loss = 1.68 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:20.221155: step 114710, loss = 1.86 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:21.389883: step 114720, loss = 2.00 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:22.560729: step 114730, loss = 1.83 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:23.719812: step 114740, loss = 1.89 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:37:24.886103: step 114750, loss = 1.75 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:26.034490: step 114760, loss = 1.92 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:37:27.208184: step 114770, loss = 1.86 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:28.375742: step 114780, loss = 1.99 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:29.534426: step 114790, loss = 1.91 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:37:30.712698: step 114800, loss = 1.84 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:31.869532: step 114810, loss = 1.89 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:37:33.018126: step 114820, loss = 1.86 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:37:34.185830: step 114830, loss = 1.74 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:35.455071: step 114840, loss = 1.90 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-05 01:37:36.546907: step 114850, loss = 1.90 (1172.3 examples/sec; 0.109 sec/batch)
2017-05-05 01:37:37.697827: step 114860, loss = 1.91 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:37:38.869337: step 114870, loss = 1.92 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:40.042984: step 114880, loss = 1.76 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:41.224822: step 114890, loss = 2.05 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:42.379676: step 114900, loss = 2.09 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:37:43.565657: step 114910, loss = 1.99 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:37:44.742053: step 114920, loss = 1.90 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:45.905049: step 114930, loss = 1.80 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:37:47.082136: step 114940, loss = 1.83 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:48.252836: step 114950, loss = 1.86 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:49.418802: step 114960, loss = 1.87 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:50.582303: step 114970, loss = 1.88 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:37:51.749110: step 114980, loss = 1.85 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:52.927060: step 114990, loss = 1.87 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:54.080041: step 115000, loss = 1.95 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:37:55.268983: step 115010, loss = 2.03 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:37:56.444888: step 115020, loss = 1.98 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:37:57.601708: step 115030, loss = 1.72 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:37:58.770710: step 115040, loss = 2.14 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:37:59.953451: step 115050, loss = 1.99 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:01.125592: step 115060, loss = 1.85 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:38:02.324473: step 115070, loss = 1.74 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:38:03.502631: step 115080, loss = 1.88 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:04.683917: step 115090, loss = 1.87 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:05.862952: step 115100, loss = 1.80 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:07.055994: step 115110, loss = 2.02 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:38:08.260563: step 115120, loss = 1.87 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:38:09.457568: step 115130, loss = 1.85 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:38:10.649444: step 115140, loss = 1.97 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:38:11.860584: step 115150, loss = 2.03 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:38:13.040491: step 115160, loss = 1.82 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:14.203020: step 115170, loss = 1.93 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:38:1E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1189 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
5.372148: step 115180, loss = 1.83 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:38:16.554389: step 115190, loss = 1.97 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:17.716960: step 115200, loss = 1.82 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:38:18.880852: step 115210, loss = 1.85 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:38:20.045469: step 115220, loss = 1.95 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:38:21.201866: step 115230, loss = 1.94 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:38:22.360454: step 115240, loss = 1.86 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:38:23.545335: step 115250, loss = 1.88 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:24.715784: step 115260, loss = 2.00 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:38:25.891686: step 115270, loss = 1.85 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:27.078499: step 115280, loss = 2.01 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:38:28.252439: step 115290, loss = 1.92 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:38:29.429323: step 115300, loss = 1.88 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:30.631390: step 115310, loss = 1.84 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:38:31.807862: step 115320, loss = 1.77 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:32.987564: step 115330, loss = 1.90 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:34.154999: step 115340, loss = 2.04 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:38:35.339838: step 115350, loss = 1.90 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:38:36.583862: step 115360, loss = 1.83 (1028.9 examples/sec; 0.124 sec/batch)
2017-05-05 01:38:37.804265: step 115370, loss = 1.87 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:38:39.031548: step 115380, loss = 1.83 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:38:40.257275: step 115390, loss = 1.84 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-05 01:38:41.450366: step 115400, loss = 1.84 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:38:42.660490: step 115410, loss = 1.88 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:38:43.859823: step 115420, loss = 1.97 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:38:45.085904: step 115430, loss = 1.93 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:38:46.291392: step 115440, loss = 1.99 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:38:47.516365: step 115450, loss = 1.96 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:38:48.724982: step 115460, loss = 1.83 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:38:49.939133: step 115470, loss = 1.95 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:38:51.161188: step 115480, loss = 1.82 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:38:52.373457: step 115490, loss = 1.86 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:38:53.579541: step 115500, loss = 1.86 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:38:54.785894: step 115510, loss = 1.95 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:38:55.999596: step 115520, loss = 1.85 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:38:57.213341: step 115530, loss = 1.83 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:38:58.412310: step 115540, loss = 1.82 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:38:59.620287: step 115550, loss = 1.78 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:39:00.838276: step 115560, loss = 1.85 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:39:02.036173: step 115570, loss = 1.88 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:39:03.262225: step 115580, loss = 1.94 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:39:04.467362: step 115590, loss = 1.88 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:39:05.658306: step 115600, loss = 1.86 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:39:06.836547: step 115610, loss = 1.81 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:08.036427: step 115620, loss = 1.91 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:39:09.241900: step 115630, loss = 1.93 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:39:10.422001: step 115640, loss = 1.81 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:11.601808: step 115650, loss = 1.90 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:12.779749: step 115660, loss = 2.03 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:13.933755: step 115670, loss = 1.81 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:39:15.114063: step 115680, loss = 1.99 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:16.293864: step 115690, loss = 1.94 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:17.466433: step 115700, loss = 1.81 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:18.636451: step 115710, loss = 1.65 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:19.789481: step 115720, loss = 1.85 (1110.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:39:20.958704: step 115730, loss = 2.09 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:22.123987: step 115740, loss = 2.02 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:23.312431: step 115750, loss = 1.83 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:39:24.486652: step 115760, loss = 2.00 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:25.667503: step 115770, loss = 1.93 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:26.861070: step 115780, loss = 1.91 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:39:28.049153: step 115790, loss = 1.85 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:39:29.245807: step 115800, loss = 1.83 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:39:30.419080: step 115810, loss = 1.78 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:31.612911: step 115820, loss = 1.79 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:39:32.894551: step 115830, loss = 2.04 (998.7 examples/sec; 0.128 sec/batch)
2017-05-05 01:39:34.002782: step 115840, loss = 1.89 (1155.0 examples/sec; 0.111 sec/batch)
2017-05-05 01:39:35.241094: step 115850, loss = 1.90 (1033.7 examples/sec; 0.124 sec/batch)
2017-05-05 01:39:36.458216: step 115860, loss = 1.94 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:39:37.646638: step 115870, loss = 1.80 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:39:38.847648: step 115880, loss = 1.88 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:39:40.044885: step 115890, loss = 1.92 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:39:41.243434: step 115900, loss = 1.84 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:39:42.419209: step 115910, loss = 1.88 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:43.601169: step 115920, loss = 1.89 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:44.782309: step 115930, loss = 2.20 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:45.928573: step 115940, loss = 2.18 (1116.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:39:47.093086: step 115950, loss = 1.88 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:39:48.261609: step 115960, loss = 1.89 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:49.431204: step 115970, loss = 1.86 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:50.588277: step 115980, loss = 1.77 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:39:51.748951: step 115990, loss = 1.96 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:39:52.932790: step 116000, loss = 1.92 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:54.106153: step 116010, loss = 1.93 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:39:55.281376: step 116020, loss = 1.83 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:56.438516: step 116030, loss = 1.89 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:39:57.614529: step 116040, loss = 1.77 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:58.795552: step 116050, loss = 1.93 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:39:59.984459: step 116060, loss = 2.03 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:40:01.173682: step 116070, loss = 1.88 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:40:02.385380: step 116080, loss = 1.85 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:40:03.568262: step 116090, loss = 1.74 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:04.745489: step 116100, loss = 1.96 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:05.897543: step 116110, loss = 1.92 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:40:07.073763: step 116120, loss = 1.95 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:08.225450: step 116130, loss = 1.85 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:40:09.404575: step 116140, loss = 1.82 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:10.587210: step 116150, loss = 1.98 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:11.756360: step 116160, loss = 1.88 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:40:12.916651: step 116170, loss = 1.93 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:40:14.072953: step 116180, loss = 2.00 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:40:15.237930: step 116190, loss = 2.00 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:40:16.407253: step 116200, loss = 1.86 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:40:17.585357: step 116210, loss = 1.93 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:18.750563: step 116220, loss = 1.84 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:40:19.909670: step 116230, loss = 1.79 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:40:21.086512: step 116240, loss = 1.84 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:22.225678: step 116250, loss = 1.94 (1123.6 examples/sec; 0.114 sec/batch)
2017-05-05 01:40:23.399838: step 116260, loss = 1.89 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:40:24.561440: step 116270, loss = 1.77 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:40:25.715484: step 116280, loss = 1.81 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:40:26.865713: step 116290, loss = 1.91 (1112.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:40:28.030992: step 116300, loss = 1.86 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:40:29.199422: step 116310, loss = 1.93 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:40:30.356318: step 116320, loss = 1.87 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:40:31.531925: step 116330, loss = 1.94 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:32.722576: step 116340, loss = 1.99 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:40:33.887794: step 116350, loss = 1.76 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:40:35.072535: step 116360, loss = 1.86 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:36.254209: step 116370, loss = 1.90 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:40:37.412587: step 116380, loss = 1.89 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:40:38.568020: step 116390, loss = 1.81 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:40:39.771625: step 116400, loss = 1.86 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:40:40.959283: step 116410, loss = 1.93 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:40:42.148635: step 116420, loss = 1.99 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:40:43.334439: step 116430, loss = 1.84 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:40:44.535530: step 116440, loss = 1.82 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:40:45.727200: step 116450, loss = 1.86 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:40:46.930574: step 116460, loss = 1.84 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:40:48.132873: step 116470, loss = 1.93 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:40:49.330581: step 116480, loss = 1.99 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:40:50.542965: step 116490, loss = 1.86 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:40:51.777257: step 116500, loss = 1.76 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-05 01:40:52.972958: step 116510, loss = 2.07 (1070.5 examples/sE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1200 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
ec; 0.120 sec/batch)
2017-05-05 01:40:54.188548: step 116520, loss = 1.88 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:40:55.380219: step 116530, loss = 1.88 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:40:56.607523: step 116540, loss = 1.85 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:40:57.796871: step 116550, loss = 2.03 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:40:59.003262: step 116560, loss = 1.83 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:41:00.207525: step 116570, loss = 1.97 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:01.396041: step 116580, loss = 1.96 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:41:02.584426: step 116590, loss = 1.92 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:41:03.792678: step 116600, loss = 1.93 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:41:04.999401: step 116610, loss = 1.81 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:41:06.219386: step 116620, loss = 1.84 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:41:07.443497: step 116630, loss = 1.80 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:41:08.664426: step 116640, loss = 1.93 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:41:09.870733: step 116650, loss = 1.82 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:41:11.073437: step 116660, loss = 1.97 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:12.309343: step 116670, loss = 1.80 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-05 01:41:13.513707: step 116680, loss = 1.96 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:14.763550: step 116690, loss = 1.79 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-05 01:41:15.994602: step 116700, loss = 1.96 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:41:17.219494: step 116710, loss = 2.05 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:41:18.418525: step 116720, loss = 1.93 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:19.634450: step 116730, loss = 1.96 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:41:20.854830: step 116740, loss = 1.92 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:41:22.069411: step 116750, loss = 1.88 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:41:23.304821: step 116760, loss = 1.95 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-05 01:41:24.544353: step 116770, loss = 1.75 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-05 01:41:25.740913: step 116780, loss = 1.84 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:26.980463: step 116790, loss = 2.00 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-05 01:41:28.197147: step 116800, loss = 1.90 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:41:29.417093: step 116810, loss = 1.99 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:41:30.729520: step 116820, loss = 1.91 (975.3 examples/sec; 0.131 sec/batch)
2017-05-05 01:41:31.812810: step 116830, loss = 1.93 (1181.6 examples/sec; 0.108 sec/batch)
2017-05-05 01:41:33.026817: step 116840, loss = 2.16 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:41:34.259637: step 116850, loss = 1.89 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-05 01:41:35.460970: step 116860, loss = 1.80 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:36.650030: step 116870, loss = 1.83 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:41:37.835945: step 116880, loss = 1.95 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:41:39.039220: step 116890, loss = 1.85 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:40.248474: step 116900, loss = 2.13 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:41:41.454096: step 116910, loss = 2.05 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:41:42.623037: step 116920, loss = 1.86 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:41:43.825198: step 116930, loss = 1.94 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:45.014268: step 116940, loss = 2.00 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:41:46.208130: step 116950, loss = 1.97 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:41:47.393076: step 116960, loss = 1.98 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:41:48.590622: step 116970, loss = 1.79 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:49.784180: step 116980, loss = 1.95 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:41:50.977570: step 116990, loss = 1.82 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:41:52.179569: step 117000, loss = 1.83 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:53.368974: step 117010, loss = 1.91 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:41:54.600862: step 117020, loss = 1.90 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:41:55.733274: step 117030, loss = 1.70 (1130.4 examples/sec; 0.113 sec/batch)
2017-05-05 01:41:56.935502: step 117040, loss = 1.97 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:41:58.109845: step 117050, loss = 2.02 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:41:59.297786: step 117060, loss = 1.77 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:00.479879: step 117070, loss = 1.96 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:01.651692: step 117080, loss = 1.83 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:02.842766: step 117090, loss = 1.89 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:04.022991: step 117100, loss = 1.88 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:05.209819: step 117110, loss = 1.95 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:06.384952: step 117120, loss = 1.84 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:07.572834: step 117130, loss = 1.83 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:08.770215: step 117140, loss = 1.90 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:42:09.956316: step 117150, loss = 2.01 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:11.154903: step 117160, loss = 1.94 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:42:12.350312: step 117170, loss = 1.84 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:42:13.537814: step 117180, loss = 1.85 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:14.756560: step 117190, loss = 1.90 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:42:15.959537: step 117200, loss = 1.94 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:42:17.182154: step 117210, loss = 1.88 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:42:18.371772: step 117220, loss = 2.06 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:19.572452: step 117230, loss = 1.96 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:42:20.761813: step 117240, loss = 1.98 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:21.950672: step 117250, loss = 1.85 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:23.141736: step 117260, loss = 1.94 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:24.341430: step 117270, loss = 1.87 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:42:25.545693: step 117280, loss = 2.11 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:42:26.729406: step 117290, loss = 1.81 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:27.911378: step 117300, loss = 1.96 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:29.091597: step 117310, loss = 1.91 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:30.257695: step 117320, loss = 1.91 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:31.433984: step 117330, loss = 1.83 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:32.597576: step 117340, loss = 1.97 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:42:33.772648: step 117350, loss = 1.83 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:34.957008: step 117360, loss = 1.78 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:36.118605: step 117370, loss = 1.87 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:42:37.289038: step 117380, loss = 1.99 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:38.456944: step 117390, loss = 1.86 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:39.632703: step 117400, loss = 1.92 (1088.7 examples/sec; 0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1211 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
.118 sec/batch)
2017-05-05 01:42:40.818596: step 117410, loss = 1.82 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:41.999924: step 117420, loss = 2.21 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:43.187560: step 117430, loss = 1.87 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:42:44.363889: step 117440, loss = 1.74 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:45.514185: step 117450, loss = 1.98 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:42:46.673396: step 117460, loss = 1.86 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:42:47.839961: step 117470, loss = 1.75 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:48.999693: step 117480, loss = 1.93 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:42:50.142139: step 117490, loss = 1.86 (1120.4 examples/sec; 0.114 sec/batch)
2017-05-05 01:42:51.300350: step 117500, loss = 1.94 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:42:52.482212: step 117510, loss = 1.95 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:53.634918: step 117520, loss = 1.93 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:42:54.797000: step 117530, loss = 1.91 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:42:55.976089: step 117540, loss = 2.02 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:42:57.129548: step 117550, loss = 1.90 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:42:58.298693: step 117560, loss = 1.94 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:42:59.486317: step 117570, loss = 1.84 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:00.659496: step 117580, loss = 1.78 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:01.820060: step 117590, loss = 1.83 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:43:02.997870: step 117600, loss = 1.90 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:43:04.171986: step 117610, loss = 1.84 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:05.347975: step 117620, loss = 1.94 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:43:06.521556: step 117630, loss = 1.86 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:07.712448: step 117640, loss = 1.84 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:08.897659: step 117650, loss = 1.99 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:10.072369: step 117660, loss = 1.94 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:11.295766: step 117670, loss = 1.85 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:43:12.514367: step 117680, loss = 1.86 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:43:13.737724: step 117690, loss = 1.83 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:43:14.943306: step 117700, loss = 1.88 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:43:16.162310: step 117710, loss = 1.80 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:43:17.369517: step 117720, loss = 1.83 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:43:18.575095: step 117730, loss = 1.76 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:43:19.786963: step 117740, loss = 1.94 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:43:20.974776: step 117750, loss = 1.86 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:22.167767: step 117760, loss = 1.77 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:23.350554: step 117770, loss = 1.95 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:43:24.553583: step 117780, loss = 1.76 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:43:25.730299: step 117790, loss = 1.94 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:43:26.953045: step 117800, loss = 1.89 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:43:28.248334: step 117810, loss = 1.88 (988.2 examples/sec; 0.130 sec/batch)
2017-05-05 01:43:29.366093: step 117820, loss = 2.03 (1145.1 examples/sec; 0.112 sec/batch)
2017-05-05 01:43:30.569987: step 117830, loss = 1.76 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:43:31.759981: step 117840, loss = 2.02 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:32.954757: step 117850, loss = 1.85 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:34.146878: step 117860, loss = 1.89 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:35.362824: step 117870, loss = 1.89 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:43:36.554800: step 117880, loss = 1.81 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:37.750084: step 117890, loss = 1.93 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:43:38.937925: step 117900, loss = 1.77 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:40.133926: step 117910, loss = 1.76 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:43:41.315051: step 117920, loss = 1.87 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:43:42.491496: step 117930, loss = 1.92 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:43:43.664885: step 117940, loss = 1.86 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:44.844175: step 117950, loss = 1.83 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:43:46.029602: step 117960, loss = 1.97 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:47.224421: step 117970, loss = 2.03 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:48.393964: step 117980, loss = 1.85 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:49.544343: step 117990, loss = 1.97 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:43:50.733163: step 118000, loss = 1.99 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:43:51.890662: step 118010, loss = 1.94 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:43:53.062521: step 118020, loss = 1.78 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:54.229194: step 118030, loss = 1.82 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:55.398252: step 118040, loss = 1.80 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:43:56.579931: step 118050, loss = 1.92 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:43:57.728651: step 118060, loss = 1.88 (1114.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:43:58.913006: step 118070, loss = 1.90 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:44:00.063496: step 118080, loss = 2.02 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:01.224041: step 118090, loss = 1.80 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:02.376634: step 118100, loss = 1.98 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:03.530007: step 118110, loss = 1.91 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:04.698759: step 118120, loss = 1.97 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:05.849915: step 118130, loss = 1.85 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:07.021454: step 118140, loss = 2.03 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:08.189420: step 118150, loss = 1.87 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:09.368035: step 118160, loss = 2.27 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:44:10.521766: step 118170, loss = 1.75 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:11.687732: step 118180, loss = 1.89 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:12.861425: step 118190, loss = 1.78 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:14.012579: step 118200, loss = 1.87 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:15.187077: step 118210, loss = 1.86 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:16.349627: step 118220, loss = 1.89 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:17.513049: step 118230, loss = 1.96 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:18.677767: step 118240, loss = 1.83 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:19.859922: step 118250, loss = 1.97 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:44:21.016941: step 118260, loss = 1.87 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:22.162090: step 118270, loss = 1.84 (1117.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:23.322934: step 118280, loss = 1.89 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:24.495525: step 118290, loss = 1.84 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:25.644208: step 118300, loss = 1.87 (1114.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:26.816679: step 118310, loss = 2.03 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:27.985608: step 118320, loss = 1.88 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:29.137795: step 118330, loss = 1.86 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:30.318702: step 118340, loss = 1.78 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:44:31.482941: step 118350, loss = 1.90 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:32.659105: step 118360, loss = 1.83 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:44:33.804197: step 118370, loss = 2.02 (1117.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:34.978703: step 118380, loss = 1.76 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:36.145299: step 118390, loss = 1.90 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:37.327487: step 118400, loss = 2.04 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:44:38.504509: step 118410, loss = 1.93 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:44:39.669122: step 118420, loss = 1.93 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:40.837687: step 118430, loss = 1.92 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:41.983427: step 118440, loss = 1.86 (1117.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:43.157418: step 118450, loss = 1.88 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:44.314637: step 118460, loss = 1.98 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:45.470491: step 118470, loss = 1.87 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:46.630764: step 118480, loss = 2.00 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:47.798597: step 118490, loss = 1.84 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:48.998975: step 118500, loss = 1.88 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:44:50.149258: step 118510, loss = 1.98 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:51.320089: step 118520, loss = 1.90 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:52.477132: step 118530, loss = 1.93 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:53.651659: step 118540, loss = 1.86 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:54.833381: step 118550, loss = 1.82 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:44:55.998750: step 118560, loss = 1.87 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:44:57.154692: step 118570, loss = 1.74 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:44:58.308561: step 118580, loss = 1.96 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:44:59.473198: step 118590, loss = 1.96 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:45:00.643923: step 118600, loss = 1.86 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:01.814548: step 118610, loss = 1.86 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:02.972654: step 118620, loss = 1.90 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:45:04.125710: step 118630, loss = 2.22 (1110.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:45:05.300623: step 118640, loss = 1.81 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:06.454732: step 118650, loss = 1.84 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:45:07.625300: step 118660, loss = 1.95 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:08.794221: step 118670, loss = 1.83 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:09.988515: step 118680, loss = 1.87 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:45:11.123584: step 118690, loss = 1.85 (1127.7 examples/sec; 0.114 sec/batch)
2017-05-05 01:45:12.293558: step 118700, loss = 1.90 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:13.458056: step 118710, loss = 1.78 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:45:14.615988: step 118720, loss = 2.06 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:45:15.795321: step 118730, loss = 1.93 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:45:16.973929: step 1187E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1222 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
40, loss = 1.89 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:45:18.134885: step 118750, loss = 1.92 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:45:19.292270: step 118760, loss = 1.96 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:45:20.457165: step 118770, loss = 1.84 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:45:21.617183: step 118780, loss = 1.99 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:45:22.775881: step 118790, loss = 1.95 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:45:24.035840: step 118800, loss = 1.89 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-05 01:45:25.114656: step 118810, loss = 1.92 (1186.5 examples/sec; 0.108 sec/batch)
2017-05-05 01:45:26.287015: step 118820, loss = 1.88 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:27.456092: step 118830, loss = 1.93 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:28.629820: step 118840, loss = 1.92 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:29.799738: step 118850, loss = 1.76 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:45:31.001731: step 118860, loss = 1.83 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:45:32.207330: step 118870, loss = 1.88 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:45:33.422906: step 118880, loss = 1.89 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:45:34.634410: step 118890, loss = 1.83 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:45:35.852628: step 118900, loss = 2.00 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:45:37.059012: step 118910, loss = 2.02 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:45:38.278706: step 118920, loss = 1.94 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:45:39.514309: step 118930, loss = 2.01 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-05 01:45:40.729451: step 118940, loss = 1.90 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:45:41.925534: step 118950, loss = 1.81 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:45:43.159716: step 118960, loss = 1.90 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:45:44.363430: step 118970, loss = 2.00 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:45:45.570158: step 118980, loss = 1.76 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:45:46.795413: step 118990, loss = 1.98 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:45:47.990286: step 119000, loss = 1.83 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:45:49.211505: step 119010, loss = 1.92 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:45:50.400282: step 119020, loss = 1.96 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:45:51.632386: step 119030, loss = 1.87 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:45:52.821147: step 119040, loss = 1.84 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:45:54.040357: step 119050, loss = 1.86 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:45:55.255896: step 119060, loss = 1.93 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:45:56.470166: step 119070, loss = 1.99 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:45:57.698073: step 119080, loss = 1.99 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:45:58.913895: step 119090, loss = 1.92 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:46:00.123461: step 119100, loss = 2.01 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:01.341111: step 119110, loss = 1.91 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:46:02.547371: step 119120, loss = 1.81 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:03.752789: step 119130, loss = 2.08 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:04.952143: step 119140, loss = 1.83 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:06.142821: step 119150, loss = 1.93 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:46:07.365004: step 119160, loss = 1.88 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:46:08.586429: step 119170, loss = 1.79 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:46:09.819152: step 119180, loss = 1.98 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:46:11.041707: step 119190, loss = 1.85 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:46:12.244560: step 119200, loss = 1.97 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:13.447093: step 119210, loss = 2.06 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:14.655512: step 119220, loss = 1.95 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:15.869382: step 119230, loss = 1.98 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:17.092490: step 119240, loss = 2.01 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:46:18.294380: step 119250, loss = 1.72 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:19.501717: step 119260, loss = 1.89 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:20.709765: step 119270, loss = 1.77 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:21.911041: step 119280, loss = 1.84 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:23.142784: step 119290, loss = 1.99 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:46:24.359795: step 119300, loss = 1.92 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:46:25.558526: step 119310, loss = 1.85 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:26.774942: step 119320, loss = 1.78 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:46:27.975825: step 119330, loss = 1.84 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:29.185106: step 119340, loss = 1.92 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:30.380643: step 119350, loss = 1.71 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:31.580307: step 119360, loss = 1.85 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:32.785313: step 119370, loss = 1.89 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:33.975302: step 119380, loss = 1.86 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:46:35.171726: step 119390, loss = 1.89 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:36.354103: step 119400, loss = 1.91 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:46:37.535881: step 119410, loss = 1.93 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:46:38.730930: step 119420, loss = 1.84 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 01:46:39.923673: step 119430, loss = 1.86 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:46:41.135948: step 119440, loss = 1.96 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:46:42.316207: step 119450, loss = 2.01 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:46:43.506154: step 119460, loss = 1.81 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:46:44.695453: step 119470, loss = 1.93 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:46:45.866507: step 119480, loss = 1.89 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:46:47.039814: step 119490, loss = 1.97 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:46:48.211637: step 119500, loss = 1.83 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:46:49.386934: step 119510, loss = 1.96 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:46:50.570056: step 119520, loss = 2.10 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:46:51.748186: step 119530, loss = 1.95 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:46:52.919536: step 119540, loss = 1.84 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:46:54.108749: step 119550, loss = 1.84 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:46:55.300705: step 119560, loss = 1.91 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:46:56.517863: step 119570, loss = 1.90 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:46:57.685684: step 119580, loss = 1.91 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:46:58.873463: step 119590, loss = 1.70 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:00.093945: step 119600, loss = 1.76 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:47:01.294279: step 119610, loss = 1.84 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:47:02.497815: step 119620, loss = 1.91 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:47:03.693742: step 119630, E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1234 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
loss = 1.96 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:47:04.909869: step 119640, loss = 1.86 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:47:06.093028: step 119650, loss = 1.92 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:07.266832: step 119660, loss = 1.93 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:47:08.473582: step 119670, loss = 1.85 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 01:47:09.633050: step 119680, loss = 1.90 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:47:10.805782: step 119690, loss = 1.83 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:47:11.993633: step 119700, loss = 1.92 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:13.190535: step 119710, loss = 1.83 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:47:14.370116: step 119720, loss = 2.12 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:15.537878: step 119730, loss = 1.96 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:47:16.717941: step 119740, loss = 1.97 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:17.891409: step 119750, loss = 1.92 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:47:19.080516: step 119760, loss = 1.86 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:20.273697: step 119770, loss = 1.91 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:21.452600: step 119780, loss = 1.79 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:22.724192: step 119790, loss = 1.85 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-05 01:47:23.795650: step 119800, loss = 1.74 (1194.6 examples/sec; 0.107 sec/batch)
2017-05-05 01:47:24.972991: step 119810, loss = 1.98 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:26.148744: step 119820, loss = 1.99 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:27.325937: step 119830, loss = 1.86 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:28.529421: step 119840, loss = 1.95 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:47:29.712817: step 119850, loss = 1.96 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:30.893887: step 119860, loss = 1.93 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:32.073832: step 119870, loss = 1.86 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:33.269744: step 119880, loss = 1.81 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:47:34.449361: step 119890, loss = 1.75 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:35.630631: step 119900, loss = 2.03 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:36.798550: step 119910, loss = 1.90 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:47:37.992692: step 119920, loss = 1.88 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:39.208665: step 119930, loss = 1.86 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:47:40.424005: step 119940, loss = 1.94 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:47:41.605665: step 119950, loss = 1.84 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:42.798598: step 119960, loss = 1.90 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:44.026302: step 119970, loss = 1.96 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:47:45.232735: step 119980, loss = 1.87 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:47:46.440762: step 119990, loss = 2.03 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:47:47.631403: step 120000, loss = 2.10 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:48.828010: step 120010, loss = 1.98 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:47:49.989463: step 120020, loss = 1.95 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:47:51.156013: step 120030, loss = 1.98 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:47:52.327199: step 120040, loss = 1.83 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:47:53.513519: step 120050, loss = 1.91 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:47:54.688931: step 120060, loss = 1.75 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:55.858441: step 120070, loss = 1.80 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:47:57.041286: step 120080, loss = 2.01 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:47:58.204750: step 120090, loss = 1.99 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:47:59.361641: step 120100, loss = 1.90 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:00.518515: step 120110, loss = 1.81 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:01.677294: step 120120, loss = 1.87 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:02.853157: step 120130, loss = 1.94 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:04.015634: step 120140, loss = 1.86 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:05.190128: step 120150, loss = 2.00 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:06.378551: step 120160, loss = 1.93 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:48:07.562664: step 120170, loss = 1.86 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:08.759068: step 120180, loss = 2.08 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:48:09.934413: step 120190, loss = 1.75 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:11.114884: step 120200, loss = 1.81 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:12.278026: step 120210, loss = 1.94 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:13.441767: step 120220, loss = 1.86 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:14.604857: step 120230, loss = 1.85 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:15.775347: step 120240, loss = 1.84 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:16.942110: step 120250, loss = 1.93 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:18.117499: step 120260, loss = 1.92 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:19.310740: step 120270, loss = 1.99 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:48:20.485351: step 120280, loss = 1.77 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:21.641129: step 120290, loss = 1.89 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:22.798652: step 120300, loss = 1.93 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:23.970956: step 120310, loss = 1.87 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:25.126031: step 120320, loss = 1.86 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:26.281491: step 120330, loss = 1.92 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:27.445560: step 120340, loss = 1.78 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:28.608664: step 120350, loss = 2.05 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:29.747074: step 120360, loss = 1.99 (1124.4 examples/sec; 0.114 sec/batch)
2017-05-05 01:48:30.922118: step 120370, loss = 1.90 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:32.093369: step 120380, loss = 1.87 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:33.270298: step 120390, loss = 1.87 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:34.417325: step 120400, loss = 1.86 (1115.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:48:35.603030: step 120410, loss = 1.90 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:48:36.824777: step 120420, loss = 1.88 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:48:37.982865: step 120430, loss = 1.96 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:39.155622: step 120440, loss = 1.88 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:40.300609: step 120450, loss = 2.02 (1117.9 examples/sec; 0.114 sec/batch)
2017-05-05 01:48:41.466051: step 120460, loss = 1.86 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:42.628286: step 120470, loss = 2.14 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:43.795462: step 120480, loss = 2.00 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:44.980535: step 120490, loss = 1.99 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:48:46.146522: step 120500, loss = 1.97 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:47.338733: step 120510, loss = 2.07 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:48:48.506027: step 120520, loss = 1.90 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:49.662179: step 120530, loss = 1.92 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:50.823470: step 120540, loss = 1.93 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:51.993479: step 120550, loss = 1.98 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:48:53.169846: step 120560, loss = 1.86 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:48:54.320301: step 120570, loss = 2.06 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:48:55.467074: step 120580, loss = 2.00 (1116.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:48:56.623993: step 120590, loss = 1.84 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:57.783071: step 120600, loss = 1.80 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:48:58.937259: step 120610, loss = 1.86 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:00.109287: step 120620, loss = 1.87 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:01.261193: step 120630, loss = 1.82 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:02.420961: step 120640, loss = 1.99 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:03.593530: step 120650, loss = 1.95 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:04.769386: step 120660, loss = 1.99 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:49:05.945105: step 120670, loss = 1.92 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:49:07.106535: step 120680, loss = 1.86 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:08.260707: step 120690, loss = 1.89 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:09.444994: step 120700, loss = 1.93 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:49:10.594177: step 120710, loss = 1.85 (1113.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:11.742340: step 120720, loss = 1.91 (1114.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:12.931014: step 120730, loss = 1.75 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:49:14.086410: step 120740, loss = 1.84 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:15.261946: step 120750, loss = 2.05 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:49:16.401826: step 120760, loss = 1.93 (1122.9 examples/sec; 0.114 sec/batch)
2017-05-05 01:49:17.574524: step 120770, loss = 1.96 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:18.818538: step 120780, loss = 2.10 (1028.9 examples/sec; 0.124 sec/batch)
2017-05-05 01:49:19.907097: step 120790, loss = 2.08 (1175.9 examples/sec; 0.109 sec/batch)
2017-05-05 01:49:21.087092: step 120800, loss = 1.90 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:49:22.228682: step 120810, loss = 1.86 (1121.2 examples/sec; 0.114 sec/batch)
2017-05-05 01:49:23.398686: step 120820, loss = 1.90 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:24.565635: step 120830, loss = 1.91 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:25.722427: step 120840, loss = 1.92 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:26.898211: step 120850, loss = 1.91 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:49:28.074917: step 120860, loss = 1.78 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:49:29.253360: step 120870, loss = 2.05 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:49:30.436764: step 120880, loss = 1.98 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:49:31.605197: step 120890, loss = 1.98 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:32.787740: step 120900, loss = 1.87 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:49:33.947923: step 120910, loss = 1.90 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:35.107592: step 120920, loss = 1.99 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:36.285345: step 120930, loss = 1.94 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:49:37.438280: step 120940, loss = 1.80 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:38.584763: step 120950, loss = 1.82 (1116.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:39.750163: step 120960, loss = 1.72 (1098.3 examples/sec; 0.117 sec/batch)
2E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1247 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
017-05-05 01:49:40.901937: step 120970, loss = 1.99 (1111.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:42.058257: step 120980, loss = 1.92 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:43.235401: step 120990, loss = 1.88 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:49:44.399359: step 121000, loss = 1.84 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:45.542414: step 121010, loss = 1.82 (1119.8 examples/sec; 0.114 sec/batch)
2017-05-05 01:49:46.736569: step 121020, loss = 1.90 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:49:47.890752: step 121030, loss = 2.07 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:49.046057: step 121040, loss = 2.00 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:50.195598: step 121050, loss = 1.91 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:51.357994: step 121060, loss = 1.70 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:52.510778: step 121070, loss = 1.95 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:53.685703: step 121080, loss = 1.93 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:54.837532: step 121090, loss = 2.01 (1111.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:49:55.993833: step 121100, loss = 1.88 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:49:57.160979: step 121110, loss = 2.04 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:58.327866: step 121120, loss = 2.09 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:49:59.496495: step 121130, loss = 1.87 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:00.670813: step 121140, loss = 1.93 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:01.838099: step 121150, loss = 2.09 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:03.016584: step 121160, loss = 1.98 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:50:04.183723: step 121170, loss = 2.15 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:05.350589: step 121180, loss = 1.79 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:06.500374: step 121190, loss = 1.89 (1113.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:50:07.668703: step 121200, loss = 1.82 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:08.836136: step 121210, loss = 1.97 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:09.996275: step 121220, loss = 1.88 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:11.154437: step 121230, loss = 2.02 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:12.343349: step 121240, loss = 1.86 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:50:13.499682: step 121250, loss = 1.89 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:14.686973: step 121260, loss = 1.80 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:50:15.839440: step 121270, loss = 1.92 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:50:17.014127: step 121280, loss = 1.92 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:18.178185: step 121290, loss = 1.74 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:19.339468: step 121300, loss = 1.78 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:20.529454: step 121310, loss = 2.05 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:50:21.711089: step 121320, loss = 1.86 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:50:22.892591: step 121330, loss = 1.98 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:50:24.053707: step 121340, loss = 2.07 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:25.204797: step 121350, loss = 1.81 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:50:26.369201: step 121360, loss = 1.87 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:27.546089: step 121370, loss = 1.84 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:50:28.720637: step 121380, loss = 1.83 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:29.874164: step 121390, loss = 1.85 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:50:31.024262: step 121400, loss = 1.89 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:50:32.177616: step 121410, loss = 1.91 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:50:33.353460: step 121420, loss = 1.70 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:50:34.538811: step 121430, loss = 1.90 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:50:35.707752: step 121440, loss = 1.86 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:36.891926: step 121450, loss = 1.85 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:50:38.032470: step 121460, loss = 1.92 (1122.3 examples/sec; 0.114 sec/batch)
2017-05-05 01:50:39.201194: step 121470, loss = 2.08 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:40.366370: step 121480, loss = 2.04 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:41.534891: step 121490, loss = 1.83 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:42.700584: step 121500, loss = 1.94 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:43.871628: step 121510, loss = 1.87 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:45.048276: step 121520, loss = 1.88 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:50:46.201271: step 121530, loss = 1.93 (1110.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:50:47.361316: step 121540, loss = 1.76 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:48.523368: step 121550, loss = 1.84 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:49.679880: step 121560, loss = 1.84 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:50.844095: step 121570, loss = 2.09 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:50:51.994923: step 121580, loss = 1.79 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:50:53.168304: step 121590, loss = 1.87 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:54.322689: step 121600, loss = 1.94 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:50:55.497673: step 121610, loss = 1.84 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:56.677760: step 121620, loss = 1.83 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:50:57.847555: step 121630, loss = 1.76 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:50:59.020091: step 121640, loss = 1.83 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:00.198499: step 121650, loss = 1.88 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:51:01.352680: step 121660, loss = 1.88 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:51:02.531104: step 121670, loss = 1.90 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:51:03.692357: step 121680, loss = 1.83 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:04.852158: step 121690, loss = 1.90 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:06.010512: step 121700, loss = 1.82 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:07.168971: step 121710, loss = 2.08 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:08.326617: step 121720, loss = 1.88 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:09.493316: step 121730, loss = 2.03 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:10.664876: step 121740, loss = 1.83 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:11.830004: step 121750, loss = 1.93 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:12.999435: step 121760, loss = 1.85 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:14.248688: step 121770, loss = 1.88 (1024.6 examples/sec; 0.125 sec/batch)
2017-05-05 01:51:15.315551: step 121780, loss = 1.83 (1199.8 examples/sec; 0.107 sec/batch)
2017-05-05 01:51:16.456179: step 121790, loss = 1.90 (1122.2 examples/sec; 0.114 sec/batch)
2017-05-05 01:51:17.631012: step 121800, loss = 1.81 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:18.805302: step 121810, loss = 1.86 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:19.967615: step 121820, loss = 1.84 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:21.136597: step 121830, loss = 1.98 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:22.305211: step 121840, loss = 1.90 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:23.471113: step 121850, loss = 1.97 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:24.632466: step 121860, loss = 1.77 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:25.798003: step 121870, loss = 1.89 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:26.972666: step 121880, loss = 1.93 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:28.146192: step 121890, loss = 1.98 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:29.310374: step 121900, loss = 1.98 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:30.464136: step 121910, loss = 1.93 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:51:31.629599: step 121920, loss = 1.80 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:32.786190: step 121930, loss = 1.94 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:33.937864: step 121940, loss = 1.94 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:51:35.093450: step 121950, loss = 1.79 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:36.275278: step 121960, loss = 1.87 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:51:37.469757: step 121970, loss = 1.87 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:51:38.629935: step 121980, loss = 1.83 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:39.785769: step 121990, loss = 1.89 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:40.963200: step 122000, loss = 1.94 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:51:42.106321: step 122010, loss = 1.76 (1119.7 examples/sec; 0.114 sec/batch)
2017-05-05 01:51:43.282281: step 122020, loss = 1.75 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:51:44.434466: step 122030, loss = 1.78 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:51:45.612531: step 122040, loss = 2.00 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:51:46.775688: step 122050, loss = 2.02 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:47.947949: step 122060, loss = 1.92 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:49.122466: step 122070, loss = 1.95 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:50.269138: step 122080, loss = 1.96 (1116.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:51:51.447951: step 122090, loss = 1.80 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:51:52.616356: step 122100, loss = 1.80 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:53.753705: step 122110, loss = 1.87 (1125.4 examples/sec; 0.114 sec/batch)
2017-05-05 01:51:54.903625: step 122120, loss = 1.80 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:51:56.069864: step 122130, loss = 2.07 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:51:57.258411: step 122140, loss = 1.93 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:51:58.416929: step 122150, loss = 1.95 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:51:59.578083: step 122160, loss = 1.98 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:52:00.734315: step 122170, loss = 2.01 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:52:01.867842: step 122180, loss = 1.84 (1129.2 examples/sec; 0.113 sec/batch)
2017-05-05 01:52:03.045210: step 122190, loss = 1.84 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:04.222389: step 122200, loss = 2.01 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:05.391549: step 122210, loss = 1.96 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:06.581801: step 122220, loss = 1.97 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:07.765157: step 122230, loss = 1.87 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:08.935959: step 122240, loss = 1.85 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:10.116021: step 122250, loss = 1.98 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:11.283459: step 122260, loss = 1.78 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:12.467148: step 122270, loss = 1.88 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:13.635036: step 122280, loss = 1.84 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:14.819850: step 122290, loss = 1.85 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:15.998200: step 122300, loss = 1.89 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1259 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
(1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:17.181363: step 122310, loss = 1.84 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:18.369704: step 122320, loss = 1.98 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:19.563152: step 122330, loss = 1.99 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:20.757148: step 122340, loss = 1.79 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:21.941080: step 122350, loss = 1.92 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:23.110606: step 122360, loss = 1.89 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:24.302336: step 122370, loss = 1.91 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:25.478953: step 122380, loss = 1.96 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:26.668501: step 122390, loss = 1.96 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:27.822126: step 122400, loss = 1.89 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:52:29.009564: step 122410, loss = 1.80 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:30.178624: step 122420, loss = 1.97 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:31.363090: step 122430, loss = 1.73 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:32.536335: step 122440, loss = 1.85 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:33.719281: step 122450, loss = 1.95 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:34.905723: step 122460, loss = 1.78 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:36.109801: step 122470, loss = 1.92 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:52:37.318282: step 122480, loss = 1.83 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:52:38.510408: step 122490, loss = 2.03 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:39.710131: step 122500, loss = 1.95 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:52:40.900578: step 122510, loss = 2.10 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:42.081879: step 122520, loss = 1.96 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:43.272161: step 122530, loss = 1.83 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:44.452006: step 122540, loss = 1.85 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:45.609787: step 122550, loss = 1.85 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:52:46.753773: step 122560, loss = 1.84 (1118.9 examples/sec; 0.114 sec/batch)
2017-05-05 01:52:47.927336: step 122570, loss = 1.88 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:49.098498: step 122580, loss = 2.05 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:50.252062: step 122590, loss = 1.86 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:52:51.437469: step 122600, loss = 2.02 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:52:52.584193: step 122610, loss = 2.07 (1116.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:52:53.748189: step 122620, loss = 1.83 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:52:54.912222: step 122630, loss = 1.74 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:52:56.080758: step 122640, loss = 1.85 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:52:57.258480: step 122650, loss = 1.89 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:52:58.410871: step 122660, loss = 1.88 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:52:59.575830: step 122670, loss = 1.81 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:53:00.742068: step 122680, loss = 1.88 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:53:01.899125: step 122690, loss = 1.72 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:53:03.071401: step 122700, loss = 1.95 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:53:04.252630: step 122710, loss = 1.90 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:53:05.401971: step 122720, loss = 1.97 (1113.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:53:06.560346: step 122730, loss = 1.87 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:53:07.740721: step 122740, loss = 1.85 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:53:08.925418: step 122750, loss = 1.80 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:53:10.184681: step 122760, loss = 1.86 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-05 01:53:11.292712: step 122770, loss = 1.98 (1155.2 examples/sec; 0.111 sec/batch)
2017-05-05 01:53:12.484826: step 122780, loss = 1.97 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:53:13.664747: step 122790, loss = 1.78 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:53:14.877363: step 122800, loss = 2.08 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:53:16.068676: step 122810, loss = 1.86 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:53:17.259333: step 122820, loss = 2.03 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:53:18.460362: step 122830, loss = 2.02 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:53:19.671308: step 122840, loss = 1.87 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:53:20.897187: step 122850, loss = 1.80 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:53:22.111393: step 122860, loss = 1.85 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:53:23.327033: step 122870, loss = 2.06 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:24.531589: step 122880, loss = 1.75 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 01:53:25.740774: step 122890, loss = 1.88 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:53:26.957550: step 122900, loss = 1.79 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:28.176254: step 122910, loss = 1.90 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:29.395486: step 122920, loss = 1.87 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:30.618998: step 122930, loss = 1.85 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:31.828560: step 122940, loss = 1.90 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:53:33.063471: step 122950, loss = 1.84 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:53:34.241551: step 122960, loss = 1.79 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:53:35.468443: step 122970, loss = 1.97 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-05 01:53:36.697014: step 122980, loss = 1.91 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-05 01:53:37.911200: step 122990, loss = 1.93 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:53:39.148369: step 123000, loss = 1.86 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-05 01:53:40.375461: step 123010, loss = 1.97 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:53:41.584218: step 123020, loss = 2.00 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:53:42.802506: step 123030, loss = 1.86 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:44.042535: step 123040, loss = 1.95 (1032.2 examples/sec; 0.124 sec/batch)
2017-05-05 01:53:45.248586: step 123050, loss = 1.73 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:53:46.472841: step 123060, loss = 1.78 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:47.689335: step 123070, loss = 1.88 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:48.912357: step 123080, loss = 1.95 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:50.117189: step 123090, loss = 1.80 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:53:51.340897: step 123100, loss = 1.86 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:52.576134: step 123110, loss = 1.88 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-05 01:53:53.784819: step 123120, loss = 1.89 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:53:55.000001: step 123130, loss = 1.85 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:53:56.225212: step 123140, loss = 1.85 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:53:57.462807: step 123150, loss = 1.83 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-05 01:53:58.634384: step 123160, loss = 1.83 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:53:59.831468: step 123170, loss = 1.95 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:54:01.046559: step 123180, loss = 1.86 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:02.271080: step 123190, loss = 1.93 (104E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1270 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
5.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:03.489577: step 123200, loss = 1.83 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:04.714002: step 123210, loss = 1.83 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:05.908555: step 123220, loss = 1.96 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:54:07.103853: step 123230, loss = 2.03 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:54:08.325411: step 123240, loss = 1.73 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:09.558461: step 123250, loss = 1.78 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-05 01:54:10.758907: step 123260, loss = 1.95 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:54:11.986456: step 123270, loss = 1.80 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-05 01:54:13.199974: step 123280, loss = 1.97 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:14.414534: step 123290, loss = 1.76 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:15.652559: step 123300, loss = 1.91 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-05 01:54:16.884981: step 123310, loss = 1.85 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:54:18.083184: step 123320, loss = 2.06 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:54:19.317592: step 123330, loss = 1.97 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-05 01:54:20.535065: step 123340, loss = 1.87 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:21.740379: step 123350, loss = 1.86 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:22.933812: step 123360, loss = 2.04 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:54:24.145389: step 123370, loss = 2.02 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:25.373342: step 123380, loss = 2.05 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:54:26.580430: step 123390, loss = 1.86 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:27.799127: step 123400, loss = 1.87 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:29.017678: step 123410, loss = 1.99 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:30.217486: step 123420, loss = 1.89 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:54:31.434850: step 123430, loss = 1.86 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:32.656080: step 123440, loss = 2.07 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:33.879777: step 123450, loss = 2.01 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:35.103403: step 123460, loss = 1.96 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:36.321553: step 123470, loss = 1.97 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:37.541968: step 123480, loss = 1.81 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:38.760537: step 123490, loss = 1.79 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:39.974757: step 123500, loss = 1.90 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:41.188456: step 123510, loss = 1.82 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:42.398041: step 123520, loss = 1.94 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:43.615056: step 123530, loss = 1.81 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:44.832219: step 123540, loss = 1.75 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:46.025359: step 123550, loss = 2.04 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:54:47.219678: step 123560, loss = 2.03 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:54:48.446729: step 123570, loss = 1.94 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 01:54:49.674442: step 123580, loss = 1.82 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 01:54:50.885051: step 123590, loss = 1.91 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:52.129758: step 123600, loss = 1.83 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-05 01:54:53.347923: step 123610, loss = 1.91 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:54:54.561252: step 123620, loss = 1.81 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:55.769927: step 123630, loss = 1.75 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:54:57.021161: step 123640, loss = 1.86 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-05 01:54:58.219958: step 123650, loss = 2.01 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:54:59.431055: step 123660, loss = 1.97 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:55:00.668469: step 123670, loss = 1.84 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-05 01:55:01.888682: step 123680, loss = 1.93 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:03.138540: step 123690, loss = 1.86 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-05 01:55:04.356553: step 123700, loss = 1.81 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:05.550414: step 123710, loss = 1.69 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:55:06.774269: step 123720, loss = 1.79 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:07.986415: step 123730, loss = 1.96 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:55:09.210334: step 123740, loss = 1.86 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:10.505127: step 123750, loss = 1.95 (988.6 examples/sec; 0.129 sec/batch)
2017-05-05 01:55:11.589414: step 123760, loss = 1.76 (1180.5 examples/sec; 0.108 sec/batch)
2017-05-05 01:55:12.806916: step 123770, loss = 1.94 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:14.015657: step 123780, loss = 1.92 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:55:15.229441: step 123790, loss = 1.91 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 01:55:16.435827: step 123800, loss = 1.75 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 01:55:17.634716: step 123810, loss = 1.94 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:55:18.839768: step 123820, loss = 1.86 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:55:20.066532: step 123830, loss = 1.83 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 01:55:21.279932: step 123840, loss = 1.82 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 01:55:22.480984: step 123850, loss = 1.72 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:55:23.706428: step 123860, loss = 1.97 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 01:55:24.924645: step 123870, loss = 2.09 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:26.123201: step 123880, loss = 1.92 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:55:27.346302: step 123890, loss = 1.81 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:28.567213: step 123900, loss = 1.85 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:29.772220: step 123910, loss = 1.99 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 01:55:30.991730: step 123920, loss = 2.01 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 01:55:32.175946: step 123930, loss = 1.77 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:55:33.358910: step 123940, loss = 1.89 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:55:34.523719: step 123950, loss = 1.97 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:55:35.710625: step 123960, loss = 2.00 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:55:36.906899: step 123970, loss = 1.84 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 01:55:38.078576: step 123980, loss = 1.76 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:55:39.265099: step 123990, loss = 1.89 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 01:55:40.448573: step 124000, loss = 1.84 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:55:41.628223: step 124010, loss = 1.98 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:55:42.809560: step 124020, loss = 1.88 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:55:43.986406: step 124030, loss = 1.94 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:55:45.136784: step 124040, loss = 1.85 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:55:46.277845: step 124050, loss = 1.98 (1121.8 examples/sec; 0.114 sec/batch)
2017-05-05 01:55:47.445175: step 124060, loss = 1.84 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:55:48.624134: step 124070, loss = 1.84 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:55:49.787212: step 124080, loss = 1.99 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:55:50.949049: step 124090, loss = 1.87 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:55:52.137902: step 124100, loss = 1.78 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:55:53.302893: step 124110, loss = 1.89 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 01:55:54.464892: step 124120, loss = 1.90 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:55:55.617404: step 124130, loss = 1.76 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:55:56.791728: step 124140, loss = 2.18 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:55:57.940012: step 124150, loss = 1.73 (1114.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:55:59.107535: step 124160, loss = 2.06 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:00.260845: step 124170, loss = 1.93 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:56:01.461310: step 124180, loss = 1.94 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 01:56:02.643187: step 124190, loss = 1.93 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:03.829903: step 124200, loss = 1.88 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:56:05.003089: step 124210, loss = 1.93 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:06.167159: step 124220, loss = 1.78 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:07.313482: step 124230, loss = 1.91 (1116.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:56:08.480612: step 124240, loss = 1.93 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:09.650450: step 124250, loss = 1.87 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:10.812461: step 124260, loss = 1.85 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:11.981197: step 124270, loss = 1.97 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:13.145358: step 124280, loss = 1.94 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:14.288510: step 124290, loss = 1.85 (1119.7 examples/sec; 0.114 sec/batch)
2017-05-05 01:56:15.449184: step 124300, loss = 1.74 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:16.611581: step 124310, loss = 1.78 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:17.788192: step 124320, loss = 1.87 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:18.970097: step 124330, loss = 1.97 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:20.161360: step 124340, loss = 1.81 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:56:21.348155: step 124350, loss = 2.07 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 01:56:22.545610: step 124360, loss = 1.90 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:56:23.747820: step 124370, loss = 2.02 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 01:56:24.944648: step 124380, loss = 1.82 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:56:26.109256: step 124390, loss = 1.64 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:27.291790: step 124400, loss = 1.89 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:28.463949: step 124410, loss = 1.90 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:29.662223: step 124420, loss = 1.71 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 01:56:30.863608: step 124430, loss = 1.87 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 01:56:32.040208: step 124440, loss = 1.94 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:33.221643: step 124450, loss = 1.95 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:34.379404: step 124460, loss = 1.91 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:35.541586: step 124470, loss = 1.90 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:36.700110: step 124480, loss = 2.02 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:37.843990: step 124490, loss = 1.91 (1119.0 examples/sec; 0.114 sec/batch)
2017-05-05 01:56:39.025969: step 124500, loss = 1.92 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:40.196780: step 124510, loss = 1.93 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:41.362669: step 124520, loss = 1.93 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:4E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1281 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
2.523044: step 124530, loss = 1.81 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:43.701829: step 124540, loss = 1.90 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:44.851929: step 124550, loss = 1.81 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-05 01:56:46.011104: step 124560, loss = 1.91 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:47.195923: step 124570, loss = 1.88 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:48.374645: step 124580, loss = 1.84 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:49.533033: step 124590, loss = 1.91 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:50.690869: step 124600, loss = 1.72 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:51.850507: step 124610, loss = 1.86 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:53.029935: step 124620, loss = 1.76 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:56:54.185066: step 124630, loss = 1.86 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:55.356918: step 124640, loss = 1.87 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:56.515383: step 124650, loss = 1.80 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:56:57.683627: step 124660, loss = 1.96 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:56:58.849063: step 124670, loss = 2.01 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:00.020516: step 124680, loss = 1.94 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:01.187659: step 124690, loss = 1.81 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:02.363997: step 124700, loss = 1.87 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:03.533950: step 124710, loss = 2.05 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:04.705357: step 124720, loss = 1.84 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:05.855730: step 124730, loss = 1.99 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 01:57:07.114147: step 124740, loss = 2.01 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-05 01:57:08.172273: step 124750, loss = 2.00 (1209.7 examples/sec; 0.106 sec/batch)
2017-05-05 01:57:09.336790: step 124760, loss = 1.89 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:10.515059: step 124770, loss = 1.99 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:11.704450: step 124780, loss = 1.94 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:57:12.871179: step 124790, loss = 1.90 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:14.013378: step 124800, loss = 1.95 (1120.6 examples/sec; 0.114 sec/batch)
2017-05-05 01:57:15.194644: step 124810, loss = 1.83 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:16.347573: step 124820, loss = 1.87 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:57:17.515701: step 124830, loss = 1.87 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:18.671819: step 124840, loss = 1.94 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:19.832414: step 124850, loss = 1.89 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:21.015326: step 124860, loss = 1.90 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:22.174795: step 124870, loss = 2.05 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:23.337111: step 124880, loss = 1.92 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:24.519095: step 124890, loss = 1.96 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:25.680603: step 124900, loss = 2.01 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:26.850606: step 124910, loss = 1.84 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:28.013609: step 124920, loss = 1.92 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:29.170707: step 124930, loss = 1.83 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:30.337633: step 124940, loss = 1.96 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:31.511181: step 124950, loss = 1.95 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:32.663760: step 124960, loss = 2.02 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:57:33.838304: step 124970, loss = 1.90 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:34.990528: step 124980, loss = 2.03 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:57:36.145464: step 124990, loss = 1.99 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:57:37.302688: step 125000, loss = 2.14 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:38.455014: step 125010, loss = 1.95 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:57:39.626517: step 125020, loss = 1.96 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:40.784586: step 125030, loss = 1.91 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:41.938172: step 125040, loss = 1.78 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:57:43.125256: step 125050, loss = 2.03 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:57:44.301925: step 125060, loss = 1.83 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:45.491168: step 125070, loss = 1.93 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:57:46.649390: step 125080, loss = 2.00 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:47.799268: step 125090, loss = 1.90 (1113.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:57:48.974191: step 125100, loss = 1.76 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:57:50.137768: step 125110, loss = 1.83 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:51.315817: step 125120, loss = 1.93 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:52.475206: step 125130, loss = 1.93 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:57:53.652290: step 125140, loss = 2.01 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:54.852072: step 125150, loss = 2.15 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 01:57:56.037658: step 125160, loss = 1.93 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 01:57:57.239740: step 125170, loss = 2.04 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:57:58.420345: step 125180, loss = 1.87 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 01:57:59.595035: step 125190, loss = 1.82 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:00.794055: step 125200, loss = 1.78 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 01:58:01.972315: step 125210, loss = 1.90 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:03.221596: step 125220, loss = 1.98 (1024.6 examples/sec; 0.125 sec/batch)
2017-05-05 01:58:04.393630: step 125230, loss = 2.01 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:05.546448: step 125240, loss = 1.96 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:58:06.694915: step 125250, loss = 1.95 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-05 01:58:07.869931: step 125260, loss = 1.88 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:09.033419: step 125270, loss = 1.86 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:10.177740: step 125280, loss = 2.04 (1118.6 examples/sec; 0.114 sec/batch)
2017-05-05 01:58:11.343708: step 125290, loss = 1.86 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:12.498356: step 125300, loss = 1.89 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:58:13.653322: step 125310, loss = 1.96 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:58:14.821887: step 125320, loss = 1.75 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:15.990566: step 125330, loss = 1.87 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:17.151824: step 125340, loss = 1.78 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:18.304008: step 125350, loss = 1.88 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:58:19.450088: step 125360, loss = 1.93 (1116.9 examples/sec; 0.115 sec/batch)
2017-05-05 01:58:20.625176: step 125370, loss = 1.93 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:21.794868: step 125380, loss = 1.91 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:22.982979: step 125390, loss = 1.95 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 01:58:24.155042: step 125400, loss = 1.77 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:25.327366: step 125410, loss = 1.95 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:26.48E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1293 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
8453: step 125420, loss = 1.78 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:27.659992: step 125430, loss = 1.75 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:28.814584: step 125440, loss = 1.91 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 01:58:29.970943: step 125450, loss = 1.93 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:31.139725: step 125460, loss = 1.77 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:32.311801: step 125470, loss = 1.97 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:33.482865: step 125480, loss = 1.77 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:34.657558: step 125490, loss = 2.02 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:35.828443: step 125500, loss = 2.03 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:37.029459: step 125510, loss = 1.80 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:58:38.183330: step 125520, loss = 2.06 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:58:39.362422: step 125530, loss = 1.88 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:40.523043: step 125540, loss = 1.75 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:41.701074: step 125550, loss = 1.89 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:42.888836: step 125560, loss = 1.89 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:58:44.080944: step 125570, loss = 1.71 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:58:45.250621: step 125580, loss = 1.88 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:46.411479: step 125590, loss = 1.83 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:47.559696: step 125600, loss = 2.06 (1114.8 examples/sec; 0.115 sec/batch)
2017-05-05 01:58:48.720842: step 125610, loss = 1.93 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:49.872667: step 125620, loss = 1.98 (1111.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:58:51.031913: step 125630, loss = 1.92 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:52.211683: step 125640, loss = 1.88 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:58:53.378146: step 125650, loss = 1.89 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:54.542553: step 125660, loss = 1.91 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:58:55.713943: step 125670, loss = 1.75 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:56.887830: step 125680, loss = 1.89 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:58:58.027627: step 125690, loss = 2.04 (1123.0 examples/sec; 0.114 sec/batch)
2017-05-05 01:58:59.202715: step 125700, loss = 1.75 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:00.371554: step 125710, loss = 1.85 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:01.541458: step 125720, loss = 1.94 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:02.813178: step 125730, loss = 1.71 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-05 01:59:03.898678: step 125740, loss = 2.02 (1179.2 examples/sec; 0.109 sec/batch)
2017-05-05 01:59:05.088006: step 125750, loss = 1.76 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:59:06.238824: step 125760, loss = 1.83 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-05 01:59:07.403181: step 125770, loss = 1.74 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:08.555154: step 125780, loss = 1.87 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:59:09.715605: step 125790, loss = 1.83 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:10.865215: step 125800, loss = 1.87 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-05 01:59:12.044099: step 125810, loss = 1.97 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:13.228520: step 125820, loss = 1.89 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:14.401346: step 125830, loss = 1.88 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:15.568858: step 125840, loss = 1.75 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:16.762688: step 125850, loss = 1.84 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 01:59:17.929159: step 125860, loss = 1.82 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:19.110284: step 125870, loss = 2.01 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:20.309061: step 125880, loss = 1.87 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 01:59:21.503080: step 125890, loss = 1.93 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 01:59:22.676369: step 125900, loss = 1.83 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:23.843299: step 125910, loss = 1.78 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:25.021906: step 125920, loss = 1.88 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:26.202326: step 125930, loss = 1.72 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:27.361283: step 125940, loss = 2.12 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:28.543855: step 125950, loss = 1.85 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:29.686617: step 125960, loss = 1.88 (1120.1 examples/sec; 0.114 sec/batch)
2017-05-05 01:59:30.841987: step 125970, loss = 1.74 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:32.027842: step 125980, loss = 1.93 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:59:33.206381: step 125990, loss = 1.87 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:34.360489: step 126000, loss = 1.88 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 01:59:35.531011: step 126010, loss = 1.93 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:36.706808: step 126020, loss = 1.92 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:37.854582: step 126030, loss = 1.81 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-05 01:59:39.032272: step 126040, loss = 1.85 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 01:59:40.221079: step 126050, loss = 1.93 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 01:59:41.390860: step 126060, loss = 1.79 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:42.549254: step 126070, loss = 1.85 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:43.720565: step 126080, loss = 2.06 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:44.891336: step 126090, loss = 2.00 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:46.033923: step 126100, loss = 1.85 (1120.3 examples/sec; 0.114 sec/batch)
2017-05-05 01:59:47.223113: step 126110, loss = 1.91 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 01:59:48.390927: step 126120, loss = 1.93 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:49.557164: step 126130, loss = 1.83 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:50.724384: step 126140, loss = 2.02 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:51.883333: step 126150, loss = 1.89 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:53.054093: step 126160, loss = 1.85 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:54.214587: step 126170, loss = 1.98 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:55.387541: step 126180, loss = 1.86 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 01:59:56.552195: step 126190, loss = 1.88 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:57.707352: step 126200, loss = 1.92 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 01:59:58.892784: step 126210, loss = 2.04 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:00:00.075882: step 126220, loss = 1.90 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:01.265518: step 126230, loss = 1.76 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:00:02.437701: step 126240, loss = 1.88 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:03.614956: step 126250, loss = 1.87 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:04.780506: step 126260, loss = 1.89 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:05.935397: step 126270, loss = 1.78 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:00:07.111365: step 126280, loss = 1.95 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:08.294635: step 126290, loss = 1.88 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:09.439281: step 126300, loss = 1.95 (1118.2 examples/sec; 0.114 sec/batch)
2017-05-05 02:00:10.609982E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1304 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
: step 126310, loss = 1.88 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:11.798204: step 126320, loss = 1.88 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:00:12.980677: step 126330, loss = 1.89 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:14.130447: step 126340, loss = 2.07 (1113.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:00:15.314865: step 126350, loss = 1.82 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:16.490596: step 126360, loss = 1.77 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:17.667520: step 126370, loss = 2.00 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:18.837294: step 126380, loss = 1.94 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:20.019346: step 126390, loss = 1.93 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:21.195629: step 126400, loss = 1.86 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:22.340820: step 126410, loss = 1.81 (1117.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:00:23.496128: step 126420, loss = 2.02 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:24.659883: step 126430, loss = 1.79 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:25.837101: step 126440, loss = 1.76 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:26.990999: step 126450, loss = 2.01 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:00:28.164974: step 126460, loss = 1.82 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:29.325697: step 126470, loss = 1.83 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:30.486050: step 126480, loss = 1.82 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:31.652521: step 126490, loss = 2.04 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:32.835633: step 126500, loss = 1.88 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:33.991214: step 126510, loss = 1.87 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:35.144116: step 126520, loss = 1.94 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:00:36.317777: step 126530, loss = 1.91 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:37.468428: step 126540, loss = 2.01 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:00:38.618520: step 126550, loss = 1.86 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:00:39.787489: step 126560, loss = 1.79 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:40.959748: step 126570, loss = 1.82 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:42.134164: step 126580, loss = 1.82 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:43.289026: step 126590, loss = 1.87 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:00:44.470497: step 126600, loss = 1.81 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:45.636368: step 126610, loss = 2.00 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:46.811422: step 126620, loss = 2.10 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:47.973607: step 126630, loss = 1.86 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:49.155125: step 126640, loss = 1.84 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:00:50.311273: step 126650, loss = 1.86 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:51.498485: step 126660, loss = 1.76 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:00:52.665488: step 126670, loss = 1.91 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:53.838196: step 126680, loss = 1.84 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:55.009122: step 126690, loss = 1.92 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:00:56.168942: step 126700, loss = 1.80 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:00:57.319844: step 126710, loss = 1.90 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:00:58.595195: step 126720, loss = 1.87 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-05 02:00:59.670468: step 126730, loss = 1.99 (1190.4 examples/sec; 0.108 sec/batch)
2017-05-05 02:01:00.851817: step 126740, loss = 2.12 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:02.011636: step 126750, loss = 1.75 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:01:03.176367: step 126760, loss = 1.91 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:01:04.335025: step 126770, loss = 1.80 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:01:05.499010: step 126780, loss = 2.05 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:01:06.667024: step 126790, loss = 1.90 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:01:07.842054: step 126800, loss = 1.84 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:09.015016: step 126810, loss = 1.82 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:01:10.172453: step 126820, loss = 1.91 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:01:11.349403: step 126830, loss = 1.94 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:12.511016: step 126840, loss = 1.85 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:01:13.673347: step 126850, loss = 1.92 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:01:14.859416: step 126860, loss = 1.85 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:16.023010: step 126870, loss = 1.87 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:01:17.171340: step 126880, loss = 2.02 (1114.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:01:18.330925: step 126890, loss = 1.82 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:01:19.482292: step 126900, loss = 1.77 (1111.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:01:20.648985: step 126910, loss = 1.77 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:01:21.801745: step 126920, loss = 2.07 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:01:22.982459: step 126930, loss = 1.91 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:24.166446: step 126940, loss = 1.94 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:25.329821: step 126950, loss = 1.81 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:01:26.485269: step 126960, loss = 1.99 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:01:27.670053: step 126970, loss = 1.78 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:28.858577: step 126980, loss = 1.80 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:30.045037: step 126990, loss = 1.83 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:31.241798: step 127000, loss = 2.00 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:01:32.422309: step 127010, loss = 1.77 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:33.604279: step 127020, loss = 1.75 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:34.807963: step 127030, loss = 1.82 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:01:36.000540: step 127040, loss = 1.94 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:37.191528: step 127050, loss = 1.89 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:38.363966: step 127060, loss = 1.84 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:01:39.567610: step 127070, loss = 1.89 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:01:40.764713: step 127080, loss = 1.83 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:01:41.943886: step 127090, loss = 1.75 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:43.131932: step 127100, loss = 1.93 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:44.330306: step 127110, loss = 1.91 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:01:45.527377: step 127120, loss = 1.82 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:01:46.738563: step 127130, loss = 1.94 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:01:47.925376: step 127140, loss = 1.98 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:49.118668: step 127150, loss = 1.98 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:50.301906: step 127160, loss = 1.87 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:51.496623: step 127170, loss = 1.86 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:52.703477: step 127180, loss = 1.85 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:01:53.890496: step 127190, loss = 1.92 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:55.073484: step 127200, loss = 1.75 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:01:56.283797: step 127210, loss = 1.96 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:01:57.482879: step 127220, loss = 1.93 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:01:58.668008: step 127230, loss = 1.91 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:01:59.864691: step 127240, loss = 1.88 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:01.045731: step 127250, loss = 2.03 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:02.250043: step 127260, loss = 1.83 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:03.448298: step 127270, loss = 1.76 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:04.635493: step 127280, loss = 1.96 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:05.838786: step 127290, loss = 2.10 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:07.039070: step 127300, loss = 2.01 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:08.229355: step 127310, loss = 1.88 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:09.418125: step 127320, loss = 2.18 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:10.599054: step 127330, loss = 1.82 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:11.791048: step 127340, loss = 1.91 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:13.004752: step 127350, loss = 1.92 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:02:14.196765: step 127360, loss = 1.88 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:15.396036: step 127370, loss = 1.98 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:16.588329: step 127380, loss = 1.91 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:17.762566: step 127390, loss = 1.95 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:02:18.946255: step 127400, loss = 1.87 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:20.133442: step 127410, loss = 1.82 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:21.307562: step 127420, loss = 1.86 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:02:22.495207: step 127430, loss = 1.88 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:23.678210: step 127440, loss = 1.90 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:24.877025: step 127450, loss = 1.94 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:26.047535: step 127460, loss = 1.94 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:02:27.242562: step 127470, loss = 1.83 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:28.421032: step 127480, loss = 1.75 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:29.596909: step 127490, loss = 1.88 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:30.801994: step 127500, loss = 1.77 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:02:31.979081: step 127510, loss = 1.75 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:33.162729: step 127520, loss = 1.80 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:34.343711: step 127530, loss = 1.80 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:35.539949: step 127540, loss = 1.90 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:36.737970: step 127550, loss = 1.90 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:37.913048: step 127560, loss = 1.77 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:39.107378: step 127570, loss = 1.91 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:40.299975: step 127580, loss = 1.90 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:41.495526: step 127590, loss = 1.83 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:02:42.686760: step 127600, loss = 1.82 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:43.897348: step 127610, loss = 1.86 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:02:45.108509: step 127620, loss = 2.00 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:02:46.316618: step 127630, loss = 1.76 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:02:47.522112: step 127640, loss = 2.17 (1061.8 examples/sec; 0.1E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1315 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
21 sec/batch)
2017-05-05 02:02:48.734802: step 127650, loss = 1.78 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:02:49.894845: step 127660, loss = 1.79 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:02:51.083605: step 127670, loss = 1.88 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:52.270652: step 127680, loss = 1.91 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:02:53.447310: step 127690, loss = 1.83 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:54.624502: step 127700, loss = 1.96 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:02:55.894483: step 127710, loss = 1.83 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-05 02:02:56.977552: step 127720, loss = 2.06 (1181.8 examples/sec; 0.108 sec/batch)
2017-05-05 02:02:58.146151: step 127730, loss = 2.02 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:02:59.317432: step 127740, loss = 1.84 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:00.510605: step 127750, loss = 1.84 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:03:01.656459: step 127760, loss = 1.90 (1117.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:03:02.803804: step 127770, loss = 1.84 (1115.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:03:03.967465: step 127780, loss = 1.91 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:05.139941: step 127790, loss = 1.84 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:06.295621: step 127800, loss = 1.94 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:07.464106: step 127810, loss = 1.75 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:08.628711: step 127820, loss = 1.82 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:09.805290: step 127830, loss = 1.96 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:10.982512: step 127840, loss = 1.85 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:12.160565: step 127850, loss = 1.93 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:13.321112: step 127860, loss = 1.82 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:14.485748: step 127870, loss = 1.90 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:15.668519: step 127880, loss = 1.92 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:16.849214: step 127890, loss = 1.80 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:18.024720: step 127900, loss = 1.83 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:19.201873: step 127910, loss = 1.85 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:20.354106: step 127920, loss = 1.94 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:03:21.514866: step 127930, loss = 1.83 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:22.681208: step 127940, loss = 1.99 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:23.861322: step 127950, loss = 1.92 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:25.032373: step 127960, loss = 1.77 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:26.205639: step 127970, loss = 1.96 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:27.372765: step 127980, loss = 1.84 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:28.559278: step 127990, loss = 1.85 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:03:29.714324: step 128000, loss = 1.91 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:30.863868: step 128010, loss = 1.95 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:03:32.023155: step 128020, loss = 1.85 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:33.218732: step 128030, loss = 1.85 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:03:34.359485: step 128040, loss = 1.89 (1122.1 examples/sec; 0.114 sec/batch)
2017-05-05 02:03:35.541075: step 128050, loss = 1.87 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:36.710330: step 128060, loss = 2.16 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:37.865906: step 128070, loss = 1.84 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:39.039354: step 128080, loss = 1.83 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:40.207411: step 128090, loss = 1.91 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:41.362040: step 128100, loss = 1.93 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:03:42.530414: step 128110, loss = 1.74 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:43.730623: step 128120, loss = 1.86 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:03:44.898193: step 128130, loss = 1.78 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:46.051089: step 128140, loss = 1.97 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:03:47.214032: step 128150, loss = 1.90 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:48.397984: step 128160, loss = 1.79 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:49.569467: step 128170, loss = 2.05 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:03:50.733779: step 128180, loss = 1.95 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:51.890902: step 128190, loss = 1.88 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:53.044684: step 128200, loss = 1.93 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:03:54.185929: step 128210, loss = 1.76 (1121.6 examples/sec; 0.114 sec/batch)
2017-05-05 02:03:55.361378: step 128220, loss = 1.93 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:03:56.517414: step 128230, loss = 1.89 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:57.673962: step 128240, loss = 1.93 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:03:58.841033: step 128250, loss = 1.78 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:00.004268: step 128260, loss = 1.98 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:01.166542: step 128270, loss = 1.87 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:02.334141: step 128280, loss = 1.92 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:03.493903: step 128290, loss = 1.72 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:04.658129: step 128300, loss = 1.88 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:05.825398: step 128310, loss = 1.86 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:06.985522: step 128320, loss = 1.84 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:08.174678: step 128330, loss = 1.89 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:04:09.356214: step 128340, loss = 1.81 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:04:10.519992: step 128350, loss = 1.82 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:11.696052: step 128360, loss = 1.94 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:04:12.854068: step 128370, loss = 1.82 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:14.000856: step 128380, loss = 1.93 (1116.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:04:15.185520: step 128390, loss = 1.81 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:04:16.362114: step 128400, loss = 1.85 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:04:17.524227: step 128410, loss = 1.74 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:18.685976: step 128420, loss = 1.98 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:19.852662: step 128430, loss = 1.91 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:21.014265: step 128440, loss = 1.88 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:22.173001: step 128450, loss = 2.12 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:23.343353: step 128460, loss = 1.82 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:24.498372: step 128470, loss = 1.67 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:25.646868: step 128480, loss = 1.89 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:04:26.816500: step 128490, loss = 1.91 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:27.980060: step 128500, loss = 1.89 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:29.178532: step 128510, loss = 1.88 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:04:30.333653: step 128520, loss = 1.80 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:31.506154: step 128530, loss = 1.78 (1091.7 examples/sec; 0.117 sE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1326 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
ec/batch)
2017-05-05 02:04:32.677480: step 128540, loss = 1.99 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:33.848993: step 128550, loss = 1.99 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:35.012486: step 128560, loss = 1.73 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:36.178908: step 128570, loss = 1.79 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:37.347172: step 128580, loss = 1.82 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:38.515467: step 128590, loss = 1.89 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:39.681206: step 128600, loss = 1.85 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:40.852259: step 128610, loss = 2.04 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:41.991777: step 128620, loss = 1.84 (1123.3 examples/sec; 0.114 sec/batch)
2017-05-05 02:04:43.154409: step 128630, loss = 1.96 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:44.321177: step 128640, loss = 2.08 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:45.473844: step 128650, loss = 1.93 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:04:46.631215: step 128660, loss = 1.80 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:47.807724: step 128670, loss = 2.01 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:04:48.987966: step 128680, loss = 1.83 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:04:50.158153: step 128690, loss = 1.97 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:51.431062: step 128700, loss = 2.01 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-05 02:04:52.516213: step 128710, loss = 1.95 (1179.6 examples/sec; 0.109 sec/batch)
2017-05-05 02:04:53.661161: step 128720, loss = 1.93 (1118.0 examples/sec; 0.114 sec/batch)
2017-05-05 02:04:54.833015: step 128730, loss = 1.89 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:04:55.997876: step 128740, loss = 1.85 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:04:57.173673: step 128750, loss = 1.79 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:04:58.325634: step 128760, loss = 1.89 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:04:59.492472: step 128770, loss = 1.84 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:00.659646: step 128780, loss = 2.02 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:01.829027: step 128790, loss = 1.74 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:02.999321: step 128800, loss = 1.70 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:04.173327: step 128810, loss = 1.97 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:05.336634: step 128820, loss = 1.84 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:06.494256: step 128830, loss = 1.87 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:07.653946: step 128840, loss = 1.76 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:08.816947: step 128850, loss = 1.94 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:09.966845: step 128860, loss = 1.93 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:05:11.130813: step 128870, loss = 1.81 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:12.298332: step 128880, loss = 2.06 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:13.448316: step 128890, loss = 1.91 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:05:14.617324: step 128900, loss = 1.95 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:15.778344: step 128910, loss = 2.08 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:16.941738: step 128920, loss = 1.81 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:18.088001: step 128930, loss = 2.00 (1116.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:05:19.259983: step 128940, loss = 1.88 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:20.458386: step 128950, loss = 1.76 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:05:21.629409: step 128960, loss = 2.03 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:22.808467: step 128970, loss = 2.00 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:05:23.964488: step 128980, loss = 1.84 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:25.125581: step 128990, loss = 1.99 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:26.275887: step 129000, loss = 1.77 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:05:27.449490: step 129010, loss = 2.04 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:28.592050: step 129020, loss = 1.96 (1120.3 examples/sec; 0.114 sec/batch)
2017-05-05 02:05:29.739410: step 129030, loss = 1.90 (1115.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:05:30.897071: step 129040, loss = 1.90 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:32.057666: step 129050, loss = 1.92 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:33.225521: step 129060, loss = 1.78 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:34.384202: step 129070, loss = 1.94 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:35.550895: step 129080, loss = 1.87 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:36.718155: step 129090, loss = 1.88 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:37.870594: step 129100, loss = 2.08 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:05:39.041074: step 129110, loss = 1.83 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:40.226853: step 129120, loss = 1.85 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:05:41.383344: step 129130, loss = 1.90 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:42.546571: step 129140, loss = 1.91 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:43.711365: step 129150, loss = 1.84 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:44.869607: step 129160, loss = 1.89 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:46.027543: step 129170, loss = 1.89 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:47.200859: step 129180, loss = 1.87 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:48.365787: step 129190, loss = 1.89 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:49.528714: step 129200, loss = 1.95 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:50.704356: step 129210, loss = 2.12 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:05:51.865413: step 129220, loss = 1.84 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:05:53.034094: step 129230, loss = 1.93 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:05:54.183910: step 129240, loss = 1.91 (1113.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:05:55.379939: step 129250, loss = 1.78 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:05:56.576198: step 129260, loss = 1.80 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:05:57.762205: step 129270, loss = 1.85 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:05:58.948266: step 129280, loss = 1.92 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:00.144798: step 129290, loss = 1.74 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:06:01.318313: step 129300, loss = 1.81 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:02.492462: step 129310, loss = 1.98 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:03.686303: step 129320, loss = 1.86 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:04.894287: step 129330, loss = 1.99 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:06:06.083074: step 129340, loss = 1.88 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:07.254595: step 129350, loss = 1.77 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:08.452337: step 129360, loss = 1.83 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:06:09.643065: step 129370, loss = 1.99 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:10.835717: step 129380, loss = 1.89 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:12.021285: step 129390, loss = 2.14 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:13.212293: step 129400, loss = 1.82 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:14.381359: step 129410, loss = 1.84 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:15.568636: step 129420, loss = 1.98 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:16.758047: step 129430, loss = 1.78 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:17.946079: step 129440, loss = 1.80 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:19.139018: step 129450, loss = 1.96 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:20.325144: step 129460, loss = 1.86 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:21.496931: step 129470, loss = 1.98 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:22.664323: step 129480, loss = 1.80 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:23.833102: step 129490, loss = 2.03 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:25.012679: step 129500, loss = 2.13 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:26.189509: step 129510, loss = 1.91 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:27.359041: step 129520, loss = 1.89 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:28.549594: step 129530, loss = 1.86 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:29.709394: step 129540, loss = 1.88 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:06:30.896654: step 129550, loss = 2.01 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:32.078197: step 129560, loss = 1.84 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:33.275934: step 129570, loss = 1.95 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:06:34.425034: step 129580, loss = 1.80 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:06:35.590547: step 129590, loss = 1.79 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:36.762112: step 129600, loss = 1.86 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:37.921425: step 129610, loss = 1.93 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:06:39.080748: step 129620, loss = 1.86 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:06:40.250396: step 129630, loss = 2.01 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:41.411406: step 129640, loss = 1.82 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:06:42.564399: step 129650, loss = 1.86 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:06:43.727863: step 129660, loss = 1.94 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:06:44.900854: step 129670, loss = 1.91 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:06:46.062068: step 129680, loss = 1.84 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:06:47.348618: step 129690, loss = 1.88 (994.9 examples/sec; 0.129 sec/batch)
2017-05-05 02:06:48.446210: step 129700, loss = 2.02 (1166.2 examples/sec; 0.110 sec/batch)
2017-05-05 02:06:49.657795: step 129710, loss = 1.88 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:06:50.854463: step 129720, loss = 1.91 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:06:52.045953: step 129730, loss = 2.11 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:53.233562: step 129740, loss = 1.99 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:54.410284: step 129750, loss = 1.89 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:06:55.607282: step 129760, loss = 1.84 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:06:56.803001: step 129770, loss = 1.78 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:06:57.990184: step 129780, loss = 1.85 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:06:59.180749: step 129790, loss = 1.90 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:07:00.385115: step 129800, loss = 1.80 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:07:01.535997: step 129810, loss = 1.82 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:07:02.729606: step 129820, loss = 1.81 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:07:03.917196: step 129830, loss = 1.84 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:07:05.076176: step 129840, loss = 1.79 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:06.222347: step 129850, loss = 1.86 (1116.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:07:07.394757: step 129860, loss = 1.98 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:08.566837: step 129870, loE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1337 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
ss = 1.95 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:09.745304: step 129880, loss = 1.89 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:11.014634: step 129890, loss = 1.86 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-05 02:07:12.145146: step 129900, loss = 2.02 (1132.2 examples/sec; 0.113 sec/batch)
2017-05-05 02:07:13.378956: step 129910, loss = 1.86 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-05 02:07:14.587573: step 129920, loss = 1.89 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:07:15.803151: step 129930, loss = 1.84 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:07:16.999940: step 129940, loss = 1.95 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:07:18.198057: step 129950, loss = 1.84 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:07:19.374847: step 129960, loss = 1.87 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:20.556464: step 129970, loss = 1.80 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:21.733675: step 129980, loss = 1.97 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:22.911327: step 129990, loss = 1.72 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:24.109192: step 130000, loss = 1.89 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:07:25.285402: step 130010, loss = 1.76 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:26.458823: step 130020, loss = 1.96 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:27.648269: step 130030, loss = 1.80 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:07:28.819130: step 130040, loss = 1.83 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:29.978676: step 130050, loss = 1.76 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:31.127492: step 130060, loss = 1.86 (1114.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:07:32.291862: step 130070, loss = 1.88 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:33.463091: step 130080, loss = 2.04 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:34.646106: step 130090, loss = 2.16 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:35.840796: step 130100, loss = 1.83 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:07:37.013332: step 130110, loss = 1.91 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:38.184971: step 130120, loss = 1.82 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:39.369176: step 130130, loss = 1.91 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:40.531453: step 130140, loss = 2.04 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:41.694718: step 130150, loss = 1.83 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:42.847924: step 130160, loss = 1.93 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:07:44.006668: step 130170, loss = 1.78 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:07:45.182363: step 130180, loss = 1.84 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:46.350091: step 130190, loss = 1.94 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:47.522178: step 130200, loss = 1.96 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:48.674725: step 130210, loss = 1.92 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:07:49.856593: step 130220, loss = 1.80 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:51.006881: step 130230, loss = 1.86 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:07:52.179422: step 130240, loss = 1.88 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:53.351248: step 130250, loss = 1.84 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:54.534530: step 130260, loss = 1.87 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:55.707416: step 130270, loss = 1.91 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:07:56.887398: step 130280, loss = 1.84 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:58.071427: step 130290, loss = 1.89 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:07:59.252446: step 130300, loss = 1.81 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:08:00.423305: step 130310, loss = 1.80 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:01.606126: step 130320, loss = 1.84 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:08:02.775740: step 130330, loss = 1.96 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:03.947778: step 130340, loss = 1.96 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:05.108720: step 130350, loss = 2.03 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:06.269204: step 130360, loss = 1.74 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:07.425093: step 130370, loss = 1.84 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:08.590578: step 130380, loss = 1.81 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:09.746778: step 130390, loss = 1.77 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:10.909532: step 130400, loss = 1.96 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:12.064834: step 130410, loss = 1.94 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:13.236204: step 130420, loss = 2.03 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:14.388878: step 130430, loss = 1.80 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:08:15.548343: step 130440, loss = 1.83 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:16.708803: step 130450, loss = 1.83 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:17.884635: step 130460, loss = 1.89 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:08:19.046857: step 130470, loss = 2.02 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:20.204788: step 130480, loss = 1.77 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:21.381286: step 130490, loss = 1.84 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:08:22.539138: step 130500, loss = 1.81 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:23.718159: step 130510, loss = 1.89 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:08:24.900999: step 130520, loss = 1.79 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:08:26.046784: step 130530, loss = 1.92 (1117.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:08:27.201731: step 130540, loss = 1.83 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:08:28.361603: step 130550, loss = 2.01 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:29.523297: step 130560, loss = 1.78 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:30.677542: step 130570, loss = 1.91 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:08:31.848170: step 130580, loss = 1.91 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:33.008395: step 130590, loss = 1.85 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:34.167653: step 130600, loss = 1.84 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:35.349192: step 130610, loss = 1.88 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:08:36.560931: step 130620, loss = 1.93 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:08:37.713002: step 130630, loss = 1.82 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:08:38.872949: step 130640, loss = 1.99 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:40.044809: step 130650, loss = 1.96 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:41.211975: step 130660, loss = 2.05 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:42.366777: step 130670, loss = 1.79 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:08:43.632823: step 130680, loss = 1.98 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-05 02:08:44.685993: step 130690, loss = 1.79 (1215.4 examples/sec; 0.105 sec/batch)
2017-05-05 02:08:45.840343: step 130700, loss = 1.90 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:08:47.022295: step 130710, loss = 1.80 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:08:48.189341: step 130720, loss = 1.89 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:49.350532: step 130730, loss = 1.80 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:50.504312: step 130740, loss = 1.86 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:08:51.673537: step 130750, loss = 2.09 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:52.832620: step 130760, loss =E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1349 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
 1.84 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:53.994044: step 130770, loss = 2.05 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:55.163878: step 130780, loss = 1.76 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:08:56.356348: step 130790, loss = 1.93 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:08:57.552499: step 130800, loss = 1.76 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:08:58.712863: step 130810, loss = 2.04 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:08:59.876769: step 130820, loss = 1.81 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:01.035298: step 130830, loss = 2.04 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:02.183071: step 130840, loss = 1.91 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:09:03.348607: step 130850, loss = 1.89 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:04.547643: step 130860, loss = 1.96 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:09:05.700912: step 130870, loss = 2.07 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:09:06.871928: step 130880, loss = 1.95 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:08.059543: step 130890, loss = 1.98 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:09:09.218016: step 130900, loss = 1.78 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:10.402656: step 130910, loss = 1.80 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:09:11.581571: step 130920, loss = 1.90 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:09:12.747734: step 130930, loss = 1.77 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:13.892191: step 130940, loss = 1.96 (1118.4 examples/sec; 0.114 sec/batch)
2017-05-05 02:09:15.056550: step 130950, loss = 1.84 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:16.235354: step 130960, loss = 1.92 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:09:17.384276: step 130970, loss = 1.90 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:09:18.551157: step 130980, loss = 1.81 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:19.715287: step 130990, loss = 1.90 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:20.883410: step 131000, loss = 1.96 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:22.035296: step 131010, loss = 2.02 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:09:23.222734: step 131020, loss = 1.81 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:09:24.381714: step 131030, loss = 1.84 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:25.536667: step 131040, loss = 1.96 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:09:26.701785: step 131050, loss = 1.90 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:27.863556: step 131060, loss = 1.98 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:29.037409: step 131070, loss = 1.84 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:30.190813: step 131080, loss = 1.86 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:09:31.365226: step 131090, loss = 1.85 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:32.532521: step 131100, loss = 2.04 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:33.686669: step 131110, loss = 1.90 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:09:34.851799: step 131120, loss = 1.77 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:36.018439: step 131130, loss = 1.93 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:37.191906: step 131140, loss = 2.09 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:38.349550: step 131150, loss = 1.89 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:39.533780: step 131160, loss = 2.11 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:09:40.708365: step 131170, loss = 1.91 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:41.868334: step 131180, loss = 1.86 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:43.025487: step 131190, loss = 1.88 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:44.194402: step 131200, loss = 1.75 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:45.356225: step 131210, loss = 1.88 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:46.500510: step 131220, loss = 1.90 (1118.6 examples/sec; 0.114 sec/batch)
2017-05-05 02:09:47.670895: step 131230, loss = 1.86 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:48.839537: step 131240, loss = 1.74 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:49.997938: step 131250, loss = 1.81 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:51.165946: step 131260, loss = 1.87 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:09:52.317579: step 131270, loss = 1.76 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:09:53.460928: step 131280, loss = 1.98 (1119.5 examples/sec; 0.114 sec/batch)
2017-05-05 02:09:54.640433: step 131290, loss = 1.91 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:09:55.824619: step 131300, loss = 1.96 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:09:57.017220: step 131310, loss = 2.00 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:09:58.179964: step 131320, loss = 1.82 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:09:59.361698: step 131330, loss = 1.99 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:00.535928: step 131340, loss = 1.93 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:01.722806: step 131350, loss = 1.97 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:02.912518: step 131360, loss = 1.96 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:04.089829: step 131370, loss = 1.79 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:05.251707: step 131380, loss = 1.93 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:10:06.416033: step 131390, loss = 2.03 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:10:07.589054: step 131400, loss = 1.75 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:08.769034: step 131410, loss = 1.89 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:09.937737: step 131420, loss = 2.05 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:11.118997: step 131430, loss = 2.04 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:12.303350: step 131440, loss = 2.00 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:13.475684: step 131450, loss = 1.91 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:14.641514: step 131460, loss = 1.74 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:15.830197: step 131470, loss = 1.85 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:17.032527: step 131480, loss = 1.94 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:10:18.200958: step 131490, loss = 1.96 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:19.397066: step 131500, loss = 1.90 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:10:20.571350: step 131510, loss = 1.85 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:21.721896: step 131520, loss = 1.92 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:10:22.892542: step 131530, loss = 1.80 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:24.076369: step 131540, loss = 1.84 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:25.262891: step 131550, loss = 1.93 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:26.452910: step 131560, loss = 1.88 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:27.641418: step 131570, loss = 1.83 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:28.811485: step 131580, loss = 2.04 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:30.003208: step 131590, loss = 1.86 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:31.212787: step 131600, loss = 1.86 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:10:32.404917: step 131610, loss = 1.76 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:33.591728: step 131620, loss = 1.90 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:34.788150: step 131630, loss = 1.89 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:10:35.982819: step 131640, loss = 1.80 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:37.180458: step 131650, loss = 1.84 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:10:38.371274: step 131660, loss = 1.70 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:39.697414: step 131670, loss = 1.91 (965.2 examples/sec; 0.133 sec/batch)
2017-05-05 02:10:40.804686: step 131680, loss = 2.01 (1156.0 examples/sec; 0.111 sec/batch)
2017-05-05 02:10:41.961957: step 131690, loss = 1.87 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:10:43.148415: step 131700, loss = 1.90 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:44.322803: step 131710, loss = 2.07 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:45.499874: step 131720, loss = 1.82 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:46.683963: step 131730, loss = 1.89 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:47.861410: step 131740, loss = 1.73 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:49.026071: step 131750, loss = 1.95 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:10:50.187443: step 131760, loss = 1.84 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:10:51.363076: step 131770, loss = 1.95 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:52.551088: step 131780, loss = 1.80 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:10:53.714670: step 131790, loss = 1.76 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:10:54.896864: step 131800, loss = 2.01 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:56.073138: step 131810, loss = 1.72 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:10:57.231531: step 131820, loss = 1.91 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:10:58.403759: step 131830, loss = 1.81 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:10:59.573798: step 131840, loss = 1.98 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:00.759782: step 131850, loss = 1.91 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:11:01.916219: step 131860, loss = 1.83 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:03.091192: step 131870, loss = 2.04 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:04.256679: step 131880, loss = 1.88 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:05.427188: step 131890, loss = 2.00 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:06.587832: step 131900, loss = 1.92 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:07.770455: step 131910, loss = 1.91 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:08.947749: step 131920, loss = 1.80 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:10.104185: step 131930, loss = 1.85 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:11.283821: step 131940, loss = 2.03 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:12.428894: step 131950, loss = 1.83 (1117.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:11:13.595447: step 131960, loss = 1.88 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:14.764456: step 131970, loss = 1.88 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:15.919551: step 131980, loss = 1.98 (1108.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:17.090730: step 131990, loss = 1.85 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:18.259917: step 132000, loss = 1.93 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:19.450880: step 132010, loss = 1.86 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:11:20.625815: step 132020, loss = 1.98 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:21.797186: step 132030, loss = 1.96 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:22.967734: step 132040, loss = 1.89 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:24.133509: step 132050, loss = 1.87 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:25.307877: step 132060, loss = 1.90 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:26.474397: step 132070, loss = 1.79 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:27.638065: step 132080, loss = 2.00 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:28.807551: step 132090, loss = 2.13 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1360 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
05 02:11:29.984849: step 132100, loss = 2.04 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:31.181995: step 132110, loss = 1.87 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:11:32.354672: step 132120, loss = 1.80 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:33.526557: step 132130, loss = 1.84 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:34.699943: step 132140, loss = 1.76 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:35.863434: step 132150, loss = 1.86 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:37.046075: step 132160, loss = 1.84 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:38.200425: step 132170, loss = 1.95 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:11:39.364550: step 132180, loss = 1.79 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:40.533891: step 132190, loss = 1.97 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:41.681223: step 132200, loss = 1.76 (1115.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:11:42.850505: step 132210, loss = 1.88 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:44.008982: step 132220, loss = 2.07 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:11:45.189674: step 132230, loss = 2.00 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:46.335207: step 132240, loss = 1.78 (1117.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:11:47.513732: step 132250, loss = 1.75 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:48.687293: step 132260, loss = 1.89 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:11:49.869734: step 132270, loss = 1.87 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:11:51.062516: step 132280, loss = 1.88 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:11:52.264915: step 132290, loss = 1.81 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:11:53.452907: step 132300, loss = 1.82 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:11:54.637967: step 132310, loss = 1.93 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:11:55.837146: step 132320, loss = 1.90 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:11:57.052415: step 132330, loss = 1.94 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:11:58.239464: step 132340, loss = 1.84 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:11:59.433513: step 132350, loss = 1.85 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:12:00.621294: step 132360, loss = 1.81 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:12:01.809205: step 132370, loss = 1.87 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:12:03.017905: step 132380, loss = 1.96 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:12:04.222351: step 132390, loss = 1.97 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:12:05.422349: step 132400, loss = 1.95 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:12:06.608056: step 132410, loss = 1.86 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:12:07.785138: step 132420, loss = 1.91 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:12:08.962984: step 132430, loss = 1.89 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:12:10.137090: step 132440, loss = 1.95 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:11.336357: step 132450, loss = 1.91 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:12:12.521779: step 132460, loss = 1.86 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:12:13.689937: step 132470, loss = 1.70 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:14.881066: step 132480, loss = 1.88 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:12:16.051487: step 132490, loss = 1.83 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:17.216775: step 132500, loss = 1.75 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:18.407100: step 132510, loss = 1.95 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:12:19.588712: step 132520, loss = 1.98 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:12:20.769797: step 132530, loss = 1.89 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:12:21.938896: step 132540, loss = 1.86 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:23.117602: step 132550, loss = 1.94 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:12:24.283848: step 132560, loss = 1.95 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:25.446006: step 132570, loss = 1.89 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:12:26.603993: step 132580, loss = 1.82 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:12:27.754379: step 132590, loss = 1.89 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:12:28.926800: step 132600, loss = 1.94 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:30.076162: step 132610, loss = 2.08 (1113.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:12:31.241928: step 132620, loss = 1.97 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:32.408180: step 132630, loss = 1.94 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:33.574784: step 132640, loss = 1.87 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:34.721396: step 132650, loss = 1.90 (1116.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:12:36.008533: step 132660, loss = 1.92 (994.5 examples/sec; 0.129 sec/batch)
2017-05-05 02:12:37.089624: step 132670, loss = 1.75 (1184.0 examples/sec; 0.108 sec/batch)
2017-05-05 02:12:38.237479: step 132680, loss = 1.89 (1115.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:12:39.425623: step 132690, loss = 2.10 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:12:40.594550: step 132700, loss = 1.93 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:41.760614: step 132710, loss = 1.88 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:42.909899: step 132720, loss = 1.87 (1113.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:12:44.075894: step 132730, loss = 1.91 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:45.236416: step 132740, loss = 1.97 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:12:46.409503: step 132750, loss = 1.97 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:47.554694: step 132760, loss = 1.71 (1117.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:12:48.714747: step 132770, loss = 2.01 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:12:49.863988: step 132780, loss = 2.07 (1113.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:12:51.037867: step 132790, loss = 1.99 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:12:52.222388: step 132800, loss = 1.79 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:12:53.397412: step 132810, loss = 1.90 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:12:54.598159: step 132820, loss = 1.86 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:12:55.796006: step 132830, loss = 1.87 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:12:56.990497: step 132840, loss = 1.79 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:12:58.202985: step 132850, loss = 1.87 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:12:59.506553: step 132860, loss = 1.84 (981.9 examples/sec; 0.130 sec/batch)
2017-05-05 02:13:00.637317: step 132870, loss = 1.94 (1132.0 examples/sec; 0.113 sec/batch)
2017-05-05 02:13:01.830315: step 132880, loss = 1.80 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:13:03.016856: step 132890, loss = 1.93 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:13:04.229487: step 132900, loss = 1.92 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:05.427616: step 132910, loss = 2.13 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:13:06.620681: step 132920, loss = 2.02 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:13:07.832143: step 132930, loss = 1.77 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:09.053559: step 132940, loss = 2.13 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:13:10.245742: step 132950, loss = 1.77 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:13:11.455086: step 132960, loss = 2.08 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:12.659920: step 132970, loss = 1.95 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:13:13.861577: step 132980, loss = 1.96 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1371 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
13:15.070934: step 132990, loss = 1.94 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:16.284000: step 133000, loss = 1.97 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:17.497200: step 133010, loss = 1.92 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:18.694571: step 133020, loss = 1.77 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:13:19.913055: step 133030, loss = 2.04 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:13:21.122276: step 133040, loss = 1.98 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:22.335736: step 133050, loss = 1.98 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:23.551673: step 133060, loss = 1.90 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:13:24.742588: step 133070, loss = 1.82 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:13:25.942205: step 133080, loss = 2.03 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:13:27.145120: step 133090, loss = 1.85 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:13:28.371350: step 133100, loss = 2.08 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:13:29.589176: step 133110, loss = 1.87 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:13:30.795758: step 133120, loss = 2.00 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:32.011756: step 133130, loss = 1.88 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:13:33.220547: step 133140, loss = 1.97 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:13:34.405072: step 133150, loss = 1.88 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:13:35.628430: step 133160, loss = 1.78 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:13:36.824777: step 133170, loss = 1.94 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:13:38.006385: step 133180, loss = 1.92 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:13:39.196568: step 133190, loss = 1.99 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:13:40.358378: step 133200, loss = 1.83 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:13:41.541045: step 133210, loss = 1.86 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:13:42.719441: step 133220, loss = 1.85 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:13:43.897282: step 133230, loss = 1.99 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:13:45.067089: step 133240, loss = 1.94 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:13:46.218586: step 133250, loss = 2.01 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:13:47.375324: step 133260, loss = 1.89 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:13:48.545624: step 133270, loss = 1.84 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:13:49.703638: step 133280, loss = 1.83 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:13:50.868845: step 133290, loss = 1.92 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:13:52.012175: step 133300, loss = 2.01 (1119.5 examples/sec; 0.114 sec/batch)
2017-05-05 02:13:53.180148: step 133310, loss = 1.91 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:13:54.347000: step 133320, loss = 1.86 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:13:55.513135: step 133330, loss = 1.84 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:13:56.675892: step 133340, loss = 1.92 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:13:57.828435: step 133350, loss = 1.97 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:13:58.987831: step 133360, loss = 1.95 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:00.150379: step 133370, loss = 1.87 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:01.313363: step 133380, loss = 1.72 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:02.458916: step 133390, loss = 1.96 (1117.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:14:03.616789: step 133400, loss = 1.98 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:04.782595: step 133410, loss = 1.97 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:05.941633: step 133420, loss = 1.69 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:07.106146: step 133430, loss = 1.89 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:08.278511: step 133440, loss = 1.88 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:09.436346: step 133450, loss = 1.88 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:10.612774: step 133460, loss = 1.87 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:11.787055: step 133470, loss = 1.86 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:12.960974: step 133480, loss = 1.98 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:14.120162: step 133490, loss = 1.98 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:15.289204: step 133500, loss = 1.87 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:16.468779: step 133510, loss = 1.80 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:17.620855: step 133520, loss = 1.89 (1111.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:14:18.800402: step 133530, loss = 1.80 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:19.959468: step 133540, loss = 1.85 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:21.131462: step 133550, loss = 1.86 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:22.291861: step 133560, loss = 1.95 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:23.482695: step 133570, loss = 1.79 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:14:24.663584: step 133580, loss = 1.87 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:25.832744: step 133590, loss = 1.86 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:27.009488: step 133600, loss = 1.78 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:28.189769: step 133610, loss = 2.00 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:29.346840: step 133620, loss = 1.80 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:14:30.514080: step 133630, loss = 1.84 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:31.691508: step 133640, loss = 1.82 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:32.961482: step 133650, loss = 1.84 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-05 02:14:34.042600: step 133660, loss = 1.96 (1184.0 examples/sec; 0.108 sec/batch)
2017-05-05 02:14:35.233931: step 133670, loss = 1.93 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:14:36.419532: step 133680, loss = 1.82 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:14:37.611569: step 133690, loss = 1.88 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:14:38.815819: step 133700, loss = 1.81 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:14:40.016165: step 133710, loss = 1.84 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:14:41.213764: step 133720, loss = 1.80 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:14:42.415092: step 133730, loss = 1.86 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:14:43.643481: step 133740, loss = 1.79 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-05 02:14:44.862626: step 133750, loss = 1.85 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:14:46.056834: step 133760, loss = 1.90 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:14:47.251083: step 133770, loss = 1.83 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:14:48.442742: step 133780, loss = 1.83 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:14:49.623607: step 133790, loss = 2.00 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:50.804644: step 133800, loss = 1.78 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:51.991732: step 133810, loss = 2.02 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:14:53.159710: step 133820, loss = 1.82 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:14:54.341834: step 133830, loss = 1.72 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:14:55.537980: step 133840, loss = 1.96 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:14:56.777812: step 133850, loss = 1.85 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-05 02:14:57.899083: step 133860, loss = 1.96 (1141.6 examples/sec; 0.112 sec/batch)
2017-05-05 02:14:59.095417: step 133870, loss = 1.86 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:00.331596: step 133880, loss = 1.75 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-05 02:15:01.516628: step 133890, loss = 1.84 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:15:02.720575: step 133900, loss = 1.96 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:03.912330: step 133910, loss = 1.98 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:15:05.107354: step 133920, loss = 1.91 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:06.316518: step 133930, loss = 1.85 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:15:07.496463: step 133940, loss = 1.90 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:08.698924: step 133950, loss = 1.96 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:09.875565: step 133960, loss = 1.88 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:11.050155: step 133970, loss = 1.89 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:12.271921: step 133980, loss = 1.79 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:15:13.469595: step 133990, loss = 1.82 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:14.673800: step 134000, loss = 1.85 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:15.849468: step 134010, loss = 1.95 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:17.039446: step 134020, loss = 1.89 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:15:18.248952: step 134030, loss = 2.11 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:15:19.456543: step 134040, loss = 2.03 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:15:20.657083: step 134050, loss = 1.77 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:21.842304: step 134060, loss = 2.01 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:15:23.014387: step 134070, loss = 1.84 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:24.197969: step 134080, loss = 1.90 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:25.377839: step 134090, loss = 2.04 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:26.573975: step 134100, loss = 1.92 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:27.768031: step 134110, loss = 1.96 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:15:28.937177: step 134120, loss = 1.85 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:30.090861: step 134130, loss = 2.07 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:15:31.253030: step 134140, loss = 1.87 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:15:32.426318: step 134150, loss = 2.03 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:33.580951: step 134160, loss = 1.93 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:15:34.728710: step 134170, loss = 1.83 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:15:35.901199: step 134180, loss = 1.85 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:37.074212: step 134190, loss = 1.90 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:38.241816: step 134200, loss = 1.90 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:39.420304: step 134210, loss = 1.89 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:40.599208: step 134220, loss = 1.90 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:41.762935: step 134230, loss = 1.76 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:15:42.950487: step 134240, loss = 1.88 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:15:44.152049: step 134250, loss = 1.79 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:15:45.312205: step 134260, loss = 1.88 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:15:46.487070: step 134270, loss = 1.95 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:47.659505: step 134280, loss = 1.91 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:48.836095: step 134290, loss = 1.85 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:50.017337: step 134300, loss = 1.93 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:51.209587: step 134310, loss = 1.84 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:15:52.396198: step 134320, loss = 1.97 (1078.7 exampE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1382 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
les/sec; 0.119 sec/batch)
2017-05-05 02:15:53.568823: step 134330, loss = 1.81 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:54.722747: step 134340, loss = 1.84 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:15:55.893713: step 134350, loss = 1.88 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:15:57.077259: step 134360, loss = 1.78 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:15:58.239587: step 134370, loss = 1.95 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:15:59.400438: step 134380, loss = 1.79 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:00.573825: step 134390, loss = 1.93 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:01.708837: step 134400, loss = 1.78 (1127.7 examples/sec; 0.114 sec/batch)
2017-05-05 02:16:02.885363: step 134410, loss = 1.82 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:04.045093: step 134420, loss = 1.85 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:05.227627: step 134430, loss = 1.89 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:06.383525: step 134440, loss = 1.95 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:07.560050: step 134450, loss = 1.79 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:08.732044: step 134460, loss = 1.88 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:09.880889: step 134470, loss = 1.97 (1114.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:16:11.069420: step 134480, loss = 1.86 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:16:12.246499: step 134490, loss = 1.85 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:13.405886: step 134500, loss = 1.96 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:14.584277: step 134510, loss = 1.85 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:15.764325: step 134520, loss = 1.87 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:16.947029: step 134530, loss = 1.89 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:18.114165: step 134540, loss = 1.86 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:19.292147: step 134550, loss = 1.93 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:20.467017: step 134560, loss = 1.99 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:21.645320: step 134570, loss = 2.04 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:22.818332: step 134580, loss = 1.94 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:23.989044: step 134590, loss = 1.90 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:25.175005: step 134600, loss = 1.76 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:16:26.338935: step 134610, loss = 1.91 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:27.512773: step 134620, loss = 1.79 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:28.685637: step 134630, loss = 1.92 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:29.932267: step 134640, loss = 1.84 (1026.8 examples/sec; 0.125 sec/batch)
2017-05-05 02:16:31.037050: step 134650, loss = 1.86 (1158.6 examples/sec; 0.110 sec/batch)
2017-05-05 02:16:32.219138: step 134660, loss = 1.86 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:33.394036: step 134670, loss = 1.77 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:34.564514: step 134680, loss = 1.92 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:35.756324: step 134690, loss = 1.84 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:16:36.920831: step 134700, loss = 2.02 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:38.082353: step 134710, loss = 1.79 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:39.267085: step 134720, loss = 1.93 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:40.447717: step 134730, loss = 1.94 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:41.606456: step 134740, loss = 1.91 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:42.778191: step 134750, loss = 2.06 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:43.943853: step 134760, loss = 2.04 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:45.114520: step 134770, loss = 1.83 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:46.272906: step 134780, loss = 2.01 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:47.430827: step 134790, loss = 1.90 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:48.590924: step 134800, loss = 1.96 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:49.748770: step 134810, loss = 1.97 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:50.929584: step 134820, loss = 1.93 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:52.093758: step 134830, loss = 1.77 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:53.266225: step 134840, loss = 1.85 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:16:54.421972: step 134850, loss = 1.97 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:55.597134: step 134860, loss = 1.92 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:16:56.757878: step 134870, loss = 1.84 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:57.917624: step 134880, loss = 1.91 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:16:59.115806: step 134890, loss = 1.89 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:17:00.279309: step 134900, loss = 1.97 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:17:01.447506: step 134910, loss = 1.92 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:02.596150: step 134920, loss = 1.85 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:17:03.775663: step 134930, loss = 1.95 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:04.949498: step 134940, loss = 1.88 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:06.099007: step 134950, loss = 2.04 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:17:07.272351: step 134960, loss = 1.77 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:08.455662: step 134970, loss = 1.82 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:09.640538: step 134980, loss = 2.06 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:10.847459: step 134990, loss = 1.81 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:17:12.048440: step 135000, loss = 1.84 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:17:13.224572: step 135010, loss = 1.96 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:14.401435: step 135020, loss = 1.93 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:15.609732: step 135030, loss = 2.01 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:17:16.790869: step 135040, loss = 1.92 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:17.967095: step 135050, loss = 1.95 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:19.152845: step 135060, loss = 1.79 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:17:20.332740: step 135070, loss = 1.82 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:21.525331: step 135080, loss = 1.80 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:17:22.694654: step 135090, loss = 1.87 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:23.847648: step 135100, loss = 1.93 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:17:25.029823: step 135110, loss = 1.72 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:26.181793: step 135120, loss = 1.89 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:17:27.382699: step 135130, loss = 2.04 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:17:28.566277: step 135140, loss = 1.88 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:29.728926: step 135150, loss = 1.89 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:17:30.883421: step 135160, loss = 1.83 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:17:32.046627: step 135170, loss = 1.80 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:17:33.211618: step 135180, loss = 1.85 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:17:34.372054: step 135190, loss = 1.93 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:17:35.545418: step 135200, loss = 2.05 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:36.714695: step 135210, loss = 1.90 (1094.7 examples/E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1393 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
sec; 0.117 sec/batch)
2017-05-05 02:17:37.868598: step 135220, loss = 1.86 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:17:39.044400: step 135230, loss = 1.80 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:40.217692: step 135240, loss = 2.02 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:41.380408: step 135250, loss = 1.94 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:17:42.552893: step 135260, loss = 1.91 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:43.722660: step 135270, loss = 2.01 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:44.886329: step 135280, loss = 1.81 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:17:46.035946: step 135290, loss = 1.93 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:17:47.225055: step 135300, loss = 2.00 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:17:48.391026: step 135310, loss = 1.85 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:49.550137: step 135320, loss = 1.83 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:17:50.707457: step 135330, loss = 1.90 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:17:51.884419: step 135340, loss = 1.84 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:53.023762: step 135350, loss = 2.06 (1123.5 examples/sec; 0.114 sec/batch)
2017-05-05 02:17:54.170510: step 135360, loss = 1.75 (1116.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:17:55.351845: step 135370, loss = 2.00 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:17:56.520963: step 135380, loss = 1.74 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:17:57.683405: step 135390, loss = 1.92 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:17:58.837840: step 135400, loss = 1.81 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:18:00.009940: step 135410, loss = 1.88 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:01.205655: step 135420, loss = 1.89 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:18:02.379627: step 135430, loss = 1.79 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:03.533790: step 135440, loss = 1.80 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:18:04.724592: step 135450, loss = 1.89 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:18:05.886913: step 135460, loss = 1.95 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:07.053222: step 135470, loss = 1.98 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:08.219979: step 135480, loss = 1.87 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:09.386545: step 135490, loss = 1.73 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:10.568988: step 135500, loss = 1.95 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:11.721882: step 135510, loss = 1.79 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:18:12.908426: step 135520, loss = 1.91 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:18:14.058649: step 135530, loss = 1.89 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:18:15.221219: step 135540, loss = 1.87 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:16.374871: step 135550, loss = 1.79 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:18:17.549870: step 135560, loss = 1.83 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:18.721998: step 135570, loss = 1.83 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:19.899701: step 135580, loss = 1.87 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:21.073164: step 135590, loss = 1.93 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:22.228614: step 135600, loss = 1.90 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:23.406951: step 135610, loss = 1.88 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:24.570070: step 135620, loss = 1.85 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:25.880947: step 135630, loss = 1.83 (976.4 examples/sec; 0.131 sec/batch)
2017-05-05 02:18:26.915610: step 135640, loss = 1.95 (1237.1 examples/sec; 0.103 sec/batch)
2017-05-05 02:18:28.091736: step 135650, loss = 1.79 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:29.258802: step 135660, loss = 1.76 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:30.432879: step 135670, loss = 1.93 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:31.601616: step 135680, loss = 1.93 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:32.761623: step 135690, loss = 1.79 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:33.923471: step 135700, loss = 1.80 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:35.104463: step 135710, loss = 1.95 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:36.322097: step 135720, loss = 1.90 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:18:37.496986: step 135730, loss = 1.75 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:38.644465: step 135740, loss = 1.88 (1115.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:18:39.823813: step 135750, loss = 1.97 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:41.012328: step 135760, loss = 2.13 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:18:42.207025: step 135770, loss = 1.85 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:18:43.410312: step 135780, loss = 1.83 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:18:44.623532: step 135790, loss = 1.93 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:18:45.784778: step 135800, loss = 2.02 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:46.952287: step 135810, loss = 1.82 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:48.143204: step 135820, loss = 1.96 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:18:49.308275: step 135830, loss = 1.82 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:50.465514: step 135840, loss = 1.94 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:51.636071: step 135850, loss = 1.93 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:52.811580: step 135860, loss = 1.93 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:18:53.964578: step 135870, loss = 1.83 (1110.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:18:55.117804: step 135880, loss = 1.86 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:18:56.284564: step 135890, loss = 1.95 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:18:57.441162: step 135900, loss = 2.03 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:58.603210: step 135910, loss = 1.81 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:18:59.759771: step 135920, loss = 1.91 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:19:00.936133: step 135930, loss = 1.86 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:19:02.103005: step 135940, loss = 1.88 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:19:03.274474: step 135950, loss = 1.89 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:19:04.436691: step 135960, loss = 1.88 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:19:05.599683: step 135970, loss = 2.05 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:19:06.765150: step 135980, loss = 1.95 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:19:07.949000: step 135990, loss = 1.90 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:19:09.137284: step 136000, loss = 1.92 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:19:10.330284: step 136010, loss = 1.92 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:19:11.553874: step 136020, loss = 1.76 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:12.748426: step 136030, loss = 2.05 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:19:13.977023: step 136040, loss = 1.74 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:19:15.177820: step 136050, loss = 1.99 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:19:16.400630: step 136060, loss = 1.90 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:17.618658: step 136070, loss = 1.83 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:18.823842: step 136080, loss = 1.89 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:20.050019: step 136090, loss = 2.02 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:19:21.262161: step 136100, loss = 1.88 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:22.466522: step 136110, loss = 1.76 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:19:23.696344: step 136120, loss = 1.82 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:19:24.907957: step 136130, loss = 1.87 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:26.111301: step 136140, loss = 1.90 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:19:27.306014: step 136150, loss = 1.98 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:19:28.520029: step 136160, loss = 1.82 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:29.718741: step 136170, loss = 1.91 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:19:30.938072: step 136180, loss = 1.97 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:32.154357: step 136190, loss = 1.80 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:33.356701: step 136200, loss = 1.85 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:19:34.567851: step 136210, loss = 1.81 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:35.778513: step 136220, loss = 2.02 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:37.000307: step 136230, loss = 1.98 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:38.195765: step 136240, loss = 1.95 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:19:39.431311: step 136250, loss = 1.81 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-05 02:19:40.650800: step 136260, loss = 1.88 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:41.865416: step 136270, loss = 1.82 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:43.079140: step 136280, loss = 1.84 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:44.317622: step 136290, loss = 1.80 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-05 02:19:45.527957: step 136300, loss = 2.22 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:46.746714: step 136310, loss = 1.77 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:47.958557: step 136320, loss = 1.99 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:49.190427: step 136330, loss = 1.84 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-05 02:19:50.407547: step 136340, loss = 1.92 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:19:51.622190: step 136350, loss = 1.84 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:52.834291: step 136360, loss = 1.93 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:54.031465: step 136370, loss = 1.85 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:19:55.258352: step 136380, loss = 1.97 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:19:56.466647: step 136390, loss = 1.93 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:19:57.648987: step 136400, loss = 1.95 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:19:58.901594: step 136410, loss = 1.86 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-05 02:20:00.095831: step 136420, loss = 1.83 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:20:01.301200: step 136430, loss = 1.99 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:02.508256: step 136440, loss = 1.95 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:03.730866: step 136450, loss = 1.81 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:04.939251: step 136460, loss = 2.05 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:06.160997: step 136470, loss = 1.90 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:07.395756: step 136480, loss = 1.81 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:08.606379: step 136490, loss = 2.02 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:09.801268: step 136500, loss = 1.92 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:20:11.040421: step 136510, loss = 1.85 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-05 02:20:12.248879: step 136520, loss = 1.81 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:13.467449: step 136530, loss = 1.78 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:14.664489: step 136540, loss = 1.79 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:20:15.866252: steE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1405 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
p 136550, loss = 1.83 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:20:17.113273: step 136560, loss = 1.87 (1026.4 examples/sec; 0.125 sec/batch)
2017-05-05 02:20:18.320129: step 136570, loss = 2.04 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:19.513880: step 136580, loss = 1.95 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:20:20.753472: step 136590, loss = 1.82 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-05 02:20:21.938251: step 136600, loss = 1.91 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:20:23.183991: step 136610, loss = 1.97 (1027.5 examples/sec; 0.125 sec/batch)
2017-05-05 02:20:24.487895: step 136620, loss = 1.91 (981.7 examples/sec; 0.130 sec/batch)
2017-05-05 02:20:25.556404: step 136630, loss = 1.96 (1197.9 examples/sec; 0.107 sec/batch)
2017-05-05 02:20:26.789897: step 136640, loss = 2.01 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:28.000358: step 136650, loss = 1.95 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:29.216698: step 136660, loss = 1.94 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:30.446989: step 136670, loss = 1.86 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:31.661455: step 136680, loss = 1.92 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:32.907872: step 136690, loss = 1.87 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-05 02:20:34.110059: step 136700, loss = 1.73 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:20:35.330143: step 136710, loss = 1.78 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:36.528257: step 136720, loss = 1.76 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:20:37.752110: step 136730, loss = 1.94 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:38.943693: step 136740, loss = 1.93 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:20:40.177035: step 136750, loss = 1.69 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:41.357432: step 136760, loss = 1.83 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:20:42.580295: step 136770, loss = 1.95 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:43.805977: step 136780, loss = 1.84 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:45.046878: step 136790, loss = 1.98 (1031.5 examples/sec; 0.124 sec/batch)
2017-05-05 02:20:46.256998: step 136800, loss = 1.82 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:47.482280: step 136810, loss = 1.87 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:48.749306: step 136820, loss = 2.01 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-05 02:20:49.877879: step 136830, loss = 1.91 (1134.2 examples/sec; 0.113 sec/batch)
2017-05-05 02:20:51.089875: step 136840, loss = 2.17 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:20:52.309267: step 136850, loss = 1.88 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:53.525033: step 136860, loss = 1.93 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:20:54.753892: step 136870, loss = 1.99 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:55.946767: step 136880, loss = 1.91 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:20:57.179373: step 136890, loss = 1.71 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-05 02:20:58.378430: step 136900, loss = 1.80 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:20:59.598800: step 136910, loss = 2.00 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:00.824564: step 136920, loss = 1.85 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-05 02:21:02.043104: step 136930, loss = 1.97 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:03.244461: step 136940, loss = 2.06 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:21:04.442559: step 136950, loss = 1.97 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:21:05.634439: step 136960, loss = 2.10 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:21:06.871743: step 136970, loss = 1.79 (1034.5 examples/sec; 0.124 sec/batch)
2017-05-05 02:21:08.080722: step 136980, loss = 1.92 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:21:09.328359: step 136990, loss = 1.97 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-05 02:21:10.524713: step 137000, loss = 1.94 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:21:11.758791: step 137010, loss = 1.77 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-05 02:21:12.968912: step 137020, loss = 1.73 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:21:14.164437: step 137030, loss = 1.81 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:21:15.385290: step 137040, loss = 1.91 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:16.605577: step 137050, loss = 1.95 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:17.814811: step 137060, loss = 2.00 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:21:19.023548: step 137070, loss = 1.95 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:21:20.231369: step 137080, loss = 1.93 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:21:21.441450: step 137090, loss = 1.81 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:21:22.659862: step 137100, loss = 1.94 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:23.879729: step 137110, loss = 1.82 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:25.095744: step 137120, loss = 1.84 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:26.292672: step 137130, loss = 1.81 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:21:27.508278: step 137140, loss = 1.99 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:21:28.716449: step 137150, loss = 1.91 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:21:29.903360: step 137160, loss = 2.00 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:21:31.097488: step 137170, loss = 1.83 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:21:32.274142: step 137180, loss = 1.92 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:21:33.461378: step 137190, loss = 1.79 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:21:34.645557: step 137200, loss = 1.75 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:21:35.858013: step 137210, loss = 1.93 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:21:37.048363: step 137220, loss = 1.83 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:21:38.259041: step 137230, loss = 2.06 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:21:39.461033: step 137240, loss = 1.94 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:21:40.669290: step 137250, loss = 2.01 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:21:41.852507: step 137260, loss = 2.16 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:21:43.015628: step 137270, loss = 1.88 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:21:44.176938: step 137280, loss = 1.82 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:21:45.355350: step 137290, loss = 1.96 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:21:46.532651: step 137300, loss = 1.78 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:21:47.704422: step 137310, loss = 1.97 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:21:48.880702: step 137320, loss = 1.73 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:21:50.044586: step 137330, loss = 1.97 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:21:51.225500: step 137340, loss = 1.78 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:21:52.384583: step 137350, loss = 1.80 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:21:53.547830: step 137360, loss = 1.95 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:21:54.705546: step 137370, loss = 1.90 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:21:55.876152: step 137380, loss = 1.79 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:21:57.060511: step 137390, loss = 1.78 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:21:58.226157: step 137400, loss = 1.84 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:21:59.393586: step 137410, loss = 2.00 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:00.573575: step 137420, loss = 1.84 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:01.747319: step 137430, loss = 1.82 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:02.896942: step 137E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1416 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
440, loss = 1.86 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:22:04.075520: step 137450, loss = 1.95 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:05.243887: step 137460, loss = 1.93 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:06.391400: step 137470, loss = 1.90 (1115.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:22:07.546755: step 137480, loss = 1.87 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:22:08.715144: step 137490, loss = 1.88 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:09.868582: step 137500, loss = 1.81 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:22:11.043358: step 137510, loss = 1.99 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:12.210836: step 137520, loss = 1.89 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:13.364777: step 137530, loss = 1.89 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:22:14.531223: step 137540, loss = 1.82 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:15.695129: step 137550, loss = 1.90 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:22:16.878779: step 137560, loss = 2.05 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:18.031395: step 137570, loss = 1.68 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:22:19.217451: step 137580, loss = 1.92 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:22:20.414986: step 137590, loss = 1.92 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:22:21.594871: step 137600, loss = 1.90 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:22.863803: step 137610, loss = 1.80 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-05 02:22:23.957609: step 137620, loss = 2.04 (1170.2 examples/sec; 0.109 sec/batch)
2017-05-05 02:22:25.150188: step 137630, loss = 2.03 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:22:26.318518: step 137640, loss = 1.81 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:27.508916: step 137650, loss = 2.01 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:22:28.704447: step 137660, loss = 1.87 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:22:29.902748: step 137670, loss = 1.86 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:22:31.090824: step 137680, loss = 1.90 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:22:32.288431: step 137690, loss = 1.81 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:22:33.476026: step 137700, loss = 1.99 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:22:34.663566: step 137710, loss = 1.83 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:22:35.861944: step 137720, loss = 1.80 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:22:37.061022: step 137730, loss = 1.81 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:22:38.268837: step 137740, loss = 2.00 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:22:39.486777: step 137750, loss = 1.82 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:22:40.685268: step 137760, loss = 1.82 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:22:41.879640: step 137770, loss = 1.82 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:22:43.066469: step 137780, loss = 1.79 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:22:44.266750: step 137790, loss = 1.83 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:22:45.445849: step 137800, loss = 1.84 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:46.619511: step 137810, loss = 1.86 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:47.803368: step 137820, loss = 1.97 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:48.988422: step 137830, loss = 1.86 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:22:50.159704: step 137840, loss = 1.87 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:51.355050: step 137850, loss = 1.91 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:22:52.533573: step 137860, loss = 1.93 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:53.711013: step 137870, loss = 1.77 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:54.889195: step 137880, loss = 1.90 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:22:56.062460: step 137890, loss = 1.72 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:57.236592: step 137900, loss = 1.84 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:22:58.376976: step 137910, loss = 1.80 (1122.4 examples/sec; 0.114 sec/batch)
2017-05-05 02:22:59.545641: step 137920, loss = 1.89 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:00.723217: step 137930, loss = 1.95 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:01.864764: step 137940, loss = 1.93 (1121.3 examples/sec; 0.114 sec/batch)
2017-05-05 02:23:03.033464: step 137950, loss = 1.83 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:04.201713: step 137960, loss = 1.98 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:05.360131: step 137970, loss = 1.90 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:23:06.521373: step 137980, loss = 1.82 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:23:07.691501: step 137990, loss = 1.73 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:08.872552: step 138000, loss = 1.77 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:10.059804: step 138010, loss = 1.89 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:23:11.243353: step 138020, loss = 2.01 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:12.412381: step 138030, loss = 1.70 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:13.585485: step 138040, loss = 1.92 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:14.762282: step 138050, loss = 1.86 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:15.936511: step 138060, loss = 1.75 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:17.101376: step 138070, loss = 1.77 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:23:18.279570: step 138080, loss = 2.02 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:19.472846: step 138090, loss = 1.86 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:23:20.650200: step 138100, loss = 1.81 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:21.820890: step 138110, loss = 2.08 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:23.008646: step 138120, loss = 1.96 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:23:24.204975: step 138130, loss = 1.84 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:23:25.385638: step 138140, loss = 2.05 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:26.568324: step 138150, loss = 1.88 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:27.759333: step 138160, loss = 1.95 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:23:28.930921: step 138170, loss = 1.82 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:30.095799: step 138180, loss = 1.86 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:23:31.277139: step 138190, loss = 2.08 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:32.453357: step 138200, loss = 1.84 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:33.612033: step 138210, loss = 1.78 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:23:34.780567: step 138220, loss = 1.90 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:35.953765: step 138230, loss = 1.88 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:37.133770: step 138240, loss = 1.92 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:38.305495: step 138250, loss = 1.90 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:39.474221: step 138260, loss = 1.81 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:40.632434: step 138270, loss = 2.02 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:23:41.802092: step 138280, loss = 1.93 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:42.990613: step 138290, loss = 1.94 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:23:44.147688: step 138300, loss = 1.95 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:23:45.324020: step 138310, loss = 1.92 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:46.483044: step 138320, loss = 1.82 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:23:47.646179: step 138330, loss = 1.95 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:23:48.794355: step 138340, loss = 1.92 (1114.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:23:49.942707: step 138350, loss = 1.89 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:23:51.119855: step 138360, loss = 1.79 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:52.272868: step 138370, loss = 1.99 (1110.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:23:53.449578: step 138380, loss = 1.89 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:54.624491: step 138390, loss = 1.95 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:23:55.776768: step 138400, loss = 1.83 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:23:56.954074: step 138410, loss = 2.00 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:23:58.117445: step 138420, loss = 1.81 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:23:59.279933: step 138430, loss = 1.94 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:24:00.451065: step 138440, loss = 1.75 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:01.609466: step 138450, loss = 1.77 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:24:02.766401: step 138460, loss = 1.91 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:24:03.928601: step 138470, loss = 1.89 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:24:05.102401: step 138480, loss = 1.93 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:06.266614: step 138490, loss = 1.96 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:24:07.435088: step 138500, loss = 1.85 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:08.591892: step 138510, loss = 1.87 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:24:09.759092: step 138520, loss = 1.92 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:10.926623: step 138530, loss = 1.86 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:12.087600: step 138540, loss = 1.97 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:24:13.253359: step 138550, loss = 1.84 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:14.419432: step 138560, loss = 1.86 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:15.581560: step 138570, loss = 1.94 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:24:16.752104: step 138580, loss = 1.87 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:17.913754: step 138590, loss = 2.04 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:24:19.176575: step 138600, loss = 1.96 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-05 02:24:20.255636: step 138610, loss = 1.82 (1186.2 examples/sec; 0.108 sec/batch)
2017-05-05 02:24:21.420562: step 138620, loss = 1.96 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:24:22.597038: step 138630, loss = 2.04 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:23.776899: step 138640, loss = 1.85 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:24.954382: step 138650, loss = 1.91 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:26.111112: step 138660, loss = 1.87 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:24:27.286686: step 138670, loss = 1.71 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:28.470089: step 138680, loss = 1.93 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:29.642361: step 138690, loss = 1.83 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:30.820227: step 138700, loss = 1.90 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:31.994222: step 138710, loss = 1.88 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:33.196143: step 138720, loss = 1.94 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:24:34.386196: step 138730, loss = 1.99 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:24:35.584343: step 138740, loss = 1.81 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:24:36.764946: step 138750, loss = 1.94 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:37.946000: step 138760, loss = 1.99 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:39.130556: step 138770, loss = 1.95 (1080.6 examples/sec; 0.118 sec/batE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1427 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
ch)
2017-05-05 02:24:40.339000: step 138780, loss = 1.83 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:24:41.518171: step 138790, loss = 2.02 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:42.761607: step 138800, loss = 2.01 (1029.4 examples/sec; 0.124 sec/batch)
2017-05-05 02:24:43.901669: step 138810, loss = 2.10 (1122.7 examples/sec; 0.114 sec/batch)
2017-05-05 02:24:45.098131: step 138820, loss = 1.95 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:24:46.270641: step 138830, loss = 1.89 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:24:47.449673: step 138840, loss = 1.74 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:48.632826: step 138850, loss = 1.89 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:49.835559: step 138860, loss = 1.86 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:24:51.036408: step 138870, loss = 1.99 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:24:52.242142: step 138880, loss = 1.84 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:24:53.458422: step 138890, loss = 1.89 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:24:54.644430: step 138900, loss = 2.02 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:24:55.830965: step 138910, loss = 1.97 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:24:57.021803: step 138920, loss = 1.78 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:24:58.206275: step 138930, loss = 1.87 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:24:59.402016: step 138940, loss = 1.90 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:00.611285: step 138950, loss = 1.81 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:01.828497: step 138960, loss = 1.89 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:03.046022: step 138970, loss = 1.87 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:04.245303: step 138980, loss = 1.86 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:05.442548: step 138990, loss = 1.94 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:06.615488: step 139000, loss = 1.79 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:25:07.821577: step 139010, loss = 1.92 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:09.053957: step 139020, loss = 2.12 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-05 02:25:10.281315: step 139030, loss = 1.87 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:25:11.508028: step 139040, loss = 1.87 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 02:25:12.728017: step 139050, loss = 1.74 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:13.953667: step 139060, loss = 1.96 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:25:15.163833: step 139070, loss = 1.85 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:16.395452: step 139080, loss = 1.89 (1039.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:25:17.605899: step 139090, loss = 1.90 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:18.806720: step 139100, loss = 1.92 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:20.051250: step 139110, loss = 1.81 (1028.5 examples/sec; 0.124 sec/batch)
2017-05-05 02:25:21.255978: step 139120, loss = 1.96 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:22.482009: step 139130, loss = 1.84 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 02:25:23.692944: step 139140, loss = 1.85 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:24.931505: step 139150, loss = 1.99 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-05 02:25:26.115098: step 139160, loss = 1.85 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:25:27.331014: step 139170, loss = 1.85 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:28.525316: step 139180, loss = 1.79 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:25:29.749732: step 139190, loss = 1.92 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:30.916508: step 139200, loss = 1.76 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:25:32.151873: step 139210, loss = 1.94 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-05 02:25:33.350114: step 139220, loss = 1.86 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:34.555473: step 139230, loss = 1.95 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:35.782784: step 139240, loss = 1.98 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:25:37.018316: step 139250, loss = 1.93 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-05 02:25:38.220132: step 139260, loss = 1.79 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:39.452735: step 139270, loss = 2.00 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 02:25:40.658383: step 139280, loss = 2.04 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:41.869757: step 139290, loss = 1.96 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:43.065706: step 139300, loss = 1.85 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:44.287538: step 139310, loss = 1.99 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:45.485083: step 139320, loss = 1.89 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:46.685606: step 139330, loss = 1.75 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:47.879939: step 139340, loss = 1.92 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:25:49.114532: step 139350, loss = 2.09 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:25:50.298488: step 139360, loss = 1.88 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:25:51.514157: step 139370, loss = 1.85 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:52.742179: step 139380, loss = 1.99 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:25:53.947964: step 139390, loss = 1.89 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:55.159032: step 139400, loss = 1.76 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:25:56.375991: step 139410, loss = 1.88 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:25:57.576575: step 139420, loss = 1.98 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:25:58.810816: step 139430, loss = 1.79 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-05 02:26:00.007039: step 139440, loss = 1.98 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:01.229949: step 139450, loss = 1.85 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:02.422124: step 139460, loss = 1.87 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:26:03.640225: step 139470, loss = 1.86 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:04.848924: step 139480, loss = 1.93 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:06.063746: step 139490, loss = 1.83 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:07.262431: step 139500, loss = 1.78 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:08.505913: step 139510, loss = 1.90 (1029.4 examples/sec; 0.124 sec/batch)
2017-05-05 02:26:09.701113: step 139520, loss = 1.86 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:10.922426: step 139530, loss = 1.91 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:12.153199: step 139540, loss = 1.79 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-05 02:26:13.372955: step 139550, loss = 1.88 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:14.580321: step 139560, loss = 2.05 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:15.800647: step 139570, loss = 1.82 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:17.002590: step 139580, loss = 1.94 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:18.314080: step 139590, loss = 1.92 (976.0 examples/sec; 0.131 sec/batch)
2017-05-05 02:26:19.410390: step 139600, loss = 1.98 (1167.6 examples/sec; 0.110 sec/batch)
2017-05-05 02:26:20.631756: step 139610, loss = 2.06 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:21.849993: step 139620, loss = 1.97 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:23.047641: step 139630, loss = 1.93 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:24.268221: step 139640, loss = 2.00 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:25.463612: step 139650, loss = 1.78 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:26.663549: step 139660, loss = 1.85 (1066.7 examples/sec; 0.120 sec/batch)
2E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1438 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
017-05-05 02:26:27.885582: step 139670, loss = 1.95 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:29.102102: step 139680, loss = 1.76 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:30.308760: step 139690, loss = 1.84 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:31.519725: step 139700, loss = 2.05 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:32.727223: step 139710, loss = 1.93 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:33.923844: step 139720, loss = 1.88 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:35.149010: step 139730, loss = 1.83 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:26:36.350832: step 139740, loss = 2.00 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:37.577834: step 139750, loss = 1.94 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 02:26:38.778727: step 139760, loss = 1.92 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:40.001485: step 139770, loss = 1.83 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:41.220495: step 139780, loss = 1.91 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:42.511654: step 139790, loss = 1.92 (991.4 examples/sec; 0.129 sec/batch)
2017-05-05 02:26:43.635905: step 139800, loss = 1.87 (1138.5 examples/sec; 0.112 sec/batch)
2017-05-05 02:26:44.824574: step 139810, loss = 1.88 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:26:46.060964: step 139820, loss = 1.87 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-05 02:26:47.256739: step 139830, loss = 2.02 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:48.471916: step 139840, loss = 1.88 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:49.674964: step 139850, loss = 1.90 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:50.904824: step 139860, loss = 2.01 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:26:52.137633: step 139870, loss = 1.90 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:26:53.341528: step 139880, loss = 1.94 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:26:54.560114: step 139890, loss = 1.96 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:26:55.773824: step 139900, loss = 1.85 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:26:57.017877: step 139910, loss = 1.89 (1028.9 examples/sec; 0.124 sec/batch)
2017-05-05 02:26:58.206417: step 139920, loss = 1.86 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:26:59.439764: step 139930, loss = 1.76 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:27:00.653160: step 139940, loss = 1.89 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:01.851430: step 139950, loss = 1.88 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:27:03.065957: step 139960, loss = 1.91 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:04.297001: step 139970, loss = 1.98 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:27:05.482838: step 139980, loss = 1.81 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:27:06.703760: step 139990, loss = 1.98 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:07.913606: step 140000, loss = 1.85 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:09.123622: step 140010, loss = 1.83 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:10.334142: step 140020, loss = 1.82 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:11.565924: step 140030, loss = 1.93 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-05 02:27:12.782195: step 140040, loss = 1.86 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:14.000976: step 140050, loss = 1.75 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:15.205308: step 140060, loss = 1.93 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:27:16.441711: step 140070, loss = 1.88 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-05 02:27:17.633931: step 140080, loss = 2.00 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:27:18.855194: step 140090, loss = 1.88 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:20.052720: step 140100, loss = 1.85 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:27:21.286118: step 140110, loss = 1.97 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:27:22.498979: step 140120, loss = 1.96 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:23.718212: step 140130, loss = 1.75 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:24.925484: step 140140, loss = 1.83 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:26.148043: step 140150, loss = 1.83 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:27.368608: step 140160, loss = 1.97 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:28.577188: step 140170, loss = 1.85 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:29.788002: step 140180, loss = 2.01 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:30.980551: step 140190, loss = 1.84 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:27:32.194291: step 140200, loss = 1.93 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:33.436942: step 140210, loss = 1.88 (1030.0 examples/sec; 0.124 sec/batch)
2017-05-05 02:27:34.623357: step 140220, loss = 1.76 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:27:35.830967: step 140230, loss = 1.95 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:37.066023: step 140240, loss = 1.92 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-05 02:27:38.261203: step 140250, loss = 2.07 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:27:39.460407: step 140260, loss = 1.97 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:27:40.683314: step 140270, loss = 1.79 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:41.871678: step 140280, loss = 1.90 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:27:43.078395: step 140290, loss = 1.77 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:44.285041: step 140300, loss = 1.88 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:45.473754: step 140310, loss = 1.77 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:27:46.693138: step 140320, loss = 1.84 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:47.913948: step 140330, loss = 1.93 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:27:49.123296: step 140340, loss = 1.93 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:50.332544: step 140350, loss = 1.93 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:27:51.536902: step 140360, loss = 1.92 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:27:52.722788: step 140370, loss = 2.04 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:27:53.901630: step 140380, loss = 1.79 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:27:55.100677: step 140390, loss = 1.98 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:27:56.263530: step 140400, loss = 2.10 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:27:57.447609: step 140410, loss = 1.94 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:27:58.626666: step 140420, loss = 1.77 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:27:59.795388: step 140430, loss = 1.75 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:00.957748: step 140440, loss = 1.88 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:02.164456: step 140450, loss = 1.90 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:28:03.341582: step 140460, loss = 1.88 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:04.503052: step 140470, loss = 1.84 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:05.666019: step 140480, loss = 1.93 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:06.834112: step 140490, loss = 2.05 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:08.008020: step 140500, loss = 1.72 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:09.170398: step 140510, loss = 1.92 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:10.334159: step 140520, loss = 1.93 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:11.520295: step 140530, loss = 2.03 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:28:12.696494: step 140540, loss = 1.94 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:13.869770: step 140550, loss = 2.07 (1091.0 examples/sec; 0.117 sec/batch)
2017-0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1450 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
5-05 02:28:15.033192: step 140560, loss = 2.00 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:16.217504: step 140570, loss = 1.89 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:17.471596: step 140580, loss = 1.89 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-05 02:28:18.542675: step 140590, loss = 1.84 (1195.1 examples/sec; 0.107 sec/batch)
2017-05-05 02:28:19.735772: step 140600, loss = 2.02 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:28:20.908728: step 140610, loss = 1.98 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:22.072306: step 140620, loss = 1.99 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:23.244315: step 140630, loss = 1.91 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:24.402825: step 140640, loss = 1.80 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:25.567598: step 140650, loss = 1.82 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:26.764535: step 140660, loss = 1.94 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:28:27.958625: step 140670, loss = 2.10 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:28:29.139209: step 140680, loss = 2.01 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:30.317788: step 140690, loss = 1.96 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:31.476271: step 140700, loss = 1.86 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:32.654148: step 140710, loss = 1.91 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:33.834838: step 140720, loss = 1.86 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:35.019622: step 140730, loss = 2.00 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:36.243627: step 140740, loss = 1.87 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:28:37.405668: step 140750, loss = 1.83 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:38.574682: step 140760, loss = 1.89 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:39.749305: step 140770, loss = 1.76 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:40.927378: step 140780, loss = 1.97 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:42.084654: step 140790, loss = 2.09 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:43.260250: step 140800, loss = 1.98 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:44.426445: step 140810, loss = 1.88 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:45.591861: step 140820, loss = 1.85 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:46.775896: step 140830, loss = 1.93 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:47.942510: step 140840, loss = 1.95 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:49.118787: step 140850, loss = 1.88 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:50.277052: step 140860, loss = 1.87 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:51.447554: step 140870, loss = 1.79 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:52.620196: step 140880, loss = 1.79 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:28:53.783322: step 140890, loss = 1.85 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:54.914539: step 140900, loss = 1.82 (1131.5 examples/sec; 0.113 sec/batch)
2017-05-05 02:28:56.075687: step 140910, loss = 1.83 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:57.256615: step 140920, loss = 1.81 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:28:58.416333: step 140930, loss = 2.08 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:28:59.604286: step 140940, loss = 1.96 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:29:00.780053: step 140950, loss = 1.81 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:29:01.932280: step 140960, loss = 1.92 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:29:03.113113: step 140970, loss = 2.00 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:29:04.271339: step 140980, loss = 1.93 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:05.433598: step 140990, loss = 1.91 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:06.592823: step 141000, loss = 1.80 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:07.750933: step 141010, loss = 1.77 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:08.912965: step 141020, loss = 1.85 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:10.075861: step 141030, loss = 1.83 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:11.252201: step 141040, loss = 1.92 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:29:12.417286: step 141050, loss = 2.04 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:13.584538: step 141060, loss = 1.79 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:14.744127: step 141070, loss = 1.87 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:15.920811: step 141080, loss = 1.82 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:29:17.098736: step 141090, loss = 1.94 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:29:18.254717: step 141100, loss = 1.82 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:19.412529: step 141110, loss = 1.91 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:20.598444: step 141120, loss = 1.79 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:29:21.755890: step 141130, loss = 1.86 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:22.937589: step 141140, loss = 1.89 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:29:24.106790: step 141150, loss = 1.93 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:25.283504: step 141160, loss = 1.89 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:29:26.438751: step 141170, loss = 1.81 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:27.617380: step 141180, loss = 1.86 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:29:28.771424: step 141190, loss = 1.96 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:29:29.923820: step 141200, loss = 1.98 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:29:31.093711: step 141210, loss = 1.88 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:32.258111: step 141220, loss = 1.79 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:33.426533: step 141230, loss = 1.92 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:34.591960: step 141240, loss = 1.85 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:35.747917: step 141250, loss = 1.86 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:36.908340: step 141260, loss = 1.82 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:38.067080: step 141270, loss = 1.89 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:39.241538: step 141280, loss = 1.93 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:40.409367: step 141290, loss = 1.86 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:41.572011: step 141300, loss = 1.91 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:42.720658: step 141310, loss = 1.88 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:29:43.886954: step 141320, loss = 1.78 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:45.079779: step 141330, loss = 1.87 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:29:46.230622: step 141340, loss = 1.98 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:29:47.389108: step 141350, loss = 1.83 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:48.576647: step 141360, loss = 1.98 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:29:49.716980: step 141370, loss = 2.00 (1122.5 examples/sec; 0.114 sec/batch)
2017-05-05 02:29:50.887194: step 141380, loss = 2.06 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:52.046692: step 141390, loss = 1.92 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:53.216852: step 141400, loss = 1.71 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:29:54.377974: step 141410, loss = 2.06 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:55.534574: step 141420, loss = 1.80 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:56.717855: step 141430, loss = 1.87 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:29:57.881564: step 141440, loss = 1.86 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:29:59.045528: step 141450, loss = 1.82 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:00.198348: step 141460, loss = 1.81 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:30:01.396534: step 141470, loss = 1.81 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:30:02.579678: step 141480, loss = 2.02 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:30:03.772412: step 141490, loss = 2.04 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:30:04.941077: step 141500, loss = 1.95 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:06.104526: step 141510, loss = 1.90 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:07.280543: step 141520, loss = 1.82 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:30:08.455372: step 141530, loss = 1.93 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:09.626960: step 141540, loss = 1.87 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:10.790043: step 141550, loss = 1.78 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:11.975105: step 141560, loss = 1.87 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:30:13.244637: step 141570, loss = 1.89 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-05 02:30:14.304509: step 141580, loss = 1.89 (1207.7 examples/sec; 0.106 sec/batch)
2017-05-05 02:30:15.461550: step 141590, loss = 1.80 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:16.604110: step 141600, loss = 1.93 (1120.3 examples/sec; 0.114 sec/batch)
2017-05-05 02:30:17.750620: step 141610, loss = 1.92 (1116.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:30:18.936467: step 141620, loss = 1.89 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:30:20.098729: step 141630, loss = 1.84 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:21.255328: step 141640, loss = 1.90 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:22.414365: step 141650, loss = 2.03 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:23.585404: step 141660, loss = 1.83 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:24.759990: step 141670, loss = 1.80 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:25.920955: step 141680, loss = 1.92 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:27.092209: step 141690, loss = 1.82 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:28.269879: step 141700, loss = 1.79 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:30:29.445894: step 141710, loss = 1.86 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:30:30.637897: step 141720, loss = 1.76 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:30:31.820606: step 141730, loss = 1.95 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:30:33.021750: step 141740, loss = 1.79 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:30:34.207643: step 141750, loss = 2.01 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:30:35.431661: step 141760, loss = 1.93 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:30:36.626683: step 141770, loss = 1.84 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:30:37.834407: step 141780, loss = 1.93 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:30:39.051302: step 141790, loss = 1.67 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:30:40.258054: step 141800, loss = 1.89 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:30:41.476651: step 141810, loss = 1.82 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:30:42.700903: step 141820, loss = 1.77 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:30:43.905739: step 141830, loss = 1.83 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:30:45.110585: step 141840, loss = 1.92 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:30:46.297116: step 141850, loss = 1.80 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:30:47.492169: step 141860, loss = 1.72 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:30:48.700705: step 141870, loss = 1.97 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:30:49.862523: step 141880, loss = 1.78 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:51.051004: step 141890, loss = 1.89 (1077E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1462 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:30:52.211687: step 141900, loss = 1.88 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:53.381661: step 141910, loss = 1.81 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:54.555452: step 141920, loss = 1.88 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:55.718873: step 141930, loss = 1.87 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:56.883618: step 141940, loss = 1.88 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:30:58.049620: step 141950, loss = 1.85 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:30:59.220291: step 141960, loss = 1.83 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:00.382841: step 141970, loss = 1.88 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:01.542864: step 141980, loss = 2.05 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:02.733683: step 141990, loss = 1.79 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:31:03.887206: step 142000, loss = 1.73 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:31:05.062072: step 142010, loss = 1.88 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:06.221182: step 142020, loss = 1.96 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:07.395749: step 142030, loss = 1.90 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:08.539160: step 142040, loss = 1.95 (1119.5 examples/sec; 0.114 sec/batch)
2017-05-05 02:31:09.694944: step 142050, loss = 1.94 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:10.864775: step 142060, loss = 1.96 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:12.015756: step 142070, loss = 1.75 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:31:13.196325: step 142080, loss = 1.95 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:31:14.353138: step 142090, loss = 1.78 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:15.518763: step 142100, loss = 2.06 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:16.690874: step 142110, loss = 1.94 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:17.848266: step 142120, loss = 1.82 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:19.032094: step 142130, loss = 1.92 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:31:20.183594: step 142140, loss = 1.87 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:31:21.358663: step 142150, loss = 2.05 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:31:22.514143: step 142160, loss = 1.68 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:23.675848: step 142170, loss = 1.93 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:24.841589: step 142180, loss = 1.76 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:25.999679: step 142190, loss = 1.85 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:27.158532: step 142200, loss = 1.86 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:28.315765: step 142210, loss = 1.77 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:29.495180: step 142220, loss = 1.93 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:31:30.663533: step 142230, loss = 2.07 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:31:31.823400: step 142240, loss = 1.85 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:32.977557: step 142250, loss = 1.86 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:31:34.132605: step 142260, loss = 2.03 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:31:35.321455: step 142270, loss = 1.90 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:31:36.507458: step 142280, loss = 1.75 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:31:37.652781: step 142290, loss = 1.89 (1117.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:31:38.836036: step 142300, loss = 1.82 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:31:40.016454: step 142310, loss = 1.71 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:31:41.227508: step 142320, loss = 2.03 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:31:42.411513: step 142330, loss = 1.82 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:31:43.618363: step 142340, loss = 1.77 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:31:44.833916: step 142350, loss = 1.93 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:31:46.042574: step 142360, loss = 1.75 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:31:47.239767: step 142370, loss = 1.86 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:31:48.443295: step 142380, loss = 1.78 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:31:49.636123: step 142390, loss = 1.69 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:31:50.830793: step 142400, loss = 1.73 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:31:52.025548: step 142410, loss = 1.69 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:31:53.234780: step 142420, loss = 1.96 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:31:54.440982: step 142430, loss = 2.02 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:31:55.653107: step 142440, loss = 1.83 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:31:56.860289: step 142450, loss = 1.81 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:31:58.061066: step 142460, loss = 2.08 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:31:59.274701: step 142470, loss = 1.85 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:00.495837: step 142480, loss = 2.02 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:01.709452: step 142490, loss = 1.96 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:02.932780: step 142500, loss = 1.81 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:04.150000: step 142510, loss = 1.82 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:05.370918: step 142520, loss = 1.92 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:06.584956: step 142530, loss = 1.88 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:07.791312: step 142540, loss = 1.92 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:09.005724: step 142550, loss = 1.82 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:10.290678: step 142560, loss = 1.81 (996.1 examples/sec; 0.128 sec/batch)
2017-05-05 02:32:11.413583: step 142570, loss = 1.84 (1139.9 examples/sec; 0.112 sec/batch)
2017-05-05 02:32:12.625938: step 142580, loss = 1.92 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:13.820285: step 142590, loss = 1.91 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:32:15.046177: step 142600, loss = 1.93 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-05 02:32:16.274180: step 142610, loss = 2.02 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:32:17.485452: step 142620, loss = 1.79 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:18.715068: step 142630, loss = 1.83 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-05 02:32:19.932641: step 142640, loss = 1.82 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:21.143299: step 142650, loss = 1.86 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:22.351686: step 142660, loss = 1.80 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:23.576483: step 142670, loss = 2.03 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:24.808404: step 142680, loss = 1.83 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-05 02:32:26.015630: step 142690, loss = 1.91 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:27.232985: step 142700, loss = 1.76 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:28.460383: step 142710, loss = 1.68 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:32:29.676028: step 142720, loss = 1.76 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:30.904869: step 142730, loss = 1.81 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-05 02:32:32.115957: step 142740, loss = 1.71 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:33.309623: step 142750, loss = 1.93 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:32:34.486607: step 142760, loss = 1.99 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:32:35.691595: step 142770, loss = 1.82 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:32:36.915782: step 142780, loss = 1.85 (1045.6 exE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1473 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
amples/sec; 0.122 sec/batch)
2017-05-05 02:32:38.128855: step 142790, loss = 1.77 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:39.341476: step 142800, loss = 1.82 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:40.574615: step 142810, loss = 1.90 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-05 02:32:41.771405: step 142820, loss = 1.96 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:32:42.990346: step 142830, loss = 1.89 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:44.219971: step 142840, loss = 2.11 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-05 02:32:45.447227: step 142850, loss = 1.87 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 02:32:46.651646: step 142860, loss = 2.05 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:32:47.859120: step 142870, loss = 1.82 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:49.072912: step 142880, loss = 1.87 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:32:50.293491: step 142890, loss = 1.74 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:32:51.498205: step 142900, loss = 1.91 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:32:52.745273: step 142910, loss = 1.84 (1026.4 examples/sec; 0.125 sec/batch)
2017-05-05 02:32:53.939510: step 142920, loss = 1.91 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:32:55.132427: step 142930, loss = 1.84 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:32:56.332718: step 142940, loss = 1.84 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:32:57.530864: step 142950, loss = 1.83 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:32:58.729894: step 142960, loss = 1.74 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:32:59.942572: step 142970, loss = 1.83 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:01.163416: step 142980, loss = 1.99 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:02.370344: step 142990, loss = 1.93 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:03.571204: step 143000, loss = 1.82 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:33:04.791453: step 143010, loss = 1.82 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:06.008139: step 143020, loss = 1.92 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:07.236287: step 143030, loss = 1.90 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-05 02:33:08.455014: step 143040, loss = 2.13 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:09.661606: step 143050, loss = 1.94 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:10.863471: step 143060, loss = 1.89 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:33:12.087933: step 143070, loss = 1.83 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:13.295538: step 143080, loss = 1.91 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:14.506559: step 143090, loss = 1.89 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:15.727565: step 143100, loss = 1.81 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:16.949167: step 143110, loss = 1.93 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:18.145086: step 143120, loss = 1.87 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:33:19.368672: step 143130, loss = 1.89 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:20.575558: step 143140, loss = 1.90 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:21.790204: step 143150, loss = 1.94 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:23.009809: step 143160, loss = 1.91 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:24.219032: step 143170, loss = 1.92 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:25.426519: step 143180, loss = 1.87 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:26.628175: step 143190, loss = 1.88 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:33:27.861554: step 143200, loss = 1.73 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:33:29.069392: step 143210, loss = 1.74 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:30.268001: step 143220, loss = 1.98 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:33:31.483549: step 143230, loss = 1.96 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:32.715586: step 143240, loss = 1.97 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:33:33.920414: step 143250, loss = 1.79 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:33:35.121437: step 143260, loss = 1.85 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:33:36.332911: step 143270, loss = 2.04 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:37.538265: step 143280, loss = 1.87 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:38.735108: step 143290, loss = 1.82 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:33:39.937130: step 143300, loss = 1.76 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:33:41.150953: step 143310, loss = 1.95 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:42.359241: step 143320, loss = 1.96 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:43.570758: step 143330, loss = 1.88 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:44.783105: step 143340, loss = 1.95 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:46.009855: step 143350, loss = 1.89 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-05 02:33:47.216384: step 143360, loss = 1.91 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:48.440482: step 143370, loss = 1.79 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:49.641801: step 143380, loss = 1.84 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:33:50.857422: step 143390, loss = 1.88 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:52.068989: step 143400, loss = 2.15 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:53.283009: step 143410, loss = 1.89 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:54.488721: step 143420, loss = 2.00 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:55.709167: step 143430, loss = 1.88 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:56.929380: step 143440, loss = 1.79 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:33:58.134539: step 143450, loss = 1.95 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:33:59.346153: step 143460, loss = 1.86 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:00.552920: step 143470, loss = 1.92 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:01.765290: step 143480, loss = 1.86 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:02.981973: step 143490, loss = 1.88 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:04.204703: step 143500, loss = 1.84 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:05.408208: step 143510, loss = 1.88 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:34:06.615974: step 143520, loss = 1.84 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:07.840830: step 143530, loss = 2.20 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:09.051255: step 143540, loss = 1.92 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:10.346149: step 143550, loss = 2.01 (988.5 examples/sec; 0.129 sec/batch)
2017-05-05 02:34:11.469170: step 143560, loss = 1.95 (1139.8 examples/sec; 0.112 sec/batch)
2017-05-05 02:34:12.673837: step 143570, loss = 1.92 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:34:13.865202: step 143580, loss = 1.82 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:34:15.072081: step 143590, loss = 1.94 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:16.294817: step 143600, loss = 1.98 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:17.497887: step 143610, loss = 1.79 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:34:18.718055: step 143620, loss = 1.80 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:19.943552: step 143630, loss = 2.05 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 02:34:21.168782: step 143640, loss = 2.07 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 02:34:22.365615: step 143650, loss = 1.90 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:34:23.585758: step 143660, loss = 1.86 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:24.812394: step 143670, loss = 1.85 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-05 02:34:26.028016: step 143680, loss = 1.75 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:27.248527: step 143690, loss = 1.85 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:28.469892: step 143700, loss = 1.78 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:29.693578: step 143710, loss = 1.91 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:30.911973: step 143720, loss = 1.84 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:32.132066: step 143730, loss = 2.06 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:33.345814: step 143740, loss = 1.83 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:34.522464: step 143750, loss = 1.80 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:34:35.727823: step 143760, loss = 1.88 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:36.967832: step 143770, loss = 1.83 (1032.3 examples/sec; 0.124 sec/batch)
2017-05-05 02:34:38.165010: step 143780, loss = 1.88 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:34:39.392021: step 143790, loss = 1.82 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 02:34:40.602739: step 143800, loss = 1.93 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:41.822818: step 143810, loss = 1.80 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:43.039732: step 143820, loss = 1.85 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:44.272418: step 143830, loss = 1.88 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-05 02:34:45.483459: step 143840, loss = 1.76 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:46.699777: step 143850, loss = 1.78 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:34:47.912078: step 143860, loss = 1.78 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:49.121040: step 143870, loss = 1.94 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:50.325681: step 143880, loss = 1.80 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:34:51.537207: step 143890, loss = 1.84 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:52.750601: step 143900, loss = 1.77 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:34:53.928411: step 143910, loss = 1.86 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:34:55.124053: step 143920, loss = 1.79 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:34:56.326395: step 143930, loss = 1.64 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:34:57.509340: step 143940, loss = 1.88 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:34:58.699831: step 143950, loss = 1.80 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:34:59.856110: step 143960, loss = 1.80 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:01.035507: step 143970, loss = 1.81 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:02.220994: step 143980, loss = 2.03 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:35:03.416432: step 143990, loss = 1.81 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:35:04.608676: step 144000, loss = 1.83 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:35:05.780542: step 144010, loss = 1.89 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:06.955203: step 144020, loss = 1.81 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:08.135339: step 144030, loss = 1.85 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:09.313180: step 144040, loss = 1.81 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:10.514601: step 144050, loss = 1.92 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:35:11.709678: step 144060, loss = 1.85 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:35:12.893617: step 144070, loss = 1.97 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:14.089947: step 144080, loss = 1.93 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:35:15.279029: step 144090, loss = 1.93 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:35:16.448755: step 144100, loss = 1.88 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:17.631511: step 144110, loss = 1.77 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:18.8011E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1483 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
56: step 144120, loss = 1.86 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:19.971075: step 144130, loss = 1.92 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:21.153868: step 144140, loss = 1.72 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:22.309592: step 144150, loss = 1.83 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:23.477712: step 144160, loss = 1.90 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:24.643948: step 144170, loss = 1.93 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:25.802558: step 144180, loss = 1.90 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:26.973735: step 144190, loss = 1.75 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:28.127336: step 144200, loss = 1.94 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:35:29.303204: step 144210, loss = 1.92 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:30.458649: step 144220, loss = 1.75 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:31.631668: step 144230, loss = 1.94 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:32.796138: step 144240, loss = 1.89 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:33.972760: step 144250, loss = 1.91 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:35.136533: step 144260, loss = 1.91 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:36.320732: step 144270, loss = 1.93 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:37.495544: step 144280, loss = 1.81 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:38.681429: step 144290, loss = 1.99 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:35:39.864518: step 144300, loss = 1.78 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:35:41.053418: step 144310, loss = 1.95 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:35:42.206362: step 144320, loss = 1.91 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:35:43.365306: step 144330, loss = 1.97 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:44.532302: step 144340, loss = 1.90 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:45.667920: step 144350, loss = 1.94 (1127.1 examples/sec; 0.114 sec/batch)
2017-05-05 02:35:46.837411: step 144360, loss = 1.82 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:48.010364: step 144370, loss = 2.00 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:49.168049: step 144380, loss = 1.79 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:50.322219: step 144390, loss = 1.88 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:35:51.493327: step 144400, loss = 1.86 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:52.639253: step 144410, loss = 1.87 (1117.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:35:53.800545: step 144420, loss = 1.93 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:35:54.969456: step 144430, loss = 1.92 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:56.143182: step 144440, loss = 1.90 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:57.329359: step 144450, loss = 1.92 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:35:58.497124: step 144460, loss = 1.88 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:35:59.660638: step 144470, loss = 1.94 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:00.842463: step 144480, loss = 1.82 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:02.004948: step 144490, loss = 1.96 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:03.170360: step 144500, loss = 1.92 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:04.328021: step 144510, loss = 1.89 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:05.485788: step 144520, loss = 1.84 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:06.665547: step 144530, loss = 1.90 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:07.924114: step 144540, loss = 1.81 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-05 02:36:08.990182: step 144550, loss = 1.88 (1200.7 examples/sec; 0.107 sec/batch)
2017-05-05 02:36:10.157518: step 144560, loss = 1.79 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:11.334849: step 144570, loss = 1.81 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:12.503626: step 144580, loss = 1.96 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:13.680553: step 144590, loss = 1.99 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:14.863100: step 144600, loss = 1.86 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:16.044695: step 144610, loss = 1.78 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:17.219415: step 144620, loss = 1.91 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:18.370286: step 144630, loss = 1.84 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:36:19.529032: step 144640, loss = 1.84 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:20.687697: step 144650, loss = 2.05 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:21.832942: step 144660, loss = 1.94 (1117.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:36:23.016988: step 144670, loss = 1.97 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:24.201040: step 144680, loss = 2.05 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:25.386639: step 144690, loss = 1.91 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:36:26.547811: step 144700, loss = 1.95 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:27.717056: step 144710, loss = 1.92 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:28.881650: step 144720, loss = 1.79 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:36:30.052965: step 144730, loss = 1.90 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:31.224280: step 144740, loss = 1.83 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:32.397952: step 144750, loss = 1.73 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:33.551137: step 144760, loss = 1.88 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:36:34.734853: step 144770, loss = 1.96 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:35.920944: step 144780, loss = 1.82 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:36:37.094447: step 144790, loss = 1.80 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:38.266006: step 144800, loss = 1.90 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:39.450329: step 144810, loss = 1.84 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:40.623259: step 144820, loss = 1.88 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:41.788333: step 144830, loss = 1.96 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:42.969512: step 144840, loss = 1.91 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:44.147659: step 144850, loss = 1.93 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:45.314805: step 144860, loss = 1.90 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:36:46.495426: step 144870, loss = 2.23 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:47.682834: step 144880, loss = 1.86 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:36:48.861448: step 144890, loss = 1.91 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:50.053440: step 144900, loss = 2.10 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:36:51.240973: step 144910, loss = 2.03 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:36:52.425890: step 144920, loss = 1.84 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:53.606035: step 144930, loss = 1.94 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:36:54.795818: step 144940, loss = 1.85 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:36:55.993774: step 144950, loss = 2.01 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:36:57.199419: step 144960, loss = 1.83 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:36:58.385596: step 144970, loss = 2.00 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:36:59.559098: step 144980, loss = 1.96 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:37:00.748375: step 144990, loss = 2.00 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:01.928138: step 145000, loss = 1.89 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:37:03.125755: E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1494 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
step 145010, loss = 1.87 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:37:04.329299: step 145020, loss = 1.85 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:37:05.527279: step 145030, loss = 2.04 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:37:06.713146: step 145040, loss = 1.88 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:07.921733: step 145050, loss = 1.83 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:37:09.120730: step 145060, loss = 1.98 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:37:10.305294: step 145070, loss = 1.90 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:37:11.518094: step 145080, loss = 1.77 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:37:12.749602: step 145090, loss = 1.79 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-05 02:37:13.951055: step 145100, loss = 1.91 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:37:15.161688: step 145110, loss = 1.70 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:37:16.377833: step 145120, loss = 1.90 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:37:17.583350: step 145130, loss = 1.95 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:37:18.775086: step 145140, loss = 1.80 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:19.968987: step 145150, loss = 1.92 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:21.174857: step 145160, loss = 1.88 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:37:22.370602: step 145170, loss = 1.81 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:37:23.578880: step 145180, loss = 1.87 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:37:24.770520: step 145190, loss = 1.91 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:25.950551: step 145200, loss = 2.03 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:37:27.136337: step 145210, loss = 1.80 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:28.297795: step 145220, loss = 1.71 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:37:29.488566: step 145230, loss = 1.82 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:30.672719: step 145240, loss = 1.81 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:37:31.839731: step 145250, loss = 1.86 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:37:33.039169: step 145260, loss = 1.76 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:37:34.226893: step 145270, loss = 1.77 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:37:35.394568: step 145280, loss = 1.85 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:37:36.549561: step 145290, loss = 1.87 (1108.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:37:37.711880: step 145300, loss = 1.85 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:37:38.883204: step 145310, loss = 1.78 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:37:40.043511: step 145320, loss = 1.93 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:37:41.205274: step 145330, loss = 2.17 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:37:42.352399: step 145340, loss = 1.86 (1115.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:37:43.517459: step 145350, loss = 1.87 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:37:44.677439: step 145360, loss = 1.86 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:37:45.822266: step 145370, loss = 1.82 (1118.1 examples/sec; 0.114 sec/batch)
2017-05-05 02:37:46.998078: step 145380, loss = 1.99 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:37:48.171392: step 145390, loss = 1.98 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:37:49.346714: step 145400, loss = 2.10 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:37:50.514356: step 145410, loss = 1.94 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:37:51.695209: step 145420, loss = 2.01 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:37:52.854119: step 145430, loss = 1.92 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:37:53.999076: step 145440, loss = 1.91 (1118.0 examples/sec; 0.114 sec/batch)
2017-05-05 02:37:55.161687: step 145450, loss = 1.76 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:37:56.339361: step 145460, loss = 1.79 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:37:57.520850: step 145470, loss = 1.85 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:37:58.684918: step 145480, loss = 1.84 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:37:59.848555: step 145490, loss = 1.94 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:00.995332: step 145500, loss = 1.99 (1116.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:38:02.157999: step 145510, loss = 1.90 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:03.322189: step 145520, loss = 2.03 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:04.576757: step 145530, loss = 1.98 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-05 02:38:05.632058: step 145540, loss = 1.94 (1212.9 examples/sec; 0.106 sec/batch)
2017-05-05 02:38:06.807377: step 145550, loss = 1.88 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:07.967203: step 145560, loss = 1.91 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:09.142836: step 145570, loss = 1.95 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:10.296777: step 145580, loss = 1.91 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:38:11.458786: step 145590, loss = 1.93 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:12.618303: step 145600, loss = 1.85 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:13.796727: step 145610, loss = 2.01 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:14.942851: step 145620, loss = 1.73 (1116.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:38:16.103327: step 145630, loss = 1.93 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:17.283050: step 145640, loss = 1.87 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:18.436041: step 145650, loss = 1.87 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:38:19.611108: step 145660, loss = 2.03 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:20.776583: step 145670, loss = 1.88 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:21.933015: step 145680, loss = 1.91 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:23.100572: step 145690, loss = 1.87 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:24.278955: step 145700, loss = 1.89 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:25.433393: step 145710, loss = 2.02 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:38:26.596854: step 145720, loss = 1.95 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:27.785395: step 145730, loss = 1.95 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:38:28.929963: step 145740, loss = 1.72 (1118.3 examples/sec; 0.114 sec/batch)
2017-05-05 02:38:30.097966: step 145750, loss = 2.03 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:31.270856: step 145760, loss = 1.86 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:32.457839: step 145770, loss = 1.96 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:38:33.634643: step 145780, loss = 2.02 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:34.813612: step 145790, loss = 1.86 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:35.991591: step 145800, loss = 1.75 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:37.202806: step 145810, loss = 1.82 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:38:38.335936: step 145820, loss = 1.70 (1129.6 examples/sec; 0.113 sec/batch)
2017-05-05 02:38:39.523893: step 145830, loss = 1.86 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:38:40.706586: step 145840, loss = 1.83 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:41.874070: step 145850, loss = 1.82 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:43.044145: step 145860, loss = 1.78 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:44.214657: step 145870, loss = 2.05 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:45.380034: step 145880, loss = 1.93 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:46.548824: step 145890, loss = 1.95 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:47.711349: step 145900, loss = 1.92 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:48.881893: step 145910, loss = 1.91 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:50.045498: step 145920, loss = 1.88 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:51.218376: step 145930, loss = 1.91 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:38:52.408320: step 145940, loss = 1.71 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:38:53.571986: step 145950, loss = 1.91 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:38:54.703734: step 145960, loss = 1.75 (1131.0 examples/sec; 0.113 sec/batch)
2017-05-05 02:38:55.881294: step 145970, loss = 1.86 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:57.058282: step 145980, loss = 1.85 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:38:58.212871: step 145990, loss = 1.75 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:38:59.387045: step 146000, loss = 1.95 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:00.557698: step 146010, loss = 1.79 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:01.728492: step 146020, loss = 1.84 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:02.924441: step 146030, loss = 1.88 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:39:04.133218: step 146040, loss = 1.84 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:39:05.345982: step 146050, loss = 1.81 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:39:06.555705: step 146060, loss = 1.79 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:39:07.768814: step 146070, loss = 1.76 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:39:08.960097: step 146080, loss = 1.89 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:39:10.136709: step 146090, loss = 1.85 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:39:11.315204: step 146100, loss = 1.91 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:39:12.517469: step 146110, loss = 1.94 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:39:13.694877: step 146120, loss = 1.85 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:39:14.867156: step 146130, loss = 1.81 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:16.042751: step 146140, loss = 1.93 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:39:17.211497: step 146150, loss = 2.10 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:18.361741: step 146160, loss = 1.95 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:39:19.527478: step 146170, loss = 1.96 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:20.692679: step 146180, loss = 1.96 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:21.847179: step 146190, loss = 1.87 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:39:23.019838: step 146200, loss = 1.98 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:24.227676: step 146210, loss = 1.87 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:39:25.385181: step 146220, loss = 1.90 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:26.541198: step 146230, loss = 1.79 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:27.701449: step 146240, loss = 1.97 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:28.893080: step 146250, loss = 2.00 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:39:30.060810: step 146260, loss = 1.88 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:31.231022: step 146270, loss = 1.89 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:32.383571: step 146280, loss = 1.94 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:39:33.569068: step 146290, loss = 1.80 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:39:34.734591: step 146300, loss = 1.95 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:35.897206: step 146310, loss = 1.87 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:37.071740: step 146320, loss = 1.85 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:38.244902: step 146330, loss = 1.90 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:39.414699: step 146340, loss = 1.90 (1094.2 examples/sec; 0.117E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1506 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
 sec/batch)
2017-05-05 02:39:40.576076: step 146350, loss = 1.74 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:41.732040: step 146360, loss = 2.01 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:42.901109: step 146370, loss = 1.98 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:44.062958: step 146380, loss = 1.70 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:45.223690: step 146390, loss = 1.94 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:46.378675: step 146400, loss = 1.97 (1108.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:39:47.555202: step 146410, loss = 1.96 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:39:48.729406: step 146420, loss = 2.13 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:49.908009: step 146430, loss = 1.78 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:39:51.091658: step 146440, loss = 1.90 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:39:52.273110: step 146450, loss = 1.89 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:39:53.428391: step 146460, loss = 2.07 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:54.579657: step 146470, loss = 1.78 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:39:55.748148: step 146480, loss = 2.00 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:39:56.902083: step 146490, loss = 2.03 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:39:58.065846: step 146500, loss = 1.89 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:39:59.226261: step 146510, loss = 2.00 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:00.497423: step 146520, loss = 1.96 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-05 02:40:01.562929: step 146530, loss = 1.82 (1201.3 examples/sec; 0.107 sec/batch)
2017-05-05 02:40:02.737008: step 146540, loss = 1.95 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:03.906118: step 146550, loss = 1.93 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:05.089714: step 146560, loss = 2.01 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:06.241523: step 146570, loss = 2.15 (1111.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:40:07.408873: step 146580, loss = 1.81 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:08.586077: step 146590, loss = 2.05 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:09.743486: step 146600, loss = 1.83 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:10.913466: step 146610, loss = 1.88 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:12.065974: step 146620, loss = 1.85 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:40:13.225513: step 146630, loss = 2.08 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:14.370361: step 146640, loss = 1.84 (1118.1 examples/sec; 0.114 sec/batch)
2017-05-05 02:40:15.536532: step 146650, loss = 1.89 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:16.730798: step 146660, loss = 1.78 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:40:17.881733: step 146670, loss = 1.75 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:40:19.053669: step 146680, loss = 1.83 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:20.218774: step 146690, loss = 1.83 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:21.405422: step 146700, loss = 1.87 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:40:22.577507: step 146710, loss = 1.91 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:23.757124: step 146720, loss = 1.86 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:24.929078: step 146730, loss = 1.97 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:26.082717: step 146740, loss = 1.81 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:40:27.264697: step 146750, loss = 1.89 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:28.444014: step 146760, loss = 1.82 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:29.621865: step 146770, loss = 1.86 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:40:30.793797: step 146780, loss = 2.06 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:31.958997: step 146790, loss = 1.98 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:33.119101: step 146800, loss = 1.87 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:34.281421: step 146810, loss = 1.80 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:35.451761: step 146820, loss = 1.78 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:36.612687: step 146830, loss = 2.11 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:37.784633: step 146840, loss = 1.97 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:38.936802: step 146850, loss = 1.97 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:40:40.094539: step 146860, loss = 1.99 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:41.264226: step 146870, loss = 1.82 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:42.420025: step 146880, loss = 1.91 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:43.580747: step 146890, loss = 1.74 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:44.739518: step 146900, loss = 1.85 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:45.893277: step 146910, loss = 1.98 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:40:47.061533: step 146920, loss = 1.93 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:48.217108: step 146930, loss = 1.85 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:49.371145: step 146940, loss = 1.88 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:40:50.528701: step 146950, loss = 1.88 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:51.698198: step 146960, loss = 1.86 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:52.887455: step 146970, loss = 1.89 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:40:54.060518: step 146980, loss = 1.86 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:55.228222: step 146990, loss = 1.91 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:56.402465: step 147000, loss = 1.99 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:40:57.545371: step 147010, loss = 1.92 (1119.9 examples/sec; 0.114 sec/batch)
2017-05-05 02:40:58.700989: step 147020, loss = 1.82 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:40:59.880770: step 147030, loss = 2.07 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:41:01.032434: step 147040, loss = 1.98 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:41:02.199068: step 147050, loss = 1.81 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:03.386288: step 147060, loss = 1.90 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:41:04.556373: step 147070, loss = 1.84 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:05.716820: step 147080, loss = 1.90 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:06.876507: step 147090, loss = 1.92 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:08.038140: step 147100, loss = 1.88 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:09.210802: step 147110, loss = 2.00 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:10.369860: step 147120, loss = 1.81 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:11.556953: step 147130, loss = 1.97 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:41:12.717948: step 147140, loss = 1.93 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:13.874716: step 147150, loss = 1.82 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:15.043739: step 147160, loss = 1.81 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:16.221914: step 147170, loss = 1.80 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:41:17.384979: step 147180, loss = 2.04 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:18.519085: step 147190, loss = 1.83 (1128.6 examples/sec; 0.113 sec/batch)
2017-05-05 02:41:19.686672: step 147200, loss = 1.91 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:20.850464: step 147210, loss = 1.85 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:22.007372: step 147220, loss = 1.75 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:23.177710: step 147230, loss = 1.81 (1093.7 examples/sec; 0.117 secE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1517 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
/batch)
2017-05-05 02:41:24.351993: step 147240, loss = 1.72 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:25.518501: step 147250, loss = 1.68 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:26.690923: step 147260, loss = 1.93 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:27.854299: step 147270, loss = 1.84 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:29.022666: step 147280, loss = 2.01 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:30.177937: step 147290, loss = 1.83 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:31.349584: step 147300, loss = 2.03 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:32.525678: step 147310, loss = 1.97 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:41:33.687867: step 147320, loss = 1.73 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:34.883346: step 147330, loss = 1.85 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:41:36.081657: step 147340, loss = 2.00 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:41:37.274990: step 147350, loss = 1.87 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:41:38.473835: step 147360, loss = 2.05 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:41:39.656465: step 147370, loss = 1.96 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:41:40.844631: step 147380, loss = 1.89 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:41:42.022539: step 147390, loss = 1.90 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:41:43.206280: step 147400, loss = 2.06 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:41:44.388548: step 147410, loss = 1.81 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:41:45.583772: step 147420, loss = 1.83 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:41:46.784734: step 147430, loss = 1.81 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:41:47.982811: step 147440, loss = 1.83 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:41:49.165202: step 147450, loss = 1.76 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:41:50.336083: step 147460, loss = 2.01 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:51.497286: step 147470, loss = 1.87 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:41:52.687879: step 147480, loss = 1.98 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:41:53.813156: step 147490, loss = 1.89 (1137.5 examples/sec; 0.113 sec/batch)
2017-05-05 02:41:54.984220: step 147500, loss = 1.88 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:41:56.241187: step 147510, loss = 1.74 (1018.3 examples/sec; 0.126 sec/batch)
2017-05-05 02:41:57.295607: step 147520, loss = 2.00 (1213.9 examples/sec; 0.105 sec/batch)
2017-05-05 02:41:58.483011: step 147530, loss = 2.13 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:41:59.651107: step 147540, loss = 1.79 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:00.812211: step 147550, loss = 1.98 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:01.947679: step 147560, loss = 2.11 (1127.3 examples/sec; 0.114 sec/batch)
2017-05-05 02:42:03.117413: step 147570, loss = 1.86 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:04.284508: step 147580, loss = 1.82 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:05.437916: step 147590, loss = 1.75 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:06.590205: step 147600, loss = 2.05 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:07.750512: step 147610, loss = 1.78 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:08.942691: step 147620, loss = 1.87 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:42:10.112417: step 147630, loss = 1.85 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:11.284818: step 147640, loss = 1.74 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:12.451094: step 147650, loss = 1.80 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:13.610537: step 147660, loss = 1.87 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:14.777469: step 147670, loss = 1.89 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:15.959410: step 147680, loss = 1.89 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:42:17.123613: step 147690, loss = 1.77 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:18.306106: step 147700, loss = 1.90 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:42:19.474881: step 147710, loss = 1.96 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:20.628350: step 147720, loss = 1.96 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:21.773490: step 147730, loss = 1.74 (1117.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:22.940969: step 147740, loss = 1.89 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:24.141413: step 147750, loss = 1.95 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:42:25.326795: step 147760, loss = 1.87 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:42:26.503399: step 147770, loss = 1.95 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:42:27.661827: step 147780, loss = 1.88 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:28.849039: step 147790, loss = 2.02 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:42:29.992323: step 147800, loss = 1.96 (1119.6 examples/sec; 0.114 sec/batch)
2017-05-05 02:42:31.164067: step 147810, loss = 1.91 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:32.325902: step 147820, loss = 2.00 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:33.500441: step 147830, loss = 1.75 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:34.651959: step 147840, loss = 2.02 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:35.806341: step 147850, loss = 1.99 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:36.968023: step 147860, loss = 1.86 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:38.135101: step 147870, loss = 1.89 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:39.291859: step 147880, loss = 1.88 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:40.453263: step 147890, loss = 1.86 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:41.610823: step 147900, loss = 1.93 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:42.772918: step 147910, loss = 1.82 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:43.949828: step 147920, loss = 1.87 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:42:45.117949: step 147930, loss = 1.88 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:46.259005: step 147940, loss = 1.86 (1121.8 examples/sec; 0.114 sec/batch)
2017-05-05 02:42:47.434262: step 147950, loss = 1.80 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:42:48.593814: step 147960, loss = 1.75 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:49.733565: step 147970, loss = 1.78 (1123.1 examples/sec; 0.114 sec/batch)
2017-05-05 02:42:50.898982: step 147980, loss = 1.82 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:52.056029: step 147990, loss = 1.94 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:53.212435: step 148000, loss = 2.03 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:54.357540: step 148010, loss = 1.91 (1117.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:55.527520: step 148020, loss = 1.98 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:42:56.687693: step 148030, loss = 1.76 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:42:57.835141: step 148040, loss = 1.81 (1115.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:42:59.007799: step 148050, loss = 1.86 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:00.192839: step 148060, loss = 1.81 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:43:01.360896: step 148070, loss = 1.90 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:02.539667: step 148080, loss = 1.86 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:03.710275: step 148090, loss = 1.82 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:04.877310: step 148100, loss = 1.86 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:06.061129: step 148110, loss = 2.04 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:07.223286: step 148120, loss = 1.95 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:08.389102: step 148130, loss = 1.88 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:09.561817: step 148140, loss = 1.90 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:10.715731: step 148150, loss = 1.98 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:43:11.884486: step 148160, loss = 2.06 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:13.054525: step 148170, loss = 1.91 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:14.214861: step 148180, loss = 1.84 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:15.385524: step 148190, loss = 1.92 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:16.543999: step 148200, loss = 1.93 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:17.697579: step 148210, loss = 1.79 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:43:18.860952: step 148220, loss = 1.85 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:20.036296: step 148230, loss = 1.96 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:21.200884: step 148240, loss = 1.90 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:22.369050: step 148250, loss = 1.92 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:23.545766: step 148260, loss = 1.90 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:24.703302: step 148270, loss = 2.08 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:25.880114: step 148280, loss = 1.94 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:27.051410: step 148290, loss = 1.96 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:28.241376: step 148300, loss = 1.83 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:43:29.411887: step 148310, loss = 1.86 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:30.587441: step 148320, loss = 1.90 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:31.770976: step 148330, loss = 1.97 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:32.946137: step 148340, loss = 1.84 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:34.110139: step 148350, loss = 1.73 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:35.274564: step 148360, loss = 1.83 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:36.453493: step 148370, loss = 1.88 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:37.606391: step 148380, loss = 1.74 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:43:38.777549: step 148390, loss = 1.96 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:39.943996: step 148400, loss = 1.97 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:41.116671: step 148410, loss = 1.89 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:42.279596: step 148420, loss = 1.74 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:43.431409: step 148430, loss = 1.86 (1111.3 examples/sec; 0.115 sec/batch)
2017-05-05 02:43:44.587708: step 148440, loss = 1.80 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:45.739910: step 148450, loss = 1.85 (1110.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:43:46.894419: step 148460, loss = 1.89 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:43:48.083836: step 148470, loss = 2.00 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:43:49.255905: step 148480, loss = 1.91 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:50.428543: step 148490, loss = 1.90 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:51.675439: step 148500, loss = 1.97 (1026.5 examples/sec; 0.125 sec/batch)
2017-05-05 02:43:52.764153: step 148510, loss = 1.92 (1175.7 examples/sec; 0.109 sec/batch)
2017-05-05 02:43:53.934964: step 148520, loss = 1.85 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:55.119588: step 148530, loss = 1.88 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:56.304369: step 148540, loss = 1.83 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:43:57.472630: step 148550, loss = 1.95 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:43:58.633835: step 148560, loss = 1.85 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:43:59.795505: step 148570, losE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1528 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
s = 1.86 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:00.968307: step 148580, loss = 1.84 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:02.126011: step 148590, loss = 1.78 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:03.307075: step 148600, loss = 1.92 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:44:04.491694: step 148610, loss = 1.99 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:44:05.636928: step 148620, loss = 1.94 (1117.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:44:06.798046: step 148630, loss = 1.96 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:07.946509: step 148640, loss = 1.93 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:44:09.114731: step 148650, loss = 1.80 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:10.275725: step 148660, loss = 1.89 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:11.436161: step 148670, loss = 2.07 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:12.612535: step 148680, loss = 1.96 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:44:13.794394: step 148690, loss = 1.97 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:44:14.958371: step 148700, loss = 1.87 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:16.128695: step 148710, loss = 1.79 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:17.298217: step 148720, loss = 1.84 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:18.441774: step 148730, loss = 1.93 (1119.3 examples/sec; 0.114 sec/batch)
2017-05-05 02:44:19.606268: step 148740, loss = 1.83 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:20.759516: step 148750, loss = 1.87 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:44:21.920456: step 148760, loss = 1.85 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:23.094865: step 148770, loss = 1.83 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:24.257352: step 148780, loss = 1.79 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:25.405801: step 148790, loss = 1.83 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:44:26.587421: step 148800, loss = 1.92 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:44:27.756008: step 148810, loss = 1.86 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:28.925930: step 148820, loss = 1.88 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:30.083823: step 148830, loss = 1.90 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:31.250142: step 148840, loss = 1.87 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:32.406692: step 148850, loss = 1.80 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:33.561924: step 148860, loss = 1.79 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:34.713648: step 148870, loss = 1.91 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:44:35.881807: step 148880, loss = 1.88 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:37.043397: step 148890, loss = 1.82 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:38.200220: step 148900, loss = 1.91 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:39.389825: step 148910, loss = 1.86 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:44:40.564667: step 148920, loss = 1.71 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:41.717130: step 148930, loss = 1.79 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:44:42.888609: step 148940, loss = 1.84 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:44.054690: step 148950, loss = 1.90 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:45.210210: step 148960, loss = 1.84 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:46.364860: step 148970, loss = 1.82 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:44:47.541668: step 148980, loss = 1.94 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:44:48.715578: step 148990, loss = 1.87 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:49.852487: step 149000, loss = 1.91 (1125.9 examples/sec; 0.114 sec/batch)
2017-05-05 02:44:51.019573: step 149010, loss = 1.93 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:52.188908: step 149020, loss = 1.97 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:53.350157: step 149030, loss = 1.79 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:54.515007: step 149040, loss = 1.94 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:44:55.684315: step 149050, loss = 1.81 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:44:56.837250: step 149060, loss = 1.85 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:44:58.016348: step 149070, loss = 1.95 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:44:59.192092: step 149080, loss = 1.98 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:00.386197: step 149090, loss = 1.94 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:45:01.548643: step 149100, loss = 1.87 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:02.720781: step 149110, loss = 1.84 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:03.883742: step 149120, loss = 1.79 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:05.058826: step 149130, loss = 1.83 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:06.231351: step 149140, loss = 1.90 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:07.397844: step 149150, loss = 1.71 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:08.561618: step 149160, loss = 1.84 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:09.706321: step 149170, loss = 1.73 (1118.2 examples/sec; 0.114 sec/batch)
2017-05-05 02:45:10.882563: step 149180, loss = 2.00 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:12.043054: step 149190, loss = 1.79 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:13.192429: step 149200, loss = 1.99 (1113.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:45:14.351314: step 149210, loss = 1.73 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:15.519896: step 149220, loss = 2.01 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:16.687118: step 149230, loss = 1.72 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:17.842464: step 149240, loss = 1.91 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:19.011724: step 149250, loss = 1.91 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:20.186279: step 149260, loss = 1.86 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:21.361061: step 149270, loss = 1.86 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:22.519293: step 149280, loss = 1.75 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:23.695761: step 149290, loss = 1.72 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:24.882999: step 149300, loss = 1.70 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:45:26.047322: step 149310, loss = 1.88 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:27.234188: step 149320, loss = 1.93 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:45:28.430457: step 149330, loss = 1.83 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:45:29.596553: step 149340, loss = 1.84 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:30.768183: step 149350, loss = 2.06 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:31.955780: step 149360, loss = 1.81 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:45:33.134777: step 149370, loss = 1.76 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:34.289534: step 149380, loss = 1.97 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:45:35.475567: step 149390, loss = 1.85 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:45:36.632875: step 149400, loss = 1.77 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:37.783316: step 149410, loss = 1.87 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:45:38.962615: step 149420, loss = 1.79 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:40.118446: step 149430, loss = 1.72 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:45:41.270931: step 149440, loss = 1.89 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:45:42.442466: step 149450, loss = 2.05 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:43.621642: step 149460, loss = E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1539 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
1.85 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:45:44.788024: step 149470, loss = 1.95 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:45:45.983920: step 149480, loss = 1.94 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:45:47.290920: step 149490, loss = 1.81 (979.3 examples/sec; 0.131 sec/batch)
2017-05-05 02:45:48.395706: step 149500, loss = 1.84 (1158.6 examples/sec; 0.110 sec/batch)
2017-05-05 02:45:49.585725: step 149510, loss = 1.85 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:45:50.779200: step 149520, loss = 1.97 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:45:51.987658: step 149530, loss = 1.92 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:45:53.185902: step 149540, loss = 2.05 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:45:54.381297: step 149550, loss = 1.67 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:45:55.571739: step 149560, loss = 1.95 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:45:56.765853: step 149570, loss = 1.84 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:45:57.958376: step 149580, loss = 2.05 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:45:59.148671: step 149590, loss = 1.92 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:46:00.352567: step 149600, loss = 1.97 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:01.530112: step 149610, loss = 1.70 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:46:02.747976: step 149620, loss = 1.83 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:46:03.949807: step 149630, loss = 1.89 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:05.156316: step 149640, loss = 1.78 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:06.350601: step 149650, loss = 2.13 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:46:07.564568: step 149660, loss = 1.89 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:08.759796: step 149670, loss = 1.82 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:09.951519: step 149680, loss = 1.99 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:46:11.152185: step 149690, loss = 1.85 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:12.352141: step 149700, loss = 1.91 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:13.564303: step 149710, loss = 1.81 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:14.750699: step 149720, loss = 1.94 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:46:15.954684: step 149730, loss = 1.84 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:17.150060: step 149740, loss = 1.88 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:18.355756: step 149750, loss = 1.76 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:19.555972: step 149760, loss = 1.96 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:20.781066: step 149770, loss = 1.72 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 02:46:22.014211: step 149780, loss = 1.86 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-05 02:46:23.238645: step 149790, loss = 1.86 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:46:24.450437: step 149800, loss = 2.17 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:25.640545: step 149810, loss = 1.92 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:46:26.837071: step 149820, loss = 1.89 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:28.042819: step 149830, loss = 1.75 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:29.239856: step 149840, loss = 1.81 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:30.449639: step 149850, loss = 1.84 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:31.656887: step 149860, loss = 1.90 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:32.860319: step 149870, loss = 1.86 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:34.062660: step 149880, loss = 1.88 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:35.276476: step 149890, loss = 1.93 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:36.482483: step 149900, loss = 1.93 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:37.695419: step 149910, loss = 1.97 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:38.891876: step 149920, loss = 1.74 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:40.091581: step 149930, loss = 1.93 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:41.292928: step 149940, loss = 1.81 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:42.482896: step 149950, loss = 1.86 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:46:43.682512: step 149960, loss = 1.95 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:44.899126: step 149970, loss = 1.89 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:46:46.106991: step 149980, loss = 1.77 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:46:47.310437: step 149990, loss = 1.74 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:48.509508: step 150000, loss = 1.93 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:46:49.682055: step 150010, loss = 2.01 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:46:50.854459: step 150020, loss = 1.94 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:46:52.031742: step 150030, loss = 1.89 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:46:53.211475: step 150040, loss = 1.82 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:46:54.376103: step 150050, loss = 1.89 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:46:55.551260: step 150060, loss = 1.90 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:46:56.738096: step 150070, loss = 1.88 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:46:57.908722: step 150080, loss = 1.96 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:46:59.081797: step 150090, loss = 1.84 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:00.246431: step 150100, loss = 1.84 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:01.416201: step 150110, loss = 1.94 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:02.616149: step 150120, loss = 1.88 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:47:03.807540: step 150130, loss = 1.88 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:47:05.021915: step 150140, loss = 1.97 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:47:06.196331: step 150150, loss = 1.87 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:07.357501: step 150160, loss = 1.89 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:08.529851: step 150170, loss = 1.84 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:09.709904: step 150180, loss = 1.94 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:10.887576: step 150190, loss = 2.06 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:12.070981: step 150200, loss = 1.82 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:13.237130: step 150210, loss = 2.01 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:14.392887: step 150220, loss = 1.90 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:15.567753: step 150230, loss = 1.89 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:16.743384: step 150240, loss = 2.07 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:17.913082: step 150250, loss = 1.80 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:19.074860: step 150260, loss = 1.97 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:20.249771: step 150270, loss = 2.10 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:21.424917: step 150280, loss = 2.01 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:22.587871: step 150290, loss = 1.93 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:23.769343: step 150300, loss = 1.88 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:24.948518: step 150310, loss = 1.88 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:26.117704: step 150320, loss = 1.86 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:27.291559: step 150330, loss = 1.82 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:28.441419: step 150340, loss = 1.74 (1113.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:47:29.612559: step 150350, loss = 2.03 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1550 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
(1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:30.773140: step 150360, loss = 1.84 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:31.926943: step 150370, loss = 1.85 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:47:33.090313: step 150380, loss = 1.89 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:34.244334: step 150390, loss = 1.89 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:47:35.409068: step 150400, loss = 2.02 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:36.566446: step 150410, loss = 1.79 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:37.714526: step 150420, loss = 1.83 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:47:38.875402: step 150430, loss = 1.87 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:40.043834: step 150440, loss = 1.90 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:41.211926: step 150450, loss = 1.86 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:42.389259: step 150460, loss = 1.79 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:43.527026: step 150470, loss = 1.97 (1125.0 examples/sec; 0.114 sec/batch)
2017-05-05 02:47:44.796882: step 150480, loss = 1.79 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-05 02:47:45.865539: step 150490, loss = 1.98 (1197.8 examples/sec; 0.107 sec/batch)
2017-05-05 02:47:47.016708: step 150500, loss = 1.95 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:47:48.190674: step 150510, loss = 1.78 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:49.371515: step 150520, loss = 1.81 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:50.520364: step 150530, loss = 1.76 (1114.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:47:51.685961: step 150540, loss = 1.82 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:47:52.864606: step 150550, loss = 2.10 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:54.028655: step 150560, loss = 1.82 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:55.209642: step 150570, loss = 1.99 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:56.394378: step 150580, loss = 2.01 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:47:57.558739: step 150590, loss = 1.88 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:58.717698: step 150600, loss = 1.85 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:47:59.887660: step 150610, loss = 1.99 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:01.058651: step 150620, loss = 1.88 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:02.222509: step 150630, loss = 1.78 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:03.386101: step 150640, loss = 1.95 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:04.562880: step 150650, loss = 1.82 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:05.712226: step 150660, loss = 1.93 (1113.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:48:06.887447: step 150670, loss = 1.76 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:08.065084: step 150680, loss = 1.80 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:09.252427: step 150690, loss = 1.90 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:48:10.404816: step 150700, loss = 2.03 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:48:11.560938: step 150710, loss = 1.83 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:12.721109: step 150720, loss = 1.89 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:13.883098: step 150730, loss = 1.84 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:15.057567: step 150740, loss = 1.84 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:16.215844: step 150750, loss = 1.92 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:17.390031: step 150760, loss = 1.87 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:18.547506: step 150770, loss = 1.91 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:19.719905: step 150780, loss = 1.83 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:20.894657: step 150790, loss = 1.96 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:22.051565: step 150800, loss = 1.84 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:23.214948: step 150810, loss = 1.76 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:24.388899: step 150820, loss = 1.87 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:25.562336: step 150830, loss = 1.94 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:26.724540: step 150840, loss = 1.90 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:27.873384: step 150850, loss = 1.84 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:48:29.061352: step 150860, loss = 1.93 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:48:30.220534: step 150870, loss = 1.87 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:31.380131: step 150880, loss = 1.83 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:32.537441: step 150890, loss = 1.97 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:33.703997: step 150900, loss = 1.91 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:34.877161: step 150910, loss = 1.87 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:36.039833: step 150920, loss = 1.93 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:37.254476: step 150930, loss = 1.94 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:48:38.426848: step 150940, loss = 1.99 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:39.607347: step 150950, loss = 1.90 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:40.765347: step 150960, loss = 1.95 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:41.918585: step 150970, loss = 1.91 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:48:43.118721: step 150980, loss = 1.92 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:48:44.297663: step 150990, loss = 1.90 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:48:45.448701: step 151000, loss = 1.97 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:48:46.611124: step 151010, loss = 1.84 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:47.778038: step 151020, loss = 1.77 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:48.945223: step 151030, loss = 1.92 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:50.098196: step 151040, loss = 1.80 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:48:51.260826: step 151050, loss = 1.85 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:52.449277: step 151060, loss = 1.74 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:48:53.610766: step 151070, loss = 1.99 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:54.784907: step 151080, loss = 1.90 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:55.949576: step 151090, loss = 1.85 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:57.114854: step 151100, loss = 1.94 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:48:58.275066: step 151110, loss = 1.90 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:48:59.444383: step 151120, loss = 1.87 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:00.616312: step 151130, loss = 1.88 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:01.780034: step 151140, loss = 1.93 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:02.945190: step 151150, loss = 1.98 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:04.080330: step 151160, loss = 1.88 (1127.6 examples/sec; 0.114 sec/batch)
2017-05-05 02:49:05.241969: step 151170, loss = 1.88 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:06.389815: step 151180, loss = 1.83 (1115.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:49:07.583810: step 151190, loss = 1.81 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:49:08.736515: step 151200, loss = 1.89 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:49:09.897667: step 151210, loss = 1.91 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:11.074693: step 151220, loss = 1.84 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:49:12.272523: step 151230, loss = 1.90 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:49:13.425710: step 151240, loss = 1.99 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:49:14.584959: step 151250, loss = 1.88 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:15.750057: step 151260, loss = 1.70 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:16.914497: step 151270, loss = 2.06 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:18.069299: step 151280, loss = 1.99 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:49:19.244799: step 151290, loss = 1.97 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:49:20.413199: step 151300, loss = 1.95 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:21.602437: step 151310, loss = 2.02 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:49:22.776923: step 151320, loss = 1.85 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:23.958344: step 151330, loss = 1.93 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:49:25.133624: step 151340, loss = 1.80 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:49:26.297434: step 151350, loss = 1.86 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:27.470699: step 151360, loss = 1.89 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:28.667740: step 151370, loss = 1.94 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:49:29.844124: step 151380, loss = 1.97 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:49:31.016752: step 151390, loss = 1.83 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:32.186953: step 151400, loss = 1.84 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:33.356866: step 151410, loss = 1.84 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:34.546493: step 151420, loss = 1.84 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:49:35.720568: step 151430, loss = 1.89 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:36.888477: step 151440, loss = 1.97 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:38.042113: step 151450, loss = 1.91 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:49:39.202658: step 151460, loss = 1.83 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:40.468832: step 151470, loss = 1.87 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-05 02:49:41.537820: step 151480, loss = 1.93 (1197.4 examples/sec; 0.107 sec/batch)
2017-05-05 02:49:42.701926: step 151490, loss = 1.86 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:43.851958: step 151500, loss = 1.92 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:49:45.022808: step 151510, loss = 1.86 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:46.177374: step 151520, loss = 1.85 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:49:47.340261: step 151530, loss = 1.81 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:48.526468: step 151540, loss = 1.82 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:49:49.689744: step 151550, loss = 1.91 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:50.847135: step 151560, loss = 1.84 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:52.014927: step 151570, loss = 1.92 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:53.181277: step 151580, loss = 1.98 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:54.344907: step 151590, loss = 1.89 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:49:55.510770: step 151600, loss = 1.82 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:56.682521: step 151610, loss = 1.89 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:49:57.836350: step 151620, loss = 1.86 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:49:58.995009: step 151630, loss = 1.85 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:00.158298: step 151640, loss = 1.85 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:01.316523: step 151650, loss = 1.81 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:02.491547: step 151660, loss = 1.80 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:03.672593: step 151670, loss = 1.78 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:04.840522: step 151680, loss = 2.06 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 02E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1562 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
:50:05.983180: step 151690, loss = 2.01 (1120.2 examples/sec; 0.114 sec/batch)
2017-05-05 02:50:07.149482: step 151700, loss = 1.95 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:08.305593: step 151710, loss = 1.92 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:09.477534: step 151720, loss = 1.88 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:10.630475: step 151730, loss = 1.80 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:50:11.811181: step 151740, loss = 1.85 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:12.975416: step 151750, loss = 1.88 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:14.149011: step 151760, loss = 1.79 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:15.333821: step 151770, loss = 1.93 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:16.522193: step 151780, loss = 1.94 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:50:17.666539: step 151790, loss = 2.13 (1118.5 examples/sec; 0.114 sec/batch)
2017-05-05 02:50:18.834676: step 151800, loss = 1.81 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:19.985863: step 151810, loss = 1.82 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:50:21.162219: step 151820, loss = 1.86 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:22.322831: step 151830, loss = 2.08 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:23.508173: step 151840, loss = 1.76 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:50:24.668215: step 151850, loss = 1.95 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:25.815577: step 151860, loss = 2.00 (1115.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:50:26.981913: step 151870, loss = 1.91 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:28.158242: step 151880, loss = 1.79 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:29.333677: step 151890, loss = 1.78 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:30.493983: step 151900, loss = 1.89 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:31.663673: step 151910, loss = 1.93 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:32.818491: step 151920, loss = 1.88 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:50:33.985444: step 151930, loss = 1.97 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:35.145172: step 151940, loss = 1.96 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:36.302837: step 151950, loss = 1.98 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:37.456100: step 151960, loss = 2.20 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-05 02:50:38.615191: step 151970, loss = 1.87 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:39.777750: step 151980, loss = 1.88 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:40.974166: step 151990, loss = 1.82 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:50:42.159299: step 152000, loss = 1.90 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:50:43.321362: step 152010, loss = 1.90 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:44.515076: step 152020, loss = 1.92 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:50:45.681207: step 152030, loss = 1.98 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:46.874611: step 152040, loss = 1.90 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:50:48.054341: step 152050, loss = 1.90 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:49.221730: step 152060, loss = 1.84 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:50.380625: step 152070, loss = 2.05 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:50:51.553442: step 152080, loss = 1.92 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:50:52.733122: step 152090, loss = 1.85 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:53.910298: step 152100, loss = 1.76 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:55.092579: step 152110, loss = 1.85 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:56.279670: step 152120, loss = 1.88 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:50:57.457568: step 152130, loss = 2.06 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:58.636710: step 152140, loss = 1.81 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:50:59.812076: step 152150, loss = 1.89 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:51:01.003856: step 152160, loss = 1.94 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:51:02.179398: step 152170, loss = 1.98 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:51:03.366329: step 152180, loss = 1.99 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:51:04.541964: step 152190, loss = 1.87 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:51:05.704649: step 152200, loss = 1.78 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:51:06.879952: step 152210, loss = 1.81 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:51:08.052279: step 152220, loss = 1.80 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:09.229377: step 152230, loss = 1.79 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:51:10.397122: step 152240, loss = 1.90 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:11.556988: step 152250, loss = 1.94 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:51:12.731274: step 152260, loss = 1.97 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:13.885609: step 152270, loss = 1.80 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:51:15.051220: step 152280, loss = 1.78 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:16.217523: step 152290, loss = 1.85 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:17.377899: step 152300, loss = 1.90 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:51:18.552734: step 152310, loss = 1.95 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:19.729200: step 152320, loss = 1.86 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:51:20.887420: step 152330, loss = 1.75 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:51:22.059524: step 152340, loss = 1.81 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:23.243190: step 152350, loss = 1.87 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:51:24.411548: step 152360, loss = 1.87 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:25.585515: step 152370, loss = 1.79 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:51:26.768012: step 152380, loss = 1.89 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:51:27.974498: step 152390, loss = 1.91 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:51:29.188603: step 152400, loss = 1.88 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:51:30.400722: step 152410, loss = 1.95 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:51:31.603197: step 152420, loss = 1.98 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:51:32.820028: step 152430, loss = 1.94 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:51:34.031085: step 152440, loss = 1.83 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:51:35.256118: step 152450, loss = 1.90 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:51:36.559552: step 152460, loss = 1.91 (982.0 examples/sec; 0.130 sec/batch)
2017-05-05 02:51:37.667584: step 152470, loss = 1.83 (1155.2 examples/sec; 0.111 sec/batch)
2017-05-05 02:51:38.867869: step 152480, loss = 1.89 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:51:40.058733: step 152490, loss = 1.82 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:51:41.280063: step 152500, loss = 1.83 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:51:42.491715: step 152510, loss = 1.78 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:51:43.721401: step 152520, loss = 1.78 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:51:44.955359: step 152530, loss = 1.99 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-05 02:51:46.152765: step 152540, loss = 1.85 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:51:47.376063: step 152550, loss = 1.90 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 02:51:48.610365: step 152560, loss = 1.91 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-05 02:51:49.824268: step 152570, loss = 2.01 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:51:5E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1573 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
1.070476: step 152580, loss = 1.85 (1027.1 examples/sec; 0.125 sec/batch)
2017-05-05 02:51:52.315503: step 152590, loss = 1.91 (1028.1 examples/sec; 0.125 sec/batch)
2017-05-05 02:51:53.532734: step 152600, loss = 1.97 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:51:54.756913: step 152610, loss = 1.98 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:51:55.967878: step 152620, loss = 1.85 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:51:57.197559: step 152630, loss = 1.82 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:51:58.413955: step 152640, loss = 1.79 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 02:51:59.639419: step 152650, loss = 2.04 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:00.816700: step 152660, loss = 1.91 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:52:02.037461: step 152670, loss = 1.76 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:03.257073: step 152680, loss = 1.76 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:04.473954: step 152690, loss = 1.89 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:05.669051: step 152700, loss = 1.79 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:52:06.894691: step 152710, loss = 1.85 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:08.117095: step 152720, loss = 1.88 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:09.329254: step 152730, loss = 1.98 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:10.518912: step 152740, loss = 1.88 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:52:11.725806: step 152750, loss = 1.87 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:12.954629: step 152760, loss = 1.84 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:14.151814: step 152770, loss = 1.71 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:52:15.363018: step 152780, loss = 1.86 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:16.581910: step 152790, loss = 1.77 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:17.787678: step 152800, loss = 2.04 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:19.005338: step 152810, loss = 1.83 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:20.252845: step 152820, loss = 1.93 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-05 02:52:21.455145: step 152830, loss = 1.89 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:52:22.677404: step 152840, loss = 1.98 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:23.929144: step 152850, loss = 1.90 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-05 02:52:25.113349: step 152860, loss = 1.86 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:52:26.338388: step 152870, loss = 2.18 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:27.550471: step 152880, loss = 1.99 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:28.769359: step 152890, loss = 1.84 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:29.981747: step 152900, loss = 1.77 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:31.209987: step 152910, loss = 1.74 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:32.441220: step 152920, loss = 2.01 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:33.637159: step 152930, loss = 1.90 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:52:34.832669: step 152940, loss = 1.89 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:52:36.051433: step 152950, loss = 1.76 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:37.257649: step 152960, loss = 1.86 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:38.460881: step 152970, loss = 1.73 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:52:39.691396: step 152980, loss = 1.92 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:40.912102: step 152990, loss = 1.92 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:42.121538: step 153000, loss = 1.88 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:43.345669: step 153010, loss = 1.81 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:44.551590: step 153020, loss = 1.82 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:45.753021: step 153030, loss = 1.82 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:52:46.980383: step 153040, loss = 2.03 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:48.205894: step 153050, loss = 1.83 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 02:52:49.373146: step 153060, loss = 1.87 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:52:50.576454: step 153070, loss = 1.78 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:52:51.790702: step 153080, loss = 1.83 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:53.007403: step 153090, loss = 1.90 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:52:54.189361: step 153100, loss = 1.96 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:52:55.396617: step 153110, loss = 1.85 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 02:52:56.593351: step 153120, loss = 1.95 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:52:57.772112: step 153130, loss = 1.79 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:52:58.960144: step 153140, loss = 1.83 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:00.132525: step 153150, loss = 1.78 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:01.314440: step 153160, loss = 1.78 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:53:02.484281: step 153170, loss = 1.93 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:03.653965: step 153180, loss = 2.10 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:04.817083: step 153190, loss = 1.79 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:05.981355: step 153200, loss = 1.80 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:07.168739: step 153210, loss = 1.85 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:08.355127: step 153220, loss = 1.85 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:09.517529: step 153230, loss = 1.81 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:10.687600: step 153240, loss = 1.98 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:11.841776: step 153250, loss = 1.94 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:53:13.007523: step 153260, loss = 1.79 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:14.178869: step 153270, loss = 1.80 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:15.336452: step 153280, loss = 1.89 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:16.514282: step 153290, loss = 1.95 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:53:17.672466: step 153300, loss = 1.98 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:18.834723: step 153310, loss = 1.82 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:19.997154: step 153320, loss = 1.92 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:21.147514: step 153330, loss = 1.91 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:53:22.326373: step 153340, loss = 1.77 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:53:23.490147: step 153350, loss = 1.69 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:24.646636: step 153360, loss = 1.94 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:25.800718: step 153370, loss = 1.72 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-05 02:53:26.964590: step 153380, loss = 1.75 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:28.119137: step 153390, loss = 1.96 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:53:29.273942: step 153400, loss = 1.82 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:53:30.432450: step 153410, loss = 1.80 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:31.619185: step 153420, loss = 1.84 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:32.792060: step 153430, loss = 1.83 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:33.955898: step 153440, loss = 1.99 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:35.241392: step 153450, loss = 1.92 (995.7 examples/sec; 0.129 sec/batch)
2017-05-05 02:53:36.316862: step 153460, loss = 2.07 (1190.2 examples/sec; 0.108 sec/batch)
2017-05-05 02:53:37.505674: step 153470, loss = 2.05 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:38.679017: step 153480, loss = 1.87 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:39.837726: step 153490, loss = 2.00 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:53:41.014046: step 153500, loss = 1.90 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:53:42.202144: step 153510, loss = 1.91 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:43.374894: step 153520, loss = 1.87 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:44.568119: step 153530, loss = 1.89 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:45.738792: step 153540, loss = 1.80 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:46.907925: step 153550, loss = 1.93 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:48.074034: step 153560, loss = 1.95 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:49.250808: step 153570, loss = 1.84 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:53:50.441722: step 153580, loss = 1.93 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:51.612767: step 153590, loss = 1.86 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:53:52.806866: step 153600, loss = 1.88 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:53.992876: step 153610, loss = 1.96 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:53:55.194288: step 153620, loss = 1.95 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:53:56.398760: step 153630, loss = 1.92 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:53:57.578108: step 153640, loss = 1.87 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:53:58.779054: step 153650, loss = 1.82 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:53:59.933458: step 153660, loss = 1.84 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:54:01.126493: step 153670, loss = 1.78 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:02.278156: step 153680, loss = 1.98 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:54:03.452396: step 153690, loss = 1.85 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:54:04.609984: step 153700, loss = 1.78 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:54:05.764513: step 153710, loss = 1.98 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:54:06.961978: step 153720, loss = 1.85 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:54:08.138424: step 153730, loss = 1.82 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:09.317029: step 153740, loss = 1.78 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:10.477943: step 153750, loss = 1.92 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:54:11.663925: step 153760, loss = 1.89 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:12.829379: step 153770, loss = 1.89 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:54:13.986714: step 153780, loss = 1.97 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:54:15.148948: step 153790, loss = 2.03 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:54:16.342268: step 153800, loss = 1.85 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:17.508880: step 153810, loss = 1.98 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:54:18.680854: step 153820, loss = 1.87 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:54:19.862714: step 153830, loss = 1.92 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:21.045050: step 153840, loss = 1.86 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:22.224068: step 153850, loss = 1.77 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:23.430081: step 153860, loss = 2.00 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 02:54:24.640264: step 153870, loss = 1.90 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:54:25.840562: step 153880, loss = 1.99 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:54:27.026780: step 153890, loss = 1.84 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:28.215721: step 153900, loss = 1.94 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:29.392690: step 153910, loss = 2.05 (1087.5 examples/sE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1584 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
ec; 0.118 sec/batch)
2017-05-05 02:54:30.567631: step 153920, loss = 1.81 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:54:31.733709: step 153930, loss = 1.84 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:54:32.914385: step 153940, loss = 1.94 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:34.071389: step 153950, loss = 1.89 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:54:35.249419: step 153960, loss = 1.85 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:36.451261: step 153970, loss = 1.99 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:54:37.621897: step 153980, loss = 1.87 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:54:38.803513: step 153990, loss = 1.83 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:39.965083: step 154000, loss = 1.84 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:54:41.126822: step 154010, loss = 1.92 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:54:42.285780: step 154020, loss = 1.92 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:54:43.463929: step 154030, loss = 1.93 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:44.616910: step 154040, loss = 1.86 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:54:45.798215: step 154050, loss = 1.79 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:46.992735: step 154060, loss = 1.89 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:48.186179: step 154070, loss = 1.91 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:49.367291: step 154080, loss = 2.00 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:50.560412: step 154090, loss = 1.93 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:51.752956: step 154100, loss = 1.84 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:52.936742: step 154110, loss = 1.74 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:54:54.124161: step 154120, loss = 1.86 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:55.344467: step 154130, loss = 1.95 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 02:54:56.571576: step 154140, loss = 2.00 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 02:54:57.766134: step 154150, loss = 1.91 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:54:58.972443: step 154160, loss = 1.85 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:55:00.192533: step 154170, loss = 1.83 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 02:55:01.375158: step 154180, loss = 1.90 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:02.603899: step 154190, loss = 1.84 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 02:55:03.804763: step 154200, loss = 1.97 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:55:04.991928: step 154210, loss = 1.80 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:06.172901: step 154220, loss = 1.86 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:07.374057: step 154230, loss = 1.87 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:55:08.582818: step 154240, loss = 1.73 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:55:09.750737: step 154250, loss = 1.85 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:55:10.942568: step 154260, loss = 1.85 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:12.106374: step 154270, loss = 1.85 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:55:13.291871: step 154280, loss = 1.83 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:14.475817: step 154290, loss = 1.96 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:15.676374: step 154300, loss = 2.00 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:55:16.866390: step 154310, loss = 1.90 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:18.086426: step 154320, loss = 1.82 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-05 02:55:19.293157: step 154330, loss = 1.81 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 02:55:20.477569: step 154340, loss = 1.89 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:21.671517: step 154350, loss = 1.88 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:22.881784: step 154360, loss = 1.84 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:55:24.064775: step 154370, loss = 1.79 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:25.260204: step 154380, loss = 1.74 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:55:26.426380: step 154390, loss = 1.79 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:55:27.609668: step 154400, loss = 2.00 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:28.810590: step 154410, loss = 1.79 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 02:55:29.991816: step 154420, loss = 1.91 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:31.177587: step 154430, loss = 1.72 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:32.439516: step 154440, loss = 1.94 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-05 02:55:33.526282: step 154450, loss = 1.92 (1177.9 examples/sec; 0.109 sec/batch)
2017-05-05 02:55:34.708305: step 154460, loss = 1.77 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:35.893192: step 154470, loss = 1.91 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:37.061972: step 154480, loss = 1.90 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:55:38.225142: step 154490, loss = 2.00 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:55:39.374940: step 154500, loss = 1.91 (1113.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:55:40.548854: step 154510, loss = 1.85 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:55:41.708569: step 154520, loss = 1.84 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:55:42.895110: step 154530, loss = 1.79 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:44.056025: step 154540, loss = 1.76 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:55:45.236682: step 154550, loss = 1.70 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:55:46.429791: step 154560, loss = 1.83 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:47.619845: step 154570, loss = 1.89 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:48.817202: step 154580, loss = 1.98 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:55:50.004562: step 154590, loss = 1.84 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:51.217578: step 154600, loss = 1.90 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:55:52.407741: step 154610, loss = 1.71 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:55:53.609463: step 154620, loss = 1.85 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:55:54.824397: step 154630, loss = 1.90 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 02:55:56.031881: step 154640, loss = 1.75 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 02:55:57.267944: step 154650, loss = 1.93 (1035.5 examples/sec; 0.124 sec/batch)
2017-05-05 02:55:58.491645: step 154660, loss = 1.83 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-05 02:55:59.678096: step 154670, loss = 2.01 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:56:00.880485: step 154680, loss = 1.89 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 02:56:02.099809: step 154690, loss = 1.83 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:56:03.302091: step 154700, loss = 1.87 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:56:04.490541: step 154710, loss = 2.04 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:56:05.667023: step 154720, loss = 1.93 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:56:06.858475: step 154730, loss = 1.87 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:56:08.060186: step 154740, loss = 2.06 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 02:56:09.267456: step 154750, loss = 1.79 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 02:56:10.466951: step 154760, loss = 1.86 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 02:56:11.666263: step 154770, loss = 1.93 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:56:12.877434: step 154780, loss = 1.89 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:56:14.071974: step 154790, loss = 1.99 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:56:15.281576: step 154800, loss = 1.92 (1058.2 examples/sec; E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1595 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
0.121 sec/batch)
2017-05-05 02:56:16.478472: step 154810, loss = 1.96 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:56:17.666135: step 154820, loss = 1.84 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:56:18.843991: step 154830, loss = 1.86 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:56:20.022758: step 154840, loss = 1.86 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:56:21.196220: step 154850, loss = 1.94 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:22.386795: step 154860, loss = 1.91 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:56:23.601178: step 154870, loss = 1.96 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 02:56:24.773029: step 154880, loss = 1.83 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:25.937425: step 154890, loss = 1.94 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:27.110392: step 154900, loss = 2.04 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:28.285470: step 154910, loss = 1.77 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:56:29.436989: step 154920, loss = 1.88 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 02:56:30.611109: step 154930, loss = 1.79 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:31.759067: step 154940, loss = 1.71 (1115.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:56:32.919516: step 154950, loss = 1.94 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:34.076408: step 154960, loss = 1.82 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:35.243930: step 154970, loss = 1.98 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:36.422657: step 154980, loss = 1.82 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:56:37.599296: step 154990, loss = 1.68 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:56:38.759549: step 155000, loss = 1.83 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:39.916689: step 155010, loss = 1.95 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:41.094292: step 155020, loss = 1.80 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:56:42.234623: step 155030, loss = 1.86 (1122.4 examples/sec; 0.114 sec/batch)
2017-05-05 02:56:43.419465: step 155040, loss = 1.75 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:56:44.582807: step 155050, loss = 2.02 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:45.733504: step 155060, loss = 2.01 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:56:46.896899: step 155070, loss = 1.98 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:48.073438: step 155080, loss = 1.85 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:56:49.237515: step 155090, loss = 1.99 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:50.401162: step 155100, loss = 1.97 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:51.567334: step 155110, loss = 1.94 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:52.741870: step 155120, loss = 2.01 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:56:53.906465: step 155130, loss = 1.84 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:55.059915: step 155140, loss = 1.83 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:56:56.236433: step 155150, loss = 1.83 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:56:57.392665: step 155160, loss = 1.92 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:56:58.530777: step 155170, loss = 1.77 (1124.7 examples/sec; 0.114 sec/batch)
2017-05-05 02:56:59.698565: step 155180, loss = 1.75 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:57:00.871439: step 155190, loss = 1.81 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:57:02.016933: step 155200, loss = 1.97 (1117.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:57:03.178358: step 155210, loss = 1.80 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:57:04.334255: step 155220, loss = 1.91 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:57:05.507015: step 155230, loss = 1.77 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:57:06.691902: step 155240, loss = 1.87 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:07.836178: step 155250, loss = 1.88 (1118.6 examples/sec; 0.114 sec/batch)
2017-05-05 02:57:09.011129: step 155260, loss = 2.01 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:57:10.183652: step 155270, loss = 1.71 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:57:11.373083: step 155280, loss = 1.94 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:12.565428: step 155290, loss = 1.74 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:13.746302: step 155300, loss = 1.92 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:14.951185: step 155310, loss = 1.87 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:57:16.118024: step 155320, loss = 1.78 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:57:17.293216: step 155330, loss = 1.87 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:18.454460: step 155340, loss = 1.92 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:57:19.633943: step 155350, loss = 1.86 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:20.829184: step 155360, loss = 1.90 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 02:57:21.994444: step 155370, loss = 1.90 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:57:23.184747: step 155380, loss = 1.75 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:24.351983: step 155390, loss = 1.87 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:57:25.516366: step 155400, loss = 1.97 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:57:26.708925: step 155410, loss = 2.02 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:27.858136: step 155420, loss = 1.78 (1113.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:57:29.115211: step 155430, loss = 1.97 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-05 02:57:30.179910: step 155440, loss = 1.79 (1202.2 examples/sec; 0.106 sec/batch)
2017-05-05 02:57:31.371894: step 155450, loss = 1.83 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:32.547187: step 155460, loss = 1.85 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:33.714012: step 155470, loss = 1.81 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:57:34.900664: step 155480, loss = 1.85 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:36.072788: step 155490, loss = 1.91 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 02:57:37.241605: step 155500, loss = 1.95 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:57:38.417314: step 155510, loss = 1.96 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:39.594704: step 155520, loss = 1.85 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:40.774531: step 155530, loss = 1.95 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:41.947924: step 155540, loss = 1.80 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:57:43.144648: step 155550, loss = 1.87 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:57:44.329439: step 155560, loss = 1.86 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:57:45.515700: step 155570, loss = 1.89 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:46.705407: step 155580, loss = 1.86 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:47.893446: step 155590, loss = 1.97 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:49.084386: step 155600, loss = 1.97 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:50.246189: step 155610, loss = 1.79 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:57:51.406963: step 155620, loss = 1.95 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:57:52.615112: step 155630, loss = 1.73 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 02:57:53.746211: step 155640, loss = 1.79 (1131.6 examples/sec; 0.113 sec/batch)
2017-05-05 02:57:54.932461: step 155650, loss = 1.86 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:57:56.144777: step 155660, loss = 2.01 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 02:57:57.344382: step 155670, loss = 1.79 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:57:58.541462: step 155680, loss = 1.76 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:57:59.752612: step 155690, loss = 1.90 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 02:58:00.936094: step 155700, loss = 1.80 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:02.123255: step 155710, loss = 1.92 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:58:03.327313: step 155720, loss = 1.99 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:58:04.522190: step 155730, loss = 1.66 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:58:05.711828: step 155740, loss = 1.79 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:58:06.931141: step 155750, loss = 1.86 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-05 02:58:08.130464: step 155760, loss = 1.76 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 02:58:09.324154: step 155770, loss = 1.94 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 02:58:10.504651: step 155780, loss = 1.97 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:11.693703: step 155790, loss = 1.97 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:58:12.865500: step 155800, loss = 2.04 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:58:14.028074: step 155810, loss = 1.90 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:58:15.204907: step 155820, loss = 1.98 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:16.393388: step 155830, loss = 1.78 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:58:17.541873: step 155840, loss = 1.89 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-05 02:58:18.704046: step 155850, loss = 1.90 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:58:19.905258: step 155860, loss = 1.93 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 02:58:21.070856: step 155870, loss = 1.92 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 02:58:22.209540: step 155880, loss = 1.80 (1124.1 examples/sec; 0.114 sec/batch)
2017-05-05 02:58:23.391219: step 155890, loss = 2.02 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:24.558484: step 155900, loss = 1.99 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:58:25.717975: step 155910, loss = 1.80 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:58:26.863474: step 155920, loss = 1.77 (1117.4 examples/sec; 0.115 sec/batch)
2017-05-05 02:58:28.037096: step 155930, loss = 1.93 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:58:29.215303: step 155940, loss = 1.92 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:30.374306: step 155950, loss = 1.92 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:58:31.540329: step 155960, loss = 1.94 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:58:32.713682: step 155970, loss = 1.76 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:58:33.863938: step 155980, loss = 1.93 (1112.8 examples/sec; 0.115 sec/batch)
2017-05-05 02:58:35.035777: step 155990, loss = 1.99 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:58:36.296418: step 156000, loss = 1.98 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-05 02:58:37.418354: step 156010, loss = 1.84 (1140.9 examples/sec; 0.112 sec/batch)
2017-05-05 02:58:38.594723: step 156020, loss = 1.80 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:39.799563: step 156030, loss = 1.79 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 02:58:40.977433: step 156040, loss = 1.97 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:42.139174: step 156050, loss = 1.96 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:58:43.331829: step 156060, loss = 1.86 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 02:58:44.524681: step 156070, loss = 1.79 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 02:58:45.701069: step 156080, loss = 1.98 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:46.898813: step 156090, loss = 1.81 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:58:48.080002: step 156100, loss = 1.81 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:49.279974: step 156110, loss = 1.78 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 02:58:50.466690: step 156120, loss = 1.93 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 02:58:51.667455: step 156130, loss = 1.94 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 02:58:52.846676: step 15E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1607 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
6140, loss = 1.84 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:54.020696: step 156150, loss = 1.95 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:58:55.198655: step 156160, loss = 1.82 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:58:56.354593: step 156170, loss = 1.69 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:58:57.506988: step 156180, loss = 1.84 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:58:58.657345: step 156190, loss = 2.01 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 02:58:59.816176: step 156200, loss = 1.96 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:00.974847: step 156210, loss = 1.90 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:02.142395: step 156220, loss = 2.05 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:03.304679: step 156230, loss = 1.89 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:04.492161: step 156240, loss = 1.89 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 02:59:05.664403: step 156250, loss = 1.86 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:06.831564: step 156260, loss = 1.80 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:08.001762: step 156270, loss = 1.84 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:09.164897: step 156280, loss = 1.95 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:10.328167: step 156290, loss = 1.80 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:11.486785: step 156300, loss = 1.94 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:12.651771: step 156310, loss = 1.94 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:13.810567: step 156320, loss = 1.90 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:14.973472: step 156330, loss = 1.87 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:16.144092: step 156340, loss = 1.85 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:17.316607: step 156350, loss = 1.83 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:18.454262: step 156360, loss = 1.94 (1125.2 examples/sec; 0.114 sec/batch)
2017-05-05 02:59:19.642734: step 156370, loss = 1.84 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:59:20.813545: step 156380, loss = 1.86 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:21.978420: step 156390, loss = 1.91 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:23.153517: step 156400, loss = 1.95 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:59:24.327492: step 156410, loss = 1.93 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:25.590279: step 156420, loss = 1.79 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-05 02:59:26.639037: step 156430, loss = 1.82 (1220.5 examples/sec; 0.105 sec/batch)
2017-05-05 02:59:27.818994: step 156440, loss = 1.84 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 02:59:28.984422: step 156450, loss = 2.11 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:30.131430: step 156460, loss = 1.91 (1116.0 examples/sec; 0.115 sec/batch)
2017-05-05 02:59:31.288468: step 156470, loss = 1.79 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:32.475296: step 156480, loss = 1.90 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 02:59:33.634821: step 156490, loss = 1.77 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:34.783626: step 156500, loss = 1.93 (1114.2 examples/sec; 0.115 sec/batch)
2017-05-05 02:59:35.955839: step 156510, loss = 1.85 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:37.118410: step 156520, loss = 1.86 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:38.279660: step 156530, loss = 1.83 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:39.455085: step 156540, loss = 1.91 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 02:59:40.617540: step 156550, loss = 1.82 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:41.795610: step 156560, loss = 1.99 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 02:59:42.980435: step 156570, loss = 1.83 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 02:59:44.157364: step 156580, loss = 1.91 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 02:59:45.334811: step 156590, loss = 1.96 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 02:59:46.491594: step 156600, loss = 1.82 (1106.5 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:47.659989: step 156610, loss = 1.72 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:48.839331: step 156620, loss = 1.96 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 02:59:49.983197: step 156630, loss = 1.82 (1119.0 examples/sec; 0.114 sec/batch)
2017-05-05 02:59:51.152081: step 156640, loss = 1.96 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:52.316584: step 156650, loss = 1.98 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:53.482484: step 156660, loss = 1.90 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:54.669851: step 156670, loss = 1.84 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 02:59:55.841410: step 156680, loss = 1.98 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:56.998396: step 156690, loss = 1.89 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 02:59:58.164395: step 156700, loss = 1.92 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 02:59:59.326032: step 156710, loss = 1.98 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:00:00.498355: step 156720, loss = 1.81 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:00:01.674096: step 156730, loss = 2.04 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:00:02.851090: step 156740, loss = 1.87 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:00:04.022709: step 156750, loss = 1.75 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:00:05.208342: step 156760, loss = 1.93 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:06.381572: step 156770, loss = 1.90 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:00:07.568046: step 156780, loss = 1.80 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:08.757766: step 156790, loss = 1.95 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:09.944733: step 156800, loss = 1.88 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:11.133467: step 156810, loss = 1.83 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:12.333151: step 156820, loss = 1.87 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:13.526601: step 156830, loss = 1.88 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:14.747540: step 156840, loss = 1.84 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:00:15.961277: step 156850, loss = 1.94 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:17.168224: step 156860, loss = 1.78 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:18.358587: step 156870, loss = 1.95 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:19.541474: step 156880, loss = 1.93 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:00:20.739500: step 156890, loss = 1.77 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:21.919934: step 156900, loss = 1.82 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:00:23.115491: step 156910, loss = 1.92 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:24.315070: step 156920, loss = 1.91 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:25.503254: step 156930, loss = 1.88 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:26.695919: step 156940, loss = 1.83 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:27.904042: step 156950, loss = 1.92 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:29.126550: step 156960, loss = 1.67 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:00:30.327288: step 156970, loss = 1.88 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:31.529535: step 156980, loss = 1.76 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:32.774344: step 156990, loss = 1.96 (1028.3 examples/sec; 0.124 sec/batch)
2017-05-05 03:00:33.978776: step 157000, loss = 1.79 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:35.207141: step 157010, loss = 2.05 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:00:36.415792: step 157020, loss = 1.89 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:37.614290: step 157030E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1618 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
, loss = 1.77 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:38.827381: step 157040, loss = 1.95 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:40.041931: step 157050, loss = 2.04 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:41.238817: step 157060, loss = 1.91 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:42.452052: step 157070, loss = 1.88 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:43.661488: step 157080, loss = 1.76 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:44.874614: step 157090, loss = 1.78 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:46.082733: step 157100, loss = 2.07 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:47.294765: step 157110, loss = 1.82 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:48.507575: step 157120, loss = 1.92 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:00:49.702459: step 157130, loss = 1.91 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:50.904033: step 157140, loss = 2.07 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:52.101661: step 157150, loss = 2.05 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:53.293596: step 157160, loss = 1.87 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:54.479312: step 157170, loss = 1.78 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:55.675253: step 157180, loss = 1.98 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:00:56.863026: step 157190, loss = 1.84 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:00:58.043184: step 157200, loss = 1.75 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:00:59.217539: step 157210, loss = 1.93 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:00.403326: step 157220, loss = 1.76 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:01:01.549886: step 157230, loss = 1.98 (1116.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:01:02.706776: step 157240, loss = 1.84 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:03.873376: step 157250, loss = 1.88 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:05.035868: step 157260, loss = 1.80 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:06.200727: step 157270, loss = 1.85 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:07.373682: step 157280, loss = 1.85 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:08.548992: step 157290, loss = 1.84 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:09.710201: step 157300, loss = 1.78 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:10.873646: step 157310, loss = 1.74 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:12.044130: step 157320, loss = 1.98 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:13.201162: step 157330, loss = 1.71 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:14.365479: step 157340, loss = 1.88 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:15.538884: step 157350, loss = 1.90 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:16.702633: step 157360, loss = 1.86 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:17.852991: step 157370, loss = 1.77 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:01:19.038427: step 157380, loss = 1.95 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:01:20.212751: step 157390, loss = 1.85 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:21.386598: step 157400, loss = 1.93 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:22.641532: step 157410, loss = 2.00 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-05 03:01:23.714502: step 157420, loss = 1.99 (1193.0 examples/sec; 0.107 sec/batch)
2017-05-05 03:01:24.884798: step 157430, loss = 1.79 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:26.047626: step 157440, loss = 1.80 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:27.236660: step 157450, loss = 2.01 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:01:28.404476: step 157460, loss = 1.88 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:29.562478: step 157470, loss = 1.88 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:30.719834: step 157480, loss = 1.87 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:31.862120: step 157490, loss = 1.85 (1120.6 examples/sec; 0.114 sec/batch)
2017-05-05 03:01:33.032477: step 157500, loss = 1.91 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:34.216680: step 157510, loss = 1.91 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:35.393521: step 157520, loss = 1.91 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:36.558072: step 157530, loss = 1.85 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:37.718726: step 157540, loss = 1.82 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:38.893394: step 157550, loss = 1.87 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:40.060396: step 157560, loss = 1.88 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:41.228823: step 157570, loss = 1.80 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:42.386470: step 157580, loss = 1.77 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:43.565514: step 157590, loss = 1.90 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:44.731354: step 157600, loss = 1.76 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:45.895273: step 157610, loss = 1.82 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:47.049448: step 157620, loss = 1.99 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:01:48.220898: step 157630, loss = 1.81 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:01:49.381645: step 157640, loss = 1.75 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:50.537074: step 157650, loss = 2.00 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:51.684412: step 157660, loss = 1.79 (1115.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:01:52.887952: step 157670, loss = 1.75 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:01:54.051326: step 157680, loss = 1.94 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:01:55.233811: step 157690, loss = 1.85 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:56.417386: step 157700, loss = 1.82 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:57.600965: step 157710, loss = 1.78 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:58.778320: step 157720, loss = 2.09 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:01:59.961672: step 157730, loss = 1.93 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:02:01.148465: step 157740, loss = 2.00 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:02.331364: step 157750, loss = 1.80 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:02:03.526084: step 157760, loss = 1.95 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:04.719892: step 157770, loss = 1.92 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:05.924807: step 157780, loss = 1.88 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:02:07.146201: step 157790, loss = 1.89 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:02:08.341731: step 157800, loss = 1.93 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:02:09.551819: step 157810, loss = 1.87 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:02:10.747294: step 157820, loss = 1.87 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:02:11.961679: step 157830, loss = 1.92 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:02:13.151888: step 157840, loss = 1.92 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:14.345785: step 157850, loss = 1.96 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:15.550122: step 157860, loss = 1.96 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:02:16.764995: step 157870, loss = 1.79 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:02:17.980277: step 157880, loss = 1.82 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:02:19.187697: step 157890, loss = 1.84 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:02:20.387406: step 157900, loss = 2.00 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:02:21.573334: step 157910, loss = 1.70 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:22.765730: step 157920, loss = 1.73 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:23.975038: step 157930, loss = 1.91 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:02:25.191098: step 157940, loss = 1.84 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:02:26.342208: step 157950, loss = 1.97 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:02:27.533214: step 157960, loss = 1.94 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:28.716027: step 157970, loss = 1.81 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:02:29.879445: step 157980, loss = 1.91 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:02:31.043469: step 157990, loss = 2.02 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:02:32.206150: step 158000, loss = 1.78 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:02:33.361968: step 158010, loss = 1.76 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:02:34.494620: step 158020, loss = 1.92 (1130.1 examples/sec; 0.113 sec/batch)
2017-05-05 03:02:35.668570: step 158030, loss = 1.75 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:02:36.826095: step 158040, loss = 1.83 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:02:37.980323: step 158050, loss = 1.84 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:02:39.162710: step 158060, loss = 1.94 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:02:40.325542: step 158070, loss = 1.88 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:02:41.475710: step 158080, loss = 1.77 (1112.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:02:42.629115: step 158090, loss = 1.85 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:02:43.792575: step 158100, loss = 1.76 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:02:44.979578: step 158110, loss = 1.69 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:02:46.118423: step 158120, loss = 1.79 (1123.9 examples/sec; 0.114 sec/batch)
2017-05-05 03:02:47.301080: step 158130, loss = 1.82 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:02:48.474737: step 158140, loss = 1.90 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:02:49.636740: step 158150, loss = 2.00 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:02:50.802467: step 158160, loss = 1.90 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:02:51.980547: step 158170, loss = 2.02 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:02:53.182077: step 158180, loss = 1.68 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:02:54.348208: step 158190, loss = 1.79 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:02:55.520126: step 158200, loss = 1.85 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:02:56.690430: step 158210, loss = 1.78 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:02:57.845963: step 158220, loss = 1.91 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:02:58.997383: step 158230, loss = 1.78 (1111.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:03:00.174097: step 158240, loss = 1.93 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:01.331184: step 158250, loss = 1.79 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:03:02.514595: step 158260, loss = 1.78 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:03.692889: step 158270, loss = 1.86 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:04.866890: step 158280, loss = 1.86 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:03:06.026741: step 158290, loss = 1.90 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:03:07.199372: step 158300, loss = 2.03 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:03:08.360012: step 158310, loss = 1.95 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:03:09.521605: step 158320, loss = 1.99 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:03:10.685860: step 158330, loss = 1.84 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:03:11.850292: step 158340, loss = 1.88 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:03:13.017298: step 158350, loss = 1.85 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:03:14.178479: step 158360, loss = 1.83 (1102.3 examples/sec; 0.116 sec/batch)E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1630 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU

2017-05-05 03:03:15.363603: step 158370, loss = 1.84 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:16.525532: step 158380, loss = 2.00 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:03:17.688900: step 158390, loss = 1.82 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:03:18.958298: step 158400, loss = 1.83 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-05 03:03:20.027492: step 158410, loss = 1.76 (1197.2 examples/sec; 0.107 sec/batch)
2017-05-05 03:03:21.190992: step 158420, loss = 1.91 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:03:22.363585: step 158430, loss = 1.84 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:03:23.541318: step 158440, loss = 1.95 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:24.726518: step 158450, loss = 1.89 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:25.878940: step 158460, loss = 1.86 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:03:27.074058: step 158470, loss = 1.95 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:03:28.261292: step 158480, loss = 1.91 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:29.474573: step 158490, loss = 1.91 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:03:30.650001: step 158500, loss = 1.80 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:31.847167: step 158510, loss = 1.96 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:03:33.027498: step 158520, loss = 1.88 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:34.224062: step 158530, loss = 1.88 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:03:35.423966: step 158540, loss = 1.84 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:03:36.613491: step 158550, loss = 1.92 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:37.813478: step 158560, loss = 1.87 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:03:39.019334: step 158570, loss = 1.86 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:03:40.198703: step 158580, loss = 2.00 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:41.395824: step 158590, loss = 1.81 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:03:42.570519: step 158600, loss = 1.88 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:03:43.744579: step 158610, loss = 1.76 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:03:44.930780: step 158620, loss = 1.85 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:46.124206: step 158630, loss = 1.86 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:47.315252: step 158640, loss = 2.07 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:48.517490: step 158650, loss = 1.93 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:03:49.710381: step 158660, loss = 1.92 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:50.912012: step 158670, loss = 1.76 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:03:52.095641: step 158680, loss = 2.01 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:53.289358: step 158690, loss = 1.92 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:54.476667: step 158700, loss = 2.01 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:55.671380: step 158710, loss = 1.90 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:03:56.855025: step 158720, loss = 1.84 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:03:58.025086: step 158730, loss = 1.97 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:03:59.204610: step 158740, loss = 1.68 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:00.385930: step 158750, loss = 1.75 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:01.537206: step 158760, loss = 1.96 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:04:02.728151: step 158770, loss = 1.85 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:04:03.895446: step 158780, loss = 2.00 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:05.067396: step 158790, loss = 1.95 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:06.237675: step 158800, loss = 1.90 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:07.409128: step 158810, loss = 1.78 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:08.598823: step 158820, loss = 2.02 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:04:09.778660: step 158830, loss = 1.89 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:10.961040: step 158840, loss = 1.89 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:12.142173: step 158850, loss = 1.99 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:13.316296: step 158860, loss = 2.01 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:14.479604: step 158870, loss = 1.85 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:15.666886: step 158880, loss = 1.95 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:04:16.828755: step 158890, loss = 1.78 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:17.986542: step 158900, loss = 1.96 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:19.160597: step 158910, loss = 1.91 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:20.324649: step 158920, loss = 1.81 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:21.502093: step 158930, loss = 1.75 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:22.678383: step 158940, loss = 1.72 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:23.846969: step 158950, loss = 1.77 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:25.002885: step 158960, loss = 1.79 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:26.147574: step 158970, loss = 1.91 (1118.2 examples/sec; 0.114 sec/batch)
2017-05-05 03:04:27.320548: step 158980, loss = 1.78 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:28.472503: step 158990, loss = 1.80 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:04:29.621055: step 159000, loss = 1.71 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:04:30.801625: step 159010, loss = 1.87 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:31.974959: step 159020, loss = 1.92 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:33.161920: step 159030, loss = 2.04 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:04:34.302436: step 159040, loss = 1.84 (1122.3 examples/sec; 0.114 sec/batch)
2017-05-05 03:04:35.455734: step 159050, loss = 1.86 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:04:36.615133: step 159060, loss = 1.79 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:37.774104: step 159070, loss = 1.78 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:38.941265: step 159080, loss = 1.87 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:40.094869: step 159090, loss = 2.04 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:04:41.259658: step 159100, loss = 1.86 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:42.414164: step 159110, loss = 1.77 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:04:43.575878: step 159120, loss = 1.97 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:44.749428: step 159130, loss = 1.88 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:45.894292: step 159140, loss = 1.91 (1118.0 examples/sec; 0.114 sec/batch)
2017-05-05 03:04:47.076926: step 159150, loss = 1.76 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:48.257084: step 159160, loss = 1.84 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:49.428770: step 159170, loss = 1.84 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:50.587752: step 159180, loss = 1.88 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:51.741765: step 159190, loss = 1.82 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:04:52.897501: step 159200, loss = 1.99 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:04:54.050332: step 159210, loss = 1.88 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:04:55.222595: step 159220, loss = 1.84 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:56.397968: step 159230, loss = 1.98 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:04:57.564167: step 159240, loss = 1.98 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:04:58.753378: step 159250, loss = 1.78 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:04:59.908010: step 159260, loss = 1.69 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:05:01.075326: step 159270, loss = 1.95 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:02.241568: step 159280, loss = 1.79 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:03.412980: step 159290, loss = 1.80 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:04.569086: step 159300, loss = 1.86 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:05.740323: step 159310, loss = 1.78 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:06.898181: step 159320, loss = 2.00 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:08.075201: step 159330, loss = 1.82 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:05:09.231010: step 159340, loss = 1.83 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:10.403014: step 159350, loss = 1.84 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:11.561089: step 159360, loss = 1.75 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:12.720098: step 159370, loss = 1.88 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:13.896020: step 159380, loss = 1.85 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:05:15.165152: step 159390, loss = 1.79 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-05 03:05:16.258934: step 159400, loss = 1.89 (1170.2 examples/sec; 0.109 sec/batch)
2017-05-05 03:05:17.428162: step 159410, loss = 1.87 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:18.599456: step 159420, loss = 1.91 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:19.792868: step 159430, loss = 1.90 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:05:20.972400: step 159440, loss = 1.93 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:05:22.146778: step 159450, loss = 1.86 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:23.328463: step 159460, loss = 1.82 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:05:24.496261: step 159470, loss = 1.89 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:25.663479: step 159480, loss = 1.92 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:26.848084: step 159490, loss = 1.77 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:05:28.014914: step 159500, loss = 1.74 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:29.189157: step 159510, loss = 1.82 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:30.366407: step 159520, loss = 1.81 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:05:31.530682: step 159530, loss = 1.77 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:32.694739: step 159540, loss = 1.94 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:33.852671: step 159550, loss = 1.83 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:35.011628: step 159560, loss = 1.84 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:36.181339: step 159570, loss = 1.84 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:37.365717: step 159580, loss = 1.89 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:05:38.523691: step 159590, loss = 2.06 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:39.685224: step 159600, loss = 1.83 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:40.854630: step 159610, loss = 1.94 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:05:42.016874: step 159620, loss = 1.79 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:43.174999: step 159630, loss = 1.92 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:05:44.357206: step 159640, loss = 1.80 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:05:45.535600: step 159650, loss = 1.91 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:05:46.731130: step 159660, loss = 2.06 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:05:47.936488: step 159670, loss = 1.94 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:05:49.144922: step 159680, loss = 1.85 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:05:50.348648: step 159690, loss = 1.78 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:05:51.564018: step 159700, loss = 1.8E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1642 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
2 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:05:52.780121: step 159710, loss = 1.97 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:05:53.999070: step 159720, loss = 1.80 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:05:55.222691: step 159730, loss = 1.92 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:05:56.450285: step 159740, loss = 1.83 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-05 03:05:57.672722: step 159750, loss = 1.94 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:05:58.870759: step 159760, loss = 1.77 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:00.114317: step 159770, loss = 1.93 (1029.3 examples/sec; 0.124 sec/batch)
2017-05-05 03:06:01.287722: step 159780, loss = 1.84 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:06:02.504992: step 159790, loss = 1.82 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:06:03.741509: step 159800, loss = 1.84 (1035.2 examples/sec; 0.124 sec/batch)
2017-05-05 03:06:04.961965: step 159810, loss = 1.85 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:06:06.163135: step 159820, loss = 1.71 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:07.376869: step 159830, loss = 1.90 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:06:08.573359: step 159840, loss = 1.81 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:09.779257: step 159850, loss = 1.81 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:06:10.971604: step 159860, loss = 1.97 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:06:12.173509: step 159870, loss = 1.93 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:13.362929: step 159880, loss = 1.79 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:06:14.559819: step 159890, loss = 1.90 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:15.764050: step 159900, loss = 1.88 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:16.964273: step 159910, loss = 1.80 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:18.155912: step 159920, loss = 1.90 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:06:19.376401: step 159930, loss = 1.93 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:06:20.590476: step 159940, loss = 1.84 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:06:21.811427: step 159950, loss = 1.85 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:06:23.037476: step 159960, loss = 1.82 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:06:24.252969: step 159970, loss = 1.86 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:06:25.472643: step 159980, loss = 1.91 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:06:26.669337: step 159990, loss = 1.89 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:06:27.857870: step 160000, loss = 1.81 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:06:29.039837: step 160010, loss = 1.95 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:06:30.227525: step 160020, loss = 1.78 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:06:31.394293: step 160030, loss = 2.14 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:06:32.583353: step 160040, loss = 1.82 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:06:33.738226: step 160050, loss = 1.88 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:06:34.906497: step 160060, loss = 2.10 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:06:36.068770: step 160070, loss = 1.89 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:06:37.250973: step 160080, loss = 1.80 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:06:38.417510: step 160090, loss = 1.99 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:06:39.589414: step 160100, loss = 1.95 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:06:40.748592: step 160110, loss = 1.99 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:06:41.907285: step 160120, loss = 2.03 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:06:43.049769: step 160130, loss = 1.88 (1120.4 examples/sec; 0.114 sec/batch)
2017-05-05 03:06:44.235141: step 160140, loss = 1.84 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:06:45.385051: step 160150, loss = 1.98 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:06:46.561349: step 160160, loss = 2.02 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:06:47.733682: step 160170, loss = 1.93 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:06:48.914579: step 160180, loss = 1.92 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:06:50.090357: step 160190, loss = 1.76 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:06:51.245862: step 160200, loss = 2.02 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:06:52.417825: step 160210, loss = 1.90 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:06:53.573536: step 160220, loss = 1.85 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:06:54.745998: step 160230, loss = 1.87 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:06:55.908144: step 160240, loss = 1.87 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:06:57.083019: step 160250, loss = 1.74 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:06:58.239200: step 160260, loss = 1.89 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:06:59.419845: step 160270, loss = 1.90 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:00.586542: step 160280, loss = 1.97 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:01.751310: step 160290, loss = 1.83 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:02.925003: step 160300, loss = 1.93 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:04.090092: step 160310, loss = 1.91 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:05.263205: step 160320, loss = 1.91 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:06.432415: step 160330, loss = 1.74 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:07.598834: step 160340, loss = 1.87 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:08.790857: step 160350, loss = 1.88 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:07:09.955188: step 160360, loss = 1.85 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:11.140416: step 160370, loss = 1.97 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:07:12.392551: step 160380, loss = 2.00 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-05 03:07:13.454337: step 160390, loss = 1.82 (1205.5 examples/sec; 0.106 sec/batch)
2017-05-05 03:07:14.611682: step 160400, loss = 1.87 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:15.793700: step 160410, loss = 1.86 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:16.951120: step 160420, loss = 1.96 (1105.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:18.096569: step 160430, loss = 1.89 (1117.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:07:19.261327: step 160440, loss = 2.05 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:20.432899: step 160450, loss = 1.88 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:21.598505: step 160460, loss = 1.82 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:22.775497: step 160470, loss = 1.79 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:23.939504: step 160480, loss = 1.84 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:25.124218: step 160490, loss = 2.01 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:26.273215: step 160500, loss = 1.77 (1114.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:07:27.441471: step 160510, loss = 1.93 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:28.603871: step 160520, loss = 1.97 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:29.778692: step 160530, loss = 1.90 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:30.957686: step 160540, loss = 1.81 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:32.118266: step 160550, loss = 1.77 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:33.319345: step 160560, loss = 1.90 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:07:34.463566: step 160570, loss = 1.97 (1118.7 examples/sec; 0.114 sec/batch)
2017-05-05 03:07:35.638705: step 160580, loss = 1.90 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:36.802752: step 160590, loss = 2.03 (1E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1654 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
099.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:37.964164: step 160600, loss = 1.82 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:39.148979: step 160610, loss = 1.75 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:40.314874: step 160620, loss = 1.80 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:41.464112: step 160630, loss = 1.95 (1113.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:07:42.622735: step 160640, loss = 1.79 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:43.792885: step 160650, loss = 1.88 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:44.946576: step 160660, loss = 1.90 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:07:46.114663: step 160670, loss = 1.96 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:47.278708: step 160680, loss = 1.72 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:07:48.451367: step 160690, loss = 1.86 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:49.621972: step 160700, loss = 1.96 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:50.773197: step 160710, loss = 1.75 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:07:51.958852: step 160720, loss = 1.86 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:07:53.132167: step 160730, loss = 1.94 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:54.304834: step 160740, loss = 1.85 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:07:55.489182: step 160750, loss = 1.80 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:56.683952: step 160760, loss = 1.84 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:07:57.868046: step 160770, loss = 1.96 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:07:59.046830: step 160780, loss = 1.77 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:08:00.220461: step 160790, loss = 1.87 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:08:01.416824: step 160800, loss = 1.81 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:08:02.610212: step 160810, loss = 1.86 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:08:03.798164: step 160820, loss = 1.87 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:08:04.988788: step 160830, loss = 1.78 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:08:06.184091: step 160840, loss = 2.06 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:08:07.374171: step 160850, loss = 1.89 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:08:08.564294: step 160860, loss = 1.86 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:08:09.749997: step 160870, loss = 1.81 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:08:10.949764: step 160880, loss = 1.84 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:08:12.147915: step 160890, loss = 1.87 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:08:13.367646: step 160900, loss = 1.78 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:14.589571: step 160910, loss = 1.83 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:15.795996: step 160920, loss = 1.87 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:17.004771: step 160930, loss = 1.83 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:18.214645: step 160940, loss = 1.99 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:19.453216: step 160950, loss = 1.88 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-05 03:08:20.674910: step 160960, loss = 1.93 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:21.882400: step 160970, loss = 1.82 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:23.076074: step 160980, loss = 1.83 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:08:24.284582: step 160990, loss = 1.80 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:25.492166: step 161000, loss = 1.77 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:26.717533: step 161010, loss = 1.83 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:08:27.936300: step 161020, loss = 1.92 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:29.148858: step 161030, loss = 2.11 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:30.362017: step 161040, loss = 1.91 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:31.574140: step 161050, loss = 1.69 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:32.794410: step 161060, loss = 1.73 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:34.024970: step 161070, loss = 1.81 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-05 03:08:35.224218: step 161080, loss = 1.82 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:08:36.470205: step 161090, loss = 1.70 (1027.3 examples/sec; 0.125 sec/batch)
2017-05-05 03:08:37.677359: step 161100, loss = 2.02 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:38.903568: step 161110, loss = 1.97 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:08:40.126086: step 161120, loss = 1.91 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:41.348526: step 161130, loss = 1.91 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:42.583374: step 161140, loss = 1.78 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:08:43.812952: step 161150, loss = 1.93 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:08:45.029795: step 161160, loss = 1.99 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:46.244153: step 161170, loss = 1.90 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:08:47.446035: step 161180, loss = 1.87 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:08:48.661744: step 161190, loss = 1.85 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:49.884920: step 161200, loss = 1.89 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:51.106729: step 161210, loss = 1.85 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:52.323955: step 161220, loss = 1.77 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:53.556988: step 161230, loss = 1.96 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-05 03:08:54.777699: step 161240, loss = 1.90 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:55.996657: step 161250, loss = 1.88 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:57.227689: step 161260, loss = 2.00 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-05 03:08:58.445382: step 161270, loss = 1.98 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:08:59.653538: step 161280, loss = 1.80 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:00.873321: step 161290, loss = 1.85 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:02.093793: step 161300, loss = 1.82 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:03.318611: step 161310, loss = 1.92 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:04.528581: step 161320, loss = 1.94 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:05.734761: step 161330, loss = 1.90 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:06.950970: step 161340, loss = 1.82 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:08.173663: step 161350, loss = 1.75 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:09.400139: step 161360, loss = 1.94 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:09:10.731077: step 161370, loss = 1.85 (961.7 examples/sec; 0.133 sec/batch)
2017-05-05 03:09:11.803704: step 161380, loss = 2.11 (1193.3 examples/sec; 0.107 sec/batch)
2017-05-05 03:09:13.015201: step 161390, loss = 1.86 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:14.224060: step 161400, loss = 1.88 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:15.422821: step 161410, loss = 1.91 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:09:16.618149: step 161420, loss = 2.01 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:09:17.832492: step 161430, loss = 1.92 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:19.059622: step 161440, loss = 1.81 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 03:09:20.283054: step 161450, loss = 1.86 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:21.500144: step 161460, loss = 1.86 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:22.718506: step 161470, loss = 1.87 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:23.932365: step 161480, loss = 1.83 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:25.150548: step 161490, loss = 2.01 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:26.332533: step 161500, loss = 1.82 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:09:27.530675: step 161510, loss = 1.82 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:09:28.713615: step 161520, loss = 2.04 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:09:29.888971: step 161530, loss = 1.81 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:09:31.078786: step 161540, loss = 1.93 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:09:32.258340: step 161550, loss = 1.92 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:09:33.451739: step 161560, loss = 1.83 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:09:34.645188: step 161570, loss = 2.03 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:09:35.880629: step 161580, loss = 1.86 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-05 03:09:37.113127: step 161590, loss = 2.00 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:09:38.332147: step 161600, loss = 1.80 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:39.565892: step 161610, loss = 2.02 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:09:40.790202: step 161620, loss = 1.89 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:42.012592: step 161630, loss = 1.83 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:43.246672: step 161640, loss = 1.73 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-05 03:09:44.456522: step 161650, loss = 1.84 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:45.676631: step 161660, loss = 2.00 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:46.911922: step 161670, loss = 1.92 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-05 03:09:48.129575: step 161680, loss = 1.93 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:49.357302: step 161690, loss = 1.74 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:09:50.567963: step 161700, loss = 1.80 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:51.776753: step 161710, loss = 1.86 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:52.995833: step 161720, loss = 1.82 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:09:54.203658: step 161730, loss = 2.00 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:55.404484: step 161740, loss = 1.76 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:09:56.629831: step 161750, loss = 1.87 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:09:57.839113: step 161760, loss = 1.80 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:09:59.028816: step 161770, loss = 1.93 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:00.233375: step 161780, loss = 1.76 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:10:01.458887: step 161790, loss = 1.90 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:10:02.676109: step 161800, loss = 1.96 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:10:03.878310: step 161810, loss = 1.86 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:10:05.098781: step 161820, loss = 1.81 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:10:06.321647: step 161830, loss = 1.94 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:10:07.547954: step 161840, loss = 1.84 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 03:10:08.766451: step 161850, loss = 1.75 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:10:09.981974: step 161860, loss = 1.97 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:10:11.202707: step 161870, loss = 1.77 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:10:12.423966: step 161880, loss = 2.04 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:10:13.642209: step 161890, loss = 1.87 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:10:14.862643: step 161900, loss = 1.75 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:10:16.086613: step 161910, loss = 1.79 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:10:17.325398: step 161920, loss = 2.04 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-05 03:10E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1665 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
:18.516940: step 161930, loss = 1.86 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:19.738363: step 161940, loss = 1.84 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:10:20.977367: step 161950, loss = 1.85 (1033.1 examples/sec; 0.124 sec/batch)
2017-05-05 03:10:22.194788: step 161960, loss = 1.88 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:10:23.374993: step 161970, loss = 1.86 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:10:24.565505: step 161980, loss = 1.80 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:25.766998: step 161990, loss = 1.80 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:10:26.960864: step 162000, loss = 1.85 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:28.148396: step 162010, loss = 1.97 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:29.337080: step 162020, loss = 1.90 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:30.493166: step 162030, loss = 1.86 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:10:31.684210: step 162040, loss = 1.89 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:32.890409: step 162050, loss = 1.80 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:10:34.065214: step 162060, loss = 2.03 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:10:35.264177: step 162070, loss = 1.89 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:10:36.462451: step 162080, loss = 1.96 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:10:37.647638: step 162090, loss = 1.93 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:38.837762: step 162100, loss = 1.94 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:40.028136: step 162110, loss = 1.77 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:41.228344: step 162120, loss = 1.94 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:10:42.403291: step 162130, loss = 1.88 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:10:43.583295: step 162140, loss = 2.00 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:10:44.767138: step 162150, loss = 1.80 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:10:45.959324: step 162160, loss = 1.88 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:47.146152: step 162170, loss = 1.87 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:48.348264: step 162180, loss = 1.81 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:10:49.517028: step 162190, loss = 1.80 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:10:50.716822: step 162200, loss = 2.01 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:10:51.891528: step 162210, loss = 1.82 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:10:53.077974: step 162220, loss = 1.94 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:10:54.239533: step 162230, loss = 1.71 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:10:55.416566: step 162240, loss = 1.81 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:10:56.591588: step 162250, loss = 1.82 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:10:57.751846: step 162260, loss = 1.90 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:10:58.935737: step 162270, loss = 1.83 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:00.093579: step 162280, loss = 1.87 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:11:01.269016: step 162290, loss = 1.82 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:02.442737: step 162300, loss = 1.89 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:03.608161: step 162310, loss = 1.94 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:04.784676: step 162320, loss = 1.93 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:05.941615: step 162330, loss = 1.86 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:11:07.124020: step 162340, loss = 1.85 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:08.299755: step 162350, loss = 1.83 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:09.589359: step 162360, loss = 1.85 (992.6 examples/sec; 0.129 sec/batch)
2017-05-05 03:11:10.651672: step 162370, loss = 1.97 (1204.9 examples/sec; 0.106 sec/batch)
2017-05-05 03:11:11.826973: step 162380, loss = 1.88 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:12.982737: step 162390, loss = 1.80 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:11:14.137292: step 162400, loss = 1.89 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:11:15.290621: step 162410, loss = 1.73 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:11:16.449468: step 162420, loss = 1.97 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:11:17.624964: step 162430, loss = 1.88 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:18.799516: step 162440, loss = 1.98 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:19.971390: step 162450, loss = 1.90 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:21.142687: step 162460, loss = 1.90 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:22.307698: step 162470, loss = 1.65 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:23.468317: step 162480, loss = 1.83 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:11:24.636280: step 162490, loss = 1.78 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:25.770710: step 162500, loss = 1.66 (1128.3 examples/sec; 0.113 sec/batch)
2017-05-05 03:11:26.926526: step 162510, loss = 1.88 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:11:28.074284: step 162520, loss = 1.99 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:11:29.237579: step 162530, loss = 1.88 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:11:30.386023: step 162540, loss = 1.94 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:11:31.560603: step 162550, loss = 1.84 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:32.748886: step 162560, loss = 1.87 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:11:33.890754: step 162570, loss = 1.82 (1121.0 examples/sec; 0.114 sec/batch)
2017-05-05 03:11:35.054547: step 162580, loss = 1.84 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:11:36.242504: step 162590, loss = 1.85 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:11:37.424289: step 162600, loss = 1.78 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:38.612315: step 162610, loss = 1.92 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:11:39.785810: step 162620, loss = 1.90 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:40.963043: step 162630, loss = 1.86 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:42.146994: step 162640, loss = 1.99 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:43.326196: step 162650, loss = 1.88 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:44.523709: step 162660, loss = 1.95 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:11:45.695545: step 162670, loss = 1.84 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:11:46.877093: step 162680, loss = 1.89 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:48.068049: step 162690, loss = 1.96 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:11:49.265439: step 162700, loss = 1.99 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:11:50.465004: step 162710, loss = 1.89 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:11:51.662001: step 162720, loss = 2.04 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:11:52.883273: step 162730, loss = 1.92 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:11:54.071668: step 162740, loss = 1.77 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:11:55.259527: step 162750, loss = 2.00 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:11:56.446564: step 162760, loss = 1.91 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:11:57.631086: step 162770, loss = 2.00 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:11:58.837202: step 162780, loss = 1.72 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:12:00.044093: step 162790, loss = 1.89 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:12:01.257885: step 162800, loss = 1.86 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:12:02.437749: step 162810, loss = 1.99 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:03.6E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1676 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
48509: step 162820, loss = 1.94 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:12:04.846169: step 162830, loss = 1.84 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:06.033450: step 162840, loss = 1.86 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:12:07.227469: step 162850, loss = 1.93 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:12:08.429509: step 162860, loss = 1.90 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:09.618121: step 162870, loss = 1.84 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:12:10.787527: step 162880, loss = 1.89 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:11.967925: step 162890, loss = 1.89 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:13.146038: step 162900, loss = 1.76 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:14.323810: step 162910, loss = 1.79 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:15.517819: step 162920, loss = 1.96 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:12:16.688348: step 162930, loss = 1.88 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:17.872674: step 162940, loss = 1.85 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:19.066210: step 162950, loss = 2.01 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:12:20.250858: step 162960, loss = 1.84 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:21.464728: step 162970, loss = 2.06 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:12:22.672528: step 162980, loss = 1.87 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:12:23.905413: step 162990, loss = 1.85 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-05 03:12:25.108969: step 163000, loss = 1.80 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:26.318148: step 163010, loss = 1.83 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:12:27.501891: step 163020, loss = 1.83 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:28.710915: step 163030, loss = 1.96 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:12:29.910712: step 163040, loss = 1.84 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:31.114479: step 163050, loss = 1.97 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:32.334958: step 163060, loss = 1.75 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:12:33.540789: step 163070, loss = 1.82 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:12:34.728209: step 163080, loss = 1.78 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:12:35.925343: step 163090, loss = 1.93 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:37.143221: step 163100, loss = 1.99 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:12:38.311744: step 163110, loss = 2.00 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:39.505772: step 163120, loss = 2.04 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:12:40.689175: step 163130, loss = 1.84 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:41.863739: step 163140, loss = 1.93 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:43.050416: step 163150, loss = 1.90 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:12:44.225367: step 163160, loss = 1.78 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:45.390730: step 163170, loss = 1.90 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:46.546731: step 163180, loss = 2.03 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:12:47.717678: step 163190, loss = 1.79 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:48.899288: step 163200, loss = 1.96 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:50.059224: step 163210, loss = 1.96 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:12:51.230527: step 163220, loss = 1.97 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:52.406528: step 163230, loss = 2.00 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:12:53.575098: step 163240, loss = 1.91 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:54.749743: step 163250, loss = 1.89 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:55.911360: step 163260, loss = 1.98 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:12:57.108622: step 163270, loss = 1.69 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:12:58.273761: step 163280, loss = 1.97 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:12:59.450089: step 163290, loss = 1.93 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:00.618372: step 163300, loss = 1.89 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:01.769473: step 163310, loss = 2.05 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:13:02.933963: step 163320, loss = 1.90 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:04.101067: step 163330, loss = 1.80 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:05.279355: step 163340, loss = 1.79 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:06.545291: step 163350, loss = 1.88 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-05 03:13:07.630019: step 163360, loss = 1.69 (1180.0 examples/sec; 0.108 sec/batch)
2017-05-05 03:13:08.793852: step 163370, loss = 1.86 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:09.960679: step 163380, loss = 1.83 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:11.154559: step 163390, loss = 1.78 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:13:12.312457: step 163400, loss = 1.92 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:13.494445: step 163410, loss = 1.87 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:14.665481: step 163420, loss = 1.86 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:15.838414: step 163430, loss = 1.91 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:17.001307: step 163440, loss = 1.91 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:18.158625: step 163450, loss = 1.99 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:19.324656: step 163460, loss = 1.80 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:20.504564: step 163470, loss = 1.89 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:21.645308: step 163480, loss = 1.82 (1122.1 examples/sec; 0.114 sec/batch)
2017-05-05 03:13:22.831358: step 163490, loss = 1.86 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:13:23.997863: step 163500, loss = 1.93 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:25.188489: step 163510, loss = 1.90 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:13:26.346580: step 163520, loss = 1.85 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:27.514087: step 163530, loss = 1.87 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:28.690785: step 163540, loss = 1.84 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:29.854821: step 163550, loss = 1.82 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:31.014112: step 163560, loss = 2.05 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:32.197755: step 163570, loss = 1.95 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:33.339112: step 163580, loss = 1.76 (1121.5 examples/sec; 0.114 sec/batch)
2017-05-05 03:13:34.486632: step 163590, loss = 1.89 (1115.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:13:35.689005: step 163600, loss = 1.86 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:13:36.846286: step 163610, loss = 1.93 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:37.989199: step 163620, loss = 1.89 (1119.9 examples/sec; 0.114 sec/batch)
2017-05-05 03:13:39.168155: step 163630, loss = 1.98 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:40.331817: step 163640, loss = 1.96 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:41.499021: step 163650, loss = 2.08 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:42.673387: step 163660, loss = 1.97 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:43.857097: step 163670, loss = 1.94 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:45.036596: step 163680, loss = 1.84 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:46.205965: step 163690, loss = 1.69 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:47.349628: step 163700, loss = 1.75 (1119.2 examples/sec; 0.114 sec/batch)
2017-05-05 03:13:48.51966E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1687 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
8: step 163710, loss = 1.85 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:49.688987: step 163720, loss = 1.78 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:50.851597: step 163730, loss = 1.95 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:52.023828: step 163740, loss = 1.85 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:53.186584: step 163750, loss = 1.77 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:54.328863: step 163760, loss = 1.95 (1120.6 examples/sec; 0.114 sec/batch)
2017-05-05 03:13:55.512923: step 163770, loss = 1.71 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:13:56.681100: step 163780, loss = 1.90 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:13:57.839818: step 163790, loss = 2.05 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:13:59.009205: step 163800, loss = 1.73 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:00.179079: step 163810, loss = 1.87 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:01.355755: step 163820, loss = 1.91 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:02.509494: step 163830, loss = 1.88 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:14:03.687440: step 163840, loss = 1.85 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:04.857402: step 163850, loss = 1.88 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:06.027810: step 163860, loss = 2.01 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:07.189794: step 163870, loss = 1.91 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:08.349319: step 163880, loss = 1.95 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:09.506994: step 163890, loss = 2.00 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:10.676570: step 163900, loss = 1.77 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:11.860316: step 163910, loss = 1.92 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:13.036527: step 163920, loss = 1.78 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:14.195913: step 163930, loss = 1.80 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:15.355095: step 163940, loss = 2.17 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:16.533075: step 163950, loss = 1.86 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:17.710375: step 163960, loss = 1.81 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:18.867591: step 163970, loss = 1.87 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:20.029508: step 163980, loss = 1.90 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:21.191356: step 163990, loss = 1.89 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:22.345056: step 164000, loss = 1.93 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:14:23.520391: step 164010, loss = 1.95 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:24.693849: step 164020, loss = 1.97 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:25.866638: step 164030, loss = 1.83 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:27.064111: step 164040, loss = 1.77 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:14:28.276802: step 164050, loss = 1.83 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:14:29.465370: step 164060, loss = 1.80 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:14:30.633567: step 164070, loss = 1.86 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:31.814438: step 164080, loss = 1.83 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:32.983025: step 164090, loss = 1.91 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:34.150071: step 164100, loss = 1.82 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:35.327888: step 164110, loss = 1.90 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:36.526206: step 164120, loss = 1.90 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:14:37.675270: step 164130, loss = 1.90 (1114.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:14:38.845386: step 164140, loss = 1.93 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:40.046405: step 164150, loss = 1.88 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:14:41.212512: step 164160, loss = 1.93 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:42.386992: step 164170, loss = 1.84 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:43.551374: step 164180, loss = 1.74 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:44.714915: step 164190, loss = 1.91 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:45.879460: step 164200, loss = 1.93 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:47.048039: step 164210, loss = 1.84 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:48.214078: step 164220, loss = 1.81 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:49.377704: step 164230, loss = 1.87 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:50.558018: step 164240, loss = 1.95 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:51.731875: step 164250, loss = 1.84 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:52.921427: step 164260, loss = 1.86 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:14:54.090685: step 164270, loss = 1.92 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:14:55.265879: step 164280, loss = 2.00 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:14:56.451616: step 164290, loss = 2.00 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:14:57.612693: step 164300, loss = 1.80 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:14:58.800443: step 164310, loss = 1.92 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:14:59.979430: step 164320, loss = 1.87 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:15:01.129757: step 164330, loss = 1.89 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:15:02.401941: step 164340, loss = 1.94 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-05 03:15:03.477431: step 164350, loss = 2.02 (1190.2 examples/sec; 0.108 sec/batch)
2017-05-05 03:15:04.653613: step 164360, loss = 1.85 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:15:05.823108: step 164370, loss = 1.71 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:06.995199: step 164380, loss = 1.78 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:08.166740: step 164390, loss = 1.95 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:09.330450: step 164400, loss = 1.82 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:10.483594: step 164410, loss = 1.92 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:15:11.653381: step 164420, loss = 1.89 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:12.815350: step 164430, loss = 1.91 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:13.960607: step 164440, loss = 1.96 (1117.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:15:15.122180: step 164450, loss = 1.79 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:16.293585: step 164460, loss = 1.81 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:17.453617: step 164470, loss = 1.82 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:18.612727: step 164480, loss = 1.83 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:19.795061: step 164490, loss = 1.90 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:15:20.949449: step 164500, loss = 1.77 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:15:22.113542: step 164510, loss = 1.85 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:23.303266: step 164520, loss = 1.90 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:15:24.499465: step 164530, loss = 1.88 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:15:25.647845: step 164540, loss = 1.87 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:15:26.824453: step 164550, loss = 1.80 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:15:27.983420: step 164560, loss = 1.86 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:29.156141: step 164570, loss = 1.99 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:30.320468: step 164580, loss = 1.91 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:31.483022: step 164590, loss = 1.95 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:32.629758: step 164600, loss = 1.87 (1116.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:15:33.786701: step 164610, loss = 1.98 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:34.952516: step 164620, loss = 1.79 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:36.109003: step 164630, loss = 1.96 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:37.269871: step 164640, loss = 1.85 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:38.451818: step 164650, loss = 1.89 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:15:39.613164: step 164660, loss = 1.84 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:40.778040: step 164670, loss = 1.83 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:41.940917: step 164680, loss = 1.80 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:43.114090: step 164690, loss = 1.83 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:44.276281: step 164700, loss = 2.11 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:45.454855: step 164710, loss = 1.76 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:15:46.617407: step 164720, loss = 1.89 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:15:47.787541: step 164730, loss = 2.06 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:48.956402: step 164740, loss = 1.90 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:50.151554: step 164750, loss = 1.86 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:15:51.319637: step 164760, loss = 1.92 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:52.490556: step 164770, loss = 1.91 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:53.674453: step 164780, loss = 1.72 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:15:54.860405: step 164790, loss = 1.83 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:15:56.031620: step 164800, loss = 1.89 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:57.196893: step 164810, loss = 1.77 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:15:58.346763: step 164820, loss = 1.98 (1113.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:15:59.515867: step 164830, loss = 1.85 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:00.673156: step 164840, loss = 1.89 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:01.831204: step 164850, loss = 1.87 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:02.995706: step 164860, loss = 1.80 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:04.158826: step 164870, loss = 1.88 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:05.330687: step 164880, loss = 1.91 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:06.501989: step 164890, loss = 1.96 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:07.700844: step 164900, loss = 2.01 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:16:08.881584: step 164910, loss = 1.88 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:10.045170: step 164920, loss = 1.82 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:11.216267: step 164930, loss = 1.84 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:12.392444: step 164940, loss = 1.93 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:13.560812: step 164950, loss = 1.97 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:14.731876: step 164960, loss = 1.92 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:15.898678: step 164970, loss = 1.93 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:17.081610: step 164980, loss = 1.78 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:18.250206: step 164990, loss = 1.90 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:19.420360: step 165000, loss = 1.89 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:20.597803: step 165010, loss = 1.98 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:21.763083: step 165020, loss = 1.91 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:22.931478: step 165030, loss = 2.05 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:24.090724: step 165040, loss = 1.84 (1104.2 examples/sec; 0.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1698 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
116 sec/batch)
2017-05-05 03:16:25.277559: step 165050, loss = 1.76 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:16:26.437906: step 165060, loss = 1.80 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:27.595503: step 165070, loss = 1.96 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:28.766139: step 165080, loss = 1.92 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:29.933171: step 165090, loss = 1.75 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:31.098530: step 165100, loss = 1.79 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:32.255165: step 165110, loss = 1.79 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:33.420314: step 165120, loss = 1.90 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:34.578580: step 165130, loss = 1.92 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:35.737612: step 165140, loss = 1.74 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:36.907289: step 165150, loss = 1.81 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:38.068354: step 165160, loss = 1.87 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:39.234221: step 165170, loss = 1.89 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:40.411596: step 165180, loss = 1.87 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:41.559084: step 165190, loss = 1.86 (1115.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:16:42.707956: step 165200, loss = 1.82 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:16:43.884442: step 165210, loss = 1.89 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:45.060728: step 165220, loss = 1.90 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:46.228021: step 165230, loss = 1.74 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:47.404815: step 165240, loss = 1.88 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:48.563436: step 165250, loss = 1.90 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:49.712697: step 165260, loss = 1.92 (1113.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:16:50.882890: step 165270, loss = 1.87 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:52.046575: step 165280, loss = 1.99 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:16:53.221194: step 165290, loss = 1.92 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:16:54.363182: step 165300, loss = 2.01 (1120.9 examples/sec; 0.114 sec/batch)
2017-05-05 03:16:55.546009: step 165310, loss = 1.80 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:56.725527: step 165320, loss = 1.89 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:16:57.974436: step 165330, loss = 1.84 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-05 03:16:59.052169: step 165340, loss = 1.75 (1187.7 examples/sec; 0.108 sec/batch)
2017-05-05 03:17:00.228045: step 165350, loss = 1.88 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:17:01.394952: step 165360, loss = 1.91 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:02.557353: step 165370, loss = 1.89 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:17:03.725567: step 165380, loss = 1.88 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:04.894165: step 165390, loss = 1.84 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:06.046187: step 165400, loss = 1.79 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:17:07.217259: step 165410, loss = 1.89 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:08.418398: step 165420, loss = 1.89 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:17:09.577276: step 165430, loss = 1.94 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:17:10.738494: step 165440, loss = 1.80 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:17:11.933304: step 165450, loss = 1.77 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:17:13.117148: step 165460, loss = 1.85 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:17:14.278566: step 165470, loss = 1.91 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:17:15.452102: step 165480, loss = 1.93 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:16.605751: step 165490, loss = 2.00 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:17:17.761602: step 165500, loss = 1.90 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:17:18.917020: step 165510, loss = 1.74 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:17:20.094380: step 165520, loss = 1.84 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:17:21.272486: step 165530, loss = 1.78 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:17:22.441688: step 165540, loss = 1.98 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:23.612860: step 165550, loss = 1.83 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:24.775703: step 165560, loss = 1.88 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:17:25.946192: step 165570, loss = 1.91 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:27.114699: step 165580, loss = 1.83 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:28.284356: step 165590, loss = 1.86 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:29.451381: step 165600, loss = 1.96 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:30.605263: step 165610, loss = 1.97 (1109.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:17:31.792056: step 165620, loss = 2.07 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:17:32.954579: step 165630, loss = 1.74 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:17:34.121709: step 165640, loss = 1.90 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:17:35.297770: step 165650, loss = 1.88 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:17:36.480413: step 165660, loss = 1.70 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:17:37.643765: step 165670, loss = 1.97 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:17:38.831410: step 165680, loss = 2.07 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:17:40.037446: step 165690, loss = 1.87 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:17:41.233437: step 165700, loss = 1.85 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:17:42.442766: step 165710, loss = 1.96 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:17:43.652940: step 165720, loss = 1.71 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:17:44.869501: step 165730, loss = 1.95 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:17:46.078270: step 165740, loss = 1.88 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:17:47.312034: step 165750, loss = 1.76 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:17:48.521627: step 165760, loss = 1.65 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:17:49.723364: step 165770, loss = 1.78 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:17:50.944546: step 165780, loss = 2.00 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:17:52.175897: step 165790, loss = 1.88 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:17:53.382323: step 165800, loss = 1.96 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:17:54.607167: step 165810, loss = 1.78 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:17:55.817464: step 165820, loss = 2.03 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:17:57.027314: step 165830, loss = 1.80 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:17:58.237334: step 165840, loss = 1.82 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:17:59.450449: step 165850, loss = 1.83 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:00.667750: step 165860, loss = 1.66 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:01.881740: step 165870, loss = 1.94 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:03.103634: step 165880, loss = 1.95 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:04.319555: step 165890, loss = 1.86 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:05.526365: step 165900, loss = 1.83 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:06.736654: step 165910, loss = 1.93 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:07.944306: step 165920, loss = 1.76 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:09.154690: step 165930, loss = 1.82 (1057.5 examples/sec; 0.121 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1710 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
sec/batch)
2017-05-05 03:18:10.362200: step 165940, loss = 1.85 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:11.586767: step 165950, loss = 1.86 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:12.816862: step 165960, loss = 1.77 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:18:14.017684: step 165970, loss = 1.89 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:18:15.233151: step 165980, loss = 1.90 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:16.449121: step 165990, loss = 1.78 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:17.669690: step 166000, loss = 1.85 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:18.874891: step 166010, loss = 1.82 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:20.090496: step 166020, loss = 1.76 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:21.301805: step 166030, loss = 2.06 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:22.510336: step 166040, loss = 1.90 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:23.712905: step 166050, loss = 1.90 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:18:24.921067: step 166060, loss = 1.83 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:26.114180: step 166070, loss = 1.87 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:18:27.303089: step 166080, loss = 1.93 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:18:28.486942: step 166090, loss = 1.82 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:18:29.653095: step 166100, loss = 1.80 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:18:30.864860: step 166110, loss = 1.88 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:32.067405: step 166120, loss = 1.79 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:18:33.279941: step 166130, loss = 1.87 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:34.489071: step 166140, loss = 1.91 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:35.702170: step 166150, loss = 1.76 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:36.948026: step 166160, loss = 1.75 (1027.4 examples/sec; 0.125 sec/batch)
2017-05-05 03:18:38.158397: step 166170, loss = 1.92 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:39.369397: step 166180, loss = 1.98 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:40.606731: step 166190, loss = 1.82 (1034.5 examples/sec; 0.124 sec/batch)
2017-05-05 03:18:41.834437: step 166200, loss = 1.92 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:18:43.060541: step 166210, loss = 1.88 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:18:44.277562: step 166220, loss = 1.76 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:18:45.490302: step 166230, loss = 1.78 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:46.702111: step 166240, loss = 2.11 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:47.916644: step 166250, loss = 1.96 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:49.129615: step 166260, loss = 1.95 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:50.317167: step 166270, loss = 1.91 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:18:51.531702: step 166280, loss = 1.84 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:52.742211: step 166290, loss = 2.10 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:18:53.941778: step 166300, loss = 1.78 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:18:55.137770: step 166310, loss = 1.90 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:18:56.446431: step 166320, loss = 1.86 (978.1 examples/sec; 0.131 sec/batch)
2017-05-05 03:18:57.554507: step 166330, loss = 1.85 (1155.2 examples/sec; 0.111 sec/batch)
2017-05-05 03:18:58.797125: step 166340, loss = 1.76 (1030.1 examples/sec; 0.124 sec/batch)
2017-05-05 03:19:00.009622: step 166350, loss = 1.79 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:01.225768: step 166360, loss = 1.82 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:02.421128: step 166370, loss = 1.91 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:19:03.643743: step 166380, loss = 1.90 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:04.854836: step 166390, loss = 1.82 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:06.045993: step 166400, loss = 2.00 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:19:07.238373: step 166410, loss = 1.88 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:19:08.470320: step 166420, loss = 2.05 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:19:09.679969: step 166430, loss = 2.10 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:10.886206: step 166440, loss = 2.00 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:12.099639: step 166450, loss = 1.87 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:13.319104: step 166460, loss = 1.97 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:14.530193: step 166470, loss = 1.96 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:15.764018: step 166480, loss = 1.69 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-05 03:19:16.976132: step 166490, loss = 1.78 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:18.177453: step 166500, loss = 1.88 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:19:19.389205: step 166510, loss = 1.78 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:20.593177: step 166520, loss = 1.92 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:19:21.796506: step 166530, loss = 1.82 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:19:23.029106: step 166540, loss = 1.75 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:19:24.245384: step 166550, loss = 1.95 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:25.451023: step 166560, loss = 1.91 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:26.671820: step 166570, loss = 1.86 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:27.884081: step 166580, loss = 1.83 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:29.114089: step 166590, loss = 1.86 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:19:30.330667: step 166600, loss = 1.89 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:31.548537: step 166610, loss = 1.83 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:32.766532: step 166620, loss = 1.77 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:33.995319: step 166630, loss = 1.82 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-05 03:19:35.215638: step 166640, loss = 1.82 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:36.434037: step 166650, loss = 1.77 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:37.635641: step 166660, loss = 1.85 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:19:38.847258: step 166670, loss = 1.92 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:40.055328: step 166680, loss = 1.77 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:41.252395: step 166690, loss = 1.85 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:19:42.442661: step 166700, loss = 1.87 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:19:43.667147: step 166710, loss = 2.05 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:44.857569: step 166720, loss = 1.88 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:19:46.067032: step 166730, loss = 1.75 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:47.270079: step 166740, loss = 1.86 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:19:48.499930: step 166750, loss = 2.11 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-05 03:19:49.701683: step 166760, loss = 1.80 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:19:50.914900: step 166770, loss = 1.81 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:52.127705: step 166780, loss = 1.88 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:53.335998: step 166790, loss = 1.82 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:54.545965: step 166800, loss = 2.02 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:55.762367: step 166810, loss = 1.66 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:19:56.974357: step 166820, loss = 1.93 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:19:58.177847: step 166830, loss = 2.11 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:19:59.372167: step 166840, loss = 1.75 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:20:00.558665: step 166850, loss = 1.82 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:20:01.753770: step 166860, loss = 1.91 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:20:02.956068: step 166870, loss = 1.82 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:20:04.142890: step 166880, loss = 1.87 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:20:05.325513: step 166890, loss = 1.74 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:20:06.468037: step 166900, loss = 1.90 (1120.3 examples/sec; 0.114 sec/batch)
2017-05-05 03:20:07.641056: step 166910, loss = 1.88 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:08.790688: step 166920, loss = 1.98 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:20:09.939592: step 166930, loss = 1.75 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:20:11.088134: step 166940, loss = 1.89 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:20:12.262061: step 166950, loss = 1.94 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:13.426415: step 166960, loss = 1.97 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:14.573754: step 166970, loss = 1.90 (1115.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:20:15.740264: step 166980, loss = 1.95 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:16.904558: step 166990, loss = 1.76 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:18.068345: step 167000, loss = 1.97 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:19.261106: step 167010, loss = 1.86 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:20:20.422270: step 167020, loss = 1.89 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:21.587698: step 167030, loss = 1.84 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:22.773021: step 167040, loss = 1.91 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:20:23.928547: step 167050, loss = 1.88 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:25.097162: step 167060, loss = 1.95 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:26.243754: step 167070, loss = 1.83 (1116.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:20:27.406484: step 167080, loss = 2.06 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:28.592729: step 167090, loss = 1.80 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:20:29.751034: step 167100, loss = 1.95 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:30.929219: step 167110, loss = 1.87 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:20:32.117913: step 167120, loss = 1.81 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:20:33.291922: step 167130, loss = 1.83 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:34.444541: step 167140, loss = 2.02 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:20:35.602974: step 167150, loss = 1.91 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:36.762473: step 167160, loss = 1.78 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:37.906134: step 167170, loss = 1.93 (1119.2 examples/sec; 0.114 sec/batch)
2017-05-05 03:20:39.086751: step 167180, loss = 1.86 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:20:40.238034: step 167190, loss = 1.83 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:20:41.400356: step 167200, loss = 1.88 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:42.582373: step 167210, loss = 1.89 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:20:43.730432: step 167220, loss = 1.74 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:20:44.902479: step 167230, loss = 1.78 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:46.058770: step 167240, loss = 1.92 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:47.205779: step 167250, loss = 1.95 (1115.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:20:48.365862: step 167260, loss = 1.87 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:49.519392: step 167270, lE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1721 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
oss = 2.04 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:20:50.681802: step 167280, loss = 1.97 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:51.845288: step 167290, loss = 1.96 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:20:53.017286: step 167300, loss = 1.83 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:54.276817: step 167310, loss = 1.81 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-05 03:20:55.358081: step 167320, loss = 1.83 (1183.8 examples/sec; 0.108 sec/batch)
2017-05-05 03:20:56.532513: step 167330, loss = 1.88 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:20:57.683089: step 167340, loss = 1.78 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:20:58.852524: step 167350, loss = 1.71 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:00.016098: step 167360, loss = 1.86 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:21:01.185328: step 167370, loss = 1.87 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:02.336422: step 167380, loss = 1.92 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:21:03.496389: step 167390, loss = 1.83 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:21:04.667682: step 167400, loss = 1.78 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:05.818310: step 167410, loss = 1.93 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:21:06.997268: step 167420, loss = 1.79 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:08.181719: step 167430, loss = 2.00 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:09.348054: step 167440, loss = 1.74 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:10.518266: step 167450, loss = 1.80 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:11.718948: step 167460, loss = 1.79 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:21:12.935199: step 167470, loss = 1.91 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:21:14.100444: step 167480, loss = 1.96 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:15.270209: step 167490, loss = 1.89 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:16.470482: step 167500, loss = 1.83 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:21:17.653605: step 167510, loss = 1.88 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:18.838273: step 167520, loss = 1.83 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:20.025505: step 167530, loss = 1.97 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:21.219002: step 167540, loss = 1.86 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:22.403621: step 167550, loss = 1.88 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:23.589405: step 167560, loss = 1.78 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:24.765654: step 167570, loss = 1.87 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:25.950273: step 167580, loss = 1.81 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:27.140453: step 167590, loss = 1.83 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:28.329844: step 167600, loss = 1.89 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:29.510669: step 167610, loss = 1.73 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:30.700309: step 167620, loss = 1.84 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:31.888715: step 167630, loss = 1.82 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:33.064496: step 167640, loss = 1.94 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:34.252126: step 167650, loss = 1.95 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:35.457550: step 167660, loss = 1.90 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:21:36.663214: step 167670, loss = 1.87 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:21:37.826445: step 167680, loss = 2.00 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:21:39.014025: step 167690, loss = 1.85 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:40.209295: step 167700, loss = 1.90 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:21:41.403875: step 167710, loss = 1.97 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:42.580413: step 167720, loss = 1.89 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:43.758223: step 167730, loss = 1.84 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:44.924875: step 167740, loss = 1.76 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:46.092129: step 167750, loss = 1.86 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:47.289044: step 167760, loss = 1.93 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:21:48.483734: step 167770, loss = 1.88 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:49.671663: step 167780, loss = 1.72 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:50.841406: step 167790, loss = 1.86 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:52.011597: step 167800, loss = 1.98 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:53.204045: step 167810, loss = 1.86 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:54.377450: step 167820, loss = 1.77 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:21:55.564942: step 167830, loss = 1.79 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:21:56.741314: step 167840, loss = 1.96 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:21:57.940205: step 167850, loss = 1.79 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:21:59.140198: step 167860, loss = 1.89 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:22:00.347362: step 167870, loss = 1.99 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:01.536811: step 167880, loss = 1.82 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:22:02.730273: step 167890, loss = 1.88 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:22:03.911273: step 167900, loss = 2.00 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:22:05.106604: step 167910, loss = 1.86 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:22:06.324070: step 167920, loss = 1.75 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:22:07.529975: step 167930, loss = 1.91 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:08.713292: step 167940, loss = 1.83 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:22:09.884343: step 167950, loss = 1.91 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:22:11.057551: step 167960, loss = 1.83 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:22:12.247509: step 167970, loss = 1.78 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:22:13.415096: step 167980, loss = 1.96 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:22:14.597781: step 167990, loss = 1.91 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:22:15.763672: step 168000, loss = 1.75 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:22:16.925783: step 168010, loss = 1.91 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:22:18.100118: step 168020, loss = 1.84 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:22:19.297107: step 168030, loss = 2.05 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:22:20.488532: step 168040, loss = 1.79 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:22:21.660548: step 168050, loss = 1.90 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:22:22.842094: step 168060, loss = 1.89 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:22:24.067188: step 168070, loss = 1.91 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 03:22:25.279449: step 168080, loss = 1.88 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:26.479896: step 168090, loss = 1.84 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:22:27.673101: step 168100, loss = 1.82 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:22:28.865533: step 168110, loss = 1.85 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:22:30.060638: step 168120, loss = 2.01 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:22:31.286841: step 168130, loss = 1.91 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:22:32.496911: step 168140, loss = 1.63 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:33.704544: step 168150, loss = 1.84 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:34.929445: step 168160, loss E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1732 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
= 1.99 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:22:36.134013: step 168170, loss = 1.73 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:22:37.358523: step 168180, loss = 1.85 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:22:38.567449: step 168190, loss = 1.82 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:39.782431: step 168200, loss = 1.79 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:41.000007: step 168210, loss = 1.98 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:22:42.196471: step 168220, loss = 1.82 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:22:43.405221: step 168230, loss = 1.78 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:44.586349: step 168240, loss = 2.08 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:22:45.769359: step 168250, loss = 1.88 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:22:46.959972: step 168260, loss = 1.95 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:22:48.145613: step 168270, loss = 1.95 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:22:49.337871: step 168280, loss = 1.95 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:22:50.534380: step 168290, loss = 1.79 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:22:51.842521: step 168300, loss = 1.94 (978.5 examples/sec; 0.131 sec/batch)
2017-05-05 03:22:52.962968: step 168310, loss = 1.80 (1142.4 examples/sec; 0.112 sec/batch)
2017-05-05 03:22:54.170784: step 168320, loss = 1.83 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:55.375801: step 168330, loss = 1.84 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:22:56.606551: step 168340, loss = 1.83 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:22:57.810684: step 168350, loss = 2.02 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:22:59.041004: step 168360, loss = 1.96 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-05 03:23:00.273734: step 168370, loss = 1.95 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-05 03:23:01.490579: step 168380, loss = 1.84 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:02.704210: step 168390, loss = 1.92 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:03.932909: step 168400, loss = 1.89 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-05 03:23:05.155882: step 168410, loss = 1.76 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:06.365187: step 168420, loss = 1.80 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:07.587919: step 168430, loss = 1.79 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:08.795747: step 168440, loss = 1.90 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:10.009751: step 168450, loss = 1.81 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:11.223042: step 168460, loss = 1.80 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:12.431700: step 168470, loss = 1.72 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:13.622881: step 168480, loss = 1.88 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:23:14.843487: step 168490, loss = 1.84 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:16.026883: step 168500, loss = 1.95 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:23:17.243947: step 168510, loss = 2.06 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:18.443313: step 168520, loss = 1.83 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:19.639536: step 168530, loss = 1.90 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:20.850590: step 168540, loss = 1.98 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:22.041841: step 168550, loss = 1.90 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:23:23.247479: step 168560, loss = 1.75 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:24.455948: step 168570, loss = 1.84 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:25.677766: step 168580, loss = 1.87 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:26.893846: step 168590, loss = 1.95 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:28.126262: step 168600, loss = 1.81 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:23:29.338170: step 168610, loss = 2.01 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:30.540865: step 168620, loss = 1.77 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:31.744121: step 168630, loss = 1.81 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:32.969159: step 168640, loss = 1.89 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:23:34.168878: step 168650, loss = 1.87 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:35.391275: step 168660, loss = 1.78 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:36.601877: step 168670, loss = 1.73 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:37.807445: step 168680, loss = 1.89 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:39.021372: step 168690, loss = 2.07 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:40.225646: step 168700, loss = 1.94 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:41.455154: step 168710, loss = 1.86 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-05 03:23:42.664303: step 168720, loss = 1.87 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:43.875158: step 168730, loss = 1.89 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:45.097847: step 168740, loss = 1.93 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:46.308083: step 168750, loss = 1.90 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:47.509308: step 168760, loss = 1.83 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:48.737837: step 168770, loss = 1.93 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:23:49.937859: step 168780, loss = 1.75 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:23:51.147562: step 168790, loss = 1.86 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:52.360000: step 168800, loss = 1.78 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:53.578649: step 168810, loss = 1.93 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:23:54.790215: step 168820, loss = 1.79 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:56.017064: step 168830, loss = 1.91 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-05 03:23:57.254760: step 168840, loss = 1.96 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-05 03:23:58.466088: step 168850, loss = 1.96 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:23:59.689027: step 168860, loss = 2.02 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:24:00.924743: step 168870, loss = 1.88 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-05 03:24:02.131734: step 168880, loss = 2.07 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:03.356827: step 168890, loss = 1.89 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-05 03:24:04.527208: step 168900, loss = 1.97 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:24:05.730972: step 168910, loss = 1.76 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:06.920005: step 168920, loss = 1.83 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:24:08.126986: step 168930, loss = 1.96 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:09.320718: step 168940, loss = 1.91 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:24:10.512523: step 168950, loss = 1.88 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:24:11.726316: step 168960, loss = 1.97 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:12.939069: step 168970, loss = 1.82 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:14.134932: step 168980, loss = 1.93 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:15.337197: step 168990, loss = 1.73 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:16.540993: step 169000, loss = 1.86 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:17.715684: step 169010, loss = 1.99 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:24:18.896562: step 169020, loss = 1.90 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:20.094115: step 169030, loss = 1.81 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:21.294381: step 169040, loss = 1.92 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:22.489697: step 169050, loss = 1.82 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:23.681536: step 169060, loss = 1.92 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:24:24.867485: step 169070, loss = 1.88 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:24:26.063169: step 169080, loss = 1.94 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:27.255803: step 169090, loss = 1.77 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:24:28.436050: step 169100, loss = 1.88 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:29.636850: step 169110, loss = 1.74 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:30.843655: step 169120, loss = 1.84 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:32.051656: step 169130, loss = 1.87 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:33.239475: step 169140, loss = 2.01 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:24:34.410090: step 169150, loss = 1.96 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:24:35.579151: step 169160, loss = 1.82 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:24:36.767211: step 169170, loss = 1.92 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:24:37.940630: step 169180, loss = 1.84 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:24:39.105863: step 169190, loss = 1.83 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:24:40.311111: step 169200, loss = 1.75 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:41.520269: step 169210, loss = 1.90 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:42.727299: step 169220, loss = 1.86 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:43.963524: step 169230, loss = 1.90 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-05 03:24:45.174173: step 169240, loss = 1.93 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:46.383591: step 169250, loss = 2.06 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:47.588271: step 169260, loss = 1.72 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:24:48.800852: step 169270, loss = 1.83 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:24:50.018678: step 169280, loss = 1.89 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:24:51.312905: step 169290, loss = 1.92 (989.0 examples/sec; 0.129 sec/batch)
2017-05-05 03:24:52.425068: step 169300, loss = 1.82 (1150.9 examples/sec; 0.111 sec/batch)
2017-05-05 03:24:53.608438: step 169310, loss = 1.84 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:54.788378: step 169320, loss = 1.80 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:55.971714: step 169330, loss = 1.82 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:57.155343: step 169340, loss = 1.84 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:24:58.342419: step 169350, loss = 1.80 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:24:59.536621: step 169360, loss = 1.90 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:25:00.724717: step 169370, loss = 2.12 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:25:01.976836: step 169380, loss = 1.82 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-05 03:25:03.197201: step 169390, loss = 1.76 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:04.429911: step 169400, loss = 2.00 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:05.647470: step 169410, loss = 1.84 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:06.852216: step 169420, loss = 1.77 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:25:08.071817: step 169430, loss = 1.85 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:09.291889: step 169440, loss = 1.77 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:10.512593: step 169450, loss = 1.93 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:11.730765: step 169460, loss = 1.81 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:12.950565: step 169470, loss = 1.87 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:14.165944: step 169480, loss = 1.86 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:15.368099: step 169490, loss = 1.92 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1743 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
05 03:25:16.569862: step 169500, loss = 1.95 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:25:17.792652: step 169510, loss = 1.74 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:19.010935: step 169520, loss = 1.90 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:20.232451: step 169530, loss = 1.89 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:21.460397: step 169540, loss = 2.02 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:22.675698: step 169550, loss = 1.70 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:23.887927: step 169560, loss = 1.96 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:25.111780: step 169570, loss = 1.90 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:26.333088: step 169580, loss = 1.90 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:27.548520: step 169590, loss = 1.92 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:28.757168: step 169600, loss = 1.89 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:29.972332: step 169610, loss = 1.77 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:31.200928: step 169620, loss = 1.84 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:32.410441: step 169630, loss = 1.84 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:33.629123: step 169640, loss = 1.72 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:34.842676: step 169650, loss = 1.84 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:36.077072: step 169660, loss = 1.85 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:37.302794: step 169670, loss = 1.94 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:38.502929: step 169680, loss = 1.70 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:25:39.691879: step 169690, loss = 1.98 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:25:40.911525: step 169700, loss = 1.91 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:42.136975: step 169710, loss = 1.97 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:43.364765: step 169720, loss = 1.80 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:44.579660: step 169730, loss = 1.91 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:45.793209: step 169740, loss = 1.92 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:47.012697: step 169750, loss = 1.82 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:48.242775: step 169760, loss = 1.86 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:49.471293: step 169770, loss = 1.72 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:50.673602: step 169780, loss = 1.86 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:25:51.886378: step 169790, loss = 1.91 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:53.111561: step 169800, loss = 1.89 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-05 03:25:54.319706: step 169810, loss = 2.00 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:25:55.537519: step 169820, loss = 1.71 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:56.756324: step 169830, loss = 1.96 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:25:57.959597: step 169840, loss = 1.74 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:25:59.179682: step 169850, loss = 1.87 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:00.396736: step 169860, loss = 1.93 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:01.617177: step 169870, loss = 1.86 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:02.849426: step 169880, loss = 1.87 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-05 03:26:04.039975: step 169890, loss = 1.92 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:26:05.267123: step 169900, loss = 1.89 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-05 03:26:06.470853: step 169910, loss = 1.92 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:26:07.682842: step 169920, loss = 1.81 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:08.897093: step 169930, loss = 1.79 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:10.102748: step 169940, loss = 2.02 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:11.348854: step 169950, loss = 1.79 (1027.2 examples/sec; 0.125 sec/batch)
2017-05-05 03:26:12.578885: step 169960, loss = 1.90 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:26:13.776829: step 169970, loss = 1.94 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:26:15.000285: step 169980, loss = 1.86 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:16.228013: step 169990, loss = 1.92 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:26:17.453921: step 170000, loss = 1.92 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-05 03:26:18.678004: step 170010, loss = 1.82 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:19.895167: step 170020, loss = 1.94 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:21.103768: step 170030, loss = 1.83 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:22.314071: step 170040, loss = 1.83 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:23.535990: step 170050, loss = 1.88 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:24.745599: step 170060, loss = 1.89 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:25.952280: step 170070, loss = 1.90 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:27.168432: step 170080, loss = 1.89 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:28.359027: step 170090, loss = 1.89 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:26:29.565999: step 170100, loss = 2.01 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:30.787769: step 170110, loss = 1.89 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:31.999529: step 170120, loss = 1.89 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:33.224013: step 170130, loss = 1.86 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:34.427957: step 170140, loss = 1.98 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:26:35.658820: step 170150, loss = 1.87 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:26:36.870104: step 170160, loss = 1.92 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:38.070910: step 170170, loss = 1.94 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:26:39.282602: step 170180, loss = 1.91 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:40.501518: step 170190, loss = 1.86 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:41.695983: step 170200, loss = 1.82 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:26:42.910573: step 170210, loss = 1.88 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:44.138515: step 170220, loss = 2.01 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-05 03:26:45.378268: step 170230, loss = 1.93 (1032.5 examples/sec; 0.124 sec/batch)
2017-05-05 03:26:46.579852: step 170240, loss = 1.80 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:26:47.796322: step 170250, loss = 1.98 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:26:49.021719: step 170260, loss = 1.99 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:26:50.230126: step 170270, loss = 1.91 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:26:51.587777: step 170280, loss = 1.91 (942.8 examples/sec; 0.136 sec/batch)
2017-05-05 03:26:52.623855: step 170290, loss = 1.81 (1235.4 examples/sec; 0.104 sec/batch)
2017-05-05 03:26:53.800606: step 170300, loss = 1.89 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:26:54.994826: step 170310, loss = 1.74 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:26:56.181423: step 170320, loss = 1.89 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:26:57.373928: step 170330, loss = 1.85 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:26:58.548334: step 170340, loss = 1.81 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:26:59.750689: step 170350, loss = 1.77 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:00.964559: step 170360, loss = 1.93 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:27:02.160153: step 170370, loss = 1.83 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:03.359343: step 170380, loss = 1.86 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 03E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1753 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
:27:04.568651: step 170390, loss = 1.87 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:27:05.771506: step 170400, loss = 1.81 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:06.963791: step 170410, loss = 1.87 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:08.174767: step 170420, loss = 1.81 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:27:09.380115: step 170430, loss = 1.82 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:27:10.604614: step 170440, loss = 1.83 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:27:11.817737: step 170450, loss = 1.85 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:27:13.025777: step 170460, loss = 1.94 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:27:14.212636: step 170470, loss = 1.96 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:15.411013: step 170480, loss = 1.79 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:16.619814: step 170490, loss = 1.84 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:27:17.811299: step 170500, loss = 2.07 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:19.024209: step 170510, loss = 1.95 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:27:20.242474: step 170520, loss = 1.87 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:27:21.437798: step 170530, loss = 1.89 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:22.618712: step 170540, loss = 1.80 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:27:23.806722: step 170550, loss = 1.89 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:25.004065: step 170560, loss = 1.84 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:26.189832: step 170570, loss = 1.99 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:27.378474: step 170580, loss = 1.80 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:28.574597: step 170590, loss = 1.89 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:29.744292: step 170600, loss = 1.88 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:27:30.915520: step 170610, loss = 1.76 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:27:32.084310: step 170620, loss = 1.92 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:27:33.249040: step 170630, loss = 1.97 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:27:34.453882: step 170640, loss = 1.91 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:35.663078: step 170650, loss = 1.83 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:27:36.861897: step 170660, loss = 1.84 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:38.040259: step 170670, loss = 1.85 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:27:39.218551: step 170680, loss = 1.78 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:27:40.412424: step 170690, loss = 1.74 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:41.602145: step 170700, loss = 1.77 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:42.800143: step 170710, loss = 1.73 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:43.978002: step 170720, loss = 1.90 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:27:45.190817: step 170730, loss = 1.93 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:27:46.366487: step 170740, loss = 1.82 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:27:47.559769: step 170750, loss = 1.78 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:27:48.727072: step 170760, loss = 1.91 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:27:49.900753: step 170770, loss = 2.01 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:27:51.085319: step 170780, loss = 1.94 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:27:52.283492: step 170790, loss = 1.98 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:27:53.462823: step 170800, loss = 1.93 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:27:54.633056: step 170810, loss = 1.99 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:27:55.798930: step 170820, loss = 1.87 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:27:56.973590: step 170830, loss = 2.04 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:27:58.125605: step 170840, loss = 1.77 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:27:59.297088: step 170850, loss = 1.90 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:28:00.453326: step 170860, loss = 1.95 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:01.621049: step 170870, loss = 1.82 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:28:02.836579: step 170880, loss = 1.88 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:28:04.012458: step 170890, loss = 1.78 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:28:05.178662: step 170900, loss = 1.95 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:28:06.334380: step 170910, loss = 1.89 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:07.548327: step 170920, loss = 1.84 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:28:08.729956: step 170930, loss = 1.96 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:28:09.892784: step 170940, loss = 1.73 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:11.081772: step 170950, loss = 1.85 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:12.257132: step 170960, loss = 1.78 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:28:13.414193: step 170970, loss = 1.89 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:14.594067: step 170980, loss = 1.87 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:28:15.772407: step 170990, loss = 1.93 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:28:16.933312: step 171000, loss = 1.83 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:18.094644: step 171010, loss = 1.85 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:19.288945: step 171020, loss = 1.96 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:20.471928: step 171030, loss = 1.81 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:28:21.635335: step 171040, loss = 1.80 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:22.797935: step 171050, loss = 1.85 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:23.990832: step 171060, loss = 1.89 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:25.181952: step 171070, loss = 1.92 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:26.337460: step 171080, loss = 1.81 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:27.501432: step 171090, loss = 1.73 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:28.706816: step 171100, loss = 1.99 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:28:29.853487: step 171110, loss = 1.95 (1116.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:28:31.060556: step 171120, loss = 1.90 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:28:32.246026: step 171130, loss = 1.92 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:33.411654: step 171140, loss = 2.02 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:28:34.596895: step 171150, loss = 1.91 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:35.755341: step 171160, loss = 1.79 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:36.961368: step 171170, loss = 1.84 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:28:38.122772: step 171180, loss = 1.84 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:39.285026: step 171190, loss = 1.67 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:40.448491: step 171200, loss = 1.97 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:41.608017: step 171210, loss = 1.81 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:42.771376: step 171220, loss = 1.85 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:43.939837: step 171230, loss = 2.00 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:28:45.104519: step 171240, loss = 1.78 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:46.267272: step 171250, loss = 1.81 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:47.425928: step 171260, loss = 2.01 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:48.691689: step 171270, loss = 1.83 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-05 03:28:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1765 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
49.741405: step 171280, loss = 1.84 (1219.4 examples/sec; 0.105 sec/batch)
2017-05-05 03:28:50.911409: step 171290, loss = 1.96 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:28:52.074601: step 171300, loss = 1.77 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:53.262639: step 171310, loss = 1.82 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:54.410722: step 171320, loss = 1.93 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:28:55.573479: step 171330, loss = 1.93 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:28:56.764864: step 171340, loss = 1.85 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:57.949952: step 171350, loss = 1.86 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:28:59.132399: step 171360, loss = 1.78 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:00.315825: step 171370, loss = 1.89 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:01.501890: step 171380, loss = 2.05 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:02.698668: step 171390, loss = 2.04 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:29:03.881322: step 171400, loss = 1.79 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:05.071368: step 171410, loss = 1.84 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:06.253222: step 171420, loss = 1.90 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:07.447337: step 171430, loss = 1.77 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:08.655986: step 171440, loss = 1.77 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:29:09.818958: step 171450, loss = 1.83 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:11.006713: step 171460, loss = 1.80 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:12.198845: step 171470, loss = 1.85 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:13.361817: step 171480, loss = 1.99 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:14.527651: step 171490, loss = 1.89 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:29:15.710395: step 171500, loss = 1.80 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:16.883240: step 171510, loss = 1.77 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:29:18.043815: step 171520, loss = 2.02 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:19.207864: step 171530, loss = 1.80 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:20.366050: step 171540, loss = 1.74 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:21.527847: step 171550, loss = 1.84 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:22.689188: step 171560, loss = 1.94 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:23.868022: step 171570, loss = 1.71 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:25.045777: step 171580, loss = 1.82 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:26.195442: step 171590, loss = 1.96 (1113.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:29:27.357380: step 171600, loss = 1.79 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:28.537050: step 171610, loss = 1.74 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:29.681048: step 171620, loss = 1.84 (1118.9 examples/sec; 0.114 sec/batch)
2017-05-05 03:29:30.853982: step 171630, loss = 1.91 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:29:32.014367: step 171640, loss = 2.06 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:33.183193: step 171650, loss = 2.03 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:29:34.333598: step 171660, loss = 1.76 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:29:35.511699: step 171670, loss = 1.84 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:36.686372: step 171680, loss = 1.97 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:29:37.867086: step 171690, loss = 1.81 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:29:39.063769: step 171700, loss = 1.88 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:29:40.220026: step 171710, loss = 1.85 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:41.379452: step 171720, loss = 1.83 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:42.546306: step 171730, loss = 1.83 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:29:43.710598: step 171740, loss = 1.92 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:44.880447: step 171750, loss = 1.93 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:29:46.043251: step 171760, loss = 1.89 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:29:47.229583: step 171770, loss = 1.71 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:48.416144: step 171780, loss = 1.92 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:49.621714: step 171790, loss = 2.15 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:29:50.822598: step 171800, loss = 1.90 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:29:52.013111: step 171810, loss = 1.80 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:53.237776: step 171820, loss = 1.81 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:29:54.423383: step 171830, loss = 1.94 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:29:55.619105: step 171840, loss = 1.85 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:29:56.825029: step 171850, loss = 1.80 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:29:58.027496: step 171860, loss = 1.98 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:29:59.217838: step 171870, loss = 1.88 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:30:00.410603: step 171880, loss = 1.91 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:30:01.583389: step 171890, loss = 1.93 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:02.776660: step 171900, loss = 1.64 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:30:03.971385: step 171910, loss = 1.73 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:30:05.162808: step 171920, loss = 1.99 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:30:06.339718: step 171930, loss = 1.91 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:07.531329: step 171940, loss = 1.74 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:30:08.746568: step 171950, loss = 1.84 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:30:09.956894: step 171960, loss = 1.88 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:30:11.176025: step 171970, loss = 1.88 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:30:12.388153: step 171980, loss = 1.85 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:30:13.599494: step 171990, loss = 1.71 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:30:14.824837: step 172000, loss = 1.84 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:30:16.034930: step 172010, loss = 1.82 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:30:17.258257: step 172020, loss = 1.89 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:30:18.439882: step 172030, loss = 2.06 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:19.659562: step 172040, loss = 1.98 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:30:20.858756: step 172050, loss = 1.80 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:30:22.067386: step 172060, loss = 1.88 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:30:23.254633: step 172070, loss = 1.74 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:30:24.449065: step 172080, loss = 1.86 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:30:25.658436: step 172090, loss = 1.77 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:30:26.866450: step 172100, loss = 1.98 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:30:28.061790: step 172110, loss = 1.84 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:30:29.259267: step 172120, loss = 2.05 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:30:30.427698: step 172130, loss = 1.87 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:31.627437: step 172140, loss = 1.85 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:30:32.818686: step 172150, loss = 2.00 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:30:34.003166: step 172160, loss = 2.07 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:35.187981: step 172170, loss = 1.81 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:36.366796: step 172180, loss = 1.87 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:37.540535: step 172190, loss = 1.75 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:38.714029: step 172200, loss = 1.90 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:39.895972: step 172210, loss = 1.83 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:41.075099: step 172220, loss = 2.03 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:42.242592: step 172230, loss = 1.89 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:43.420257: step 172240, loss = 1.86 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:44.587341: step 172250, loss = 1.86 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:45.868181: step 172260, loss = 1.85 (999.3 examples/sec; 0.128 sec/batch)
2017-05-05 03:30:46.948215: step 172270, loss = 1.77 (1185.2 examples/sec; 0.108 sec/batch)
2017-05-05 03:30:48.120479: step 172280, loss = 1.89 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:49.281732: step 172290, loss = 1.81 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:30:50.442691: step 172300, loss = 1.93 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:30:51.613739: step 172310, loss = 1.87 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:52.789478: step 172320, loss = 1.84 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:53.960223: step 172330, loss = 1.93 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:55.142121: step 172340, loss = 1.89 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:30:56.312572: step 172350, loss = 1.69 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:30:57.467052: step 172360, loss = 1.77 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:30:58.618342: step 172370, loss = 1.82 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:30:59.775217: step 172380, loss = 1.85 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:31:00.949870: step 172390, loss = 1.91 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:31:02.099622: step 172400, loss = 1.84 (1113.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:31:03.273108: step 172410, loss = 1.78 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:31:04.451397: step 172420, loss = 1.90 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:31:05.608705: step 172430, loss = 1.90 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:31:06.763256: step 172440, loss = 1.91 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:31:07.951208: step 172450, loss = 1.81 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:31:09.113847: step 172460, loss = 1.89 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:31:10.270944: step 172470, loss = 1.84 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:31:11.437706: step 172480, loss = 1.99 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:31:12.634693: step 172490, loss = 1.80 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:13.809335: step 172500, loss = 1.95 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:31:15.010320: step 172510, loss = 1.82 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:16.201513: step 172520, loss = 1.76 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:31:17.406087: step 172530, loss = 1.84 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:18.632933: step 172540, loss = 1.75 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-05 03:31:19.825586: step 172550, loss = 1.80 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:31:21.023582: step 172560, loss = 1.79 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:22.233766: step 172570, loss = 1.86 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:31:23.457180: step 172580, loss = 1.93 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:31:24.660065: step 172590, loss = 1.78 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:25.860891: step 172600, loss = 1.92 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:27.056129: step 172610, loss = 1.80 (1070.9 examples/E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1776 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
sec; 0.120 sec/batch)
2017-05-05 03:31:28.244977: step 172620, loss = 1.95 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:31:29.430836: step 172630, loss = 1.74 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:31:30.594773: step 172640, loss = 1.95 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:31:31.755032: step 172650, loss = 1.96 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:31:32.954944: step 172660, loss = 1.87 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:34.163823: step 172670, loss = 1.93 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:31:35.357618: step 172680, loss = 1.88 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:31:36.554657: step 172690, loss = 2.04 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:37.732106: step 172700, loss = 1.87 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:31:38.939470: step 172710, loss = 1.77 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:31:40.109647: step 172720, loss = 1.82 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:31:41.309913: step 172730, loss = 1.94 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:42.477308: step 172740, loss = 1.85 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:31:43.687108: step 172750, loss = 1.84 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:31:44.886050: step 172760, loss = 1.96 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:46.081719: step 172770, loss = 1.99 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:47.310158: step 172780, loss = 2.03 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:31:48.519995: step 172790, loss = 1.95 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:31:49.710472: step 172800, loss = 1.90 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:31:50.918438: step 172810, loss = 1.85 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:31:52.122867: step 172820, loss = 1.75 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:53.334186: step 172830, loss = 1.84 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:31:54.516907: step 172840, loss = 1.79 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:31:55.719989: step 172850, loss = 1.92 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:31:56.938329: step 172860, loss = 2.18 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:31:58.147479: step 172870, loss = 1.98 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:31:59.363157: step 172880, loss = 1.98 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:32:00.565203: step 172890, loss = 1.77 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:32:01.755625: step 172900, loss = 1.85 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:02.952113: step 172910, loss = 1.76 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:32:04.128047: step 172920, loss = 1.86 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:32:05.319178: step 172930, loss = 1.87 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:06.517637: step 172940, loss = 1.88 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:32:07.721164: step 172950, loss = 1.93 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:32:08.908551: step 172960, loss = 1.91 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:10.096525: step 172970, loss = 1.85 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:11.272347: step 172980, loss = 1.87 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:32:12.468044: step 172990, loss = 1.86 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:32:13.632097: step 173000, loss = 1.79 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:32:14.826709: step 173010, loss = 1.94 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:16.005586: step 173020, loss = 1.95 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:32:17.196836: step 173030, loss = 1.93 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:18.356404: step 173040, loss = 1.78 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:32:19.543750: step 173050, loss = 1.88 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:20.713366: step 173060, loss = 1.87 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:32:21.874427: step 173070, loss = 1.91 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:32:23.062261: step 173080, loss = 1.83 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:24.242888: step 173090, loss = 1.77 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:32:25.414707: step 173100, loss = 1.86 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:32:26.580493: step 173110, loss = 1.81 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:32:27.753435: step 173120, loss = 1.73 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:32:28.918263: step 173130, loss = 1.74 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:32:30.079882: step 173140, loss = 1.84 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:32:31.252114: step 173150, loss = 1.85 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:32:32.429078: step 173160, loss = 1.77 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:32:33.607709: step 173170, loss = 2.06 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:32:34.796937: step 173180, loss = 1.95 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:35.977593: step 173190, loss = 1.95 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:32:37.173898: step 173200, loss = 1.93 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:32:38.377006: step 173210, loss = 1.80 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:32:39.603341: step 173220, loss = 1.93 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 03:32:40.821457: step 173230, loss = 1.97 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:32:41.996870: step 173240, loss = 1.80 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:32:43.289681: step 173250, loss = 1.87 (990.1 examples/sec; 0.129 sec/batch)
2017-05-05 03:32:44.400798: step 173260, loss = 1.88 (1152.0 examples/sec; 0.111 sec/batch)
2017-05-05 03:32:45.623764: step 173270, loss = 1.84 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:32:46.813600: step 173280, loss = 1.88 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:48.025629: step 173290, loss = 1.92 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:32:49.245014: step 173300, loss = 1.87 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:32:50.431558: step 173310, loss = 1.89 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:51.618834: step 173320, loss = 1.92 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:52.811092: step 173330, loss = 1.83 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:32:53.985977: step 173340, loss = 1.90 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:32:55.166594: step 173350, loss = 1.85 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:32:56.337371: step 173360, loss = 1.81 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:32:57.498991: step 173370, loss = 1.79 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:32:58.654735: step 173380, loss = 1.83 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:32:59.837787: step 173390, loss = 1.89 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:01.012751: step 173400, loss = 1.93 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:02.162298: step 173410, loss = 1.95 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:33:03.335092: step 173420, loss = 1.88 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:04.501551: step 173430, loss = 1.92 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:05.646939: step 173440, loss = 2.02 (1117.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:33:06.807297: step 173450, loss = 1.80 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:07.978121: step 173460, loss = 1.67 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:09.141692: step 173470, loss = 1.71 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:10.285018: step 173480, loss = 1.99 (1119.5 examples/sec; 0.114 sec/batch)
2017-05-05 03:33:11.473786: step 173490, loss = 1.88 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:33:12.655894: step 173500, loss = 1.91 (1082.8 examples/sec; E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1787 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
0.118 sec/batch)
2017-05-05 03:33:13.806792: step 173510, loss = 1.91 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:33:14.982544: step 173520, loss = 1.88 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:16.155194: step 173530, loss = 1.87 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:17.315985: step 173540, loss = 1.71 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:18.479461: step 173550, loss = 1.83 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:19.644160: step 173560, loss = 1.76 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:20.807467: step 173570, loss = 2.12 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:21.978926: step 173580, loss = 2.00 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:23.155704: step 173590, loss = 1.84 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:24.314611: step 173600, loss = 1.80 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:25.486028: step 173610, loss = 1.96 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:26.675959: step 173620, loss = 1.75 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:33:27.867456: step 173630, loss = 1.84 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:33:29.047021: step 173640, loss = 1.77 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:30.219621: step 173650, loss = 1.84 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:31.394845: step 173660, loss = 1.84 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:32.576212: step 173670, loss = 1.87 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:33.746060: step 173680, loss = 1.89 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:34.915392: step 173690, loss = 1.86 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:36.088631: step 173700, loss = 1.78 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:37.249286: step 173710, loss = 1.88 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:38.397728: step 173720, loss = 1.85 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:33:39.564443: step 173730, loss = 1.73 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:40.745494: step 173740, loss = 1.91 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:41.895939: step 173750, loss = 1.79 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:33:43.054527: step 173760, loss = 1.89 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:44.220993: step 173770, loss = 1.87 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:45.373280: step 173780, loss = 2.10 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:33:46.535721: step 173790, loss = 1.80 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:33:47.709281: step 173800, loss = 2.13 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:48.882022: step 173810, loss = 1.72 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:50.030925: step 173820, loss = 1.89 (1114.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:33:51.176278: step 173830, loss = 1.78 (1117.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:33:52.351678: step 173840, loss = 2.03 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:53.530993: step 173850, loss = 2.18 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:54.697110: step 173860, loss = 1.95 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:55.871835: step 173870, loss = 1.91 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:33:57.049030: step 173880, loss = 1.88 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:33:58.200791: step 173890, loss = 1.89 (1111.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:33:59.373175: step 173900, loss = 1.93 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:00.546761: step 173910, loss = 1.90 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:01.699676: step 173920, loss = 1.97 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:34:02.879577: step 173930, loss = 1.84 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:04.075907: step 173940, loss = 1.77 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:34:05.258995: step 173950, loss = 1.76 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:06.427632: step 173960, loss = 1.81 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:07.608007: step 173970, loss = 1.81 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:08.783381: step 173980, loss = 1.72 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:09.930104: step 173990, loss = 1.94 (1116.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:34:11.109851: step 174000, loss = 1.89 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:12.275602: step 174010, loss = 1.80 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:13.452434: step 174020, loss = 1.81 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:14.621446: step 174030, loss = 1.99 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:15.782756: step 174040, loss = 1.79 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:16.964310: step 174050, loss = 1.74 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:18.120561: step 174060, loss = 1.97 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:19.284533: step 174070, loss = 1.82 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:20.464136: step 174080, loss = 1.80 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:21.635435: step 174090, loss = 1.88 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:22.799068: step 174100, loss = 1.85 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:23.965950: step 174110, loss = 1.76 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:25.144593: step 174120, loss = 1.84 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:26.304060: step 174130, loss = 1.81 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:27.475646: step 174140, loss = 1.91 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:28.642323: step 174150, loss = 1.84 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:29.791407: step 174160, loss = 1.78 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:34:30.953436: step 174170, loss = 1.72 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:32.120014: step 174180, loss = 1.83 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:33.280944: step 174190, loss = 1.78 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:34.427416: step 174200, loss = 1.88 (1116.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:34:35.604001: step 174210, loss = 1.89 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:36.756502: step 174220, loss = 1.88 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:34:37.911715: step 174230, loss = 1.94 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:39.184739: step 174240, loss = 1.80 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-05 03:34:40.256612: step 174250, loss = 1.87 (1194.2 examples/sec; 0.107 sec/batch)
2017-05-05 03:34:41.427144: step 174260, loss = 1.92 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:42.570177: step 174270, loss = 1.97 (1119.8 examples/sec; 0.114 sec/batch)
2017-05-05 03:34:43.738623: step 174280, loss = 1.85 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:44.920783: step 174290, loss = 1.95 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:46.082637: step 174300, loss = 1.89 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:47.260689: step 174310, loss = 1.86 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:48.437702: step 174320, loss = 1.79 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:49.609837: step 174330, loss = 1.85 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:34:50.793347: step 174340, loss = 1.79 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:51.950386: step 174350, loss = 1.91 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:53.109213: step 174360, loss = 1.77 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:54.284827: step 174370, loss = 1.92 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:34:55.444712: step 174380, loss = 2.04 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:56.633654: step 174390, loss = 2.09 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:34:57.794881: step 174400, loss = 1.73 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:34:58.945806: step 174410, loss = 1.83 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:35:00.114998: step 174420, loss = 1.83 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:01.287087: step 174430, loss = 1.95 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:02.448010: step 174440, loss = 1.88 (1102.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:03.617703: step 174450, loss = 1.89 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:04.768639: step 174460, loss = 1.92 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:35:05.942103: step 174470, loss = 1.89 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:07.107415: step 174480, loss = 1.73 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:08.265812: step 174490, loss = 2.02 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:09.425544: step 174500, loss = 1.81 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:10.601027: step 174510, loss = 1.96 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:11.787909: step 174520, loss = 1.83 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:35:12.987594: step 174530, loss = 1.83 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:35:14.162498: step 174540, loss = 1.75 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:15.339592: step 174550, loss = 1.75 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:16.514844: step 174560, loss = 1.85 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:17.673163: step 174570, loss = 1.94 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:18.840717: step 174580, loss = 1.80 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:20.030403: step 174590, loss = 1.80 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:35:21.197584: step 174600, loss = 1.97 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:22.363588: step 174610, loss = 1.84 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:23.520682: step 174620, loss = 1.89 (1106.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:24.698505: step 174630, loss = 1.82 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:25.865294: step 174640, loss = 1.92 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:27.030015: step 174650, loss = 1.99 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:28.212097: step 174660, loss = 1.78 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:29.387222: step 174670, loss = 1.63 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:30.572061: step 174680, loss = 1.79 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:31.748521: step 174690, loss = 1.85 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:32.901622: step 174700, loss = 1.88 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:35:34.061312: step 174710, loss = 1.84 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:35.221345: step 174720, loss = 1.75 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:36.395706: step 174730, loss = 1.75 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:37.550680: step 174740, loss = 1.94 (1108.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:35:38.708694: step 174750, loss = 1.81 (1105.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:39.882158: step 174760, loss = 1.82 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:41.049626: step 174770, loss = 1.93 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:42.215643: step 174780, loss = 1.93 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:43.388988: step 174790, loss = 2.01 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:44.564497: step 174800, loss = 1.83 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:45.731396: step 174810, loss = 1.86 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:46.904472: step 174820, loss = 1.88 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:48.087520: step 174830, loss = 1.84 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:49.252709: step 17E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1798 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
4840, loss = 1.90 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:50.404064: step 174850, loss = 1.75 (1111.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:35:51.596676: step 174860, loss = 1.94 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:35:52.781211: step 174870, loss = 1.74 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:53.939277: step 174880, loss = 1.83 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:35:55.112456: step 174890, loss = 1.89 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:35:56.289066: step 174900, loss = 1.83 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:57.475174: step 174910, loss = 1.80 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:35:58.653166: step 174920, loss = 1.87 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:35:59.833943: step 174930, loss = 1.88 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:01.015650: step 174940, loss = 1.78 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:02.204058: step 174950, loss = 1.80 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:03.392066: step 174960, loss = 1.72 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:04.599944: step 174970, loss = 1.80 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:36:05.786276: step 174980, loss = 1.96 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:06.968106: step 174990, loss = 1.73 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:08.151107: step 175000, loss = 1.88 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:09.331438: step 175010, loss = 1.90 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:10.522849: step 175020, loss = 1.80 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:11.718309: step 175030, loss = 1.85 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:36:12.907887: step 175040, loss = 1.97 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:14.089710: step 175050, loss = 1.75 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:15.281734: step 175060, loss = 2.03 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:36:16.492868: step 175070, loss = 1.88 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:36:17.669870: step 175080, loss = 1.92 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:18.885620: step 175090, loss = 1.93 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:36:20.093056: step 175100, loss = 1.80 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:36:21.314943: step 175110, loss = 1.85 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:36:22.499138: step 175120, loss = 1.98 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:23.710593: step 175130, loss = 1.88 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:36:24.906190: step 175140, loss = 1.80 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:36:26.087928: step 175150, loss = 1.72 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:27.283064: step 175160, loss = 1.88 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:36:28.445590: step 175170, loss = 2.10 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:36:29.613123: step 175180, loss = 1.84 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:36:30.789712: step 175190, loss = 1.83 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:31.953796: step 175200, loss = 1.89 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:36:33.121983: step 175210, loss = 1.80 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:36:34.264471: step 175220, loss = 2.05 (1120.4 examples/sec; 0.114 sec/batch)
2017-05-05 03:36:35.513027: step 175230, loss = 1.77 (1025.2 examples/sec; 0.125 sec/batch)
2017-05-05 03:36:36.607990: step 175240, loss = 1.86 (1169.0 examples/sec; 0.109 sec/batch)
2017-05-05 03:36:37.790695: step 175250, loss = 1.80 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:38.945496: step 175260, loss = 1.84 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:36:40.093169: step 175270, loss = 1.79 (1115.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:36:41.272508: step 175280, loss = 1.64 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:42.433214: step 175290, loss = 1.89 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:36:43.587511: step 175300, loss = 1.93 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:36:44.769220: step 175310, loss = 1.83 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:45.917597: step 175320, loss = 1.88 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:36:47.071607: step 175330, loss = 1.74 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:36:48.236274: step 175340, loss = 1.84 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:36:49.395812: step 175350, loss = 1.66 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:36:50.556622: step 175360, loss = 2.00 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:36:51.714450: step 175370, loss = 1.89 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:36:52.897101: step 175380, loss = 2.16 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:54.053827: step 175390, loss = 1.92 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:36:55.235137: step 175400, loss = 1.96 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:36:56.386208: step 175410, loss = 2.19 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:36:57.546898: step 175420, loss = 1.86 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:36:58.751873: step 175430, loss = 1.83 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:36:59.899137: step 175440, loss = 1.81 (1115.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:37:01.082779: step 175450, loss = 1.83 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:37:02.248903: step 175460, loss = 1.88 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:03.445323: step 175470, loss = 1.99 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:37:04.629472: step 175480, loss = 1.84 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:37:05.794836: step 175490, loss = 2.10 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:06.990854: step 175500, loss = 1.89 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:37:08.182075: step 175510, loss = 1.95 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:09.366510: step 175520, loss = 1.76 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:37:10.544801: step 175530, loss = 1.68 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:37:11.721960: step 175540, loss = 1.86 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:37:12.913586: step 175550, loss = 1.78 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:14.106064: step 175560, loss = 2.08 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:15.303390: step 175570, loss = 1.88 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:37:16.487495: step 175580, loss = 2.18 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:37:17.694007: step 175590, loss = 1.87 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:37:18.859554: step 175600, loss = 1.83 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:20.032749: step 175610, loss = 2.12 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:21.195416: step 175620, loss = 1.90 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:22.354378: step 175630, loss = 2.00 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:23.526858: step 175640, loss = 1.88 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:24.718658: step 175650, loss = 1.92 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:25.905969: step 175660, loss = 1.83 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:27.089529: step 175670, loss = 1.87 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:37:28.277110: step 175680, loss = 2.01 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:29.452633: step 175690, loss = 1.88 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:37:30.625832: step 175700, loss = 1.93 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:31.791215: step 175710, loss = 1.91 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:32.956792: step 175720, loss = 1.93 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:34.114693: step 175730E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1809 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
, loss = 1.93 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:35.282969: step 175740, loss = 1.88 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:36.462624: step 175750, loss = 1.78 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:37:37.631945: step 175760, loss = 1.78 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:38.813428: step 175770, loss = 1.96 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:37:40.005472: step 175780, loss = 1.93 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:41.192468: step 175790, loss = 1.87 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:37:42.342319: step 175800, loss = 1.78 (1113.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:37:43.509786: step 175810, loss = 1.77 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:44.671878: step 175820, loss = 1.86 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:45.817060: step 175830, loss = 1.85 (1117.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:37:46.989730: step 175840, loss = 1.82 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:48.156099: step 175850, loss = 1.89 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:49.327256: step 175860, loss = 1.83 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:50.481021: step 175870, loss = 1.81 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:37:51.650451: step 175880, loss = 1.93 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:52.801038: step 175890, loss = 2.03 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:37:53.942547: step 175900, loss = 1.90 (1121.3 examples/sec; 0.114 sec/batch)
2017-05-05 03:37:55.111739: step 175910, loss = 1.91 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:56.276719: step 175920, loss = 1.82 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:57.438212: step 175930, loss = 1.82 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:37:58.604486: step 175940, loss = 1.93 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:37:59.790171: step 175950, loss = 1.89 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:00.973955: step 175960, loss = 1.83 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:02.133097: step 175970, loss = 1.87 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:38:03.295863: step 175980, loss = 1.75 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:38:04.468557: step 175990, loss = 1.81 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:38:05.648551: step 176000, loss = 1.90 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:06.831991: step 176010, loss = 1.93 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:08.026099: step 176020, loss = 1.81 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:09.198922: step 176030, loss = 1.98 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:38:10.412682: step 176040, loss = 1.76 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:38:11.594016: step 176050, loss = 1.98 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:12.766983: step 176060, loss = 1.95 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:38:13.940069: step 176070, loss = 1.74 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:38:15.138044: step 176080, loss = 1.85 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:38:16.334086: step 176090, loss = 1.95 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:38:17.513176: step 176100, loss = 1.88 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:18.716077: step 176110, loss = 1.86 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:38:19.900412: step 176120, loss = 1.79 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:21.117525: step 176130, loss = 1.85 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:38:22.310856: step 176140, loss = 1.85 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:23.517902: step 176150, loss = 1.85 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:38:24.712059: step 176160, loss = 1.74 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:25.896919: step 176170, loss = 1.91 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:27.090485: step 176180, loss = 1.81 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:28.290603: step 176190, loss = 1.94 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:38:29.468535: step 176200, loss = 1.99 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:30.655927: step 176210, loss = 1.91 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:31.934421: step 176220, loss = 2.00 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-05 03:38:33.024531: step 176230, loss = 1.94 (1174.2 examples/sec; 0.109 sec/batch)
2017-05-05 03:38:34.225857: step 176240, loss = 1.89 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:38:35.431646: step 176250, loss = 1.91 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:38:36.640969: step 176260, loss = 1.96 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:38:37.807858: step 176270, loss = 1.85 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:38:38.993687: step 176280, loss = 1.91 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:40.173452: step 176290, loss = 1.79 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:41.366421: step 176300, loss = 1.85 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:42.532274: step 176310, loss = 1.85 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:38:43.697961: step 176320, loss = 1.96 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:38:44.874980: step 176330, loss = 1.84 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:46.050738: step 176340, loss = 2.01 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:47.226549: step 176350, loss = 1.89 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:48.385374: step 176360, loss = 2.00 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:38:49.568214: step 176370, loss = 1.73 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:38:50.763571: step 176380, loss = 1.85 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:38:51.954456: step 176390, loss = 1.85 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:53.141078: step 176400, loss = 1.84 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:54.333846: step 176410, loss = 2.09 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:38:55.621180: step 176420, loss = 1.90 (994.3 examples/sec; 0.129 sec/batch)
2017-05-05 03:38:56.753213: step 176430, loss = 1.86 (1130.7 examples/sec; 0.113 sec/batch)
2017-05-05 03:38:57.968367: step 176440, loss = 1.80 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-05 03:38:59.164162: step 176450, loss = 1.93 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:39:00.357534: step 176460, loss = 2.00 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:39:01.547583: step 176470, loss = 1.79 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:39:02.731041: step 176480, loss = 1.86 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:39:03.917212: step 176490, loss = 1.78 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:39:05.087447: step 176500, loss = 1.76 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:06.248222: step 176510, loss = 1.87 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:07.423819: step 176520, loss = 1.93 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:39:08.600429: step 176530, loss = 1.80 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:39:09.751930: step 176540, loss = 1.74 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:39:10.919317: step 176550, loss = 1.85 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:12.110607: step 176560, loss = 1.93 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:39:13.291710: step 176570, loss = 1.79 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:39:14.432919: step 176580, loss = 1.91 (1121.6 examples/sec; 0.114 sec/batch)
2017-05-05 03:39:15.601300: step 176590, loss = 1.84 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:16.777908: step 176600, loss = 1.85 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:39:17.940493: step 176610, loss = 1.81 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:19.107846: step 176620, loss = 1.91 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:20.270814: step 176630, loss = 1.82 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:21.429870: step 176640, loss = 1.72 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:22.579794: step 176650, loss = 1.93 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:39:23.752374: step 176660, loss = 1.90 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:24.915000: step 176670, loss = 1.98 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:26.080390: step 176680, loss = 1.77 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:27.256211: step 176690, loss = 1.91 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:39:28.418605: step 176700, loss = 1.85 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:29.574673: step 176710, loss = 1.75 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:30.734457: step 176720, loss = 1.82 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:31.896666: step 176730, loss = 1.78 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:33.064731: step 176740, loss = 1.99 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:34.218077: step 176750, loss = 1.93 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:39:35.397875: step 176760, loss = 1.67 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:39:36.539318: step 176770, loss = 1.77 (1121.4 examples/sec; 0.114 sec/batch)
2017-05-05 03:39:37.705901: step 176780, loss = 1.92 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:38.837085: step 176790, loss = 1.96 (1131.6 examples/sec; 0.113 sec/batch)
2017-05-05 03:39:39.989022: step 176800, loss = 1.90 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:39:41.164165: step 176810, loss = 1.85 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:39:42.337374: step 176820, loss = 1.97 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:43.520307: step 176830, loss = 1.86 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:39:44.696049: step 176840, loss = 1.74 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:39:45.870757: step 176850, loss = 1.81 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:47.034494: step 176860, loss = 1.93 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:48.208087: step 176870, loss = 2.03 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:39:49.370457: step 176880, loss = 1.88 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:50.525425: step 176890, loss = 2.02 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:39:51.681289: step 176900, loss = 1.92 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:52.839824: step 176910, loss = 1.90 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:39:53.987461: step 176920, loss = 2.00 (1115.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:39:55.164132: step 176930, loss = 1.85 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:39:56.340686: step 176940, loss = 1.80 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:39:57.483185: step 176950, loss = 1.73 (1120.4 examples/sec; 0.114 sec/batch)
2017-05-05 03:39:58.632465: step 176960, loss = 1.94 (1113.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:39:59.808290: step 176970, loss = 1.71 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:00.989206: step 176980, loss = 1.71 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:02.148856: step 176990, loss = 1.94 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:03.342477: step 177000, loss = 1.70 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:40:04.508011: step 177010, loss = 2.19 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:05.672661: step 177020, loss = 1.82 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:06.830166: step 177030, loss = 1.87 (1105.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:07.989455: step 177040, loss = 1.93 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:09.164120: step 177050, loss = 1.83 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:10.298712: step 177060, loss = 1.97 (1128.2 examples/sec; 0.113 sec/batch)
E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1821 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
2017-05-05 03:40:11.454513: step 177070, loss = 1.83 (1107.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:12.610407: step 177080, loss = 1.72 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:13.751535: step 177090, loss = 1.76 (1121.7 examples/sec; 0.114 sec/batch)
2017-05-05 03:40:14.926810: step 177100, loss = 2.03 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:16.103888: step 177110, loss = 1.92 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:17.265966: step 177120, loss = 1.89 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:18.426409: step 177130, loss = 1.88 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:19.592386: step 177140, loss = 1.90 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:20.784787: step 177150, loss = 1.82 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:40:21.956831: step 177160, loss = 2.02 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:23.137606: step 177170, loss = 1.76 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:24.308075: step 177180, loss = 1.87 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:25.474645: step 177190, loss = 1.88 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:26.652445: step 177200, loss = 1.89 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:27.936369: step 177210, loss = 1.87 (996.9 examples/sec; 0.128 sec/batch)
2017-05-05 03:40:29.020542: step 177220, loss = 1.80 (1180.6 examples/sec; 0.108 sec/batch)
2017-05-05 03:40:30.206840: step 177230, loss = 1.75 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:40:31.389404: step 177240, loss = 1.90 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:32.562585: step 177250, loss = 1.84 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:33.735552: step 177260, loss = 1.93 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:34.909898: step 177270, loss = 1.83 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:36.093096: step 177280, loss = 1.90 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:37.273888: step 177290, loss = 1.71 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:38.422507: step 177300, loss = 1.95 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:40:39.594931: step 177310, loss = 1.93 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:40.753525: step 177320, loss = 2.06 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:41.914145: step 177330, loss = 1.80 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:43.102557: step 177340, loss = 1.86 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:40:44.287806: step 177350, loss = 1.78 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:40:45.466520: step 177360, loss = 1.80 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:46.652548: step 177370, loss = 1.65 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:40:47.826415: step 177380, loss = 1.90 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:48.984172: step 177390, loss = 2.02 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:50.136123: step 177400, loss = 1.86 (1111.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:40:51.303710: step 177410, loss = 1.89 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:40:52.452807: step 177420, loss = 1.88 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:40:53.632474: step 177430, loss = 1.93 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:54.812152: step 177440, loss = 1.86 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:55.971604: step 177450, loss = 1.79 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:57.132746: step 177460, loss = 1.78 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:40:58.308043: step 177470, loss = 2.16 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:40:59.467043: step 177480, loss = 1.93 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:41:00.648396: step 177490, loss = 1.87 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:41:01.814941: step 177500, loss = 1.83 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:41:02.976477: step 177510, loss = 1.83 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:41:04.126372: step 177520, loss = 1.79 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:41:05.289697: step 177530, loss = 2.00 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:41:06.435536: step 177540, loss = 1.97 (1117.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:41:07.617669: step 177550, loss = 1.82 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:41:08.783312: step 177560, loss = 1.96 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:41:09.949604: step 177570, loss = 1.92 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:41:11.126292: step 177580, loss = 1.74 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:41:12.286007: step 177590, loss = 1.90 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:41:13.467817: step 177600, loss = 1.88 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:41:14.658536: step 177610, loss = 1.92 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:41:15.857053: step 177620, loss = 1.86 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:41:17.052975: step 177630, loss = 1.75 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:41:18.234571: step 177640, loss = 1.78 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:41:19.429637: step 177650, loss = 1.96 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:41:20.626070: step 177660, loss = 1.70 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:41:21.823143: step 177670, loss = 1.79 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:41:23.031431: step 177680, loss = 1.82 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:41:24.237134: step 177690, loss = 1.96 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:41:25.461911: step 177700, loss = 1.95 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:41:26.674269: step 177710, loss = 1.83 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:41:27.901174: step 177720, loss = 1.86 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-05 03:41:29.126772: step 177730, loss = 1.87 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-05 03:41:30.344502: step 177740, loss = 1.89 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:41:31.584894: step 177750, loss = 1.69 (1031.9 examples/sec; 0.124 sec/batch)
2017-05-05 03:41:32.824525: step 177760, loss = 1.91 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-05 03:41:34.027129: step 177770, loss = 1.93 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:41:35.247182: step 177780, loss = 2.02 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:41:36.465130: step 177790, loss = 1.95 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:41:37.671492: step 177800, loss = 1.93 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:41:38.836050: step 177810, loss = 1.81 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:41:40.046755: step 177820, loss = 1.95 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:41:41.256870: step 177830, loss = 1.81 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:41:42.457031: step 177840, loss = 1.80 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:41:43.677800: step 177850, loss = 1.81 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:41:44.903154: step 177860, loss = 1.82 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-05 03:41:46.115491: step 177870, loss = 1.94 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:41:47.330318: step 177880, loss = 1.74 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:41:48.568401: step 177890, loss = 1.84 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-05 03:41:49.794571: step 177900, loss = 1.72 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:41:51.022969: step 177910, loss = 1.87 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:41:52.246423: step 177920, loss = 1.90 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-05 03:41:53.471285: step 177930, loss = 1.85 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:41:54.689928: step 177940, loss = 1.80 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:41:55.926275: step 177950, loss = 1.80 (1035.3 examples/sec; 0.124 sec/batch)
2017-E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1833 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
05-05 03:41:57.164647: step 177960, loss = 1.81 (1033.6 examples/sec; 0.124 sec/batch)
2017-05-05 03:41:58.378862: step 177970, loss = 1.75 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:41:59.592194: step 177980, loss = 1.88 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:42:00.806853: step 177990, loss = 1.93 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:42:02.012391: step 178000, loss = 1.97 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:42:03.213078: step 178010, loss = 1.86 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:42:04.405490: step 178020, loss = 1.75 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:05.623199: step 178030, loss = 1.94 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:42:06.839869: step 178040, loss = 1.94 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:42:08.059336: step 178050, loss = 1.89 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:42:09.269202: step 178060, loss = 2.04 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:42:10.485630: step 178070, loss = 1.84 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-05 03:42:11.712631: step 178080, loss = 1.84 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 03:42:12.924407: step 178090, loss = 1.89 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:42:14.115046: step 178100, loss = 1.81 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:15.325573: step 178110, loss = 1.91 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:42:16.541286: step 178120, loss = 1.74 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:42:17.737911: step 178130, loss = 1.86 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:42:18.962201: step 178140, loss = 1.85 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:42:20.172742: step 178150, loss = 1.85 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:42:21.375477: step 178160, loss = 1.78 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:42:22.554121: step 178170, loss = 1.73 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:23.732733: step 178180, loss = 1.85 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:24.916346: step 178190, loss = 1.87 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:26.176153: step 178200, loss = 1.95 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-05 03:42:27.284688: step 178210, loss = 1.91 (1154.7 examples/sec; 0.111 sec/batch)
2017-05-05 03:42:28.486866: step 178220, loss = 1.84 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:42:29.685299: step 178230, loss = 1.80 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:42:30.877671: step 178240, loss = 1.93 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:32.073718: step 178250, loss = 1.77 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:42:33.260007: step 178260, loss = 1.84 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:34.463888: step 178270, loss = 1.81 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:42:35.653728: step 178280, loss = 1.81 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:36.841573: step 178290, loss = 1.96 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:38.027170: step 178300, loss = 1.95 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:39.203728: step 178310, loss = 1.79 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:40.395013: step 178320, loss = 1.86 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:41.568735: step 178330, loss = 1.91 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:42:42.731963: step 178340, loss = 1.91 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:42:43.901584: step 178350, loss = 1.93 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:42:45.085194: step 178360, loss = 1.91 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:46.275041: step 178370, loss = 1.91 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:47.457087: step 178380, loss = 1.79 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:48.623927: step 178390, loss = 1.80 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:42:49.799576: step 178400, loss = 1.82 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:50.978142: step 178410, loss = 2.00 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:52.151583: step 178420, loss = 1.92 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:42:53.335194: step 178430, loss = 1.95 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:54.523113: step 178440, loss = 1.85 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:42:55.704179: step 178450, loss = 1.76 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:56.902779: step 178460, loss = 1.87 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:42:58.083558: step 178470, loss = 1.88 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:42:59.280609: step 178480, loss = 1.76 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:00.461739: step 178490, loss = 2.15 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:01.671352: step 178500, loss = 1.92 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:43:02.857335: step 178510, loss = 1.81 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:04.056502: step 178520, loss = 2.01 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:05.242269: step 178530, loss = 2.05 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:06.460607: step 178540, loss = 1.87 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:43:07.662987: step 178550, loss = 1.83 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:08.853433: step 178560, loss = 1.95 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:10.032475: step 178570, loss = 1.88 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:11.215611: step 178580, loss = 1.85 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:12.406083: step 178590, loss = 1.98 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:13.580729: step 178600, loss = 1.79 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:14.748991: step 178610, loss = 1.78 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:15.937623: step 178620, loss = 2.09 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:17.123663: step 178630, loss = 1.89 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:18.275334: step 178640, loss = 1.71 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:43:19.461167: step 178650, loss = 1.80 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:20.645828: step 178660, loss = 1.83 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:21.841462: step 178670, loss = 1.83 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:23.042093: step 178680, loss = 1.74 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:24.249328: step 178690, loss = 1.92 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:43:25.460905: step 178700, loss = 1.83 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:43:26.655036: step 178710, loss = 2.00 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:27.888861: step 178720, loss = 1.78 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-05 03:43:29.120183: step 178730, loss = 1.81 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-05 03:43:30.320912: step 178740, loss = 1.81 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:31.516776: step 178750, loss = 1.98 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:32.705967: step 178760, loss = 1.84 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:33.885478: step 178770, loss = 1.80 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:35.089354: step 178780, loss = 1.71 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:36.276080: step 178790, loss = 1.97 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:37.458064: step 178800, loss = 1.78 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:38.628448: step 178810, loss = 1.91 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:39.815955: step 178820, loss = 1.89 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:40.999165: step 178830, loss = 1.95 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:42.180990: step 178840, loss = 1.91 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:43.362617: step 178850, loss = 1.68 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:44.554816: step 178860, loss = 1.77 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:45.720177: step 178870, loss = 1.86 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:46.892462: step 178880, loss = 1.91 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:48.064705: step 178890, loss = 1.93 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:49.251818: step 178900, loss = 2.05 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:50.425514: step 178910, loss = 1.97 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:51.608388: step 178920, loss = 1.86 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:43:52.775683: step 178930, loss = 1.82 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:53.968920: step 178940, loss = 1.65 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:43:55.138596: step 178950, loss = 1.82 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:56.336486: step 178960, loss = 1.76 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:43:57.506543: step 178970, loss = 1.97 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:58.679886: step 178980, loss = 1.95 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:43:59.847803: step 178990, loss = 1.91 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:44:01.049473: step 179000, loss = 2.02 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:02.253986: step 179010, loss = 1.84 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:03.448333: step 179020, loss = 1.87 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:04.638475: step 179030, loss = 1.81 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:05.825992: step 179040, loss = 1.88 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:07.042951: step 179050, loss = 1.84 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:44:08.239160: step 179060, loss = 1.94 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:09.438541: step 179070, loss = 1.98 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:10.630532: step 179080, loss = 1.95 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:11.817732: step 179090, loss = 1.94 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:12.998314: step 179100, loss = 1.75 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:44:14.183137: step 179110, loss = 1.89 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:44:15.357324: step 179120, loss = 1.82 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:44:16.534978: step 179130, loss = 1.76 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:44:17.705268: step 179140, loss = 1.84 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:44:18.899188: step 179150, loss = 1.92 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:20.101034: step 179160, loss = 1.96 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:21.287310: step 179170, loss = 1.91 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:22.473675: step 179180, loss = 1.97 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:23.767752: step 179190, loss = 1.78 (989.1 examples/sec; 0.129 sec/batch)
2017-05-05 03:44:24.878272: step 179200, loss = 1.86 (1152.7 examples/sec; 0.111 sec/batch)
2017-05-05 03:44:26.083932: step 179210, loss = 1.76 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:27.285178: step 179220, loss = 1.79 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:28.511265: step 179230, loss = 2.02 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:44:29.738538: step 179240, loss = 1.79 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-05 03:44:30.965525: step 179250, loss = 1.73 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-05 03:44:32.169738: step 179260, loss = 1.82 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:33.393874: step 179270, loss = 1.99 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-05 03:44:34.618221: step 179280, loss = 1.80 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:44:35.842989: step 179290, loss = 1.86 (1045E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1845 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:44:37.042506: step 179300, loss = 1.96 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:38.252745: step 179310, loss = 1.86 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:39.456079: step 179320, loss = 1.87 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:40.668616: step 179330, loss = 1.85 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:41.876843: step 179340, loss = 1.87 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:43.101549: step 179350, loss = 1.96 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:44:44.315340: step 179360, loss = 1.86 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:45.533144: step 179370, loss = 1.76 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-05 03:44:46.740271: step 179380, loss = 1.93 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:47.941651: step 179390, loss = 1.86 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:44:49.161992: step 179400, loss = 1.83 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-05 03:44:50.352475: step 179410, loss = 1.79 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:51.569199: step 179420, loss = 1.90 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:44:52.787097: step 179430, loss = 1.98 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-05 03:44:53.967788: step 179440, loss = 1.85 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:44:55.173342: step 179450, loss = 1.92 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:44:56.399035: step 179460, loss = 1.86 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-05 03:44:57.587135: step 179470, loss = 1.89 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:44:58.768666: step 179480, loss = 1.80 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:44:59.967217: step 179490, loss = 1.97 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 03:45:01.157954: step 179500, loss = 1.76 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:45:02.342099: step 179510, loss = 1.79 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:03.514474: step 179520, loss = 1.87 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:04.710288: step 179530, loss = 1.73 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:45:05.888195: step 179540, loss = 1.82 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:07.080807: step 179550, loss = 1.87 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:45:08.257871: step 179560, loss = 2.04 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:09.419364: step 179570, loss = 1.99 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:10.579990: step 179580, loss = 1.79 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:11.763476: step 179590, loss = 1.79 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:12.931693: step 179600, loss = 1.87 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:14.092460: step 179610, loss = 1.95 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:15.260635: step 179620, loss = 1.92 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:16.410104: step 179630, loss = 1.71 (1113.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:45:17.567701: step 179640, loss = 1.77 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:18.723824: step 179650, loss = 1.97 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:19.877646: step 179660, loss = 1.97 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:45:21.044680: step 179670, loss = 1.83 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:22.207428: step 179680, loss = 1.88 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:23.377868: step 179690, loss = 1.88 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:24.553949: step 179700, loss = 1.80 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:25.722984: step 179710, loss = 1.88 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:26.887526: step 179720, loss = 1.84 (1099.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:28.083953: step 179730, loss = 2.04 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:45:29.254687: step 179740, loss = 1.78 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:30.415880: step 179750, loss = 1.93 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:31.584214: step 179760, loss = 1.86 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:32.766184: step 179770, loss = 1.70 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:33.933270: step 179780, loss = 1.78 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:35.143152: step 179790, loss = 1.75 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:45:36.322462: step 179800, loss = 1.92 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:37.522649: step 179810, loss = 1.72 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-05 03:45:38.704196: step 179820, loss = 1.84 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:39.870786: step 179830, loss = 1.78 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:41.036554: step 179840, loss = 1.90 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:42.192772: step 179850, loss = 1.85 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:43.359613: step 179860, loss = 1.85 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:44.525810: step 179870, loss = 1.98 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:45.699904: step 179880, loss = 1.93 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:46.880903: step 179890, loss = 1.90 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:48.069107: step 179900, loss = 1.79 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:45:49.238617: step 179910, loss = 1.78 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:50.415426: step 179920, loss = 1.85 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:45:51.605415: step 179930, loss = 1.90 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:45:52.765551: step 179940, loss = 1.86 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:53.928666: step 179950, loss = 1.83 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:55.102721: step 179960, loss = 1.89 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:56.267376: step 179970, loss = 1.81 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:57.426311: step 179980, loss = 1.88 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:45:58.600305: step 179990, loss = 1.95 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:45:59.741284: step 180000, loss = 1.82 (1121.8 examples/sec; 0.114 sec/batch)
2017-05-05 03:46:00.895061: step 180010, loss = 1.99 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:46:02.070080: step 180020, loss = 1.82 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:03.214812: step 180030, loss = 1.88 (1118.2 examples/sec; 0.114 sec/batch)
2017-05-05 03:46:04.410860: step 180040, loss = 1.76 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:46:05.572366: step 180050, loss = 1.66 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:06.736671: step 180060, loss = 1.88 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:07.899314: step 180070, loss = 1.81 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:09.065604: step 180080, loss = 1.99 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:10.228867: step 180090, loss = 1.90 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:11.385537: step 180100, loss = 1.79 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:12.551475: step 180110, loss = 1.90 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:13.697771: step 180120, loss = 1.83 (1116.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:46:14.870135: step 180130, loss = 1.79 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:16.013118: step 180140, loss = 1.85 (1119.9 examples/sec; 0.114 sec/batch)
2017-05-05 03:46:17.193867: step 180150, loss = 1.89 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:18.360161: step 180160, loss = 1.85 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:19.541759: step 180170, loss = 1.93 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:20.833015: step 180180, loss = 1.97 (991.3 examples/sec; 0.129 sec/batch)
2017-05-05 03:46:21.918998: step 180190, loss = 1.93 (1178.7 examples/sec; 0.109 sec/batch)
2017-05-05 03:46:23.098368: step 180200, loss = 1.84 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:24.304356: step 180210, loss = 1.71 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:46:25.514705: step 180220, loss = 1.84 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:46:26.722832: step 180230, loss = 1.97 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:46:27.905219: step 180240, loss = 1.81 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:29.080873: step 180250, loss = 1.82 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:30.266208: step 180260, loss = 1.81 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:46:31.468846: step 180270, loss = 1.76 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 03:46:32.644273: step 180280, loss = 1.92 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:33.812756: step 180290, loss = 1.72 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:34.974198: step 180300, loss = 1.78 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:36.145327: step 180310, loss = 1.82 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:37.311833: step 180320, loss = 2.08 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:38.484372: step 180330, loss = 1.97 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:39.659626: step 180340, loss = 1.97 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:40.823308: step 180350, loss = 1.69 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:41.977992: step 180360, loss = 1.77 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:46:43.137989: step 180370, loss = 1.76 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:44.309056: step 180380, loss = 1.70 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:45.474720: step 180390, loss = 1.94 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:46.637591: step 180400, loss = 1.92 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:47.816844: step 180410, loss = 1.76 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:48.988134: step 180420, loss = 2.09 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:50.168583: step 180430, loss = 1.80 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:51.333430: step 180440, loss = 1.92 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:52.510326: step 180450, loss = 1.78 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:46:53.669139: step 180460, loss = 2.00 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:54.826447: step 180470, loss = 1.78 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:46:56.011952: step 180480, loss = 1.72 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-05 03:46:57.165329: step 180490, loss = 1.80 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:46:58.338528: step 180500, loss = 1.85 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:46:59.501732: step 180510, loss = 1.85 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:00.656516: step 180520, loss = 1.90 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:01.812362: step 180530, loss = 1.80 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:02.978132: step 180540, loss = 1.85 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:04.148892: step 180550, loss = 1.92 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:05.318780: step 180560, loss = 1.82 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:06.476114: step 180570, loss = 1.91 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:07.682545: step 180580, loss = 1.84 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:47:08.851309: step 180590, loss = 1.91 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:10.022735: step 180600, loss = 1.74 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:11.171715: step 180610, loss = 1.89 (1114.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:12.330850: step 180620, loss = 1.84 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:13E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1857 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
.485605: step 180630, loss = 1.79 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:14.636004: step 180640, loss = 1.90 (1112.7 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:15.800423: step 180650, loss = 1.98 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:16.956975: step 180660, loss = 1.84 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:18.125291: step 180670, loss = 1.87 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:19.278213: step 180680, loss = 1.75 (1110.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:20.444664: step 180690, loss = 1.85 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:21.596499: step 180700, loss = 1.86 (1111.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:22.770202: step 180710, loss = 1.75 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:23.935003: step 180720, loss = 1.87 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:25.105933: step 180730, loss = 1.78 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:26.280538: step 180740, loss = 1.70 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:27.459651: step 180750, loss = 1.89 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:47:28.623980: step 180760, loss = 1.87 (1099.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:29.806573: step 180770, loss = 1.81 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:47:30.989107: step 180780, loss = 1.86 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:47:32.177487: step 180790, loss = 1.73 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:47:33.342779: step 180800, loss = 1.88 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:34.506173: step 180810, loss = 1.92 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:35.664877: step 180820, loss = 1.76 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:36.822480: step 180830, loss = 1.92 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:37.961662: step 180840, loss = 1.78 (1123.6 examples/sec; 0.114 sec/batch)
2017-05-05 03:47:39.133490: step 180850, loss = 1.86 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:40.311397: step 180860, loss = 1.83 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-05 03:47:41.477734: step 180870, loss = 1.92 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:47:42.632345: step 180880, loss = 1.92 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:43.786989: step 180890, loss = 1.76 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:44.935683: step 180900, loss = 1.74 (1114.3 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:46.075784: step 180910, loss = 1.96 (1122.7 examples/sec; 0.114 sec/batch)
2017-05-05 03:47:47.230532: step 180920, loss = 1.87 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:48.390268: step 180930, loss = 1.80 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:49.538893: step 180940, loss = 1.97 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:50.699255: step 180950, loss = 1.75 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:51.860526: step 180960, loss = 1.98 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:53.010684: step 180970, loss = 1.84 (1112.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:54.167956: step 180980, loss = 1.81 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:47:55.322194: step 180990, loss = 1.91 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:56.473917: step 181000, loss = 1.91 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:57.622514: step 181010, loss = 1.86 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:58.775168: step 181020, loss = 1.75 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:47:59.930040: step 181030, loss = 1.80 (1108.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:01.088840: step 181040, loss = 1.77 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:02.238094: step 181050, loss = 1.97 (1113.8 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:03.380788: step 181060, loss = 1.74 (1120.2 examples/sec; 0.114 sec/batch)
2017-05-05 03:48:04.539034: step 181070, loss = 1.95 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:05.688171: step 181080, loss = 1.81 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:06.856451: step 181090, loss = 1.87 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:08.028518: step 181100, loss = 1.98 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:09.195799: step 181110, loss = 1.90 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:10.380474: step 181120, loss = 1.83 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:48:11.556872: step 181130, loss = 1.93 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:48:12.733570: step 181140, loss = 2.01 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 03:48:13.892962: step 181150, loss = 1.84 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:15.063837: step 181160, loss = 1.89 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:16.332651: step 181170, loss = 1.83 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-05 03:48:17.395443: step 181180, loss = 1.86 (1204.4 examples/sec; 0.106 sec/batch)
2017-05-05 03:48:18.566772: step 181190, loss = 1.81 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:19.728663: step 181200, loss = 1.88 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:20.913989: step 181210, loss = 1.79 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:48:22.050870: step 181220, loss = 1.75 (1125.9 examples/sec; 0.114 sec/batch)
2017-05-05 03:48:23.223608: step 181230, loss = 1.92 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:24.406595: step 181240, loss = 1.84 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:48:25.530370: step 181250, loss = 1.74 (1139.0 examples/sec; 0.112 sec/batch)
2017-05-05 03:48:26.698839: step 181260, loss = 1.87 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:27.870587: step 181270, loss = 1.87 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:29.044481: step 181280, loss = 1.76 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:30.217766: step 181290, loss = 1.92 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:31.383942: step 181300, loss = 1.79 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:32.549115: step 181310, loss = 1.91 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:33.697184: step 181320, loss = 1.99 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:34.851467: step 181330, loss = 1.91 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:35.997767: step 181340, loss = 1.83 (1116.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:37.173159: step 181350, loss = 1.87 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:48:38.316933: step 181360, loss = 1.83 (1119.1 examples/sec; 0.114 sec/batch)
2017-05-05 03:48:39.490125: step 181370, loss = 1.81 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:40.649528: step 181380, loss = 1.90 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:41.828218: step 181390, loss = 1.93 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-05 03:48:42.989648: step 181400, loss = 1.77 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:44.164039: step 181410, loss = 1.81 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:45.322763: step 181420, loss = 1.81 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:46.476489: step 181430, loss = 1.84 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:47.633855: step 181440, loss = 1.91 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:48.783383: step 181450, loss = 1.79 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:49.932435: step 181460, loss = 1.82 (1114.0 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:51.094596: step 181470, loss = 1.87 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:52.273872: step 181480, loss = 1.99 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-05 03:48:53.443076: step 181490, loss = 1.94 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 03:48:54.593541: step 181500, loss = 1.76 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-05 03:48:55.752978: step 181510, loss = 1.82 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:56.914840: step 181520, loss = 1.82 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:48:58.052644: step 181530, loss = 1.77 (1125.0 examples/sec; 0.114 sec/batch)
2017-05-05 03:48:59.222346: step 181540, loss = 2.04 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:00.385851: step 181550, loss = 1.89 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:49:01.547529: step 181560, loss = 1.82 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:49:02.726970: step 181570, loss = 1.87 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:49:03.897729: step 181580, loss = 1.84 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:05.069610: step 181590, loss = 1.87 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:06.233377: step 181600, loss = 1.99 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:49:07.406737: step 181610, loss = 2.04 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:08.573834: step 181620, loss = 1.96 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:09.740563: step 181630, loss = 1.81 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:10.923962: step 181640, loss = 1.84 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:49:12.109915: step 181650, loss = 1.80 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-05 03:49:13.291573: step 181660, loss = 1.78 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-05 03:49:14.450818: step 181670, loss = 1.72 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 03:49:15.622673: step 181680, loss = 1.83 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:16.810727: step 181690, loss = 1.87 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:49:17.984866: step 181700, loss = 1.91 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:19.134650: step 181710, loss = 1.93 (1113.2 examples/sec; 0.115 sec/batch)
2017-05-05 03:49:20.305511: step 181720, loss = 1.90 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:21.470899: step 181730, loss = 2.01 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:22.637416: step 181740, loss = 1.84 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:23.789463: step 181750, loss = 1.76 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 03:49:24.944674: step 181760, loss = 1.75 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 03:49:26.124286: step 181770, loss = 1.86 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:49:27.284813: step 181780, loss = 2.07 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-05 03:49:28.452611: step 181790, loss = 1.88 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-05 03:49:29.612987: step 181800, loss = 1.93 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 03:49:30.845725: step 181810, loss = 1.91 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-05 03:49:32.079980: step 181820, loss = 1.77 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-05 03:49:33.341662: step 181830, loss = 1.86 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-05 03:49:34.584865: step 181840, loss = 1.99 (1029.6 examples/sec; 0.124 sec/batch)
2017-05-05 03:49:35.845916: step 181850, loss = 1.85 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-05 03:49:37.100527: step 181860, loss = 1.77 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-05 03:49:38.343055: step 181870, loss = 1.92 (1030.2 examples/sec; 0.124 sec/batch)
2017-05-05 03:49:39.617374: step 181880, loss = 1.82 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-05 03:49:40.861556: step 181890, loss = 1.81 (1028.8 examples/sec; 0.124 sec/batch)
2017-05-05 03:49:42.064683: step 181900, loss = 1.74 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:49:43.280540: step 181910, loss = 1.89 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:49:44.465111: step 181920, loss = 1.84 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:49:45.690815: step 181930, loss = 1.97 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-05 03:49:46.970222: step 181940, loss = 1.84 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-05 03:49:48.224704: step 181950, loss = 1.82 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-05 03:49:49.472151: step 181960, loss = 1.94 (1026.1 examples/sE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1870 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
ec; 0.125 sec/batch)
2017-05-05 03:49:50.658659: step 181970, loss = 1.88 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 03:49:51.856249: step 181980, loss = 1.77 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 03:49:53.071132: step 181990, loss = 1.86 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-05 03:49:54.259642: step 182000, loss = 1.79 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-05 03:49:55.471435: step 182010, loss = 1.89 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 03:49:56.684458: step 182020, loss = 2.12 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:49:57.893772: step 182030, loss = 1.93 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:49:59.110838: step 182040, loss = 1.86 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 03:50:00.316031: step 182050, loss = 1.89 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:01.517641: step 182060, loss = 2.03 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:02.713450: step 182070, loss = 1.75 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:03.933133: step 182080, loss = 1.85 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:50:05.116043: step 182090, loss = 1.83 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-05 03:50:06.324559: step 182100, loss = 1.82 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:07.510335: step 182110, loss = 1.71 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-05 03:50:08.704156: step 182120, loss = 1.87 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-05 03:50:09.902169: step 182130, loss = 1.82 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:11.107410: step 182140, loss = 1.79 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:12.301010: step 182150, loss = 1.98 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 03:50:13.569550: step 182160, loss = 1.82 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-05 03:50:14.670739: step 182170, loss = 1.92 (1162.4 examples/sec; 0.110 sec/batch)
2017-05-05 03:50:15.872558: step 182180, loss = 1.82 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:17.092168: step 182190, loss = 1.78 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-05 03:50:18.287681: step 182200, loss = 1.76 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:19.498931: step 182210, loss = 1.90 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:20.711363: step 182220, loss = 1.88 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:21.918479: step 182230, loss = 1.72 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:23.144665: step 182240, loss = 1.81 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-05 03:50:24.347186: step 182250, loss = 1.83 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:25.553053: step 182260, loss = 2.02 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:26.751870: step 182270, loss = 1.88 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:27.966628: step 182280, loss = 2.00 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:29.179981: step 182290, loss = 1.85 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:30.394567: step 182300, loss = 1.88 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:31.589802: step 182310, loss = 1.90 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:32.790138: step 182320, loss = 1.80 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:33.983212: step 182330, loss = 1.67 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-05 03:50:35.200179: step 182340, loss = 1.98 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 03:50:36.407611: step 182350, loss = 1.81 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-05 03:50:37.567289: step 182360, loss = 1.95 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 03:50:38.754051: step 182370, loss = 1.88 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:50:39.956640: step 182380, loss = 1.80 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:41.161477: step 182390, loss = 1.81 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:42.338455: step 182400, loss = 1.71 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-05 03:50:43.541537: step 182410, loss = 1.77 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 03:50:44.771497: step 182420, loss = 1.86 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-05 03:50:45.944384: step 182430, loss = 1.93 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 03:50:47.135512: step 182440, loss = 1.99 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 03:50:48.320000: step 182450, loss = 1.92 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 03:50:49.477606: step 182460, loss = 1.81 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-05 03:50:50.635497: step 182470, loss = 1.99 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-05 03:50:51.809925: step 182480, loss = 1.96 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-05 03:50:52.987161: step 182490, loss = 1.84 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 03:50:54.173339: step 182500, loss = 1.77 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-05 03:50:55.556971: step 182510, loss = 1.87 (925.1 examples/sec; 0.138 sec/batch)
2017-05-05 03:50:57.023045: step 182520, loss = 2.09 (873.1 examples/sec; 0.147 sec/batch)
2017-05-05 03:50:58.566590: step 182530, loss = 1.90 (829.3 examples/sec; 0.154 sec/batch)
2017-05-05 03:51:00.136365: step 182540, loss = 1.79 (815.4 examples/sec; 0.157 sec/batch)
2017-05-05 03:51:01.659381: step 182550, loss = 1.85 (840.4 examples/sec; 0.152 sec/batch)
2017-05-05 03:51:03.191314: step 182560, loss = 1.83 (835.5 examples/sec; 0.153 sec/batch)
2017-05-05 03:51:04.761786: step 182570, loss = 1.95 (815.0 examples/sec; 0.157 sec/batch)
2017-05-05 03:51:06.271466: step 182580, loss = 1.79 (847.9 examples/sec; 0.151 sec/batch)
2017-05-05 03:51:07.772697: step 182590, loss = 1.82 (852.6 examples/sec; 0.150 sec/batch)
2017-05-05 03:51:09.304491: step 182600, loss = 1.75 (835.6 examples/sec; 0.153 sec/batch)
2017-05-05 03:51:10.793728: step 182610, loss = 1.89 (859.5 examples/sec; 0.149 sec/batch)
2017-05-05 03:51:12.311369: step 182620, loss = 1.69 (843.4 examples/sec; 0.152 sec/batch)
2017-05-05 03:51:13.791775: step 182630, loss = 1.84 (864.6 examples/sec; 0.148 sec/batch)
2017-05-05 03:51:15.313317: step 182640, loss = 1.85 (841.3 examples/sec; 0.152 sec/batch)
2017-05-05 03:51:16.796140: step 182650, loss = 1.71 (863.2 examples/sec; 0.148 sec/batch)
2017-05-05 03:51:18.355642: step 182660, loss = 1.98 (820.8 examples/sec; 0.156 sec/batch)
2017-05-05 03:51:19.848444: step 182670, loss = 1.84 (857.4 examples/sec; 0.149 sec/batch)
2017-05-05 03:51:21.351922: step 182680, loss = 1.88 (851.4 examples/sec; 0.150 sec/batch)
2017-05-05 03:51:22.873361: step 182690, loss = 1.87 (841.3 examples/sec; 0.152 sec/batch)
2017-05-05 03:51:24.338687: step 182700, loss = 1.85 (873.5 examples/sec; 0.147 sec/batch)
2017-05-05 03:51:25.839882: step 182710, loss = 1.78 (852.7 examples/sec; 0.150 sec/batch)
2017-05-05 03:51:27.296473: step 182720, loss = 1.74 (878.8 examples/sec; 0.146 sec/batch)
2017-05-05 03:51:28.826511: step 182730, loss = 1.85 (836.6 examples/sec; 0.153 sec/batch)
2017-05-05 03:51:30.365093: step 182740, loss = 1.83 (831.9 examples/sec; 0.154 sec/batch)
2017-05-05 03:51:31.880125: step 182750, loss = 1.75 (844.9 examples/sec; 0.152 sec/batch)
2017-05-05 03:51:33.338395: step 182760, loss = 1.90 (877.8 examples/sec; 0.146 sec/batch)
2017-05-05 03:51:34.870302: step 182770, loss = 1.92 (835.6 examples/sec; 0.153 sec/batch)
2017-05-05 03:51:36.366912: step 182780, loss = 1.93 (855.3 examples/sec; 0.150 sec/batch)
2017-05-05 03:51:37.839278: step 182790, loss = 1.89 (869.3 examples/sec; 0.147 sec/batch)
2017-05-05 03:51:39.387214: step 182800, loss = 1.89 (826.9 examples/sec; 0.155 sec/batch)
2017-05-05 03:51:40.903763: step 182810, loss = 1.79 (844.0 examples/sec; 0.152 sec/batch)
2017-05-05 03:51:42.454495: step 182820, loss = 2.02 (825.4 examples/sec; 0.155 sec/batch)
2017-05-05 03:51:43.954593: step 182830, loss = 1.77 (853.3 examples/sec; 0.150 sec/batch)
2017-05-05 03:51:45.445011: step 182840, loss = 1.90 (858.8 examples/sec; 0.149 sec/batch)
2017-05-05 03:51:46.955678: step 182850, loss = 1.92 (847.3 examples/sec; 0.151 sec/batch)
2017-05-05 03:51:4E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1880 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
8.452225: step 182860, loss = 1.99 (855.3 examples/sec; 0.150 sec/batch)
2017-05-05 03:51:49.983721: step 182870, loss = 1.85 (835.8 examples/sec; 0.153 sec/batch)
2017-05-05 03:51:51.497707: step 182880, loss = 1.76 (845.5 examples/sec; 0.151 sec/batch)
2017-05-05 03:51:53.013488: step 182890, loss = 1.85 (844.4 examples/sec; 0.152 sec/batch)
2017-05-05 03:51:54.540525: step 182900, loss = 1.88 (838.2 examples/sec; 0.153 sec/batch)
2017-05-05 03:51:56.091674: step 182910, loss = 1.94 (825.2 examples/sec; 0.155 sec/batch)
2017-05-05 03:51:57.619224: step 182920, loss = 1.91 (837.9 examples/sec; 0.153 sec/batch)
2017-05-05 03:51:59.205670: step 182930, loss = 1.89 (806.8 examples/sec; 0.159 sec/batch)
2017-05-05 03:52:00.774543: step 182940, loss = 1.84 (815.9 examples/sec; 0.157 sec/batch)
2017-05-05 03:52:02.289441: step 182950, loss = 1.79 (844.9 examples/sec; 0.151 sec/batch)
2017-05-05 03:52:03.803143: step 182960, loss = 1.95 (845.6 examples/sec; 0.151 sec/batch)
2017-05-05 03:52:05.302662: step 182970, loss = 1.91 (853.6 examples/sec; 0.150 sec/batch)
2017-05-05 03:52:06.862512: step 182980, loss = 1.85 (820.6 examples/sec; 0.156 sec/batch)
2017-05-05 03:52:08.356942: step 182990, loss = 1.88 (856.5 examples/sec; 0.149 sec/batch)
2017-05-05 03:52:09.914846: step 183000, loss = 1.95 (821.6 examples/sec; 0.156 sec/batch)
2017-05-05 03:52:11.413175: step 183010, loss = 1.88 (854.3 examples/sec; 0.150 sec/batch)
2017-05-05 03:52:12.963954: step 183020, loss = 1.80 (825.4 examples/sec; 0.155 sec/batch)
2017-05-05 03:52:14.512305: step 183030, loss = 2.04 (826.7 examples/sec; 0.155 sec/batch)
2017-05-05 03:52:16.111429: step 183040, loss = 1.85 (800.4 examples/sec; 0.160 sec/batch)
2017-05-05 03:52:17.664944: step 183050, loss = 1.95 (823.9 examples/sec; 0.155 sec/batch)
2017-05-05 03:52:19.251449: step 183060, loss = 1.85 (806.8 examples/sec; 0.159 sec/batch)
2017-05-05 03:52:20.762325: step 183070, loss = 1.80 (847.2 examples/sec; 0.151 sec/batch)
2017-05-05 03:52:22.282980: step 183080, loss = 1.83 (841.7 examples/sec; 0.152 sec/batch)
2017-05-05 03:52:23.871597: step 183090, loss = 1.93 (805.7 examples/sec; 0.159 sec/batch)
2017-05-05 03:52:25.379772: step 183100, loss = 2.00 (848.7 examples/sec; 0.151 sec/batch)
2017-05-05 03:52:26.930573: step 183110, loss = 1.80 (825.4 examples/sec; 0.155 sec/batch)
2017-05-05 03:52:28.464416: step 183120, loss = 1.77 (834.5 examples/sec; 0.153 sec/batch)
2017-05-05 03:52:30.061200: step 183130, loss = 1.95 (801.6 examples/sec; 0.160 sec/batch)
2017-05-05 03:52:31.630511: step 183140, loss = 1.83 (815.6 examples/sec; 0.157 sec/batch)
2017-05-05 03:52:33.267149: step 183150, loss = 1.81 (782.1 examples/sec; 0.164 sec/batch)
2017-05-05 03:52:34.688405: step 183160, loss = 1.93 (900.6 examples/sec; 0.142 sec/batch)
2017-05-05 03:52:36.208095: step 183170, loss = 1.80 (842.3 examples/sec; 0.152 sec/batch)
2017-05-05 03:52:37.740830: step 183180, loss = 1.87 (835.1 examples/sec; 0.153 sec/batch)
2017-05-05 03:52:39.313548: step 183190, loss = 1.70 (813.9 examples/sec; 0.157 sec/batch)
2017-05-05 03:52:40.880905: step 183200, loss = 1.82 (816.7 examples/sec; 0.157 sec/batch)
2017-05-05 03:52:42.445342: step 183210, loss = 1.97 (818.2 examples/sec; 0.156 sec/batch)
2017-05-05 03:52:44.010409: step 183220, loss = 1.97 (817.9 examples/sec; 0.157 sec/batch)
2017-05-05 03:52:45.568579: step 183230, loss = 1.98 (821.5 examples/sec; 0.156 sec/batch)
2017-05-05 03:52:47.120597: step 183240, loss = 1.79 (824.7 examples/sec; 0.155 sec/batch)
2017-05-05 03:52:48.612658: step 183250, loss = 1.84 (857.9 examples/sec; 0.149 sec/batch)
2017-05-05 03:52:50.181714: step 183260, loss = 1.84 (815.8 examples/sec; 0.157 sec/batch)
2017-05-05 03:52:51.725039: step 183270, loss = 1.76 (829.4 examples/sec; 0.154 sec/batch)
2017-05-05 03:52:53.266278: step 183280, loss = 1.93 (830.5 examples/sec; 0.154 sec/batch)
2017-05-05 03:52:54.788442: step 183290, loss = 1.76 (840.9 examples/sec; 0.152 sec/batch)
2017-05-05 03:52:56.322385: step 183300, loss = 1.79 (834.4 examples/sec; 0.153 sec/batch)
2017-05-05 03:52:57E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1888 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
.848613: step 183310, loss = 1.87 (838.7 examples/sec; 0.153 sec/batch)
2017-05-05 03:52:59.434832: step 183320, loss = 1.79 (806.9 examples/sec; 0.159 sec/batch)
2017-05-05 03:53:00.942650: step 183330, loss = 1.70 (848.9 examples/sec; 0.151 sec/batch)
2017-05-05 03:53:02.524117: step 183340, loss = 1.98 (809.4 examples/sec; 0.158 sec/batch)
2017-05-05 03:53:04.062508: step 183350, loss = 1.88 (832.0 examples/sec; 0.154 sec/batch)
2017-05-05 03:53:05.546963: step 183360, loss = 1.89 (862.3 examples/sec; 0.148 sec/batch)
2017-05-05 03:53:07.049936: step 183370, loss = 1.78 (851.6 examples/sec; 0.150 sec/batch)
2017-05-05 03:53:08.556949: step 183380, loss = 1.96 (849.4 examples/sec; 0.151 sec/batch)
2017-05-05 03:53:10.077943: step 183390, loss = 1.73 (841.6 examples/sec; 0.152 sec/batch)
2017-05-05 03:53:11.645221: step 183400, loss = 1.98 (816.7 examples/sec; 0.157 sec/batch)
2017-05-05 03:53:13.143297: step 183410, loss = 1.88 (854.4 examples/sec; 0.150 sec/batch)
2017-05-05 03:53:14.711494: step 183420, loss = 1.96 (816.2 examples/sec; 0.157 sec/batch)
2017-05-05 03:53:16.282783: step 183430, loss = 1.99 (814.6 examples/sec; 0.157 sec/batch)
2017-05-05 03:53:17.838572: step 183440, loss = 1.80 (822.7 examples/sec; 0.156 sec/batch)
2017-05-05 03:53:19.407906: step 183450, loss = 1.88 (815.6 examples/sec; 0.157 sec/batch)
2017-05-05 03:53:20.937213: step 183460, loss = 2.03 (837.0 examples/sec; 0.153 sec/batch)
2017-05-05 03:53:22.486499: step 183470, loss = 1.82 (826.2 examples/sec; 0.155 sec/batch)
2017-05-05 03:53:24.024756: step 183480, loss = 1.85 (832.1 examples/sec; 0.154 sec/batch)
2017-05-05 03:53:25.534613: step 183490, loss = 1.86 (847.8 examples/sec; 0.151 sec/batch)
2017-05-05 03:53:26.998839: step 183500, loss = 1.63 (874.2 examples/sec; 0.146 sec/batch)
2017-05-05 03:53:28.567178: step 183510, loss = 1.70 (816.2 examples/sec; 0.157 sec/batch)
2017-05-05 03:53:30.113405: step 183520, loss = 1.85 (827.8 examples/sec; 0.155 sec/batch)
2017-05-05 03:53:31.605195: step 183530, loss = 1.98 (858.0 examples/sec; 0.149 sec/batch)
2017-05-05 03:53:33.109368: step 183540, loss = 1.69 (851.0 examples/sec; 0.150 sec/batch)
2017-05-05 03:53:34.615914: step 183550, loss = 1.98 (849.6 examples/sec; 0.151 sec/batch)
2017-05-05 03:53:36.224938: step 183560, loss = 1.83 (795.5 examples/sec; 0.161 sec/batch)
2017-05-05 03:53:37.776315: step 183570, loss = 1.80 (825.1 examples/sec; 0.155 sec/batch)
2017-05-05 03:53:39.301642: step 183580, loss = 1.90 (839.2 examples/sec; 0.153 sec/batch)
2017-05-05 03:53:40.795648: step 183590, loss = 1.86 (856.8 examples/sec; 0.149 sec/batch)
2017-05-05 03:53:42.298492: step 183600, loss = 1.87 (851.7 examples/sec; 0.150 sec/batch)
2017-05-05 03:53:43.813989: step 183610, loss = 1.90 (844.6 examples/sec; 0.152 sec/batch)
2017-05-05 03:53:45.320181: step 183620, loss = 1.96 (849.8 examples/sec; 0.151 sec/batch)
2017-05-05 03:53:46.875626: step 183630, loss = 1.81 (822.9 examples/sec; 0.156 sec/batch)
2017-05-05 03:53:48.409279: step 183640, loss = 1.82 (834.6 examples/sec; 0.153 sec/batch)
2017-05-05 03:53:49.914990: step 183650, loss = 1.71 (850.1 examples/sec; 0.151 sec/batch)
2017-05-05 03:53:51.492264: step 183660, loss = 1.97 (811.5 examples/sec; 0.158 sec/batch)
2017-05-05 03:53:53.011514: step 183670, loss = 1.87 (842.5 examples/sec; 0.152 sec/batch)
2017-05-05 03:53:54.530455: step 183680, loss = 1.90 (842.7 examples/sec; 0.152 sec/batch)
2017-05-05 03:53:56.034913: step 183690, loss = 1.79 (850.8 examples/sec; 0.150 sec/batch)
2017-05-05 03:53:57.574071: step 183700, loss = 1.85 (831.6 examples/sec; 0.154 sec/batch)
2017-05-05 03:53:59.053686: step 183710, loss = 1.91 (865.1 examples/sec; 0.148 sec/batch)
2017-05-05 03:54:00.557344: step 183720, loss = 1.93 (851.3 examples/sec; 0.150 sec/batch)
2017-05-05 03:54:02.046943: step 183730, loss = 2.01 (859.3 examples/sec; 0.149 sec/batch)
2017-05-05 03:54:03.567491: step 183740, loss = 1.97 (841.8 examples/sec; 0.152 sec/batch)
2017-05-05 03:54:05.052812: step 183750, loss = 1.89 (861.8 examples/sec; 0.149 sec/batch)
2017-05-05 03:54:06.554513: step 183760, loss = 1.86 (852.4 examples/sec; 0.150 sec/batch)
2017-05-05 03:54:08.105409: step 183770, loss = 1.83 (825.3 examples/sec; 0.155 sec/batch)
2017-05-05 03:54:09.575146: step 183780, loss = 1.87 (870.9 examples/sec; 0.147 sec/batch)
2017-05-05 03:54:11.116872: step 183790, loss = 1.84 (830.2 examples/sec; 0.154 sec/batch)
2017-05-05 03:54:12.641555: step 183800, loss = 1.88 (839.5 examples/sec; 0.152 sec/batch)
2017-05-05 03:54:14.159998: step 183810, loss = 1.92 (843.0 examples/sec; 0.152 sec/batch)
2017-05-05 03:54:15.641714: step 183820, loss = 1.92 (863.9 examples/sec; 0.148 sec/batch)
2017-05-05 03:54:17.143295: step 183830, loss = 1.82 (852.4 examples/sec; 0.150 sec/batch)
2017-05-05 03:54:18.657618: step 183840, loss = 1.79 (845.3 examples/sec; 0.151 sec/batch)
2017-05-05 03:54:20.129130: step 183850, loss = 2.05 (869.9 examples/sec; 0.147 sec/batch)
2017-05-05 03:54:21.627573: step 183860, loss = 1.87 (854.2 examples/sec; 0.150 sec/batch)
2017-05-05 03:54:23.115088: step 183870, loss = 1.95 (860.5 examples/sec; 0.149 sec/batch)
2017-05-05 03:54:24.587345: step 183880, loss = 1.78 (869.4 examples/sec; 0.147 sec/batch)
2017-05-05 03:54:26.106771: step 183890, loss = 1.80 (842.4 examples/sec; 0.152 sec/batch)
2017-05-05 03:54:27.646637: step 183900, loss = 2.05 (831.2 examples/sec; 0.154 sec/batch)
2017-05-05 03:54:29.108705: step 183910, loss = 1.80 (875.5 examples/sec; 0.146 sec/batch)
2017-05-05 03:54:30.584741: step 183920, loss = 1.80 (867.2 examples/sec; 0.148 sec/batch)
2017-05-05 03:54:32.075764: step 183930, loss = 1.71 (858.5 examples/sec; 0.149 sec/batch)
2017-05-05 03:54:33.608162: step 183940, loss = 1.90 (835.3 examples/sec; 0.153 sec/batch)
2017-05-05 03:54:35.160226: step 183950, loss = 1.91 (824.7 examples/sec; 0.155 sec/batch)
2017-05-05 03:54:36.724727: step 183960, loss = 1.79 (818.1 examples/sec; 0.156 sec/batch)
2017-05-05 03:54:38.248809: step 183970, loss = 1.83 (839.9 examples/sec; 0.152 sec/batch)
2017-05-05 03:54:39.799129: step 183980, loss = 1.83 (825.6 examples/sec; 0.155 sec/batch)
2017-05-05 03:54:41.333989: step 183990, loss = 1.91 (834.0 examples/sec; 0.153 sec/batch)
2017-05-05 03:54:42.849937: step 184000, loss = 1.78 (844.4 examples/sec; 0.152 sec/batch)
2017-05-05 03:54:44.382322: step 184010, loss = 1.81 (835.3 examples/sec; 0.153 sec/batch)
2017-05-05 03:54:45.923714: step 184020, loss = 2.00 (830.4 examples/sec; 0.154 sec/batch)
2017-05-05 03:54:47.487735: step 184030, loss = 1.77 (818.4 examples/sec; 0.156 sec/batch)
2017-05-05 03:54:49.001153: step 184040, loss = 1.86 (845.8 examples/sec; 0.151 sec/batch)
2017-05-05 03:54:50.627887: step 184050, loss = 2.02 (786.9 examples/sec; 0.163 sec/batch)
2017-05-05 03:54:52.154427: step 184060, loss = 2.14 (838.5 examples/sec; 0.153 sec/batch)
2017-05-05 03:54:53.711938: step 184070, loss = 1.88 (821.8 examples/sec; 0.156 sec/batch)
2017-05-05 03:54:55.237051: step 184080, loss = 1.77 (839.3 examples/sec; 0.153 sec/batch)
2017-05-05 03:54:56.792167: step 184090, loss = 1.92 (823.1 examples/sec; 0.156 sec/batch)
2017-05-05 03:54:58.309934: step 184100, loss = 1.82 (843.3 examples/sec; 0.152 sec/batch)
2017-05-05 03:54:59.858913: step 184110, loss = 1.97 (826.3 examples/sec; 0.155 sec/batch)
2017-05-05 03:55:01.394908: step 184120, loss = 1.85 (833.3 examples/sec; 0.154 sec/batch)
2017-05-05 03:55:02.936611: step 184130, loss = 1.81 (830.2 examples/sec; 0.154 sec/batch)
2017-05-05 03:55:04.540438: step 184140, loss = 1.87 (798.1 examples/sec; 0.160 sec/batch)
2017-05-05 03:55:05.947583: step 184150, loss = 1.83 (909.6 examples/sec; 0.141 sec/batch)
2017-05-05 03:55:07.460655: step 184160, loss = 1.88 (846.0 examples/sec; 0.151 sec/batch)
2017-05-05 03:55:08.965898: step 184170, loss = 1.87 (850.4 examples/sec; 0.151 sec/batch)
2017-05-05 03:55:10.488755: step 184180, loss = 1.89 (840.5 examples/sec; 0.152 sec/batch)
2017-05-05 03:55:12.007974: step 184190, loss = 1.81 (842.5 examples/sec; 0.152 sec/batch)
2017-05-05 03:55:13.454087: step 184200, loss = 1.83 (885.1 examples/sec; 0.145 sec/batch)
2017-05-05 03:55:15.0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1896 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
43016: step 184210, loss = 1.86 (805.6 examples/sec; 0.159 sec/batch)
2017-05-05 03:55:16.586096: step 184220, loss = 1.74 (829.5 examples/sec; 0.154 sec/batch)
2017-05-05 03:55:18.128338: step 184230, loss = 2.05 (830.0 examples/sec; 0.154 sec/batch)
2017-05-05 03:55:19.612510: step 184240, loss = 1.73 (862.4 examples/sec; 0.148 sec/batch)
2017-05-05 03:55:21.175808: step 184250, loss = 1.85 (818.8 examples/sec; 0.156 sec/batch)
2017-05-05 03:55:22.705422: step 184260, loss = 1.90 (836.8 examples/sec; 0.153 sec/batch)
2017-05-05 03:55:24.214828: step 184270, loss = 1.80 (848.0 examples/sec; 0.151 sec/batch)
2017-05-05 03:55:25.767139: step 184280, loss = 2.02 (824.6 examples/sec; 0.155 sec/batch)
2017-05-05 03:55:27.299450: step 184290, loss = 1.96 (835.3 examples/sec; 0.153 sec/batch)
2017-05-05 03:55:28.810362: step 184300, loss = 1.83 (847.2 examples/sec; 0.151 sec/batch)
2017-05-05 03:55:30.300941: step 184310, loss = 1.84 (858.7 examples/sec; 0.149 sec/batch)
2017-05-05 03:55:31.833544: step 184320, loss = 1.78 (835.2 examples/sec; 0.153 sec/batch)
2017-05-05 03:55:33.320122: step 184330, loss = 2.03 (861.0 examples/sec; 0.149 sec/batch)
2017-05-05 03:55:34.862431: step 184340, loss = 1.87 (829.9 examples/sec; 0.154 sec/batch)
2017-05-05 03:55:36.377645: step 184350, loss = 1.81 (844.8 examples/sec; 0.152 sec/batch)
2017-05-05 03:55:37.843629: step 184360, loss = 1.82 (873.1 examples/sec; 0.147 sec/batch)
2017-05-05 03:55:39.341419: step 184370, loss = 1.87 (854.6 examples/sec; 0.150 sec/batch)
2017-05-05 03:55:40.861587: step 184380, loss = 1.88 (842.0 examples/sec; 0.152 sec/batch)
2017-05-05 03:55:42.374077: step 184390, loss = 1.79 (846.3 examples/sec; 0.151 sec/batch)
2017-05-05 03:55:43.903873: step 184400, loss = 1.73 (836.7 examples/sec; 0.153 sec/batch)
2017-05-05 03:55:45.440523: step 184410, loss = 1.84 (833.0 examples/sec; 0.154 sec/batch)
2017-05-05 03:55:46.955821: step 184420, loss = 1.68 (844.7 examples/sec; 0.152 sec/batch)
2017-05-05 03:55:48.442816: step 184430, loss = 1.92 (860.8 examples/sec; 0.149 sec/batch)
2017-05-05 03:55:49.962388: step 184440, loss = 1.79 (842.3 examples/sec; 0.152 sec/batch)
2017-05-05 03:55:51.460910: step 184450, loss = 1.91 (854.2 examples/sec; 0.150 sec/batch)
2017-05-05 03:55:52.972873: step 184460, loss = 1.82 (846.6 examples/sec; 0.151 sec/batch)
2017-05-05 03:55:54.514999: step 184470, loss = 1.88 (830.0 examples/sec; 0.154 sec/batch)
2017-05-05 03:55:56.031933: step 184480, loss = 1.77 (843.8 examples/sec; 0.152 sec/batch)
2017-05-05 03:55:57.540600: step 184490, loss = 1.82 (848.4 examples/sec; 0.151 sec/batch)
2017-05-05 03:55:59.026496: step 184500, loss = 1.76 (861.4 examples/sec; 0.149 sec/batch)
2017-05-05 03:56:00.522432: step 184510, loss = 1.82 (855.7 examples/sec; 0.150 sec/batch)
2017-05-05 03:56:02.001391: step 184520, loss = 1.82 (865.5 examples/sec; 0.148 sec/batch)
2017-05-05 03:56:03.596407: step 184530, loss = 1.93 (802.5 examples/sec; 0.160 sec/batch)
2017-05-05 03:56:05.122028: step 184540, loss = 1.69 (839.0 examples/sec; 0.153 sec/batch)
2017-05-05 03:56:06.621275: step 184550, loss = 1.93 (853.8 examples/sec; 0.150 sec/batch)
2017-05-05 03:56:08.128934: step 184560, loss = 1.97 (849.0 examples/sec; 0.151 sec/batch)
2017-05-05 03:56:09.638860: step 184570, loss = 1.97 (847.7 examples/sec; 0.151 sec/batch)
2017-05-05 03:56:11.131547: step 184580, loss = 1.80 (857.5 examples/sec; 0.149 sec/batch)
2017-05-05 03:56:12.647302: step 184590, loss = 2.04 (844.5 examples/sec; 0.152 sec/batch)
2017-05-05 03:56:14.167926: step 184600, loss = 1.87 (841.8 examples/sec; 0.152 sec/batch)
2017-05-05 03:56:15.687031: step 184610, loss = 1.74 (842.6 examples/sec; 0.152 sec/batch)
2017-05-05 03:56:17.275827: step 184620, loss = 1.74 (805.6 examples/sec; 0.159 sec/batch)
2017-05-05 03:56:18.814848: step 184630, loss = 1.88 (831.7 examples/sec; 0.154 sec/batch)
2017-05-05 03:56:20.306714: step 184640, loss = 1.74 (858.0 examples/sec; 0.149 sec/batch)
2017-05-05 03:56:21.837478: step 184650, loss = 1.87 (836.2 examples/sec; 0.153 sec/batch)
2017-05-05 03:56:23.403337: step 184660, loss = 1.80 (817.4 examples/sec; 0.157 sec/batch)
2017-05-05 03:56:24.937381: step 184670, loss = 1.76 (834.4 examples/sec; 0.153 sec/batch)
2017-05-05 03:56:26.446939: step 184680, loss = 1.90 (847.9 examples/sec; 0.151 sec/batch)
2017-05-05 03:56:27.997444: step 184690, loss = 1.84 (825.5 examples/sec; 0.155 sec/batch)
2017-05-05 03:56:29.506539: step 184700, loss = 1.85 (848.2 examples/sec; 0.151 sec/batch)
2017-05-05 03:56:31.065991: step 184710, loss = 1.82 (820.8 examples/sec; 0.156 sec/batch)
2017-05-05 03:56:32.648171: step 184720, loss = 1.78 (809.0 examples/sec; 0.158 sec/batch)
2017-05-05 03:56:34.168399: step 184730, loss = 1.86 (842.0 examples/sec; 0.152 sec/batch)
2017-05-05 03:56:35.663541: step 184740, loss = 1.78 (856.1 examples/sec; 0.150 sec/batch)
2017-05-05 03:56:37.223740: step 184750, loss = 1.88 (820.4 examples/sec; 0.156 sec/batch)
2017-05-05 03:56:38.708310: step 184760, loss = 1.95 (862.2 examples/sec; 0.148 sec/batch)
2017-05-05 03:56:40.221405: step 184770, loss = 1.91 (845.9 examples/sec; 0.151 sec/batch)
2017-05-05 03:56:41.781144: step 184780, loss = 1.99 (820.7 examples/sec; 0.156 sec/batch)
2017-05-05 03:56:43.301154: step 184790, loss = 1.88 (842.1 examples/sec; 0.152 sec/batch)
2017-05-05 03:56:44.836634: step 184800, loss = 1.78 (833.6 examples/sec; 0.154 sec/batch)
2017-05-05 03:56:46.343601: step 184810, loss = 1.89 (849.4 examples/sec; 0.151 sec/batch)
2017-05-05 03:56:47.839782: step 184820, loss = 1.90 (855.5 examples/sec; 0.150 sec/batch)
2017-05-05 03:56:49.371679: step 184830, loss = 1.89 (835.6 examples/sec; 0.153 sec/batch)
2017-05-05 03:56:50.925589: step 184840, loss = 1.71 (823.7 examples/sec; 0.155 sec/batch)
2017-05-05 03:56:52.467331: step 184850, loss = 1.84 (830.2 examples/sec; 0.154 sec/batch)
2017-05-05 03:56:54.041259: step 184860, loss = 1.73 (813.3 examples/sec; 0.157 sec/batch)
2017-05-05 03:56:55.561819: step 184870, loss = 1.87 (841.8 examples/sec; 0.152 sec/batch)
2017-05-05 03:56:57.049736: step 184880, loss = 1.87 (860.3 examples/sec; 0.149 sec/batch)
2017-05-05 03:56:58.590747: step 184890, loss = 1.77 (830.6 examples/sec; 0.154 sec/batch)
2017-05-05 03:57:00.099328: step 184900, loss = 1.76 (848.5 examples/sec; 0.151 sec/batch)
2017-05-05 03:57:01.669422: step 184910, loss = 1.94 (815.2 examples/sec; 0.157 sec/batch)
2017-05-05 03:57:03.218356: step 184920, loss = 2.02 (826.4 examples/sec; 0.155 sec/batch)
2017-05-05 03:57:04.748418: step 184930, loss = 1.92 (836.6 examples/sec; 0.153 sec/batch)
2017-05-05 03:57:06.264782: step 184940, loss = 1.72 (844.1 examples/sec; 0.152 sec/batch)
2017-05-05 03:57:07.794880: step 184950, loss = 1.93 (836.5 examples/sec; 0.153 sec/batch)
2017-05-05 03:57:09.332217: step 184960, loss = 1.88 (832.6 examples/sec; 0.154 sec/batch)
2017-05-05 03:57:10.890606: step 184970, loss = 1.85 (821.4 examples/sec; 0.156 sec/batch)
2017-05-05 03:57:12.383566: step 184980, loss = 1.92 (857.4 examples/sec; 0.149 sec/batch)
2017-05-05 03:57:13.928047: step 184990, loss = 1.92 (828.8 examples/sec; 0.154 sec/batch)
2017-05-05 03:57:15.474748: step 185000, loss = 1.83 (827.6 examples/sec; 0.155 sec/batch)
2017-05-05 03:57:17.026428: step 185010, loss = 1.83 (824.9 examples/sec; 0.155 sec/batch)
2017-05-05 03:57:18.529969: step 185020, loss = 1.88 (851.3 examples/sec; 0.150 sec/batch)
2017-05-05 03:57:20.069704: step 185030, loss = 1.87 (831.3 examples/sec; 0.154 sec/batch)
2017-05-05 03:57:21.640520: step 185040, loss = 1.80 (814.9 examples/sec; 0.157 sec/batch)
2017-05-05 03:57:23.207803: step 185050, loss = 1.76 (816.7 examples/sec; 0.157 sec/batch)
2017-05-05 03:57:24.698501: step 185060, loss = 1.88 (858.7 examples/sec; 0.149 sec/batch)
2017-05-05 03:57:26.250950: step 185070, loss = 1.94 (824.5 examples/sec; 0.155 sec/batch)
2017-05-05 03:57:27.798983: step 185080, loss = 1.89 (826.9 examples/sec; 0.155 sec/batch)
2017-05-05 03:57:29.339744: step 185090, loss = 1.85 (830.8 examples/sec; 0.154 sec/batch)
2017-05-05 03:57:30.902810: step 185100, loss = 1.83 (818.9 examples/sec; 0.156 sec/batch)
2017-05-05 03:57:32.443E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1904 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
790: step 185110, loss = 1.87 (830.6 examples/sec; 0.154 sec/batch)
2017-05-05 03:57:33.944462: step 185120, loss = 1.83 (853.0 examples/sec; 0.150 sec/batch)
2017-05-05 03:57:35.583581: step 185130, loss = 1.99 (780.9 examples/sec; 0.164 sec/batch)
2017-05-05 03:57:36.987075: step 185140, loss = 1.82 (912.0 examples/sec; 0.140 sec/batch)
2017-05-05 03:57:38.526973: step 185150, loss = 1.82 (831.2 examples/sec; 0.154 sec/batch)
2017-05-05 03:57:40.052683: step 185160, loss = 1.83 (839.0 examples/sec; 0.153 sec/batch)
2017-05-05 03:57:41.512998: step 185170, loss = 1.88 (876.5 examples/sec; 0.146 sec/batch)
2017-05-05 03:57:43.059885: step 185180, loss = 1.90 (827.5 examples/sec; 0.155 sec/batch)
2017-05-05 03:57:44.613056: step 185190, loss = 1.74 (824.1 examples/sec; 0.155 sec/batch)
2017-05-05 03:57:46.165660: step 185200, loss = 1.78 (824.4 examples/sec; 0.155 sec/batch)
2017-05-05 03:57:47.681006: step 185210, loss = 1.84 (844.7 examples/sec; 0.152 sec/batch)
2017-05-05 03:57:49.205223: step 185220, loss = 1.97 (839.8 examples/sec; 0.152 sec/batch)
2017-05-05 03:57:50.713323: step 185230, loss = 1.87 (848.7 examples/sec; 0.151 sec/batch)
2017-05-05 03:57:52.245614: step 185240, loss = 1.81 (835.4 examples/sec; 0.153 sec/batch)
2017-05-05 03:57:53.783545: step 185250, loss = 1.77 (832.3 examples/sec; 0.154 sec/batch)
2017-05-05 03:57:55.305661: step 185260, loss = 1.95 (840.9 examples/sec; 0.152 sec/batch)
2017-05-05 03:57:56.831730: step 185270, loss = 1.95 (838.8 examples/sec; 0.153 sec/batch)
2017-05-05 03:57:58.380674: step 185280, loss = 1.85 (826.4 examples/sec; 0.155 sec/batch)
2017-05-05 03:57:59.920714: step 185290, loss = 1.86 (831.2 examples/sec; 0.154 sec/batch)
2017-05-05 03:58:01.422803: step 185300, loss = 1.86 (852.1 examples/sec; 0.150 sec/batch)
2017-05-05 03:58:02.947098: step 185310, loss = 1.84 (839.7 examples/sec; 0.152 sec/batch)
2017-05-05 03:58:04.504407: step 185320, loss = 1.75 (821.9 examples/sec; 0.156 sec/batch)
2017-05-05 03:58:06.014751: step 185330, loss = 1.88 (847.5 examples/sec; 0.151 sec/batch)
2017-05-05 03:58:07.502527: step 185340, loss = 1.83 (860.4 examples/sec; 0.149 sec/batch)
2017-05-05 03:58:09.062484: step 185350, loss = 1.81 (820.5 examples/sec; 0.156 sec/batch)
2017-05-05 03:58:10.607610: step 185360, loss = 1.83 (828.4 examples/sec; 0.155 sec/batch)
2017-05-05 03:58:12.104818: step 185370, loss = 2.10 (854.9 examples/sec; 0.150 sec/batch)
2017-05-05 03:58:13.649748: step 185380, loss = 1.91 (828.5 examples/sec; 0.154 sec/batch)
2017-05-05 03:58:15.192928: step 185390, loss = 1.92 (829.5 examples/sec; 0.154 sec/batch)
2017-05-05 03:58:16.701765: step 185400, loss = 1.91 (848.3 examples/sec; 0.151 sec/batch)
2017-05-05 03:58:18.200176: step 185410, loss = 1.94 (854.2 examples/sec; 0.150 sec/batch)
2017-05-05 03:58:19.714300: step 185420, loss = 1.93 (845.4 examples/sec; 0.151 sec/batch)
2017-05-05 03:58:21.218344: step 185430, loss = 1.78 (851.0 examples/sec; 0.150 sec/batch)
2017-05-05 03:58:22.787784: step 185440, loss = 1.85 (815.6 examples/sec; 0.157 sec/batch)
2017-05-05 03:58:24.299143: step 185450, loss = 1.80 (846.9 examples/sec; 0.151 sec/batch)
2017-05-05 03:58:25.846104: step 185460, loss = 1.89 (827.4 examples/sec; 0.155 sec/batch)
2017-05-05 03:58:27.408606: step 185470, loss = 1.86 (819.2 examples/sec; 0.156 sec/batch)
2017-05-05 03:58:28.943850: step 185480, loss = 1.83 (833.7 examples/sec; 0.154 sec/batch)
2017-05-05 03:58:30.484461: step 185490, loss = 1.84 (830.8 examples/sec; 0.154 sec/batch)
2017-05-05 03:58:32.004674: step 185500, loss = 1.90 (842.0 examples/sec; 0.152 sec/batch)
2017-05-05 03:58:33.576328: step 185510, loss = 1.78 (814.4 examples/sec; 0.157 sec/batch)
2017-05-05 03:58:35.154331: step 185520, loss = 1.91 (811.2 examples/sec; 0.158 sec/batch)
2017-05-05 03:58:36.797762: step 185530, loss = 1.91 (778.9 examples/sec; 0.164 sec/batch)
2017-05-05 03:58:38.340033: step 185540, loss = 1.86 (829.9 examples/sec; 0.154 sec/batch)
2017-05-05 03:58:39.864751: step 185550, loss = 1.74 (839.5 examples/sec; 0.152 sec/batch)
2017-05-05 03:58:41.399087: step 185560, loss = 1.87 (834.2 examples/sec; 0.153 sec/batch)
2017-05-05 03:58:42.939060: step 185570, loss = 1.88 (831.2 examples/sec; 0.154 sec/batch)
2017-05-05 03:58:44.482362: step 185580, loss = 2.05 (829.4 examples/sec; 0.154 sec/batch)
2017-05-05 03:58:45.976514: step 185590, loss = 1.96 (856.7 examples/sec; 0.149 sec/batch)
2017-05-05 03:58:47.461424: step 185600, loss = 1.88 (862.0 examples/sec; 0.148 sec/batch)
2017-05-05 03:58:49.065998: step 185610, loss = 1.73 (797.7 examples/sec; 0.160 sec/batch)
2017-05-05 03:58:50.624499: step 185620, loss = 2.00 (821.3 examples/sec; 0.156 sec/batch)
2017-05-05 03:58:52.183539: step 185630, loss = 1.87 (821.0 examples/sec; 0.156 sec/batch)
2017-05-05 03:58:53.707658: step 185640, loss = 1.91 (839.8 examples/sec; 0.152 sec/batch)
2017-05-05 03:58:55.208807: step 185650, loss = 1.94 (852.7 examples/sec; 0.150 sec/batch)
2017-05-05 03:58:56.739314: step 185660, loss = 1.86 (836.3 examples/sec; 0.153 sec/batch)
2017-05-05 03:58:58.291928: step 185670, loss = 1.99 (824.4 examples/sec; 0.155 sec/batch)
2017-05-05 03:58:59.825611: step 185680, loss = 1.89 (834.6 examples/sec; 0.153 sec/batch)
2017-05-05 03:59:01.329989: step 185690, loss = 1.92 (850.8 examples/sec; 0.150 sec/batch)
2017-05-05 03:59:02.893931: step 185700, loss = 1.82 (818.4 examples/sec; 0.156 sec/batch)
2017-05-05 03:59:04.439721: step 185710, loss = 1.72 (828.1 examples/sec; 0.155 sec/batch)
2017-05-05 03:59:05.948967: step 185720, loss = 1.97 (848.1 examples/sec; 0.151 sec/batch)
2017-05-05 03:59:07.490664: step 185730, loss = 1.70 (830.3 examples/sec; 0.154 sec/batch)
2017-05-05 03:59:09.053215: step 185740, loss = 1.82 (819.2 examples/sec; 0.156 sec/batch)
2017-05-05 03:59:10.550757: step 185750, loss = 1.93 (854.7 examples/sec; 0.150 sec/batch)
2017-05-05 03:59:12.107182: step 185760, loss = 1.73 (822.4 examples/sec; 0.156 sec/batch)
2017-05-05 03:59:13.661706: step 185770, loss = 1.94 (823.4 examples/sec; 0.155 sec/batch)
2017-05-05 03:59:15.165679: step 185780, loss = 2.11 (851.1 examples/sec; 0.150 sec/batch)
2017-05-05 03:59:16.727311: step 185790, loss = 1.74 (819.7 examples/sec; 0.156 sec/batch)
2017-05-05 03:59:18.253213: step 185800, loss = 1.90 (838.9 examples/sec; 0.153 sec/batch)
2017-05-05 03:59:19.756571: step 185810, loss = 2.04 (851.4 examples/sec; 0.150 sec/batch)
2017-05-05 03:59:21.345615: step 185820, loss = 1.97 (805.5 examples/sec; 0.159 sec/batch)
2017-05-05 03:59:22.873732: step 185830, loss = 1.69 (837.6 examples/sec; 0.153 sec/batch)
2017-05-05 03:59:24.444077: step 185840, loss = 1.94 (815.1 examples/sec; 0.157 sec/batch)
2017-05-05 03:59:26.013155: step 185850, loss = 1.73 (815.8 examples/sec; 0.157 sec/batch)
2017-05-05 03:59:27.550723: step 185860, loss = 1.79 (832.5 examples/sec; 0.154 sec/batch)
2017-05-05 03:59:29.102432: step 185870, loss = 1.89 (824.9 examples/sec; 0.155 sec/batch)
2017-05-05 03:59:30.596391: step 185880, loss = 1.88 (856.8 examples/sec; 0.149 sec/batch)
2017-05-05 03:59:32.119545: step 185890, loss = 1.72 (840.4 examples/sec; 0.152 sec/batch)
2017-05-05 03:59:33.652412: step 185900, loss = 1.95 (835.0 examples/sec; 0.153 sec/batch)
2017-05-05 03:59:35.197759: step 185910, loss = 1.97 (828.3 examples/sec; 0.155 sec/batch)
2017-05-05 03:59:36.731671: step 185920, loss = 1.85 (834.5 examples/sec; 0.153 sec/batch)
2017-05-05 03:59:38.240507: step 185930, loss = 2.03 (848.3 examples/sec; 0.151 sec/batch)
2017-05-05 03:59:39.813093: step 185940, loss = 2.09 (813.9 examples/sec; 0.157 sec/batch)
2017-05-05 03:59:41.372532: step 185950, loss = 1.87 (820.8 examples/sec; 0.156 sec/batch)
2017-05-05 03:59:42.857375: step 185960, loss = 1.91 (862.0 examples/sec; 0.148 sec/batch)
2017-05-05 03:59:44.419478: step 185970, loss = 1.88 (819.4 examples/sec; 0.156 sec/batch)
2017-05-05 03:59:45.890526: step 185980, loss = 1.73 (870.1 examples/sec; 0.147 sec/batch)
2017-05-05 03:59:47.456774: step 185990, loss = 2.05 (817.2 examples/sec; 0.157 sec/batch)
2017-05-05 03:59:48.999559: step 186000, loss = 1.73 (829.7 examples/sec; 0.154 sec/batch)
2017-05-05 03:59:50.53798E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1913 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
1: step 186010, loss = 1.87 (832.0 examples/sec; 0.154 sec/batch)
2017-05-05 03:59:52.063976: step 186020, loss = 1.88 (838.8 examples/sec; 0.153 sec/batch)
2017-05-05 03:59:53.635051: step 186030, loss = 1.91 (814.7 examples/sec; 0.157 sec/batch)
2017-05-05 03:59:55.139263: step 186040, loss = 1.87 (851.0 examples/sec; 0.150 sec/batch)
2017-05-05 03:59:56.703847: step 186050, loss = 1.99 (818.1 examples/sec; 0.156 sec/batch)
2017-05-05 03:59:58.206355: step 186060, loss = 1.93 (851.9 examples/sec; 0.150 sec/batch)
2017-05-05 03:59:59.731086: step 186070, loss = 1.94 (839.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:00:01.241137: step 186080, loss = 1.91 (847.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:00:02.788741: step 186090, loss = 1.81 (827.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:00:04.307304: step 186100, loss = 1.82 (842.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:00:05.846199: step 186110, loss = 1.98 (831.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:00:07.510676: step 186120, loss = 1.82 (769.0 examples/sec; 0.166 sec/batch)
2017-05-05 04:00:08.924277: step 186130, loss = 1.94 (905.5 examples/sec; 0.141 sec/batch)
2017-05-05 04:00:10.505479: step 186140, loss = 1.76 (809.5 examples/sec; 0.158 sec/batch)
2017-05-05 04:00:12.053481: step 186150, loss = 1.81 (826.9 examples/sec; 0.155 sec/batch)
2017-05-05 04:00:13.653616: step 186160, loss = 1.90 (799.9 examples/sec; 0.160 sec/batch)
2017-05-05 04:00:15.215063: step 186170, loss = 1.91 (819.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:00:16.771871: step 186180, loss = 1.84 (822.2 examples/sec; 0.156 sec/batch)
2017-05-05 04:00:18.302715: step 186190, loss = 1.76 (836.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:00:19.845511: step 186200, loss = 1.93 (829.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:00:21.385183: step 186210, loss = 1.89 (831.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:00:22.943088: step 186220, loss = 1.84 (821.6 examples/sec; 0.156 sec/batch)
2017-05-05 04:00:24.516453: step 186230, loss = 1.99 (813.5 examples/sec; 0.157 sec/batch)
2017-05-05 04:00:26.010514: step 186240, loss = 1.88 (856.7 examples/sec; 0.149 sec/batch)
2017-05-05 04:00:27.570226: step 186250, loss = 1.85 (820.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:00:29.138106: step 186260, loss = 1.78 (816.4 examples/sec; 0.157 sec/batch)
2017-05-05 04:00:30.681287: step 186270, loss = 1.86 (829.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:00:32.207485: step 186280, loss = 1.87 (838.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:00:33.799686: step 186290, loss = 1.80 (803.9 examples/sec; 0.159 sec/batch)
2017-05-05 04:00:35.335363: step 186300, loss = 1.85 (833.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:00:36.948911: step 186310, loss = 1.91 (793.3 examples/sec; 0.161 sec/batch)
2017-05-05 04:00:38.477425: step 186320, loss = 1.91 (837.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:00:40.046160: step 186330, loss = 1.88 (815.9 examples/sec; 0.157 sec/batch)
2017-05-05 04:00:41.616845: step 186340, loss = 1.92 (814.9 examples/sec; 0.157 sec/batch)
2017-05-05 04:00:43.135015: step 186350, loss = 1.83 (843.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:00:44.668647: step 186360, loss = 1.76 (834.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:00:46.194969: step 186370, loss = 1.91 (838.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:00:47.734100: step 186380, loss = 1.79 (831.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:00:49.303798: step 186390, loss = 1.84 (815.4 examples/sec; 0.157 sec/batch)
2017-05-05 04:00:50.880772: step 186400, loss = 1.79 (811.7 examples/sec; 0.158 sec/batch)
2017-05-05 04:00:52.437706: step 186410, loss = 1.98 (822.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:00:54.003690: step 186420, loss = 1.90 (817.4 examples/sec; 0.157 sec/batch)
2017-05-05 04:00:55.517720: step 186430, loss = 1.94 (845.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:00:57.069915: step 186440, loss = 1.90 (824.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:00:58.604820: step 186450, loss = 1.84 (833.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:01:00.111950: step 186460, loss = 1.85 (849.3 examples/sec; 0.151 sec/batch)
2017-05-05 04:01:01.687016: step 186470, loss = 1.97 (812.7 examples/sec; 0.158 sec/batch)
2017-05-05 04:01:03.269190: step 186480, loss = 1.92 (809.0 examples/sec; 0.158 sec/batch)
2017-05-05 04:01:04.848791: step 186490, loss = 1.88 (810.3 examples/sec; 0.158 sec/batch)
2017-05-05 04:01:06.367985: step 186500, loss = 1.79 (842.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:01:07.899874: step 186510, loss = 1.79 (835.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:01:09.417948: step 186520, loss = 1.88 (843.2 examples/sec; 0.152 sec/batch)
2017-05-05 04:01:10.928002: step 186530, loss = 1.90 (847.7 examples/sec; 0.151 sec/batch)
2017-05-05 04:01:12.466677: step 186540, loss = 2.05 (831.9 examples/sec; 0.154 sec/batch)
2017-05-05 04:01:14.033469: step 186550, loss = 1.90 (817.0 examples/sec; 0.157 sec/batch)
2017-05-05 04:01:15.614444: step 186560, loss = 2.04 (809.6 examples/sec; 0.158 sec/batch)
2017-05-05 04:01:17.186714: step 186570, loss = 1.87 (814.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:01:18.728252: step 186580, loss = 1.93 (830.4 examples/sec; 0.154 sec/batch)
2017-05-05 04:01:20.302185: step 186590, loss = 1.89 (813.2 examples/sec; 0.157 sec/batch)
2017-05-05 04:01:21.783016: step 186600, loss = 1.87 (864.4 examples/sec; 0.148 sec/batch)
2017-05-05 04:01:23.317305: step 186610, loss = 1.85 (834.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:01:24.818391: step 186620, loss = 1.93 (852.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:01:26.338672: step 186630, loss = 1.91 (842.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:01:27.860584: step 186640, loss = 2.02 (841.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:01:29.436820: step 186650, loss = 1.98 (812.1 examples/sec; 0.158 sec/batch)
2017-05-05 04:01:30.986169: step 186660, loss = 1.74 (826.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:01:32.550362: step 186670, loss = 1.77 (818.3 examples/sec; 0.156 sec/batch)
2017-05-05 04:01:34.107434: step 186680, loss = 1.84 (822.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:01:35.651511: step 186690, loss = 1.88 (829.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:01:37.201955: step 186700, loss = 1.99 (825.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:01:38.720973: step 186710, loss = 1.82 (842.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:01:40.240748: step 186720, loss = 2.01 (842.2 examples/sec; 0.152 sec/batch)
2017-05-05 04:01:41.791462: step 186730, loss = 1.72 (825.4 examples/sec; 0.155 sec/batch)
2017-05-05 04:01:43.378121: step 186740, loss = 1.85 (806.7 examples/sec; 0.159 sec/batch)
2017-05-05 04:01:44.940446: step 186750, loss = 2.01 (819.3 examples/sec; 0.156 sec/batch)
2017-05-05 04:01:46.501799: step 186760, loss = 1.89 (819.8 examples/sec; 0.156 sec/batch)
2017-05-05 04:01:48.027643: step 186770, loss = 1.88 (838.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:01:49.545880: step 186780, loss = 1.96 (843.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:01:51.079378: step 186790, loss = 1.83 (834.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:01:52.624986: step 186800, loss = 1.76 (828.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:01:54.179305: step 186810, loss = 1.74 (823.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:01:55.747749: step 186820, loss = 1.90 (816.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:01:57.261174: step 186830, loss = 1.95 (845.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:01:58.844200: step 186840, loss = 1.90 (808.6 examples/sec; 0.158 sec/batch)
2017-05-05 04:02:00.361195: step 186850, loss = 1.87 (843.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:02:01.848312: step 186860, loss = 1.77 (860.7 examples/sec; 0.149 sec/batch)
2017-05-05 04:02:03.356147: step 186870, loss = 1.80 (848.9 examples/sec; 0.151 sec/batch)
2017-05-05 04:02:04.922697: step 186880, loss = 1.77 (817.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:02:06.442442: step 186890, loss = 1.96 (842.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:02:07.941607: step 186900, loss = 1.96 (853.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:02:09.539394:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1921 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
 step 186910, loss = 1.89 (801.1 examples/sec; 0.160 sec/batch)
2017-05-05 04:02:11.072680: step 186920, loss = 1.95 (834.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:02:12.646428: step 186930, loss = 2.00 (813.3 examples/sec; 0.157 sec/batch)
2017-05-05 04:02:14.196051: step 186940, loss = 1.95 (826.0 examples/sec; 0.155 sec/batch)
2017-05-05 04:02:15.718339: step 186950, loss = 1.84 (840.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:02:17.218334: step 186960, loss = 1.87 (853.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:02:18.748811: step 186970, loss = 1.93 (836.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:02:20.255781: step 186980, loss = 1.93 (849.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:02:21.776060: step 186990, loss = 1.99 (841.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:02:23.295004: step 187000, loss = 2.01 (842.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:02:24.828518: step 187010, loss = 1.88 (834.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:02:26.381573: step 187020, loss = 1.98 (824.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:02:27.940131: step 187030, loss = 1.94 (821.3 examples/sec; 0.156 sec/batch)
2017-05-05 04:02:29.483294: step 187040, loss = 2.00 (829.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:02:30.996147: step 187050, loss = 1.86 (846.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:02:32.618753: step 187060, loss = 1.86 (788.9 examples/sec; 0.162 sec/batch)
2017-05-05 04:02:34.124765: step 187070, loss = 1.81 (849.9 examples/sec; 0.151 sec/batch)
2017-05-05 04:02:35.633097: step 187080, loss = 1.93 (848.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:02:37.177712: step 187090, loss = 1.70 (828.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:02:38.728878: step 187100, loss = 1.84 (825.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:02:40.378273: step 187110, loss = 1.83 (776.0 examples/sec; 0.165 sec/batch)
2017-05-05 04:02:41.814619: step 187120, loss = 1.98 (891.2 examples/sec; 0.144 sec/batch)
2017-05-05 04:02:43.363664: step 187130, loss = 1.90 (826.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:02:44.854334: step 187140, loss = 1.93 (858.7 examples/sec; 0.149 sec/batch)
2017-05-05 04:02:46.365120: step 187150, loss = 1.90 (847.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:02:47.905901: step 187160, loss = 1.76 (830.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:02:49.442841: step 187170, loss = 1.69 (832.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:02:50.962256: step 187180, loss = 2.00 (842.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:02:52.530557: step 187190, loss = 1.87 (816.2 examples/sec; 0.157 sec/batch)
2017-05-05 04:02:54.036628: step 187200, loss = 2.01 (849.9 examples/sec; 0.151 sec/batch)
2017-05-05 04:02:55.639346: step 187210, loss = 1.79 (798.6 examples/sec; 0.160 sec/batch)
2017-05-05 04:02:57.161472: step 187220, loss = 1.65 (840.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:02:58.728864: step 187230, loss = 1.67 (816.6 examples/sec; 0.157 sec/batch)
2017-05-05 04:03:00.274459: step 187240, loss = 1.79 (828.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:03:01.809091: step 187250, loss = 1.82 (833.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:03:03.369395: step 187260, loss = 1.93 (820.3 examples/sec; 0.156 sec/batch)
2017-05-05 04:03:04.932565: step 187270, loss = 1.88 (818.8 examples/sec; 0.156 sec/batch)
2017-05-05 04:03:06.454681: step 187280, loss = 1.88 (840.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:03:07.999963: step 187290, loss = 1.90 (828.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:03:09.525401: step 187300, loss = 1.94 (839.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:03:11.040326: step 187310, loss = 1.78 (844.9 examples/sec; 0.151 sec/batch)
2017-05-05 04:03:12.573453: step 187320, loss = 1.80 (834.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:03:14.071778: step 187330, loss = 2.09 (854.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:03:15.615512: step 187340, loss = 1.90 (829.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:03:17.110152: step 187350, loss = 1.91 (856.4 examples/sec; 0.149 sec/batch)
2017-05-05 04:03:18.606775: E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1929 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
step 187360, loss = 1.96 (855.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:03:20.156319: step 187370, loss = 1.77 (826.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:03:21.673574: step 187380, loss = 1.85 (843.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:03:23.174987: step 187390, loss = 1.84 (852.5 examples/sec; 0.150 sec/batch)
2017-05-05 04:03:24.758468: step 187400, loss = 1.69 (808.3 examples/sec; 0.158 sec/batch)
2017-05-05 04:03:26.275620: step 187410, loss = 1.82 (843.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:03:27.834961: step 187420, loss = 1.80 (820.9 examples/sec; 0.156 sec/batch)
2017-05-05 04:03:29.390757: step 187430, loss = 1.95 (822.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:03:30.921895: step 187440, loss = 1.84 (836.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:03:32.475952: step 187450, loss = 1.97 (823.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:03:34.008051: step 187460, loss = 1.89 (835.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:03:35.546313: step 187470, loss = 1.88 (832.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:03:37.037788: step 187480, loss = 1.99 (858.2 examples/sec; 0.149 sec/batch)
2017-05-05 04:03:38.603720: step 187490, loss = 1.88 (817.4 examples/sec; 0.157 sec/batch)
2017-05-05 04:03:40.134460: step 187500, loss = 1.88 (836.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:03:41.645125: step 187510, loss = 1.92 (847.3 examples/sec; 0.151 sec/batch)
2017-05-05 04:03:43.186146: step 187520, loss = 1.93 (830.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:03:44.713788: step 187530, loss = 2.02 (837.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:03:46.202860: step 187540, loss = 1.83 (859.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:03:47.693396: step 187550, loss = 1.77 (858.8 examples/sec; 0.149 sec/batch)
2017-05-05 04:03:49.241977: step 187560, loss = 1.95 (826.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:03:50.785914: step 187570, loss = 1.86 (829.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:03:52.286218: step 187580, loss = 1.83 (853.2 examples/sec; 0.150 sec/batch)
2017-05-05 04:03:53.847329: step 187590, loss = 2.07 (819.9 examples/sec; 0.156 sec/batch)
2017-05-05 04:03:55.390756: step 187600, loss = 1.95 (829.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:03:56.910842: step 187610, loss = 1.78 (842.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:03:58.406665: step 187620, loss = 2.16 (855.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:03:59.950690: step 187630, loss = 1.87 (829.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:04:01.445737: step 187640, loss = 1.82 (856.2 examples/sec; 0.150 sec/batch)
2017-05-05 04:04:02.984870: step 187650, loss = 1.89 (831.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:04:04.499336: step 187660, loss = 1.79 (845.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:04:05.961567: step 187670, loss = 2.05 (875.4 examples/sec; 0.146 sec/batch)
2017-05-05 04:04:07.501624: step 187680, loss = 1.83 (831.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:04:09.076046: step 187690, loss = 1.99 (813.0 examples/sec; 0.157 sec/batch)
2017-05-05 04:04:10.595610: step 187700, loss = 1.88 (842.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:04:12.124984: step 187710, loss = 1.80 (836.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:04:13.656631: step 187720, loss = 1.86 (835.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:04:15.127917: step 187730, loss = 1.79 (870.0 examples/sec; 0.147 sec/batch)
2017-05-05 04:04:16.674045: step 187740, loss = 1.79 (827.9 examples/sec; 0.155 sec/batch)
2017-05-05 04:04:18.194780: step 187750, loss = 1.75 (841.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:04:19.672378: step 187760, loss = 1.83 (866.3 examples/sec; 0.148 sec/batch)
2017-05-05 04:04:21.156546: step 187770, loss = 1.95 (862.4 examples/sec; 0.148 sec/batch)
2017-05-05 04:04:22.612127: step 187780, loss = 1.93 (879.4 examples/sec; 0.146 sec/batch)
2017-05-05 04:04:24.133318: step 187790, loss = 1.84 (841.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:04:25.637792: step 187800, loss = 1.99 (850.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:04:27.178358: step 187810, loss = 1.94 (830.9 examples/sec; 0.154 sec/batch)
2017-05-05 04:04:28.666577: step 187820, loss = 1.85 (860.1 examples/sec; 0.149 sec/batch)
2017-05-05 04:04:30.160083: step 187830, loss = 1.80 (857.0 examples/sec; 0.149 sec/batch)
2017-05-05 04:04:31.614525: step 187840, loss = 1.78 (880.1 examples/sec; 0.145 sec/batch)
2017-05-05 04:04:33.081192: step 187850, loss = 2.03 (872.7 examples/sec; 0.147 sec/batch)
2017-05-05 04:04:34.606405: step 187860, loss = 1.83 (839.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:04:36.091046: step 187870, loss = 1.87 (862.2 examples/sec; 0.148 sec/batch)
2017-05-05 04:04:37.577056: step 187880, loss = 1.71 (861.4 examples/sec; 0.149 sec/batch)
2017-05-05 04:04:39.041507: step 187890, loss = 1.78 (874.1 examples/sec; 0.146 sec/batch)
2017-05-05 04:04:40.605888: step 187900, loss = 1.87 (818.2 examples/sec; 0.156 sec/batch)
2017-05-05 04:04:42.142819: step 187910, loss = 1.80 (832.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:04:43.597900: step 187920, loss = 1.77 (879.7 examples/sec; 0.146 sec/batch)
2017-05-05 04:04:45.088444: step 187930, loss = 1.79 (858.7 examples/sec; 0.149 sec/batch)
2017-05-05 04:04:46.572708: step 187940, loss = 1.86 (862.4 examples/sec; 0.148 sec/batch)
2017-05-05 04:04:48.124794: step 187950, loss = 1.81 (824.7 examples/sec; 0.155 sec/batch)
2017-05-05 04:04:49.626416: step 187960, loss = 1.99 (852.4 examples/sec; 0.150 sec/batch)
2017-05-05 04:04:51.128957: step 187970, loss = 1.89 (851.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:04:52.647778: step 187980, loss = 1.74 (842.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:04:54.208139: step 187990, loss = 1.90 (820.3 examples/sec; 0.156 sec/batch)
2017-05-05 04:04:55.652370: step 188000, loss = 1.68 (886.3 examples/sec; 0.144 sec/batch)
2017-05-05 04:04:57.220812: step 188010, loss = 1.80 (816.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:04:58.780584: step 188020, loss = 1.86 (820.6 examples/sec; 0.156 sec/batch)
2017-05-05 04:05:00.343277: step 188030, loss = 1.75 (819.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:05:01.909198: step 188040, loss = 1.78 (817.4 examples/sec; 0.157 sec/batch)
2017-05-05 04:05:03.441701: step 188050, loss = 1.93 (835.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:05:04.945291: step 188060, loss = 1.78 (851.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:05:06.483382: step 188070, loss = 1.87 (832.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:05:07.989800: step 188080, loss = 1.73 (849.7 examples/sec; 0.151 sec/batch)
2017-05-05 04:05:09.489935: step 188090, loss = 1.93 (853.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:05:11.153040: step 188100, loss = 1.89 (769.6 examples/sec; 0.166 sec/batch)
2017-05-05 04:05:12.538380: step 188110, loss = 1.98 (924.0 examples/sec; 0.139 sec/batch)
2017-05-05 04:05:14.001692: step 188120, loss = 1.88 (874.7 examples/sec; 0.146 sec/batch)
2017-05-05 04:05:15.521820: step 188130, loss = 1.96 (842.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:05:17.027827: step 188140, loss = 1.97 (849.9 examples/sec; 0.151 sec/batch)
2017-05-05 04:05:18.571115: step 188150, loss = 1.88 (829.4 examples/sec; 0.154 sec/batch)
2017-05-05 04:05:20.097932: step 188160, loss = 1.85 (838.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:05:21.610432: step 188170, loss = 1.99 (846.3 examples/sec; 0.151 sec/batch)
2017-05-05 04:05:23.164525: step 188180, loss = 1.75 (823.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:05:24.694436: step 188190, loss = 1.75 (836.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:05:26.223352: step 188200, loss = 1.78 (837.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:05:27.733304: step 188210, loss = 1.86 (847.7 examples/sec; 0.151 sec/batch)
2017-05-05 04:05:29.282199: step 188220, loss = 1.80 (826.4 examples/sec; 0.155 sec/batch)
2017-05-05 04:05:30.774295: step 188230, loss = 1.75 (857.9 examples/sec; 0.149 sec/batch)
2017-05-05 04:05:32.324303: step 188240, loss = 1.81 (825.8 examples/sec; 0.155 sec/batch)
2017-05-05 04:05:33.871968: step 188250, loss = 1.85 (827.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:05:35.372639: stE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1937 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
ep 188260, loss = 1.93 (853.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:05:36.899719: step 188270, loss = 1.84 (838.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:05:38.445020: step 188280, loss = 1.80 (828.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:05:39.988791: step 188290, loss = 1.77 (829.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:05:41.532172: step 188300, loss = 1.89 (829.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:05:43.048270: step 188310, loss = 1.81 (844.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:05:44.614714: step 188320, loss = 1.76 (817.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:05:46.142121: step 188330, loss = 1.97 (838.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:05:47.605441: step 188340, loss = 1.93 (874.7 examples/sec; 0.146 sec/batch)
2017-05-05 04:05:49.126914: step 188350, loss = 1.70 (841.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:05:50.687884: step 188360, loss = 1.94 (820.0 examples/sec; 0.156 sec/batch)
2017-05-05 04:05:52.206479: step 188370, loss = 1.89 (842.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:05:53.710126: step 188380, loss = 1.85 (851.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:05:55.249076: step 188390, loss = 1.87 (831.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:05:56.798993: step 188400, loss = 1.86 (825.8 examples/sec; 0.155 sec/batch)
2017-05-05 04:05:58.325221: step 188410, loss = 1.87 (838.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:05:59.796225: step 188420, loss = 1.78 (870.2 examples/sec; 0.147 sec/batch)
2017-05-05 04:06:01.365731: step 188430, loss = 1.92 (815.5 examples/sec; 0.157 sec/batch)
2017-05-05 04:06:02.874986: step 188440, loss = 1.97 (848.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:06:04.385426: step 188450, loss = 1.88 (847.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:06:05.900551: step 188460, loss = 1.96 (844.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:06:07.405351: step 188470, loss = 1.84 (850.6 examples/sec; 0.150 sec/batch)
2017-05-05 04:06:08.908288: step 188480, loss = 1.82 (851.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:06:10.491706: step 188490, loss = 1.82 (808.4 examples/sec; 0.158 sec/batch)
2017-05-05 04:06:12.034153: step 188500, loss = 1.80 (829.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:06:13.517596: step 188510, loss = 1.87 (862.9 examples/sec; 0.148 sec/batch)
2017-05-05 04:06:15.004109: step 188520, loss = 1.88 (861.1 examples/sec; 0.149 sec/batch)
2017-05-05 04:06:16.517000: step 188530, loss = 1.81 (846.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:06:18.002905: step 188540, loss = 1.83 (861.4 examples/sec; 0.149 sec/batch)
2017-05-05 04:06:19.508411: step 188550, loss = 1.96 (850.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:06:21.005812: step 188560, loss = 1.93 (854.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:06:22.485549: step 188570, loss = 1.82 (865.0 examples/sec; 0.148 sec/batch)
2017-05-05 04:06:24.025813: step 188580, loss = 1.90 (831.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:06:25.526777: step 188590, loss = 1.88 (852.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:06:27.038142: step 188600, loss = 1.81 (846.9 examples/sec; 0.151 sec/batch)
2017-05-05 04:06:28.547594: step 188610, loss = 1.94 (848.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:06:30.060913: step 188620, loss = 1.82 (845.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:06:31.598260: step 188630, loss = 1.95 (832.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:06:33.072215: step 188640, loss = 1.93 (868.4 examples/sec; 0.147 sec/batch)
2017-05-05 04:06:34.602310: step 188650, loss = 1.73 (836.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:06:36.059790: step 188660, loss = 1.85 (878.2 examples/sec; 0.146 sec/batch)
2017-05-05 04:06:37.563084: step 188670, loss = 1.93 (851.5 examples/sec; 0.150 sec/batch)
2017-05-05 04:06:39.104464: step 188680, loss = 1.82 (830.4 examples/sec; 0.154 sec/batch)
2017-05-05 04:06:40.626364: step 188690, loss = 1.81 (841.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:06:42.106520: step 188700, loss = 1.94 (864.8 examples/sec; 0.148 sec/batch)
2017-05-05 04:06:43.652271: step 188710, loss = 1.78 (828.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:06:45.163487: step 188720, loss = 1.94 (847.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:06:46.666883: step 188730, loss = 1.86 (851.4 examples/sec; 0.150 sec/batch)
2017-05-05 04:06:48.234277: step 188740, loss = 1.94 (816.6 examples/sec; 0.157 sec/batch)
2017-05-05 04:06:49.762702: step 188750, loss = 1.97 (837.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:06:51.329410: step 188760, loss = 1.84 (817.0 examples/sec; 0.157 sec/batch)
2017-05-05 04:06:52.837979: step 188770, loss = 1.76 (848.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:06:54.276376: step 188780, loss = 1.85 (889.9 examples/sec; 0.144 sec/batch)
2017-05-05 04:06:55.779053: step 188790, loss = 1.97 (851.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:06:57.351031: step 188800, loss = 1.98 (814.3 examples/sec; 0.157 sec/batch)
2017-05-05 04:06:58.835218: step 188810, loss = 1.95 (862.4 examples/sec; 0.148 sec/batch)
2017-05-05 04:07:00.314622: step 188820, loss = 1.90 (865.2 examples/sec; 0.148 sec/batch)
2017-05-05 04:07:01.862985: step 188830, loss = 1.88 (826.7 examples/sec; 0.155 sec/batch)
2017-05-05 04:07:03.410845: step 188840, loss = 1.96 (826.9 examples/sec; 0.155 sec/batch)
2017-05-05 04:07:04.900168: step 188850, loss = 1.85 (859.5 examples/sec; 0.149 sec/batch)
2017-05-05 04:07:06.454563: step 188860, loss = 1.79 (823.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:07:08.022480: step 188870, loss = 1.93 (816.4 examples/sec; 0.157 sec/batch)
2017-05-05 04:07:09.521440: step 188880, loss = 2.02 (853.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:07:11.061530: step 188890, loss = 1.77 (831.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:07:12.637963: step 188900, loss = 1.88 (812.0 examples/sec; 0.158 sec/batch)
2017-05-05 04:07:14.182085: step 188910, loss = 1.80 (828.9 examples/sec; 0.154 sec/batch)
2017-05-05 04:07:15.680704: step 188920, loss = 1.74 (854.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:07:17.288489: step 188930, loss = 1.77 (796.1 examples/sec; 0.161 sec/batch)
2017-05-05 04:07:18.858035: step 188940, loss = 1.80 (815.5 examples/sec; 0.157 sec/batch)
2017-05-05 04:07:20.324989: step 188950, loss = 1.91 (872.6 examples/sec; 0.147 sec/batch)
2017-05-05 04:07:21.861354: step 188960, loss = 1.94 (833.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:07:23.354098: step 188970, loss = 1.95 (857.5 examples/sec; 0.149 sec/batch)
2017-05-05 04:07:24.937091: step 188980, loss = 2.06 (808.6 examples/sec; 0.158 sec/batch)
2017-05-05 04:07:26.481760: step 188990, loss = 1.89 (828.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:07:27.992347: step 189000, loss = 2.07 (847.3 examples/sec; 0.151 sec/batch)
2017-05-05 04:07:29.558205: step 189010, loss = 1.88 (817.4 examples/sec; 0.157 sec/batch)
2017-05-05 04:07:31.132176: step 189020, loss = 1.77 (813.2 examples/sec; 0.157 sec/batch)
2017-05-05 04:07:32.643985: step 189030, loss = 1.88 (846.7 examples/sec; 0.151 sec/batch)
2017-05-05 04:07:34.193422: step 189040, loss = 1.96 (826.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:07:35.702676: step 189050, loss = 1.96 (848.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:07:37.210425: step 189060, loss = 1.95 (848.9 examples/sec; 0.151 sec/batch)
2017-05-05 04:07:38.717960: step 189070, loss = 1.80 (849.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:07:40.243392: step 189080, loss = 1.82 (839.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:07:41.895003: step 189090, loss = 1.82 (775.0 examples/sec; 0.165 sec/batch)
2017-05-05 04:07:43.275903: step 189100, loss = 1.96 (926.9 examples/sec; 0.138 sec/batch)
2017-05-05 04:07:44.754440: step 189110, loss = 2.02 (865.7 examples/sec; 0.148 sec/batch)
2017-05-05 04:07:46.257171: step 189120, loss = 1.87 (851.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:07:47.826679: step 189130, loss = 1.80 (815.5 examples/sec; 0.157 sec/batch)
2017-05-05 04:07:49.342291: step 189140, loss = 1.81 (844.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:07:50.900055: step 189150, loss = 1.86 (821.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:07:52.432942: stepE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1945 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
 189160, loss = 1.96 (835.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:07:53.990839: step 189170, loss = 1.94 (821.6 examples/sec; 0.156 sec/batch)
2017-05-05 04:07:55.491607: step 189180, loss = 1.88 (852.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:07:57.067702: step 189190, loss = 1.82 (812.1 examples/sec; 0.158 sec/batch)
2017-05-05 04:07:58.554098: step 189200, loss = 1.92 (861.2 examples/sec; 0.149 sec/batch)
2017-05-05 04:08:00.170457: step 189210, loss = 2.03 (791.9 examples/sec; 0.162 sec/batch)
2017-05-05 04:08:01.687666: step 189220, loss = 1.84 (843.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:08:03.242735: step 189230, loss = 1.86 (823.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:08:04.763554: step 189240, loss = 1.95 (841.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:08:06.307061: step 189250, loss = 1.90 (829.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:08:07.802008: step 189260, loss = 1.91 (856.2 examples/sec; 0.149 sec/batch)
2017-05-05 04:08:09.377870: step 189270, loss = 1.87 (812.3 examples/sec; 0.158 sec/batch)
2017-05-05 04:08:10.945665: step 189280, loss = 2.04 (816.4 examples/sec; 0.157 sec/batch)
2017-05-05 04:08:12.510846: step 189290, loss = 1.91 (817.8 examples/sec; 0.157 sec/batch)
2017-05-05 04:08:13.991425: step 189300, loss = 1.86 (864.5 examples/sec; 0.148 sec/batch)
2017-05-05 04:08:15.553990: step 189310, loss = 1.91 (819.2 examples/sec; 0.156 sec/batch)
2017-05-05 04:08:17.145114: step 189320, loss = 1.84 (804.5 examples/sec; 0.159 sec/batch)
2017-05-05 04:08:18.640768: step 189330, loss = 1.78 (855.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:08:20.170294: step 189340, loss = 1.85 (836.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:08:21.725821: step 189350, loss = 1.86 (822.9 examples/sec; 0.156 sec/batch)
2017-05-05 04:08:23.308345: step 189360, loss = 1.86 (808.8 examples/sec; 0.158 sec/batch)
2017-05-05 04:08:24.890049: step 189370, loss = 1.74 (809.3 examples/sec; 0.158 sec/batch)
2017-05-05 04:08:26.443736: step 189380, loss = 1.82 (823.8 examples/sec; 0.155 sec/batch)
2017-05-05 04:08:27.961834: step 189390, loss = 1.88 (843.2 examples/sec; 0.152 sec/batch)
2017-05-05 04:08:29.509907: step 189400, loss = 1.83 (826.8 examples/sec; 0.155 sec/batch)
2017-05-05 04:08:31.037760: step 189410, loss = 1.95 (837.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:08:32.572132: step 189420, loss = 1.89 (834.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:08:34.067356: step 189430, loss = 1.95 (856.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:08:35.623877: step 189440, loss = 1.66 (822.3 examples/sec; 0.156 sec/batch)
2017-05-05 04:08:37.174103: step 189450, loss = 1.87 (825.7 examples/sec; 0.155 sec/batch)
2017-05-05 04:08:38.654967: step 189460, loss = 1.81 (864.4 examples/sec; 0.148 sec/batch)
2017-05-05 04:08:40.146133: step 189470, loss = 1.95 (858.4 examples/sec; 0.149 sec/batch)
2017-05-05 04:08:41.665642: step 189480, loss = 1.84 (842.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:08:43.197915: step 189490, loss = 1.71 (835.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:08:44.754419: step 189500, loss = 1.77 (822.4 examples/sec; 0.156 sec/batch)
2017-05-05 04:08:46.257106: step 189510, loss = 1.86 (851.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:08:47.827441: step 189520, loss = 1.90 (815.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:08:49.329630: step 189530, loss = 1.72 (852.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:08:50.870905: step 189540, loss = 1.85 (830.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:08:52.343013: step 189550, loss = 1.77 (869.5 examples/sec; 0.147 sec/batch)
2017-05-05 04:08:53.819415: step 189560, loss = 1.94 (867.0 examples/sec; 0.148 sec/batch)
2017-05-05 04:08:55.397014: step 189570, loss = 2.07 (811.4 examples/sec; 0.158 sec/batch)
2017-05-05 04:08:56.912047: step 189580, loss = 1.76 (844.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:08:58.424904: step 189590, loss = 1.81 (846.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:08:59.929744: step 189600, loss = 2.05 (850.6 examples/sec; 0.150 sec/batch)
2017-05-05 04:09:01.479619: step 189610, loss = 1.93 (825.9 examples/sec; 0.155 sec/batch)
2017-05-05 04:09:03.018455: step 189620, loss = 1.93 (831.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:09:04.535612: step 189630, loss = 1.85 (843.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:09:06.056152: step 189640, loss = 1.96 (841.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:09:07.641586: step 189650, loss = 2.03 (807.4 examples/sec; 0.159 sec/batch)
2017-05-05 04:09:09.132610: step 189660, loss = 1.87 (858.5 examples/sec; 0.149 sec/batch)
2017-05-05 04:09:10.679192: step 189670, loss = 1.85 (827.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:09:12.205913: step 189680, loss = 1.66 (838.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:09:13.740037: step 189690, loss = 2.07 (834.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:09:15.304936: step 189700, loss = 1.83 (817.9 examples/sec; 0.156 sec/batch)
2017-05-05 04:09:16.805648: step 189710, loss = 1.74 (852.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:09:18.345808: step 189720, loss = 1.95 (831.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:09:19.905453: step 189730, loss = 1.86 (820.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:09:21.449812: step 189740, loss = 1.84 (828.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:09:22.950714: step 189750, loss = 1.92 (852.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:09:24.464095: step 189760, loss = 1.91 (845.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:09:25.998730: step 189770, loss = 1.82 (834.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:09:27.565444: step 189780, loss = 1.82 (817.0 examples/sec; 0.157 sec/batch)
2017-05-05 04:09:29.138996: step 189790, loss = 1.84 (813.4 examples/sec; 0.157 sec/batch)
2017-05-05 04:09:30.628023: step 189800, loss = 1.88 (859.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:09:32.201186: step 189810, loss = 1.80 (813.6 examples/sec; 0.157 sec/batch)
2017-05-05 04:09:33.707464: step 189820, loss = 1.84 (849.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:09:35.223547: step 189830, loss = 1.79 (844.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:09:36.742717: step 189840, loss = 1.92 (842.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:09:38.296660: step 189850, loss = 1.72 (823.7 examples/sec; 0.155 sec/batch)
2017-05-05 04:09:39.843835: step 189860, loss = 1.82 (827.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:09:41.376314: step 189870, loss = 1.76 (835.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:09:42.869368: step 189880, loss = 1.88 (857.3 examples/sec; 0.149 sec/batch)
2017-05-05 04:09:44.352349: step 189890, loss = 1.90 (863.1 examples/sec; 0.148 sec/batch)
2017-05-05 04:09:45.893740: step 189900, loss = 1.84 (830.4 examples/sec; 0.154 sec/batch)
2017-05-05 04:09:47.467458: step 189910, loss = 1.82 (813.4 examples/sec; 0.157 sec/batch)
2017-05-05 04:09:49.009566: step 189920, loss = 1.81 (830.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:09:50.532041: step 189930, loss = 1.78 (840.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:09:52.119370: step 189940, loss = 1.75 (806.4 examples/sec; 0.159 sec/batch)
2017-05-05 04:09:53.645954: step 189950, loss = 1.92 (838.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:09:55.135122: step 189960, loss = 1.79 (859.5 examples/sec; 0.149 sec/batch)
2017-05-05 04:09:56.699579: step 189970, loss = 1.85 (818.2 examples/sec; 0.156 sec/batch)
2017-05-05 04:09:58.184607: step 189980, loss = 1.80 (861.9 examples/sec; 0.149 sec/batch)
2017-05-05 04:09:59.714601: step 189990, loss = 1.88 (836.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:10:01.237875: step 190000, loss = 1.74 (840.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:10:02.738044: step 190010, loss = 1.85 (853.2 examples/sec; 0.150 sec/batch)
2017-05-05 04:10:04.265846: step 190020, loss = 1.77 (837.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:10:05.758327: step 190030, loss = 1.80 (857.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:10:07.309700: step 190040, loss = 1.81 (825.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:10:08.906341: step 190050, loss = 1.85 (801.7 examples/sec; 0.160 sec/batch)
2017-05-05 04:10:10.503367: step 1E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1954 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
90060, loss = 1.81 (801.5 examples/sec; 0.160 sec/batch)
2017-05-05 04:10:11.969747: step 190070, loss = 1.86 (872.9 examples/sec; 0.147 sec/batch)
2017-05-05 04:10:13.627895: step 190080, loss = 2.05 (771.9 examples/sec; 0.166 sec/batch)
2017-05-05 04:10:15.140962: step 190090, loss = 1.81 (846.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:10:16.667444: step 190100, loss = 1.76 (838.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:10:18.153755: step 190110, loss = 1.78 (861.2 examples/sec; 0.149 sec/batch)
2017-05-05 04:10:19.701211: step 190120, loss = 1.92 (827.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:10:21.231751: step 190130, loss = 2.06 (836.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:10:22.714368: step 190140, loss = 1.86 (863.3 examples/sec; 0.148 sec/batch)
2017-05-05 04:10:24.204009: step 190150, loss = 1.87 (859.3 examples/sec; 0.149 sec/batch)
2017-05-05 04:10:25.713456: step 190160, loss = 1.86 (848.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:10:27.292285: step 190170, loss = 1.82 (810.7 examples/sec; 0.158 sec/batch)
2017-05-05 04:10:28.836115: step 190180, loss = 1.90 (829.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:10:30.399438: step 190190, loss = 2.02 (818.8 examples/sec; 0.156 sec/batch)
2017-05-05 04:10:31.964915: step 190200, loss = 1.85 (817.6 examples/sec; 0.157 sec/batch)
2017-05-05 04:10:33.456990: step 190210, loss = 1.77 (857.9 examples/sec; 0.149 sec/batch)
2017-05-05 04:10:35.061150: step 190220, loss = 1.80 (797.9 examples/sec; 0.160 sec/batch)
2017-05-05 04:10:36.607031: step 190230, loss = 2.05 (828.0 examples/sec; 0.155 sec/batch)
2017-05-05 04:10:38.167283: step 190240, loss = 1.96 (820.4 examples/sec; 0.156 sec/batch)
2017-05-05 04:10:39.712334: step 190250, loss = 1.92 (828.4 examples/sec; 0.155 sec/batch)
2017-05-05 04:10:41.239684: step 190260, loss = 1.90 (838.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:10:42.738443: step 190270, loss = 1.97 (854.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:10:44.230991: step 190280, loss = 1.87 (857.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:10:45.792209: step 190290, loss = 1.91 (819.9 examples/sec; 0.156 sec/batch)
2017-05-05 04:10:47.374007: step 190300, loss = 1.84 (809.2 examples/sec; 0.158 sec/batch)
2017-05-05 04:10:48.849622: step 190310, loss = 1.83 (867.4 examples/sec; 0.148 sec/batch)
2017-05-05 04:10:50.418797: step 190320, loss = 1.81 (815.7 examples/sec; 0.157 sec/batch)
2017-05-05 04:10:51.985644: step 190330, loss = 1.98 (816.9 examples/sec; 0.157 sec/batch)
2017-05-05 04:10:53.493124: step 190340, loss = 1.80 (849.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:10:54.980193: step 190350, loss = 1.79 (860.8 examples/sec; 0.149 sec/batch)
2017-05-05 04:10:56.539151: step 190360, loss = 1.83 (821.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:10:58.117367: step 190370, loss = 1.94 (811.0 examples/sec; 0.158 sec/batch)
2017-05-05 04:10:59.682055: step 190380, loss = 1.92 (818.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:11:01.246917: step 190390, loss = 1.80 (818.0 examples/sec; 0.156 sec/batch)
2017-05-05 04:11:02.771525: step 190400, loss = 1.99 (839.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:11:04.292619: step 190410, loss = 1.88 (841.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:11:05.802890: step 190420, loss = 1.87 (847.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:11:07.324905: step 190430, loss = 1.90 (841.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:11:08.875213: step 190440, loss = 1.94 (825.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:11:10.439106: step 190450, loss = 1.94 (818.5 examples/sec; 0.156 sec/batch)
2017-05-05 04:11:12.028658: step 190460, loss = 1.82 (805.3 examples/sec; 0.159 sec/batch)
2017-05-05 04:11:13.540572: step 190470, loss = 1.79 (846.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:11:15.060527: step 190480, loss = 1.90 (842.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:11:16.611402: step 190490, loss = 1.94 (825.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:11:18.111193: step 190500, loss = 1.81 (853.5 examples/sec; 0.150 sec/batch)
2017-05-05 04:11:19.639821: step 19E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1962 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
0510, loss = 1.89 (837.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:11:21.176189: step 190520, loss = 1.75 (833.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:11:22.759311: step 190530, loss = 1.82 (808.5 examples/sec; 0.158 sec/batch)
2017-05-05 04:11:24.309837: step 190540, loss = 1.84 (825.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:11:25.825222: step 190550, loss = 1.78 (844.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:11:27.376663: step 190560, loss = 1.85 (825.0 examples/sec; 0.155 sec/batch)
2017-05-05 04:11:28.878039: step 190570, loss = 1.80 (852.6 examples/sec; 0.150 sec/batch)
2017-05-05 04:11:30.373719: step 190580, loss = 1.85 (855.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:11:31.955259: step 190590, loss = 1.86 (809.3 examples/sec; 0.158 sec/batch)
2017-05-05 04:11:33.538110: step 190600, loss = 1.70 (808.7 examples/sec; 0.158 sec/batch)
2017-05-05 04:11:35.095507: step 190610, loss = 1.76 (821.9 examples/sec; 0.156 sec/batch)
2017-05-05 04:11:36.631383: step 190620, loss = 1.85 (833.4 examples/sec; 0.154 sec/batch)
2017-05-05 04:11:38.205545: step 190630, loss = 1.85 (813.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:11:39.708422: step 190640, loss = 2.03 (851.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:11:41.241607: step 190650, loss = 1.84 (834.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:11:42.743457: step 190660, loss = 2.07 (852.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:11:44.320522: step 190670, loss = 1.85 (811.6 examples/sec; 0.158 sec/batch)
2017-05-05 04:11:45.826342: step 190680, loss = 1.92 (850.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:11:47.316014: step 190690, loss = 1.91 (859.2 examples/sec; 0.149 sec/batch)
2017-05-05 04:11:48.886521: step 190700, loss = 1.67 (815.0 examples/sec; 0.157 sec/batch)
2017-05-05 04:11:50.373152: step 190710, loss = 2.00 (861.0 examples/sec; 0.149 sec/batch)
2017-05-05 04:11:51.873359: step 190720, loss = 1.88 (853.2 examples/sec; 0.150 sec/batch)
2017-05-05 04:11:53.415039: step 190730, loss = 2.00 (830.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:11:54.905436: step 190740, loss = 1.88 (858.8 examples/sec; 0.149 sec/batch)
2017-05-05 04:11:56.460941: step 190750, loss = 1.82 (822.9 examples/sec; 0.156 sec/batch)
2017-05-05 04:11:58.021642: step 190760, loss = 1.76 (820.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:11:59.547672: step 190770, loss = 1.86 (838.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:12:01.073597: step 190780, loss = 1.82 (838.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:12:02.609061: step 190790, loss = 1.84 (833.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:12:04.161941: step 190800, loss = 1.78 (824.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:12:05.660923: step 190810, loss = 1.85 (853.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:12:07.210103: step 190820, loss = 1.99 (826.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:12:08.747127: step 190830, loss = 1.76 (832.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:12:10.278179: step 190840, loss = 1.99 (836.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:12:11.823637: step 190850, loss = 1.93 (828.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:12:13.341038: step 190860, loss = 1.80 (843.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:12:14.878941: step 190870, loss = 1.79 (832.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:12:16.415425: step 190880, loss = 1.90 (833.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:12:17.878985: step 190890, loss = 1.93 (874.6 examples/sec; 0.146 sec/batch)
2017-05-05 04:12:19.422849: step 190900, loss = 1.78 (829.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:12:20.931431: step 190910, loss = 1.80 (848.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:12:22.432379: step 190920, loss = 1.92 (852.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:12:24.007524: step 190930, loss = 1.84 (812.6 examples/sec; 0.158 sec/batch)
2017-05-05 04:12:25.510521: step 190940, loss = 1.92 (851.6 examples/sec; 0.150 sec/batch)
2017-05-05 04:12:27.029412: step 190950, loss = 2.02 (842.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:12:28.524845: step 190960, loss = 1.86 (855.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:12:30.027956: step 190970, loss = 1.79 (851.6 examples/sec; 0.150 sec/batch)
2017-05-05 04:12:31.570782: step 190980, loss = 1.96 (829.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:12:33.089863: step 190990, loss = 1.95 (842.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:12:34.609310: step 191000, loss = 1.84 (842.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:12:36.106999: step 191010, loss = 1.85 (854.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:12:37.594967: step 191020, loss = 1.79 (860.2 examples/sec; 0.149 sec/batch)
2017-05-05 04:12:39.118205: step 191030, loss = 1.85 (840.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:12:40.597933: step 191040, loss = 1.81 (865.0 examples/sec; 0.148 sec/batch)
2017-05-05 04:12:42.113095: step 191050, loss = 1.87 (844.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:12:43.619617: step 191060, loss = 1.87 (849.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:12:45.283606: step 191070, loss = 1.87 (769.2 examples/sec; 0.166 sec/batch)
2017-05-05 04:12:46.702905: step 191080, loss = 1.98 (901.9 examples/sec; 0.142 sec/batch)
2017-05-05 04:12:48.236987: step 191090, loss = 1.84 (834.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:12:49.804819: step 191100, loss = 1.83 (816.4 examples/sec; 0.157 sec/batch)
2017-05-05 04:12:51.340599: step 191110, loss = 1.74 (833.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:12:52.846462: step 191120, loss = 1.80 (850.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:12:54.374827: step 191130, loss = 1.83 (837.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:12:55.873281: step 191140, loss = 1.65 (854.2 examples/sec; 0.150 sec/batch)
2017-05-05 04:12:57.443624: step 191150, loss = 1.93 (815.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:12:58.963219: step 191160, loss = 1.90 (842.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:13:00.486413: step 191170, loss = 1.84 (840.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:13:02.028385: step 191180, loss = 1.92 (830.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:13:03.509771: step 191190, loss = 1.89 (864.0 examples/sec; 0.148 sec/batch)
2017-05-05 04:13:05.039111: step 191200, loss = 1.93 (837.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:13:06.575046: step 191210, loss = 1.80 (833.4 examples/sec; 0.154 sec/batch)
2017-05-05 04:13:08.118575: step 191220, loss = 1.80 (829.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:13:09.636257: step 191230, loss = 1.71 (843.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:13:11.179382: step 191240, loss = 1.92 (829.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:13:12.781772: step 191250, loss = 1.78 (798.8 examples/sec; 0.160 sec/batch)
2017-05-05 04:13:14.287694: step 191260, loss = 1.81 (850.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:13:15.810764: step 191270, loss = 1.77 (840.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:13:17.355752: step 191280, loss = 1.90 (828.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:13:18.876448: step 191290, loss = 1.80 (841.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:13:20.410450: step 191300, loss = 1.94 (834.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:13:21.935075: step 191310, loss = 1.90 (839.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:13:23.500365: step 191320, loss = 1.82 (817.7 examples/sec; 0.157 sec/batch)
2017-05-05 04:13:25.075743: step 191330, loss = 1.99 (812.5 examples/sec; 0.158 sec/batch)
2017-05-05 04:13:26.625676: step 191340, loss = 1.83 (825.8 examples/sec; 0.155 sec/batch)
2017-05-05 04:13:28.199413: step 191350, loss = 1.88 (813.3 examples/sec; 0.157 sec/batch)
2017-05-05 04:13:29.743377: step 191360, loss = 1.87 (829.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:13:31.278883: step 191370, loss = 1.86 (833.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:13:32.794351: step 191380, loss = 1.93 (844.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:13:34.307224: step 191390, loss = 1.81 (846.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:13:35.832750: step 191400, loss = 1.88 (839.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:13:37.308892: step 1914E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1970 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
10, loss = 1.90 (867.1 examples/sec; 0.148 sec/batch)
2017-05-05 04:13:38.812100: step 191420, loss = 1.74 (851.5 examples/sec; 0.150 sec/batch)
2017-05-05 04:13:40.338383: step 191430, loss = 1.87 (838.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:13:41.868547: step 191440, loss = 1.86 (836.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:13:43.424338: step 191450, loss = 1.90 (822.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:13:44.906761: step 191460, loss = 1.96 (863.5 examples/sec; 0.148 sec/batch)
2017-05-05 04:13:46.422488: step 191470, loss = 1.73 (844.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:13:47.935438: step 191480, loss = 1.79 (846.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:13:49.474205: step 191490, loss = 1.84 (831.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:13:51.023997: step 191500, loss = 1.86 (825.9 examples/sec; 0.155 sec/batch)
2017-05-05 04:13:52.532262: step 191510, loss = 1.86 (848.7 examples/sec; 0.151 sec/batch)
2017-05-05 04:13:53.998193: step 191520, loss = 2.05 (873.2 examples/sec; 0.147 sec/batch)
2017-05-05 04:13:55.473939: step 191530, loss = 1.71 (867.4 examples/sec; 0.148 sec/batch)
2017-05-05 04:13:56.973970: step 191540, loss = 1.87 (853.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:13:58.469215: step 191550, loss = 1.87 (856.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:13:59.980354: step 191560, loss = 1.99 (847.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:14:01.503007: step 191570, loss = 1.85 (840.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:14:02.999676: step 191580, loss = 1.90 (855.2 examples/sec; 0.150 sec/batch)
2017-05-05 04:14:04.554786: step 191590, loss = 1.90 (823.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:14:06.077427: step 191600, loss = 1.92 (840.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:14:07.606420: step 191610, loss = 1.71 (837.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:14:09.113873: step 191620, loss = 1.97 (849.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:14:10.598989: step 191630, loss = 1.87 (861.9 examples/sec; 0.149 sec/batch)
2017-05-05 04:14:12.152996: step 191640, loss = 1.93 (823.7 examples/sec; 0.155 sec/batch)
2017-05-05 04:14:13.638370: step 191650, loss = 1.87 (861.7 examples/sec; 0.149 sec/batch)
2017-05-05 04:14:15.224770: step 191660, loss = 1.81 (806.9 examples/sec; 0.159 sec/batch)
2017-05-05 04:14:16.825971: step 191670, loss = 1.96 (799.4 examples/sec; 0.160 sec/batch)
2017-05-05 04:14:18.328736: step 191680, loss = 1.98 (851.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:14:19.884430: step 191690, loss = 1.85 (822.8 examples/sec; 0.156 sec/batch)
2017-05-05 04:14:21.418683: step 191700, loss = 1.74 (834.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:14:22.943735: step 191710, loss = 1.97 (839.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:14:24.436403: step 191720, loss = 1.94 (857.5 examples/sec; 0.149 sec/batch)
2017-05-05 04:14:26.011296: step 191730, loss = 1.92 (812.8 examples/sec; 0.157 sec/batch)
2017-05-05 04:14:27.523137: step 191740, loss = 1.93 (846.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:14:29.054118: step 191750, loss = 1.76 (836.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:14:30.514154: step 191760, loss = 1.87 (876.7 examples/sec; 0.146 sec/batch)
2017-05-05 04:14:32.141142: step 191770, loss = 1.73 (786.7 examples/sec; 0.163 sec/batch)
2017-05-05 04:14:33.667826: step 191780, loss = 1.95 (838.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:14:35.175995: step 191790, loss = 1.70 (848.7 examples/sec; 0.151 sec/batch)
2017-05-05 04:14:36.700850: step 191800, loss = 1.82 (839.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:14:38.233953: step 191810, loss = 2.00 (834.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:14:39.753053: step 191820, loss = 1.95 (842.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:14:41.334215: step 191830, loss = 1.74 (809.5 examples/sec; 0.158 sec/batch)
2017-05-05 04:14:42.834283: step 191840, loss = 1.89 (853.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:14:44.349365: step 191850, loss = 1.68 (844.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:14:45.867749: step 191860, loss = 1.76 (843.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:14:47.487540: step 191870, loss = 1.99 (790.2 examples/sec; 0.162 sec/batch)
2017-05-05 04:14:48.997728: step 191880, loss = 1.67 (847.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:14:50.518923: step 191890, loss = 2.12 (841.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:14:52.103817: step 191900, loss = 1.88 (807.6 examples/sec; 0.158 sec/batch)
2017-05-05 04:14:53.620861: step 191910, loss = 1.85 (843.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:14:55.169964: step 191920, loss = 1.85 (826.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:14:56.720536: step 191930, loss = 2.01 (825.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:14:58.244451: step 191940, loss = 1.69 (839.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:14:59.733454: step 191950, loss = 1.99 (859.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:15:01.259999: step 191960, loss = 1.88 (838.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:15:02.820830: step 191970, loss = 1.88 (820.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:15:04.372328: step 191980, loss = 1.91 (825.0 examples/sec; 0.155 sec/batch)
2017-05-05 04:15:05.927046: step 191990, loss = 1.90 (823.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:15:07.441059: step 192000, loss = 1.87 (845.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:15:08.965455: step 192010, loss = 1.79 (839.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:15:10.465339: step 192020, loss = 1.86 (853.4 examples/sec; 0.150 sec/batch)
2017-05-05 04:15:12.017778: step 192030, loss = 1.78 (824.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:15:13.548114: step 192040, loss = 1.92 (836.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:15:15.080895: step 192050, loss = 1.88 (835.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:15:16.742964: step 192060, loss = 1.87 (770.1 examples/sec; 0.166 sec/batch)
2017-05-05 04:15:18.183360: step 192070, loss = 1.96 (888.6 examples/sec; 0.144 sec/batch)
2017-05-05 04:15:19.704302: step 192080, loss = 1.92 (841.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:15:21.205562: step 192090, loss = 1.84 (852.6 examples/sec; 0.150 sec/batch)
2017-05-05 04:15:22.704459: step 192100, loss = 1.91 (854.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:15:24.273447: step 192110, loss = 1.75 (815.8 examples/sec; 0.157 sec/batch)
2017-05-05 04:15:25.775828: step 192120, loss = 1.84 (852.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:15:27.296975: step 192130, loss = 1.75 (841.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:15:28.806755: step 192140, loss = 1.79 (847.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:15:30.373208: step 192150, loss = 2.02 (817.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:15:31.933592: step 192160, loss = 1.79 (820.3 examples/sec; 0.156 sec/batch)
2017-05-05 04:15:33.434423: step 192170, loss = 1.75 (852.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:15:35.005840: step 192180, loss = 1.75 (814.6 examples/sec; 0.157 sec/batch)
2017-05-05 04:15:36.575462: step 192190, loss = 1.81 (815.5 examples/sec; 0.157 sec/batch)
2017-05-05 04:15:38.084449: step 192200, loss = 1.83 (848.3 examples/sec; 0.151 sec/batch)
2017-05-05 04:15:39.655672: step 192210, loss = 1.97 (814.7 examples/sec; 0.157 sec/batch)
2017-05-05 04:15:41.195386: step 192220, loss = 2.04 (831.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:15:42.744115: step 192230, loss = 1.74 (826.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:15:44.289874: step 192240, loss = 1.89 (828.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:15:45.800741: step 192250, loss = 1.85 (847.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:15:47.374447: step 192260, loss = 1.89 (813.4 examples/sec; 0.157 sec/batch)
2017-05-05 04:15:48.955623: step 192270, loss = 1.97 (809.5 examples/sec; 0.158 sec/batch)
2017-05-05 04:15:50.509934: step 192280, loss = 1.93 (823.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:15:52.037803: step 192290, loss = 1.92 (837.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:15:53.570370: step 192300, loss = 1.78 (835.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:15:55.076616: step 192310E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1978 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
, loss = 1.81 (849.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:15:56.587026: step 192320, loss = 1.82 (847.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:15:58.126290: step 192330, loss = 1.85 (831.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:15:59.663411: step 192340, loss = 1.88 (832.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:16:01.192190: step 192350, loss = 1.80 (837.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:16:02.695981: step 192360, loss = 1.77 (851.2 examples/sec; 0.150 sec/batch)
2017-05-05 04:16:04.215937: step 192370, loss = 1.96 (842.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:16:05.758425: step 192380, loss = 1.96 (829.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:16:07.241749: step 192390, loss = 1.84 (862.9 examples/sec; 0.148 sec/batch)
2017-05-05 04:16:08.803054: step 192400, loss = 1.82 (819.8 examples/sec; 0.156 sec/batch)
2017-05-05 04:16:10.339521: step 192410, loss = 1.90 (833.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:16:11.869715: step 192420, loss = 1.93 (836.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:16:13.395679: step 192430, loss = 2.08 (838.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:16:14.936043: step 192440, loss = 1.92 (831.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:16:16.467790: step 192450, loss = 1.82 (835.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:16:18.019690: step 192460, loss = 1.78 (824.8 examples/sec; 0.155 sec/batch)
2017-05-05 04:16:19.564661: step 192470, loss = 1.89 (828.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:16:21.092197: step 192480, loss = 1.83 (838.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:16:22.616438: step 192490, loss = 1.82 (839.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:16:24.183281: step 192500, loss = 1.85 (817.0 examples/sec; 0.157 sec/batch)
2017-05-05 04:16:25.710365: step 192510, loss = 1.75 (838.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:16:27.274990: step 192520, loss = 1.98 (818.1 examples/sec; 0.156 sec/batch)
2017-05-05 04:16:28.771619: step 192530, loss = 1.80 (855.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:16:30.351862: step 192540, loss = 2.00 (810.0 examples/sec; 0.158 sec/batch)
2017-05-05 04:16:31.862202: step 192550, loss = 1.69 (847.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:16:33.400383: step 192560, loss = 1.85 (832.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:16:34.920071: step 192570, loss = 1.83 (842.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:16:36.440878: step 192580, loss = 1.93 (841.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:16:38.011217: step 192590, loss = 1.82 (815.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:16:39.541717: step 192600, loss = 1.94 (836.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:16:41.071964: step 192610, loss = 1.85 (836.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:16:42.561781: step 192620, loss = 1.80 (859.2 examples/sec; 0.149 sec/batch)
2017-05-05 04:16:44.099929: step 192630, loss = 2.04 (832.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:16:45.654627: step 192640, loss = 1.89 (823.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:16:47.194334: step 192650, loss = 2.07 (831.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:16:48.715199: step 192660, loss = 1.81 (841.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:16:50.264133: step 192670, loss = 1.83 (826.4 examples/sec; 0.155 sec/batch)
2017-05-05 04:16:51.782738: step 192680, loss = 1.85 (842.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:16:53.324308: step 192690, loss = 1.99 (830.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:16:54.823279: step 192700, loss = 1.78 (853.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:16:56.401621: step 192710, loss = 1.93 (811.0 examples/sec; 0.158 sec/batch)
2017-05-05 04:16:57.975842: step 192720, loss = 1.96 (813.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:16:59.465677: step 192730, loss = 1.94 (859.2 examples/sec; 0.149 sec/batch)
2017-05-05 04:17:00.968124: step 192740, loss = 1.80 (851.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:17:02.462944: step 192750, loss = 1.86 (856.3 examples/sec; 0.149 sec/batch)
2017-05-05 04:17:04.058040: step 192760, loss = 1.91 (802.5 examples/sec; 0.160 sec/batch)
2017-05-05 04:17:05.604820: step 192770, loss = 1.80 (827.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:17:07.136483: step 192780, loss = 1.91 (835.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:17:08.697971: step 192790, loss = 1.80 (819.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:17:10.247973: step 192800, loss = 1.77 (825.8 examples/sec; 0.155 sec/batch)
2017-05-05 04:17:11.736985: step 192810, loss = 1.78 (859.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:17:13.235263: step 192820, loss = 1.75 (854.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:17:14.784555: step 192830, loss = 2.01 (826.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:17:16.263548: step 192840, loss = 1.72 (865.5 examples/sec; 0.148 sec/batch)
2017-05-05 04:17:17.750958: step 192850, loss = 1.86 (860.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:17:19.308646: step 192860, loss = 2.00 (821.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:17:20.849804: step 192870, loss = 1.99 (830.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:17:22.401217: step 192880, loss = 1.79 (825.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:17:23.918128: step 192890, loss = 1.88 (843.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:17:25.462754: step 192900, loss = 1.77 (828.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:17:26.986679: step 192910, loss = 1.84 (839.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:17:28.522258: step 192920, loss = 1.85 (833.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:17:30.028091: step 192930, loss = 1.98 (850.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:17:31.600655: step 192940, loss = 1.81 (814.0 examples/sec; 0.157 sec/batch)
2017-05-05 04:17:33.154369: step 192950, loss = 2.04 (823.8 examples/sec; 0.155 sec/batch)
2017-05-05 04:17:34.738934: step 192960, loss = 1.64 (807.8 examples/sec; 0.158 sec/batch)
2017-05-05 04:17:36.293379: step 192970, loss = 2.00 (823.4 examples/sec; 0.155 sec/batch)
2017-05-05 04:17:37.804606: step 192980, loss = 1.81 (847.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:17:39.395222: step 192990, loss = 1.84 (804.7 examples/sec; 0.159 sec/batch)
2017-05-05 04:17:40.906741: step 193000, loss = 1.80 (846.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:17:42.458376: step 193010, loss = 1.86 (824.9 examples/sec; 0.155 sec/batch)
2017-05-05 04:17:44.002137: step 193020, loss = 1.88 (829.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:17:45.534163: step 193030, loss = 1.84 (835.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:17:47.027177: step 193040, loss = 1.92 (857.3 examples/sec; 0.149 sec/batch)
2017-05-05 04:17:48.690891: step 193050, loss = 1.85 (769.4 examples/sec; 0.166 sec/batch)
2017-05-05 04:17:50.087425: step 193060, loss = 1.90 (916.6 examples/sec; 0.140 sec/batch)
2017-05-05 04:17:51.661856: step 193070, loss = 1.72 (813.0 examples/sec; 0.157 sec/batch)
2017-05-05 04:17:53.187391: step 193080, loss = 1.82 (839.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:17:54.726908: step 193090, loss = 1.91 (831.4 examples/sec; 0.154 sec/batch)
2017-05-05 04:17:56.252007: step 193100, loss = 1.73 (839.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:17:57.775304: step 193110, loss = 1.74 (840.3 examples/sec; 0.152 sec/batch)
2017-05-05 04:17:59.309027: step 193120, loss = 1.92 (834.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:18:00.815877: step 193130, loss = 1.97 (849.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:18:02.386342: step 193140, loss = 1.82 (815.0 examples/sec; 0.157 sec/batch)
2017-05-05 04:18:03.957332: step 193150, loss = 1.85 (814.8 examples/sec; 0.157 sec/batch)
2017-05-05 04:18:05.453827: step 193160, loss = 1.80 (855.3 examples/sec; 0.150 sec/batch)
2017-05-05 04:18:07.016415: step 193170, loss = 1.71 (819.2 examples/sec; 0.156 sec/batch)
2017-05-05 04:18:08.598984: step 193180, loss = 1.86 (808.8 examples/sec; 0.158 sec/batch)
2017-05-05 04:18:10.140450: step 193190, loss = 2.00 (830.4 examples/sec; 0.154 sec/batch)
2017-05-05 04:18:11.674512: step 193200, loss = 1.90 (834.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:18:13.201208: step 193210, E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1986 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
loss = 1.91 (838.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:18:14.771668: step 193220, loss = 1.96 (815.0 examples/sec; 0.157 sec/batch)
2017-05-05 04:18:16.279293: step 193230, loss = 1.94 (849.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:18:17.793161: step 193240, loss = 1.75 (845.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:18:19.353739: step 193250, loss = 1.75 (820.2 examples/sec; 0.156 sec/batch)
2017-05-05 04:18:20.906194: step 193260, loss = 1.92 (824.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:18:22.380528: step 193270, loss = 1.98 (868.2 examples/sec; 0.147 sec/batch)
2017-05-05 04:18:23.927968: step 193280, loss = 1.83 (827.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:18:25.480460: step 193290, loss = 1.84 (824.5 examples/sec; 0.155 sec/batch)
2017-05-05 04:18:26.950271: step 193300, loss = 1.77 (870.9 examples/sec; 0.147 sec/batch)
2017-05-05 04:18:28.456483: step 193310, loss = 1.71 (849.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:18:29.959836: step 193320, loss = 1.85 (851.4 examples/sec; 0.150 sec/batch)
2017-05-05 04:18:31.434430: step 193330, loss = 1.99 (868.0 examples/sec; 0.147 sec/batch)
2017-05-05 04:18:32.947760: step 193340, loss = 1.87 (845.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:18:34.448901: step 193350, loss = 1.90 (852.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:18:35.955508: step 193360, loss = 1.94 (849.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:18:37.531757: step 193370, loss = 1.92 (812.1 examples/sec; 0.158 sec/batch)
2017-05-05 04:18:39.031131: step 193380, loss = 1.80 (853.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:18:40.558847: step 193390, loss = 1.86 (837.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:18:41.997151: step 193400, loss = 1.70 (889.9 examples/sec; 0.144 sec/batch)
2017-05-05 04:18:43.537690: step 193410, loss = 1.78 (830.9 examples/sec; 0.154 sec/batch)
2017-05-05 04:18:45.034782: step 193420, loss = 2.06 (855.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:18:46.566702: step 193430, loss = 1.83 (835.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:18:48.077841: step 193440, loss = 1.92 (847.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:18:49.600598: step 193450, loss = 1.74 (840.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:18:51.113815: step 193460, loss = 1.76 (845.9 examples/sec; 0.151 sec/batch)
2017-05-05 04:18:52.653186: step 193470, loss = 1.87 (831.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:18:54.208600: step 193480, loss = 1.83 (822.9 examples/sec; 0.156 sec/batch)
2017-05-05 04:18:55.771542: step 193490, loss = 1.86 (819.0 examples/sec; 0.156 sec/batch)
2017-05-05 04:18:57.347310: step 193500, loss = 1.94 (812.3 examples/sec; 0.158 sec/batch)
2017-05-05 04:18:58.896844: step 193510, loss = 1.97 (826.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:19:00.424885: step 193520, loss = 1.76 (837.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:19:01.918631: step 193530, loss = 1.87 (856.9 examples/sec; 0.149 sec/batch)
2017-05-05 04:19:03.496507: step 193540, loss = 1.98 (811.2 examples/sec; 0.158 sec/batch)
2017-05-05 04:19:05.006217: step 193550, loss = 1.82 (847.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:19:06.483988: step 193560, loss = 1.88 (866.2 examples/sec; 0.148 sec/batch)
2017-05-05 04:19:08.022751: step 193570, loss = 1.81 (831.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:19:09.569879: step 193580, loss = 1.83 (827.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:19:11.094747: step 193590, loss = 1.89 (839.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:19:12.665431: step 193600, loss = 1.70 (814.9 examples/sec; 0.157 sec/batch)
2017-05-05 04:19:14.256966: step 193610, loss = 1.87 (804.3 examples/sec; 0.159 sec/batch)
2017-05-05 04:19:15.815702: step 193620, loss = 1.82 (821.2 examples/sec; 0.156 sec/batch)
2017-05-05 04:19:17.328300: step 193630, loss = 1.87 (846.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:19:18.848927: step 193640, loss = 1.90 (841.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:19:20.357288: step 193650, loss = 2.04 (848.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:19:21.873906: step 193660, lE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1995 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
oss = 1.84 (844.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:19:23.418657: step 193670, loss = 1.82 (828.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:19:24.978534: step 193680, loss = 1.86 (820.6 examples/sec; 0.156 sec/batch)
2017-05-05 04:19:26.505343: step 193690, loss = 1.76 (838.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:19:28.052075: step 193700, loss = 1.94 (827.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:19:29.627857: step 193710, loss = 1.87 (812.3 examples/sec; 0.158 sec/batch)
2017-05-05 04:19:31.193435: step 193720, loss = 1.92 (817.6 examples/sec; 0.157 sec/batch)
2017-05-05 04:19:32.753921: step 193730, loss = 1.76 (820.3 examples/sec; 0.156 sec/batch)
2017-05-05 04:19:34.263068: step 193740, loss = 1.75 (848.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:19:35.753700: step 193750, loss = 1.79 (858.7 examples/sec; 0.149 sec/batch)
2017-05-05 04:19:37.316871: step 193760, loss = 1.69 (818.8 examples/sec; 0.156 sec/batch)
2017-05-05 04:19:38.846743: step 193770, loss = 1.67 (836.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:19:40.382896: step 193780, loss = 1.75 (833.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:19:41.912639: step 193790, loss = 1.94 (836.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:19:43.439165: step 193800, loss = 1.79 (838.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:19:45.012778: step 193810, loss = 1.88 (813.4 examples/sec; 0.157 sec/batch)
2017-05-05 04:19:46.533894: step 193820, loss = 1.85 (841.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:19:48.129116: step 193830, loss = 1.76 (802.4 examples/sec; 0.160 sec/batch)
2017-05-05 04:19:49.648974: step 193840, loss = 1.87 (842.2 examples/sec; 0.152 sec/batch)
2017-05-05 04:19:51.166071: step 193850, loss = 1.82 (843.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:19:52.713086: step 193860, loss = 1.87 (827.4 examples/sec; 0.155 sec/batch)
2017-05-05 04:19:54.217338: step 193870, loss = 2.05 (850.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:19:55.759192: step 193880, loss = 1.82 (830.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:19:57.254060: step 193890, loss = 1.88 (856.3 examples/sec; 0.149 sec/batch)
2017-05-05 04:19:58.728110: step 193900, loss = 1.89 (868.4 examples/sec; 0.147 sec/batch)
2017-05-05 04:20:00.291082: step 193910, loss = 1.88 (819.0 examples/sec; 0.156 sec/batch)
2017-05-05 04:20:01.796583: step 193920, loss = 1.75 (850.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:20:03.293492: step 193930, loss = 1.81 (855.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:20:04.826176: step 193940, loss = 1.81 (835.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:20:06.329273: step 193950, loss = 1.97 (851.6 examples/sec; 0.150 sec/batch)
2017-05-05 04:20:07.865318: step 193960, loss = 1.86 (833.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:20:09.447494: step 193970, loss = 1.89 (809.0 examples/sec; 0.158 sec/batch)
2017-05-05 04:20:11.007989: step 193980, loss = 1.84 (820.2 examples/sec; 0.156 sec/batch)
2017-05-05 04:20:12.553135: step 193990, loss = 1.80 (828.4 examples/sec; 0.155 sec/batch)
2017-05-05 04:20:14.077847: step 194000, loss = 1.89 (839.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:20:15.675075: step 194010, loss = 1.99 (801.4 examples/sec; 0.160 sec/batch)
2017-05-05 04:20:17.192668: step 194020, loss = 1.81 (843.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:20:18.714323: step 194030, loss = 1.80 (841.2 examples/sec; 0.152 sec/batch)
2017-05-05 04:20:20.372622: step 194040, loss = 1.83 (771.9 examples/sec; 0.166 sec/batch)
2017-05-05 04:20:21.756632: step 194050, loss = 1.79 (924.9 examples/sec; 0.138 sec/batch)
2017-05-05 04:20:23.240465: step 194060, loss = 1.89 (862.6 examples/sec; 0.148 sec/batch)
2017-05-05 04:20:24.759770: step 194070, loss = 1.84 (842.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:20:26.259156: step 194080, loss = 1.90 (853.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:20:27.805721: step 194090, loss = 1.93 (827.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:20:29.335960: step 194100, loss = 1.97 (836.5 examples/sec; 0.153 sec/batch)
2017-05-05 04:20:30.811587: step 194110, loss = 1.75 (867.4 examples/sec; 0.148 sec/batch)
2017-05-05 04:20:32.275753: step 194120, loss = 1.76 (874.2 examples/sec; 0.146 sec/batch)
2017-05-05 04:20:33.783281: step 194130, loss = 1.86 (849.1 examples/sec; 0.151 sec/batch)
2017-05-05 04:20:35.302784: step 194140, loss = 1.76 (842.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:20:36.801585: step 194150, loss = 1.82 (854.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:20:38.336272: step 194160, loss = 1.79 (834.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:20:39.826125: step 194170, loss = 1.85 (859.1 examples/sec; 0.149 sec/batch)
2017-05-05 04:20:41.376303: step 194180, loss = 1.81 (825.7 examples/sec; 0.155 sec/batch)
2017-05-05 04:20:42.902379: step 194190, loss = 2.01 (838.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:20:44.441076: step 194200, loss = 1.86 (831.9 examples/sec; 0.154 sec/batch)
2017-05-05 04:20:45.912739: step 194210, loss = 1.82 (869.8 examples/sec; 0.147 sec/batch)
2017-05-05 04:20:47.407924: step 194220, loss = 1.87 (856.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:20:48.920900: step 194230, loss = 1.83 (846.0 examples/sec; 0.151 sec/batch)
2017-05-05 04:20:50.403861: step 194240, loss = 2.00 (863.1 examples/sec; 0.148 sec/batch)
2017-05-05 04:20:51.872163: step 194250, loss = 1.82 (871.8 examples/sec; 0.147 sec/batch)
2017-05-05 04:20:53.411937: step 194260, loss = 1.96 (831.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:20:54.971802: step 194270, loss = 1.69 (820.6 examples/sec; 0.156 sec/batch)
2017-05-05 04:20:56.524588: step 194280, loss = 1.87 (824.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:20:58.008201: step 194290, loss = 1.85 (862.8 examples/sec; 0.148 sec/batch)
2017-05-05 04:20:59.562677: step 194300, loss = 1.80 (823.4 examples/sec; 0.155 sec/batch)
2017-05-05 04:21:01.097546: step 194310, loss = 1.96 (833.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:21:02.606607: step 194320, loss = 1.97 (848.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:21:04.138798: step 194330, loss = 2.02 (835.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:21:05.626618: step 194340, loss = 1.82 (860.3 examples/sec; 0.149 sec/batch)
2017-05-05 04:21:07.168434: step 194350, loss = 1.70 (830.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:21:08.680661: step 194360, loss = 2.17 (846.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:21:10.243814: step 194370, loss = 1.83 (818.9 examples/sec; 0.156 sec/batch)
2017-05-05 04:21:11.727203: step 194380, loss = 1.80 (862.9 examples/sec; 0.148 sec/batch)
2017-05-05 04:21:13.244048: step 194390, loss = 1.81 (843.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:21:14.800324: step 194400, loss = 1.83 (822.5 examples/sec; 0.156 sec/batch)
2017-05-05 04:21:16.374016: step 194410, loss = 1.83 (813.4 examples/sec; 0.157 sec/batch)
2017-05-05 04:21:17.924364: step 194420, loss = 1.95 (825.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:21:19.456528: step 194430, loss = 2.02 (835.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:21:20.992382: step 194440, loss = 1.90 (833.4 examples/sec; 0.154 sec/batch)
2017-05-05 04:21:22.567271: step 194450, loss = 1.83 (812.8 examples/sec; 0.157 sec/batch)
2017-05-05 04:21:24.064915: step 194460, loss = 2.06 (854.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:21:25.559550: step 194470, loss = 1.76 (856.4 examples/sec; 0.149 sec/batch)
2017-05-05 04:21:27.095305: step 194480, loss = 1.86 (833.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:21:28.614841: step 194490, loss = 1.93 (842.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:21:30.117010: step 194500, loss = 1.90 (852.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:21:31.625562: step 194510, loss = 1.94 (848.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:21:33.190569: step 194520, loss = 1.88 (817.9 examples/sec; 0.157 sec/batch)
2017-05-05 04:21:34.707671: step 194530, loss = 1.74 (843.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:21:36.275333: step 194540, loss = 1.88 (816.5 examples/sec; 0.157 sec/batch)
2017-05-05 04:21:37.834476: step 194550, loss = 1.85 (821.0 examples/sec; 0.156 sec/batch)
2017-05-05 04:21:39.335711: step 194560, losE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 2003 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
s = 1.70 (852.6 examples/sec; 0.150 sec/batch)
2017-05-05 04:21:40.859665: step 194570, loss = 1.89 (839.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:21:42.400867: step 194580, loss = 1.73 (830.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:21:43.935138: step 194590, loss = 1.94 (834.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:21:45.449016: step 194600, loss = 1.93 (845.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:21:46.979018: step 194610, loss = 1.82 (836.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:21:48.512211: step 194620, loss = 1.89 (834.9 examples/sec; 0.153 sec/batch)
2017-05-05 04:21:50.038140: step 194630, loss = 1.73 (838.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:21:51.542836: step 194640, loss = 1.97 (850.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:21:53.085087: step 194650, loss = 1.83 (830.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:21:54.616831: step 194660, loss = 1.93 (835.7 examples/sec; 0.153 sec/batch)
2017-05-05 04:21:56.137806: step 194670, loss = 1.95 (841.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:21:57.626316: step 194680, loss = 1.77 (859.9 examples/sec; 0.149 sec/batch)
2017-05-05 04:21:59.144854: step 194690, loss = 1.83 (842.9 examples/sec; 0.152 sec/batch)
2017-05-05 04:22:00.681034: step 194700, loss = 1.86 (833.2 examples/sec; 0.154 sec/batch)
2017-05-05 04:22:02.246223: step 194710, loss = 1.80 (817.8 examples/sec; 0.157 sec/batch)
2017-05-05 04:22:03.773678: step 194720, loss = 1.85 (838.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:22:05.387620: step 194730, loss = 1.74 (793.1 examples/sec; 0.161 sec/batch)
2017-05-05 04:22:06.911897: step 194740, loss = 1.87 (839.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:22:08.425682: step 194750, loss = 1.79 (845.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:22:09.930628: step 194760, loss = 1.97 (850.5 examples/sec; 0.150 sec/batch)
2017-05-05 04:22:11.492370: step 194770, loss = 1.87 (819.6 examples/sec; 0.156 sec/batch)
2017-05-05 04:22:13.059505: step 194780, loss = 1.90 (816.8 examples/sec; 0.157 sec/batch)
2017-05-05 04:22:14.576886: step 194790, loss = 1.94 (843.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:22:16.117651: step 194800, loss = 1.96 (830.8 examples/sec; 0.154 sec/batch)
2017-05-05 04:22:17.673121: step 194810, loss = 1.86 (822.9 examples/sec; 0.156 sec/batch)
2017-05-05 04:22:19.170908: step 194820, loss = 1.86 (854.6 examples/sec; 0.150 sec/batch)
2017-05-05 04:22:20.651299: step 194830, loss = 1.96 (864.6 examples/sec; 0.148 sec/batch)
2017-05-05 04:22:22.213096: step 194840, loss = 1.76 (819.6 examples/sec; 0.156 sec/batch)
2017-05-05 04:22:23.762570: step 194850, loss = 1.84 (826.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:22:25.294433: step 194860, loss = 1.74 (835.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:22:26.823460: step 194870, loss = 1.81 (837.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:22:28.323677: step 194880, loss = 1.85 (853.2 examples/sec; 0.150 sec/batch)
2017-05-05 04:22:29.839554: step 194890, loss = 1.84 (844.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:22:31.343954: step 194900, loss = 1.86 (850.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:22:32.921118: step 194910, loss = 1.84 (811.6 examples/sec; 0.158 sec/batch)
2017-05-05 04:22:34.418672: step 194920, loss = 1.82 (854.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:22:35.942836: step 194930, loss = 1.97 (839.8 examples/sec; 0.152 sec/batch)
2017-05-05 04:22:37.494791: step 194940, loss = 2.01 (824.8 examples/sec; 0.155 sec/batch)
2017-05-05 04:22:39.047027: step 194950, loss = 1.63 (824.6 examples/sec; 0.155 sec/batch)
2017-05-05 04:22:40.597107: step 194960, loss = 1.85 (825.8 examples/sec; 0.155 sec/batch)
2017-05-05 04:22:42.131227: step 194970, loss = 1.92 (834.4 examples/sec; 0.153 sec/batch)
2017-05-05 04:22:43.693456: step 194980, loss = 1.97 (819.3 examples/sec; 0.156 sec/batch)
2017-05-05 04:22:45.208851: step 194990, loss = 1.94 (844.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:22:46.752357: step 195000, loss = 1.82 (829.3 examples/sec; 0.154 sec/batch)
2017-05-05 04:22:48.243972: step 195010, loss = 1.92 (858.1 examples/sec; 0.149 sec/batch)
2017-05-05 04:22:49.741979: step 195020, loss = 1.88 (854.5 examples/sec; 0.150 sec/batch)
2017-05-05 04:22:51.411064: step 195030, loss = 1.83 (766.9 examples/sec; 0.167 sec/batch)
2017-05-05 04:22:52.877141: step 195040, loss = 1.90 (873.1 examples/sec; 0.147 sec/batch)
2017-05-05 04:22:54.406824: step 195050, loss = 1.89 (836.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:22:55.936022: step 195060, loss = 2.04 (837.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:22:57.504352: step 195070, loss = 1.84 (816.2 examples/sec; 0.157 sec/batch)
2017-05-05 04:22:58.996746: step 195080, loss = 1.78 (857.7 examples/sec; 0.149 sec/batch)
2017-05-05 04:23:00.501473: step 195090, loss = 1.94 (850.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:23:02.050757: step 195100, loss = 1.78 (826.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:23:03.586128: step 195110, loss = 1.93 (833.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:23:05.092976: step 195120, loss = 1.85 (849.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:23:06.614784: step 195130, loss = 1.70 (841.1 examples/sec; 0.152 sec/batch)
2017-05-05 04:23:08.116261: step 195140, loss = 1.82 (852.5 examples/sec; 0.150 sec/batch)
2017-05-05 04:23:09.619140: step 195150, loss = 1.86 (851.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:23:11.120486: step 195160, loss = 1.76 (852.6 examples/sec; 0.150 sec/batch)
2017-05-05 04:23:12.637010: step 195170, loss = 1.94 (844.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:23:14.124327: step 195180, loss = 1.85 (860.6 examples/sec; 0.149 sec/batch)
2017-05-05 04:23:15.625947: step 195190, loss = 1.80 (852.4 examples/sec; 0.150 sec/batch)
2017-05-05 04:23:17.126911: step 195200, loss = 1.85 (852.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:23:18.618574: step 195210, loss = 1.82 (858.1 examples/sec; 0.149 sec/batch)
2017-05-05 04:23:20.144586: step 195220, loss = 2.01 (838.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:23:21.650867: step 195230, loss = 1.75 (849.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:23:23.153755: step 195240, loss = 1.59 (851.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:23:24.697850: step 195250, loss = 1.93 (829.0 examples/sec; 0.154 sec/batch)
2017-05-05 04:23:26.208032: step 195260, loss = 1.97 (847.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:23:27.707898: step 195270, loss = 1.77 (853.4 examples/sec; 0.150 sec/batch)
2017-05-05 04:23:29.209013: step 195280, loss = 1.78 (852.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:23:30.726723: step 195290, loss = 1.77 (843.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:23:32.279678: step 195300, loss = 1.84 (824.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:23:33.784937: step 195310, loss = 2.05 (850.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:23:35.347058: step 195320, loss = 1.82 (819.4 examples/sec; 0.156 sec/batch)
2017-05-05 04:23:36.830667: step 195330, loss = 1.93 (862.8 examples/sec; 0.148 sec/batch)
2017-05-05 04:23:38.339739: step 195340, loss = 1.81 (848.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:23:39.890829: step 195350, loss = 1.82 (825.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:23:41.432117: step 195360, loss = 1.89 (830.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:23:42.953380: step 195370, loss = 1.83 (841.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:23:44.508597: step 195380, loss = 1.82 (823.0 examples/sec; 0.156 sec/batch)
2017-05-05 04:23:46.070046: step 195390, loss = 1.90 (819.8 examples/sec; 0.156 sec/batch)
2017-05-05 04:23:47.646777: step 195400, loss = 1.81 (811.8 examples/sec; 0.158 sec/batch)
2017-05-05 04:23:49.171827: step 195410, loss = 1.80 (839.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:23:50.665086: step 195420, loss = 1.87 (857.2 examples/sec; 0.149 sec/batch)
2017-05-05 04:23:52.160836: step 195430, loss = 1.84 (855.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:23:53.737951: step 195440, loss = 2.02 (811.6 examples/sec; 0.158 sec/batch)
2017-05-05 04:23:55.238425: step 195450, loss = 1.91 (853.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:23:56.794954: step 195460, loss E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 2011 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
= 1.92 (822.3 examples/sec; 0.156 sec/batch)
2017-05-05 04:23:58.361882: step 195470, loss = 1.76 (816.9 examples/sec; 0.157 sec/batch)
2017-05-05 04:23:59.900308: step 195480, loss = 1.71 (832.1 examples/sec; 0.154 sec/batch)
2017-05-05 04:24:01.431689: step 195490, loss = 1.98 (835.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:24:03.011072: step 195500, loss = 1.72 (810.4 examples/sec; 0.158 sec/batch)
2017-05-05 04:24:04.512119: step 195510, loss = 1.77 (852.7 examples/sec; 0.150 sec/batch)
2017-05-05 04:24:06.079155: step 195520, loss = 2.04 (816.8 examples/sec; 0.157 sec/batch)
2017-05-05 04:24:07.655604: step 195530, loss = 1.89 (811.9 examples/sec; 0.158 sec/batch)
2017-05-05 04:24:09.166548: step 195540, loss = 1.72 (847.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:24:10.742077: step 195550, loss = 1.80 (812.4 examples/sec; 0.158 sec/batch)
2017-05-05 04:24:12.323106: step 195560, loss = 2.02 (809.6 examples/sec; 0.158 sec/batch)
2017-05-05 04:24:13.884592: step 195570, loss = 1.89 (819.7 examples/sec; 0.156 sec/batch)
2017-05-05 04:24:15.455341: step 195580, loss = 1.87 (814.9 examples/sec; 0.157 sec/batch)
2017-05-05 04:24:17.028552: step 195590, loss = 1.98 (813.6 examples/sec; 0.157 sec/batch)
2017-05-05 04:24:18.517225: step 195600, loss = 1.81 (859.8 examples/sec; 0.149 sec/batch)
2017-05-05 04:24:20.085592: step 195610, loss = 1.92 (816.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:24:21.603148: step 195620, loss = 1.87 (843.5 examples/sec; 0.152 sec/batch)
2017-05-05 04:24:23.100208: step 195630, loss = 1.80 (855.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:24:24.631686: step 195640, loss = 1.91 (835.8 examples/sec; 0.153 sec/batch)
2017-05-05 04:24:26.158869: step 195650, loss = 1.88 (838.2 examples/sec; 0.153 sec/batch)
2017-05-05 04:24:27.673316: step 195660, loss = 1.89 (845.2 examples/sec; 0.151 sec/batch)
2017-05-05 04:24:29.219786: step 195670, loss = 1.75 (827.7 examples/sec; 0.155 sec/batch)
2017-05-05 04:24:30.728263: step 195680, loss = 1.96 (848.6 examples/sec; 0.151 sec/batch)
2017-05-05 04:24:32.303350: step 195690, loss = 1.89 (812.6 examples/sec; 0.158 sec/batch)
2017-05-05 04:24:33.844668: step 195700, loss = 1.82 (830.5 examples/sec; 0.154 sec/batch)
2017-05-05 04:24:35.369079: step 195710, loss = 1.80 (839.7 examples/sec; 0.152 sec/batch)
2017-05-05 04:24:36.937468: step 195720, loss = 1.76 (816.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:24:38.485784: step 195730, loss = 1.85 (826.7 examples/sec; 0.155 sec/batch)
2017-05-05 04:24:39.999758: step 195740, loss = 1.85 (845.5 examples/sec; 0.151 sec/batch)
2017-05-05 04:24:41.521080: step 195750, loss = 1.97 (841.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:24:43.056923: step 195760, loss = 1.94 (833.4 examples/sec; 0.154 sec/batch)
2017-05-05 04:24:44.541825: step 195770, loss = 1.96 (862.0 examples/sec; 0.148 sec/batch)
2017-05-05 04:24:46.099930: step 195780, loss = 1.99 (821.5 examples/sec; 0.156 sec/batch)
2017-05-05 04:24:47.606365: step 195790, loss = 1.76 (849.7 examples/sec; 0.151 sec/batch)
2017-05-05 04:24:49.151826: step 195800, loss = 1.91 (828.2 examples/sec; 0.155 sec/batch)
2017-05-05 04:24:50.635598: step 195810, loss = 1.98 (862.7 examples/sec; 0.148 sec/batch)
2017-05-05 04:24:52.206024: step 195820, loss = 1.74 (815.1 examples/sec; 0.157 sec/batch)
2017-05-05 04:24:53.743375: step 195830, loss = 1.86 (832.6 examples/sec; 0.154 sec/batch)
2017-05-05 04:24:55.276198: step 195840, loss = 1.85 (835.1 examples/sec; 0.153 sec/batch)
2017-05-05 04:24:56.878666: step 195850, loss = 1.83 (798.8 examples/sec; 0.160 sec/batch)
2017-05-05 04:24:58.432519: step 195860, loss = 1.77 (823.8 examples/sec; 0.155 sec/batch)
2017-05-05 04:24:59.955520: step 195870, loss = 1.88 (840.4 examples/sec; 0.152 sec/batch)
2017-05-05 04:25:01.463455: step 195880, loss = 1.79 (848.8 examples/sec; 0.151 sec/batch)
2017-05-05 04:25:03.078341: step 195890, loss = 1.83 (792.6 examples/sec; 0.161 sec/batch)
2017-05-05 04:25:04.610734: step 195900, loss = 1.79 (835.3 examples/sec; 0.153 sec/batch)
2017-05-05 04:25:06.221642: step 195910, loss = 1.95 (794.6 examples/sec; 0.161 sec/batch)
2017-05-05 04:25:07.793643: step 195920, loss = 1.87 (814.2 examples/sec; 0.157 sec/batch)
2017-05-05 04:25:09.344954: step 195930, loss = 1.78 (825.1 examples/sec; 0.155 sec/batch)
2017-05-05 04:25:10.855433: step 195940, loss = 1.81 (847.4 examples/sec; 0.151 sec/batch)
2017-05-05 04:25:12.437184: step 195950, loss = 1.85 (809.2 examples/sec; 0.158 sec/batch)
2017-05-05 04:25:13.952671: step 195960, loss = 1.84 (844.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:25:15.541604: step 195970, loss = 1.74 (805.6 examples/sec; 0.159 sec/batch)
2017-05-05 04:25:17.086856: step 195980, loss = 1.77 (828.3 examples/sec; 0.155 sec/batch)
2017-05-05 04:25:18.587844: step 195990, loss = 1.80 (852.8 examples/sec; 0.150 sec/batch)
2017-05-05 04:25:20.130581: step 196000, loss = 1.93 (829.7 examples/sec; 0.154 sec/batch)
2017-05-05 04:25:21.658109: step 196010, loss = 1.83 (838.0 examples/sec; 0.153 sec/batch)
2017-05-05 04:25:23.290343: step 196020, loss = 1.81 (784.2 examples/sec; 0.163 sec/batch)
2017-05-05 04:25:24.677542: step 196030, loss = 1.91 (922.7 examples/sec; 0.139 sec/batch)
2017-05-05 04:25:26.207549: step 196040, loss = 1.82 (836.6 examples/sec; 0.153 sec/batch)
2017-05-05 04:25:27.706152: step 196050, loss = 1.82 (854.1 examples/sec; 0.150 sec/batch)
2017-05-05 04:25:29.184149: step 196060, loss = 1.89 (866.0 examples/sec; 0.148 sec/batch)
2017-05-05 04:25:30.623589: step 196070, loss = 1.82 (889.2 examples/sec; 0.144 sec/batch)
2017-05-05 04:25:32.144523: step 196080, loss = 1.71 (841.6 examples/sec; 0.152 sec/batch)
2017-05-05 04:25:33.652263: step 196090, loss = 1.91 (848.9 examples/sec; 0.151 sec/batch)
2017-05-05 04:25:35.131883: step 196100, loss = 1.95 (865.1 examples/sec; 0.148 sec/batch)
2017-05-05 04:25:36.618555: step 196110, loss = 1.82 (861.0 examples/sec; 0.149 sec/batch)
2017-05-05 04:25:38.111329: step 196120, loss = 1.80 (857.5 examples/sec; 0.149 sec/batch)
2017-05-05 04:25:39.608600: step 196130, loss = 1.91 (854.9 examples/sec; 0.150 sec/batch)
2017-05-05 04:25:41.107094: step 196140, loss = 1.82 (854.2 examples/sec; 0.150 sec/batch)
2017-05-05 04:25:42.699755: step 196150, loss = 1.92 (803.7 examples/sec; 0.159 sec/batch)
2017-05-05 04:25:44.143050: step 196160, loss = 1.73 (886.9 examples/sec; 0.144 sec/batch)
2017-05-05 04:25:45.638437: step 196170, loss = 1.89 (856.0 examples/sec; 0.150 sec/batch)
2017-05-05 04:25:47.188095: step 196180, loss = 1.81 (826.0 examples/sec; 0.155 sec/batch)
2017-05-05 04:25:48.732288: step 196190, loss = 1.91 (828.9 examples/sec; 0.154 sec/batch)
2017-05-05 04:25:50.248858: step 196200, loss = 1.79 (844.0 examples/sec; 0.152 sec/batch)
2017-05-05 04:25:51.732130: step 196210, loss = 1.87 (863.0 examples/sec; 0.148 sec/batch)
2017-05-05 04:25:53.220190: step 196220, loss = 1.88 (860.2 examples/sec; 0.149 sec/batch)
2017-05-05 04:25:54.339930: step 196230, loss = 1.81 (1143.1 examples/sec; 0.112 sec/batch)
2017-05-05 04:25:55.542697: step 196240, loss = 1.90 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:25:56.725636: step 196250, loss = 1.77 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-05 04:25:57.910255: step 196260, loss = 1.64 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-05 04:25:59.101497: step 196270, loss = 1.84 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-05 04:26:00.295684: step 196280, loss = 1.77 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 04:26:01.466504: step 196290, loss = 1.85 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 04:26:02.647723: step 196300, loss = 1.71 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-05 04:26:03.832708: step 196310, loss = 1.83 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-05 04:26:05.019022: step 196320, loss = 1.88 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-05 04:26:06.179364: step 196330, loss = 1.90 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:07.373673: step 196340, loss = 1.79 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 04:26:08.557979: step 196350, loss = 1.94 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-05 04:26:09.721000: step 19E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 2020 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
6360, loss = 1.86 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:10.899014: step 196370, loss = 1.91 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 04:26:12.094002: step 196380, loss = 1.85 (1071.1 examples/sec; 0.119 sec/batch)
2017-05-05 04:26:13.254152: step 196390, loss = 1.80 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:14.403681: step 196400, loss = 1.83 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:15.570418: step 196410, loss = 1.73 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-05 04:26:16.743479: step 196420, loss = 1.78 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-05 04:26:17.915605: step 196430, loss = 1.81 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-05 04:26:19.067584: step 196440, loss = 1.90 (1111.1 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:20.240209: step 196450, loss = 1.83 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-05 04:26:21.395812: step 196460, loss = 1.78 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:22.550022: step 196470, loss = 1.82 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:23.704466: step 196480, loss = 1.74 (1108.8 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:24.885405: step 196490, loss = 1.81 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-05 04:26:26.021194: step 196500, loss = 1.85 (1127.0 examples/sec; 0.114 sec/batch)
2017-05-05 04:26:27.184172: step 196510, loss = 1.81 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:28.335611: step 196520, loss = 1.88 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:29.487988: step 196530, loss = 1.84 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:30.643176: step 196540, loss = 1.72 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:31.798525: step 196550, loss = 1.94 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:32.957789: step 196560, loss = 1.83 (1104.2 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:34.097770: step 196570, loss = 1.89 (1122.8 examples/sec; 0.114 sec/batch)
2017-05-05 04:26:35.247108: step 196580, loss = 1.70 (1113.7 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:36.393554: step 196590, loss = 1.85 (1116.5 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:37.539532: step 196600, loss = 1.85 (1117.0 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:38.696484: step 196610, loss = 1.91 (1106.3 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:39.856141: step 196620, loss = 1.89 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:41.007007: step 196630, loss = 1.92 (1112.2 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:42.159567: step 196640, loss = 1.84 (1110.6 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:43.304890: step 196650, loss = 1.87 (1117.6 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:44.459839: step 196660, loss = 1.76 (1108.2 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:45.611362: step 196670, loss = 2.01 (1111.6 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:46.767976: step 196680, loss = 1.86 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:47.940698: step 196690, loss = 1.83 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-05 04:26:49.109586: step 196700, loss = 1.80 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 04:26:50.269981: step 196710, loss = 1.86 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:51.432132: step 196720, loss = 1.89 (1101.4 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:52.582760: step 196730, loss = 1.88 (1112.4 examples/sec; 0.115 sec/batch)
2017-05-05 04:26:53.717892: step 196740, loss = 1.85 (1127.6 examples/sec; 0.114 sec/batch)
2017-05-05 04:26:54.883411: step 196750, loss = 2.01 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-05 04:26:56.060685: step 196760, loss = 1.88 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-05 04:26:57.223798: step 196770, loss = 1.93 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:58.385605: step 196780, loss = 1.88 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-05 04:26:59.526454: step 196790, loss = 1.91 (1122.0 examples/sec; 0.114 sec/batch)
2017-05-05 04:27:00.686644: step 196800, loss = 1.99 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-05 04:27:01.845709: step 196810, loss = 1.94 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-05 04:27:03.002424: step 196820, loss = 1.81 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 04:27:04.171306: step 196830, loss = 1.80 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-05 04:27:05.346574: step 196840, loss = 1.77 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-05 04:27:06.507768: step 196850, loss = 1.95 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-05 04:27:07.685765: step 196860, loss = 1.90 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-05 04:27:08.845421: step 196870, loss = 1.84 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-05 04:27:10.010413: step 196880, loss = 2.01 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-05 04:27:11.168877: step 196890, loss = 1.82 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-05 04:27:12.325548: step 196900, loss = 1.71 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-05 04:27:13.504434: step 196910, loss = 1.89 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 04:27:14.686521: step 196920, loss = 1.74 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 04:27:15.868640: step 196930, loss = 1.82 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 04:27:17.068529: step 196940, loss = 2.06 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:18.289474: step 196950, loss = 1.75 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-05 04:27:19.496599: step 196960, loss = 1.75 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:27:20.688955: step 196970, loss = 1.84 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-05 04:27:21.878377: step 196980, loss = 1.72 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-05 04:27:23.074652: step 196990, loss = 1.86 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:24.267455: step 197000, loss = 1.73 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 04:27:25.567402: step 197010, loss = 1.84 (984.7 examples/sec; 0.130 sec/batch)
2017-05-05 04:27:26.671795: step 197020, loss = 1.79 (1159.0 examples/sec; 0.110 sec/batch)
2017-05-05 04:27:27.861364: step 197030, loss = 1.69 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-05 04:27:29.056704: step 197040, loss = 1.84 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:30.254037: step 197050, loss = 1.87 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:31.471261: step 197060, loss = 1.77 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 04:27:32.666592: step 197070, loss = 1.77 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:33.864947: step 197080, loss = 1.79 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:35.084731: step 197090, loss = 1.87 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-05 04:27:36.275337: step 197100, loss = 1.79 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 04:27:37.507915: step 197110, loss = 1.93 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-05 04:27:38.716930: step 197120, loss = 1.94 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:27:39.917464: step 197130, loss = 1.83 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:41.121226: step 197140, loss = 1.97 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:42.294094: step 197150, loss = 1.89 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-05 04:27:43.490849: step 197160, loss = 1.98 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:44.695881: step 197170, loss = 1.88 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:27:45.883356: step 197180, loss = 1.97 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 04:27:47.073453: step 197190, loss = 1.76 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-05 04:27:48.294948: step 197200, loss = 1.95 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-05 04:27:49.508162: step 197210, loss = 1.96 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:27:50.700332: step 197220, loss = 1.94 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 04:27:51.923820: step 197230, loss = 1.79 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-05 04:27:53.151223: step 197240, loss = 1.91 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-05 04:27:54.344827: step 197250, loss = 1.85 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 04:27:55.555322: step 197260, loss = 1.85 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:27:56.753709: step 197270, loss = 1.87 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 04:27:57.963321: step 197280, loss = 1.81 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:27:59.179310: step 197290, loss = 1.86 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-05 04:28:00.386513: step 197300, loss = 1.70 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:01.582695: step 197310, loss = 1.89 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:02.809039: step 197320, loss = 1.84 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-05 04:28:04.033792: step 197330, loss = 1.72 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-05 04:28:05.235483: step 197340, loss = 1.94 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:06.421940: step 197350, loss = 1.88 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:07.617501: step 197360, loss = 1.85 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:08.813616: step 197370, loss = 1.69 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:10.024330: step 197380, loss = 1.89 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:11.235938: step 197390, loss = 1.73 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:12.439601: step 197400, loss = 1.96 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:13.609100: step 197410, loss = 1.80 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-05 04:28:14.826703: step 197420, loss = 2.08 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 04:28:16.021669: step 197430, loss = 1.94 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:17.228431: step 197440, loss = 1.77 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:18.424938: step 197450, loss = 1.87 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:19.621345: step 197460, loss = 1.85 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:20.823345: step 197470, loss = 1.87 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:22.021359: step 197480, loss = 1.97 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:23.217811: step 197490, loss = 1.82 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:24.423900: step 197500, loss = 1.88 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:25.612021: step 197510, loss = 1.87 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:26.813506: step 197520, loss = 1.95 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:28.017756: step 197530, loss = 1.78 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:29.217163: step 197540, loss = 1.97 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:30.399595: step 197550, loss = 1.76 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-05 04:28:31.616709: step 197560, loss = 1.87 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 04:28:32.821308: step 197570, loss = 1.90 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:34.014156: step 197580, loss = 2.00 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:35.233286: step 197590, loss = 1.92 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-05 04:28:36.417510: step 197600, loss = 1.95 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 04:28:37.643210: step 197610, loss = 1.97 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-05 04:28:38.849082: step 197620, loss = 1.76 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:40.058448: step 197630, loss = 1.98 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:41.279200: step 197640, loss = 1.82 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 04:28:42.481569: step 197650, loss = 1.76 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:43.678346: step 197660, loss = 1.86 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:44.886697: step 197670, loss = 2.06 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:46.084271: step 197680, loss = 1.81 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:47.309321: step 197690, loss = 1.91 (1044.9 examples/sec; 0.123 sec/batE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 2033 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
ch)
2017-05-05 04:28:48.486151: step 197700, loss = 1.78 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-05 04:28:49.694197: step 197710, loss = 1.90 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:50.885377: step 197720, loss = 1.94 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:52.093326: step 197730, loss = 1.99 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:53.314091: step 197740, loss = 1.83 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-05 04:28:54.532886: step 197750, loss = 1.86 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-05 04:28:55.730817: step 197760, loss = 1.93 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-05 04:28:56.922171: step 197770, loss = 1.73 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-05 04:28:58.130820: step 197780, loss = 1.73 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 04:28:59.332099: step 197790, loss = 1.91 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:00.535275: step 197800, loss = 1.84 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:01.719190: step 197810, loss = 1.93 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-05 04:29:02.918001: step 197820, loss = 1.90 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:04.102167: step 197830, loss = 1.85 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-05 04:29:05.306012: step 197840, loss = 1.95 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:06.513180: step 197850, loss = 1.83 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:07.726382: step 197860, loss = 1.78 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:08.928464: step 197870, loss = 1.88 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:10.127344: step 197880, loss = 1.76 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:11.333628: step 197890, loss = 1.77 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:12.537510: step 197900, loss = 2.00 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:13.742454: step 197910, loss = 1.78 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:14.938187: step 197920, loss = 1.98 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:16.149345: step 197930, loss = 1.82 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:17.367719: step 197940, loss = 1.98 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-05 04:29:18.571866: step 197950, loss = 1.90 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:19.774087: step 197960, loss = 1.80 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:20.983442: step 197970, loss = 1.98 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:22.199327: step 197980, loss = 1.79 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 04:29:23.416044: step 197990, loss = 1.82 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-05 04:29:24.717098: step 198000, loss = 1.84 (983.8 examples/sec; 0.130 sec/batch)
2017-05-05 04:29:25.804167: step 198010, loss = 1.79 (1177.5 examples/sec; 0.109 sec/batch)
2017-05-05 04:29:27.008639: step 198020, loss = 1.88 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:28.227687: step 198030, loss = 1.84 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 04:29:29.430419: step 198040, loss = 1.86 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:30.619514: step 198050, loss = 1.86 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-05 04:29:31.833166: step 198060, loss = 1.94 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:33.044360: step 198070, loss = 1.95 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:34.250302: step 198080, loss = 1.85 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:35.465298: step 198090, loss = 1.77 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:36.656028: step 198100, loss = 1.79 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-05 04:29:37.879569: step 198110, loss = 1.87 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-05 04:29:39.089206: step 198120, loss = 1.89 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:40.306205: step 198130, loss = 1.87 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-05 04:29:41.509548: step 198140, loss = 1.84 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:42.709950: step 198150, loss = 1.78 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:43.922365: step 198160, loss = 1.87 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:45.135454: step 198170, loss = 1.85 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:46.337729: step 198180, loss = 1.88 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:47.553693: step 198190, loss = 1.92 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-05 04:29:48.731031: step 198200, loss = 1.86 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-05 04:29:49.934967: step 198210, loss = 1.83 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:51.148989: step 198220, loss = 2.01 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:29:52.352106: step 198230, loss = 1.87 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:53.546095: step 198240, loss = 1.83 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 04:29:54.740121: step 198250, loss = 1.72 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-05 04:29:55.941483: step 198260, loss = 1.91 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:57.161731: step 198270, loss = 1.80 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-05 04:29:58.360681: step 198280, loss = 1.78 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 04:29:59.569682: step 198290, loss = 1.93 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:00.779875: step 198300, loss = 1.88 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:01.974133: step 198310, loss = 1.91 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-05 04:30:03.180469: step 198320, loss = 1.81 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:04.378010: step 198330, loss = 2.02 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:05.597067: step 198340, loss = 1.87 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-05 04:30:06.800900: step 198350, loss = 1.96 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:08.003120: step 198360, loss = 1.86 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:09.209104: step 198370, loss = 1.90 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:10.404055: step 198380, loss = 1.88 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 04:30:11.582907: step 198390, loss = 2.02 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-05 04:30:12.759601: step 198400, loss = 1.76 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-05 04:30:13.944096: step 198410, loss = 1.94 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-05 04:30:15.134537: step 198420, loss = 2.05 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-05 04:30:16.330511: step 198430, loss = 1.83 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:17.509593: step 198440, loss = 1.90 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-05 04:30:18.689888: step 198450, loss = 1.86 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 04:30:19.872014: step 198460, loss = 2.01 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-05 04:30:21.052333: step 198470, loss = 1.81 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-05 04:30:22.229444: step 198480, loss = 1.89 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-05 04:30:23.431396: step 198490, loss = 1.76 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:24.631910: step 198500, loss = 1.90 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:25.842036: step 198510, loss = 2.12 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:27.064264: step 198520, loss = 1.85 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-05 04:30:28.263386: step 198530, loss = 2.00 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:29.468777: step 198540, loss = 1.81 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:30.677192: step 198550, loss = 1.77 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:31.874266: step 198560, loss = 1.81 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:33.084093: step 198570, loss = 1.86 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:34.283263: step 198580, loss = 1.81 (1067.4 examples/sec; 0.120 sec/batch)
2E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 2044 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
017-05-05 04:30:35.495223: step 198590, loss = 1.88 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:36.690112: step 198600, loss = 1.94 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-05 04:30:37.888705: step 198610, loss = 1.91 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:39.098707: step 198620, loss = 1.87 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:40.311697: step 198630, loss = 1.79 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:41.521394: step 198640, loss = 1.82 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:42.729703: step 198650, loss = 1.76 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:43.951281: step 198660, loss = 1.84 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-05 04:30:45.153816: step 198670, loss = 1.84 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:46.371013: step 198680, loss = 1.72 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-05 04:30:47.592376: step 198690, loss = 1.84 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-05 04:30:48.795379: step 198700, loss = 1.86 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:50.005042: step 198710, loss = 1.81 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:51.226153: step 198720, loss = 1.86 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-05 04:30:52.433990: step 198730, loss = 1.80 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:53.633180: step 198740, loss = 1.74 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:54.824952: step 198750, loss = 1.89 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 04:30:56.035034: step 198760, loss = 1.81 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-05 04:30:57.235426: step 198770, loss = 1.75 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:58.434399: step 198780, loss = 1.81 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-05 04:30:59.636655: step 198790, loss = 1.88 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:00.818835: step 198800, loss = 1.89 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-05 04:31:02.017309: step 198810, loss = 1.76 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:03.240550: step 198820, loss = 1.91 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-05 04:31:04.438050: step 198830, loss = 1.73 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:05.629856: step 198840, loss = 1.81 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-05 04:31:06.830825: step 198850, loss = 2.01 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:08.033085: step 198860, loss = 1.83 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:09.238162: step 198870, loss = 1.84 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:10.432701: step 198880, loss = 1.89 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-05 04:31:11.642692: step 198890, loss = 1.86 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:12.839905: step 198900, loss = 1.90 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:14.046416: step 198910, loss = 1.91 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:15.253698: step 198920, loss = 1.86 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:16.463157: step 198930, loss = 1.88 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:17.651137: step 198940, loss = 1.82 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-05 04:31:18.862923: step 198950, loss = 1.79 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:20.077916: step 198960, loss = 1.83 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:21.277554: step 198970, loss = 1.86 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:22.482642: step 198980, loss = 1.85 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:23.813412: step 198990, loss = 1.84 (961.8 examples/sec; 0.133 sec/batch)
2017-05-05 04:31:24.876000: step 199000, loss = 1.75 (1204.6 examples/sec; 0.106 sec/batch)
2017-05-05 04:31:26.074716: step 199010, loss = 1.83 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:27.284262: step 199020, loss = 1.80 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:28.491735: step 199030, loss = 1.90 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:29.687030: step 199040, loss = 1.82 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:30.874780: step 199050, loss = 2.03 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 04:31:32.097536: step 199060, loss = 1.86 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-05 04:31:33.299524: step 199070, loss = 1.86 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:34.502913: step 199080, loss = 1.95 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:35.716043: step 199090, loss = 1.87 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:36.917270: step 199100, loss = 1.89 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:38.114387: step 199110, loss = 1.90 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:39.338857: step 199120, loss = 1.75 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 04:31:40.557004: step 199130, loss = 1.92 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-05 04:31:41.755420: step 199140, loss = 1.70 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:42.947887: step 199150, loss = 1.85 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-05 04:31:44.167289: step 199160, loss = 1.85 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-05 04:31:45.372263: step 199170, loss = 1.75 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:46.576024: step 199180, loss = 1.82 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:47.762692: step 199190, loss = 1.79 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-05 04:31:48.968328: step 199200, loss = 1.97 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:50.174005: step 199210, loss = 1.77 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:51.362081: step 199220, loss = 1.85 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-05 04:31:52.557612: step 199230, loss = 1.96 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:53.757316: step 199240, loss = 1.76 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:54.965404: step 199250, loss = 1.97 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-05 04:31:56.163911: step 199260, loss = 1.84 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:57.368265: step 199270, loss = 1.92 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:31:58.583439: step 199280, loss = 1.85 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-05 04:31:59.795087: step 199290, loss = 1.79 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:01.019534: step 199300, loss = 1.93 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-05 04:32:02.213082: step 199310, loss = 1.86 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:03.409853: step 199320, loss = 1.64 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:04.623324: step 199330, loss = 1.80 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:05.808596: step 199340, loss = 1.93 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:07.017338: step 199350, loss = 1.73 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:08.218882: step 199360, loss = 1.78 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:09.432750: step 199370, loss = 1.83 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:10.649855: step 199380, loss = 1.80 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-05 04:32:11.823697: step 199390, loss = 1.86 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-05 04:32:13.036591: step 199400, loss = 2.00 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:14.227133: step 199410, loss = 1.79 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:15.443972: step 199420, loss = 1.95 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-05 04:32:16.657481: step 199430, loss = 1.90 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:17.862425: step 199440, loss = 1.86 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:19.060671: step 199450, loss = 2.00 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:20.252786: step 199460, loss = 2.02 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:21.461019: step 199470, loss = 1.73 (1059.4 examples/sec; 0.121 sec/batch)
2017-0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 2054 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
5-05 04:32:22.666907: step 199480, loss = 1.89 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:23.873194: step 199490, loss = 1.97 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:25.082915: step 199500, loss = 2.04 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:26.270450: step 199510, loss = 1.94 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:27.476053: step 199520, loss = 1.72 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:28.682072: step 199530, loss = 1.80 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:29.883645: step 199540, loss = 1.82 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:31.090744: step 199550, loss = 1.90 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:32.313908: step 199560, loss = 1.94 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-05 04:32:33.510321: step 199570, loss = 1.87 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:34.703433: step 199580, loss = 1.92 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:35.897618: step 199590, loss = 1.84 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:37.103972: step 199600, loss = 2.02 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:38.305762: step 199610, loss = 1.76 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:39.506700: step 199620, loss = 2.12 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:40.710163: step 199630, loss = 2.08 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:41.921352: step 199640, loss = 1.88 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:43.128147: step 199650, loss = 1.85 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:44.321249: step 199660, loss = 1.93 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:45.527565: step 199670, loss = 1.97 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:46.724662: step 199680, loss = 1.67 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:47.944780: step 199690, loss = 1.86 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-05 04:32:49.143018: step 199700, loss = 1.84 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:50.343753: step 199710, loss = 1.88 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:51.556292: step 199720, loss = 1.75 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:52.760185: step 199730, loss = 1.73 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:53.961031: step 199740, loss = 1.81 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:55.166906: step 199750, loss = 1.80 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:56.370287: step 199760, loss = 2.04 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-05 04:32:57.576055: step 199770, loss = 1.97 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-05 04:32:58.766467: step 199780, loss = 1.87 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-05 04:32:59.947492: step 199790, loss = 1.69 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-05 04:33:01.176507: step 199800, loss = 1.87 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-05 04:33:02.367544: step 199810, loss = 1.86 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-05 04:33:03.585138: step 199820, loss = 1.83 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-05 04:33:04.797180: step 199830, loss = 1.89 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-05 04:33:05.984854: step 199840, loss = 1.89 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-05 04:33:07.187559: step 199850, loss = 1.86 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-05 04:33:08.383683: step 199860, loss = 1.93 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-05 04:33:09.578759: step 199870, loss = 1.79 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-05 04:33:10.779351: step 199880, loss = 1.75 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-05 04:33:11.975079: step 199890, loss = 1.79 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-05 04:33:13.149768: step 199900, loss = 1.80 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-05 04:33:14.320656: step 199910, loss = 2.10 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-05 04:33:15.478923: step 199920, loss = 1.86 (E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 2056 events to /tmp/cifar10_train/events.out.tfevents.1493948901.GHC34.GHC.ANDREW.CMU.EDU
1105.1 examples/sec; 0.116 sec/batch)
2017-05-05 04:33:16.665007: step 199930, loss = 1.76 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-05 04:33:17.821076: step 199940, loss = 1.91 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-05 04:33:18.995386: step 199950, loss = 1.96 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-05 04:33:20.158478: step 199960, loss = 1.80 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-05 04:33:21.327635: step 199970, loss = 1.89 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-05 04:33:22.561435: step 199980, loss = 1.83 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-05 04:33:23.616147: step 199990, loss = 1.80 (1213.6 examples/sec; 0.105 sec/batch)
--- 24303.667588 seconds ---
