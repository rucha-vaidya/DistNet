I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:04:00.0
Total memory: 7.92GiB
Free memory: 2.77GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3a3c170
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Quadro K620
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:03:00.0
Total memory: 1.95GiB
Free memory: 1.83GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Ignoring gpu device (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0) with Cuda multiprocessor count: 3. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
Connecting to port  17002
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Received variable values from ps
2017-05-09 00:34:58.865190: step 0, loss = 4.67 (77.1 examples/sec; 1.661 sec/batch)
2017-05-09 00:34:59.906536: step 10, loss = 4.36 (1229.2 examples/sec; 0.104 sec/batch)
2017-05-09 00:35:01.170227: step 20, loss = 4.55 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 00:35:02.500542: step 30, loss = 4.46 (962.2 examples/sec; 0.133 sec/batch)
2017-05-09 00:35:03.781151: step 40, loss = 4.26 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:35:05.060495: step 50, loss = 4.38 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:35:06.327824: step 60, loss = 4.33 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:35:07.623288: step 70, loss = 4.17 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:35:08.891069: step 80, loss = 4.12 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:35:10.164773: step 90, loss = 4.06 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 00:35:11.545744: step 100, loss = 4.04 (926.9 examples/sec; 0.138 sec/batch)
2017-05-09 00:35:12.706921: step 110, loss = 3.95 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-09 00:35:13.971225: step 120, loss = 4.11 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 00:35:15.228150: step 130, loss = 3.77 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-09 00:35:16.496655: step 140, loss = 3.91 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:35:17.768242: step 150, loss = 3.90 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:35:19.034133: step 160, loss = 3.77 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:35:20.316253: step 170, loss = 3.72 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:35:21.631653: step 180, loss = 3.90 (973.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:35:22.914833: step 190, loss = 3.50 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:35:24.289984: step 200, loss = 3.57 (930.8 examples/sec; 0.138 sec/batch)
2017-05-09 00:35:25.440956: step 210, loss = 3.73 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-09 00:35:26.705589: step 220, loss = 3.41 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-09 00:35:27.993743: step 230, loss = 3.90 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:35:29.255575: step 240, loss = 3.58 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-09 00:35:30.575040: step 250, loss = 3.62 (970.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:35:31.886279: step 260, loss = 3.41 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:35:33.202292: step 270, loss = 3.51 (972.6 examples/sec; 0.132 sec/batch)
2017-05-09 00:35:34.473013: step 280, loss = 3.35 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 00:35:35.759734: step 290, loss = 3.29 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:35:37.168245: step 300, loss = 3.24 (908.8 examples/sec; 0.141 sec/batch)
2017-05-09 00:35:38.316617: step 310, loss = 3.32 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-09 00:35:39.578899: step 320, loss = 3.01 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-09 00:35:40.859834: step 330, loss = 3.18 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:35:42.136440: step 340, loss = 3.37 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:35:43.430792: step 350, loss = 3.08 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:35:44.731713: step 360, loss = 3.02 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:35:45.997368: step 370, loss = 3.20 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-09 00:35:47.303280: step 380, loss = 3.14 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:35:48.587877: step 390, loss = 3.04 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:35:50.008419: step 400, loss = 2.94 (901.1 examples/sec; 0.142 sec/batch)
2017-05-09 00:35:51.174832: step 410, loss = 2.85 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-09 00:35:52.481571: step 420, loss = 3.11 (979.5 examples/sec; 0.131 sec/batch)
2017-05-09 00:35:53.763935: step 430, loss = 3.02 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:35:55.053833: step 440, loss = 2.74 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:35:56.327115: step 450, loss = 2.76 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 00:35:57.596720: step 460, loss = 2.80 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 00:35:58.866706: step 470, loss = 2.75 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 00:36:00.162801: step 480, loss = 2.78 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:36:01.438672: step 490, loss = 2.83 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:36:02.823852: step 500, loss = 2.74 (924.1 examples/sec; 0.139 sec/batch)
2017-05-09 00:36:04.001014: step 510, loss = 2.64 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-09 00:36:05.276848: step 520, loss = 2.77 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:36:06.575651: step 530, loss = 2.90 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:36:07.868886: step 540, loss = 2.80 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:09.180014: step 550, loss = 2.79 (976.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:36:10.485171: step 560, loss = 2.72 (980.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:36:11.790031: step 570, loss = 2.55 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:36:13.091572: step 580, loss = 2.53 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:36:14.384884: step 590, loss = 3.36 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:15.774967: step 600, loss = 2.48 (920.8 examples/sec; 0.139 sec/batch)
2017-05-09 00:36:16.944714: step 610, loss = 2.47 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-09 00:36:18.207354: step 620, loss = 2.48 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-09 00:36:19.466382: step 630, loss = 2.36 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-09 00:36:20.734511: step 640, loss = 2.41 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:36:22.034877: step 650, loss = 2.65 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:36:23.315858: step 660, loss = 2.68 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:36:24.615930: step 670, loss = 2.42 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:36:25.902210: step 680, loss = 2.58 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:27.166780: step 690, loss = 2.30 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-09 00:36:28.552516: step 700, loss = 2.13 (923.7 examples/sec; 0.139 sec/batch)
2017-05-09 00:36:29.701117: step 710, loss = 2.37 (1114.4 examples/sec; 0.115 sec/batch)
2017-05-09 00:36:30.984382: step 720, loss = 2.44 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:36:32.288584: step 730, loss = 2.24 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:36:33.583083: step 740, loss = 2.72 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:34.865048: step 750, loss = 2.85 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:36:36.131593: step 760, loss = 2.35 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:36:37.400584: step 770, loss = 2.24 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 00:36:38.690422: step 780, loss = 2.20 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:39.971640: step 790, loss = 2.42 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:36:41.352576: step 800, loss = 2.21 (926.9 examples/sec; 0.138 sec/batch)
2017-05-09 00:36:42.553091: step 810, loss = 2.25 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-09 00:36:43.851863: step 820, loss = 1.97 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:36:45.147935: step 830, loss = 2.22 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:36:46.420749: step 840, loss = 2.02 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:36:47.714977: step 850, loss = 2.14 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:49.033738: step 860, loss = 2.19 (970.6 examples/sec; 0.132 sec/batch)
2017-05-09 00:36:50.300393: step 870, loss = 2.06 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:36:51.593074: step 880, loss = 2.21 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:52.878255: step 890, loss = 2.15 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:54.257956: step 900, loss = 2.12 (927.7 examples/sec; 0.138 sec/batch)
2017-05-09 00:36:55.430399: step 910, loss = 2.27 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-09 00:36:56.711010: step 920, loss = 2.14 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:36:57.994741: step 930, loss = 2.02 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:36:59.273524: step 940, loss = 2.18 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:37:00.561528: step 950, loss = 2.11 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:37:01.829073: step 960, loss = 2.21 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:37:03.132572: step 970, loss = 1.93 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:37:04.457410: step 980, loss = 2.12 (966.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:37:05.757387: step 990, loss = 1.97 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:37:07.162672: step 1000, loss = 1.87 (910.8 examples/sec; 0.141 sec/batch)
2017-05-09 00:37:08.338453: step 1010, loss = 2.03 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-09 00:37:09.611072: step 1020, loss = 1.89 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:37:10.939877: step 1030, loss = 1.88 (963.3 examples/sec; 0.133 sec/batch)
2017-05-09 00:37:12.294139: step 1040, loss = 1.88 (945.2 examples/sec; 0.135 sec/batch)
2017-05-09 00:37:13.586247: step 1050, loss = 1.86 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:37:14.869865: step 1060, loss = 1.86 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:37:16.182363: step 1070, loss = 1.83 (975.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:37:17.476965: step 1080, loss = 1.65 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:37:18.772653: step 1090, loss = 1.90 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:37:20.187479: step 1100, loss = 1.90 (904.7 examples/sec; 0.141 sec/batch)
2017-05-09 00:37:21.384862: step 1110, loss = 1.84 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-09 00:37:22.677520: step 1120, loss = 1.78 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:37:24.028184: step 1130, loss = 1.63 (947.7 examples/sec; 0.135 sec/batch)
2017-05-09 00:37:25.353996: step 1140, loss = 1.99 (965.4 examples/sec; 0.133 sec/batch)
2017-05-09 00:37:26.627148: step 1150, loss = 1.82 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:37:27.963621: step 1160, loss = 1.91 (957.7 examples/sec; 0.134 sec/batch)
2017-05-09 00:37:29.275028: step 1170, loss = 1.83 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:37:30.563790: step 1180, loss = 1.94 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:37:31.870232: step 1190, loss = 1.80 (979.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:37:33.282823: step 1200, loss = 1.79 (906.1 examples/sec; 0.141 sec/batch)
2017-05-09 00:37:34.508986: step 1210, loss = 1.86 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-09 00:37:35.820732: step 1220, loss = 1.96 (975.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:37:37.119024: step 1230, loss = 1.95 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:37:38.428535: step 1240, loss = 1.79 (977.5 examples/sec; 0.131 sec/batch)
2017-05-09 00:37:39.759063: step 1250, loss = 1.67 (962.0 examples/sec; 0.133 sec/batch)
2017-05-09 00:37:41.110127: step 1260, loss = 1.67 (947.4 examples/sec; 0.135 sec/batch)
2017-05-09 00:37:42.437561: step 1270, loss = 1.57 (964.3 examples/sec; 0.133 sec/batch)
2017-05-09 00:37:43.780209: step 1280, loss = 1.81 (953.3 examples/sec; 0.134 sec/batch)
2017-05-09 00:37:45.076643: step 1290, loss = 1.79 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:37:46.484773: step 1300, loss = 1.83 (909.0 examples/sec; 0.141 sec/batch)
2017-05-09 00:37:47.704931: step 1310, loss = 1.70 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-09 00:37:49.014812: step 1320, loss = 1.79 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:37:50.336811: step 1330, loss = 1.52 (968.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:37:51.618129: step 1340, loss = 1.48 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:37:52.933350: step 1350, loss = 1.40 (973.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:37:54.263006: step 1360, loss = 1.78 (962.6 examples/sec; 0.133 sec/batch)
2017-05-09 00:37:55.600598: step 1370, loss = 1.40 (956.9 examples/sec; 0.134 sec/batch)
2017-05-09 00:37:56.947921: step 1380, loss = 1.78 (950.0 examples/sec; 0.135 sec/batch)
2017-05-09 00:37:58.250825: step 1390, loss = 1.54 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:37:59.674352: step 1400, loss = 1.74 (899.2 examples/sec; 0.142 sec/batch)
2017-05-09 00:38:00.879584: step 1410, loss = 1.68 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-09 00:38:02.196151: step 1420, loss = 1.87 (972.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:38:03.492429: step 1430, loss = 1.62 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:38:04.823435: step 1440, loss = 1.71 (961.7 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:06.105598: step 1450, loss = 1.57 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:38:07.419483: step 1460, loss = 1.45 (974.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:38:08.716583: step 1470, loss = 1.56 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:38:10.005387: step 1480, loss = 1.63 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:38:11.314819: step 1490, loss = 1.55 (977.5 examples/sec; 0.131 sec/batch)
2017-05-09 00:38:12.740548: step 1500, loss = 1.43 (897.8 examples/sec; 0.143 sec/batch)
2017-05-09 00:38:13.956209: step 1510, loss = 1.51 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-09 00:38:15.304425: step 1520, loss = 1.61 (949.4 examples/sec; 0.135 sec/batch)
2017-05-09 00:38:16.630879: step 1530, loss = 1.66 (965.0 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:17.960485: step 1540, loss = 1.45 (962.7 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:19.264125: step 1550, loss = 1.54 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:38:20.595049: step 1560, loss = 1.57 (961.7 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:21.924658: step 1570, loss = 1.38 (962.7 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:23.276761: step 1580, loss = 1.57 (946.7 examples/sec; 0.135 sec/batch)
2017-05-09 00:38:24.591684: step 1590, loss = 1.40 (973.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:38:25.992143: step 1600, loss = 1.39 (914.0 examples/sec; 0.140 sec/batch)
2017-05-09 00:38:27.203979: step 1610, loss = 1.48 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-09 00:38:28.484509: step 1620, loss = 1.39 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:38:29.770863: step 1630, loss = 1.75 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:38:31.111981: step 1640, loss = 1.52 (954.4 examples/sec; 0.134 sec/batch)
2017-05-09 00:38:32.435771: step 1650, loss = 1.34 (966.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:38:33.796978: step 1660, loss = 1.48 (940.3 examples/sec; 0.136 sec/batch)
2017-05-09 00:38:35.157405: step 1670, loss = 1.86 (940.9 examples/sec; 0.136 sec/batch)
2017-05-09 00:38:36.489278: step 1680, loss = 1.47 (961.1 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:37.795879: step 1690, loss = 1.44 (979.6 examples/sec; 0.131 sec/batch)
2017-05-09 00:38:39.172849: step 1700, loss = 1.53 (929.6 examples/sec; 0.138 sec/batch)
2017-05-09 00:38:40.340190: step 1710, loss = 1.35 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-09 00:38:41.623284: step 1720, loss = 1.43 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:38:42.920890: step 1730, loss = 1.40 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:38:44.229117: step 1740, loss = 1.41 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:38:45.557094: step 1750, loss = 1.50 (963.9 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:46.869664: step 1760, loss = 1.38 (975.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:38:48.212262: step 1770, loss = 1.54 (953.4 examples/sec; 0.134 sec/batch)
2017-05-09 00:38:49.518162: step 1780, loss = 1.49 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:38:50.843881: step 1790, loss = 1.46 (965.5 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:52.251201: step 1800, loss = 1.20 (909.5 examples/sec; 0.141 sec/batch)
2017-05-09 00:38:53.472153: step 1810, loss = 1.37 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-09 00:38:54.764932: step 1820, loss = 1.42 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:38:56.097003: step 1830, loss = 1.53 (960.9 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:57.412446: step 1840, loss = 1.44 (973.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:38:58.741996: step 1850, loss = 1.89 (962.7 examples/sec; 0.133 sec/batch)
2017-05-09 00:39:00.027424: step 1860, loss = 1.45 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:39:01.303005: step 1870, loss = 1.30 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:02.605350: step 1880, loss = 1.47 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:39:03.883790: step 1890, loss = 1.46 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:05.264214: step 1900, loss = 1.60 (927.3 examples/sec; 0.138 sec/batch)
2017-05-09 00:39:06.440054: step 1910, loss = 1.48 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-09 00:39:07.715838: step 1920, loss = 1.43 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:09.005655: step 1930, loss = 1.24 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:39:10.288293: step 1940, loss = 1.45 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:11.592035: step 1950, loss = 1.24 (981.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:39:12.906617: step 1960, loss = 1.39 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:39:14.173472: step 1970, loss = 1.34 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:39:15.433870: step 1980, loss = 1.34 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-09 00:39:16.710216: step 1990, loss = 1.44 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:18.088800: step 2000, loss = 1.34 (928.5 examples/sec; 0.138 sec/batch)
2017-05-09 00:39:19.270943: step 2010, loss = 1.30 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-09 00:39:20.546961: step 2020, loss = 1.13 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:21.811864: step 2030, loss = 1.37 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-09 00:39:23.095826: step 2040, loss = 1.26 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:24.375618: step 2050, loss = 1.63 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:25.670525: step 2060, loss = 1.94 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:39:26.943092: step 2070, loss = 1.53 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:39:28.255208: step 2080, loss = 1.44 (975.5 examples/sec; 0.131 sec/batch)
2017-05-09 00:39:29.517636: step 2090, loss = 1.41 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-09 00:39:30.900849: step 2100, loss = 1.42 (925.4 examples/sec; 0.138 sec/batch)
2017-05-09 00:39:32.107383: step 2110, loss = 1.25 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-09 00:39:33.382653: step 2120, loss = 1.13 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:34.665919: step 2130, loss = 1.22 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:35.973205: step 2140, loss = 1.47 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:39:37.270657: step 2150, loss = 1.37 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:39:38.559378: step 2160, loss = 1.31 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:39:39.853860: step 2170, loss = 1.35 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:39:41.119781: step 2180, loss = 1.25 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:39:42.380490: step 2190, loss = 1.27 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-09 00:39:43.756172: step 2200, loss = 1.26 (930.4 examples/sec; 0.138 sec/batch)
2017-05-09 00:39:44.928014: step 2210, loss = 1.20 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-09 00:39:46.198007: step 2220, loss = 1.58 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 00:39:47.516368: step 2230, loss = 1.48 (970.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:39:48.820833: step 2240, loss = 1.43 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:39:50.109027: step 2250, loss = 1.14 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:39:51.368758: step 2260, loss = 1.08 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-09 00:39:52.626384: step 2270, loss = 1.17 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-09 00:39:53.891521: step 2280, loss = 1.26 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 00:39:55.164881: step 2290, loss = 1.18 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 00:39:56.535448: step 2300, loss = 1.32 (933.9 examples/sec; 0.137 sec/batch)
2017-05-09 00:39:57.733268: step 2310, loss = 1.31 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-09 00:39:59.024628: step 2320, loss = 1.56 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:40:00.325048: step 2330, loss = 1.35 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:01.603419: step 2340, loss = 1.26 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:40:02.915770: step 2350, loss = 1.30 (975.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:40:04.225220: step 2360, loss = 1.56 (977.5 examples/sec; 0.131 sec/batch)
2017-05-09 00:40:05.481682: step 2370, loss = 1.47 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-09 00:40:06.744906: step 2380, loss = 1.25 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-09 00:40:08.024457: step 2390, loss = 1.23 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:40:09.387725: step 2400, loss = 1.33 (938.9 examples/sec; 0.136 sec/batch)
2017-05-09 00:40:10.564226: step 2410, loss = 1.12 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-09 00:40:11.839701: step 2420, loss = 1.13 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:40:13.136080: step 2430, loss = 1.21 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:14.412708: step 2440, loss = 1.17 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:40:15.686291: step 2450, loss = 1.68 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:40:16.973271: step 2460, loss = 1.08 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:40:18.248919: step 2470, loss = 0.95 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:40:19.522962: step 2480, loss = 1.14 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-09 00:40:20.779623: step 2490, loss = 1.36 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-09 00:40:22.146470: step 2500, loss = 1.56 (936.5 examples/sec; 0.137 sec/batch)
2017-05-09 00:40:23.341662: step 2510, loss = 1.08 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-09 00:40:24.643551: step 2520, loss = 1.21 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:25.946260: step 2530, loss = 1.30 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:27.246517: step 2540, loss = 1.16 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:28.559496: step 2550, loss = 1.06 (974.9 examples/sec; 0.131 sec/batch)
2017-05-09 00:40:29.856310: step 2560, loss = 1.12 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:31.167798: step 2570, loss = 1.12 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:40:32.505772: step 2580, loss = 1.26 (956.7 examples/sec; 0.134 sec/batch)
2017-05-09 00:40:33.820408: step 2590, loss = 1.20 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:40:35.248762: step 2600, loss = 1.19 (896.1 examples/sec; 0.143 sec/batch)
2017-05-09 00:40:36.465707: step 2610, loss = 1.36 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-09 00:40:37.753608: step 2620, loss = 1.00 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:40:39.016308: step 2630, loss = 1.21 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-09 00:40:40.299495: step 2640, loss = 1.27 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:40:41.595799: step 2650, loss = 1.32 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:42.892576: step 2660, loss = 1.07 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:44.195558: step 2670, loss = 1.08 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:45.498677: step 2680, loss = 0.98 (982.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:46.810109: step 2690, loss = 1.38 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:40:48.207191: step 2700, loss = 1.37 (916.2 examples/sec; 0.140 sec/batch)
2017-05-09 00:40:49.419183: step 2710, loss = 1.16 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-09 00:40:50.731709: step 2720, loss = 1.20 (975.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:40:52.039161: step 2730, loss = 1.31 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:40:53.351988: step 2740, loss = 1.18 (975.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:40:54.665653: step 2750, loss = 1.21 (974.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:40:55.973734: step 2760, loss = 1.01 (978.5 examples/sec; 0.131 sec/batch)
2017-05-09 00:40:57.267156: step 2770, loss = 1.42 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:40:58.571780: step 2780, loss = 1.09 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:59.870909: step 2790, loss = 1.52 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:41:01.263940: step 2800, loss = 1.47 (918.9 examples/sec; 0.139 sec/batch)
2017-05-09 00:41:02.424419: step 2810, loss = 1.37 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-09 00:41:03.719611: step 2820, loss = 1.39 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:41:05.050927: step 2830, loss = 1.19 (961.5 examples/sec; 0.133 sec/batch)
2017-05-09 00:41:06.340350: step 2840, loss = 1.19 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:41:07.625416: step 2850, loss = 1.11 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:41:08.949666: step 2860, loss = 1.38 (966.6 examples/sec; 0.132 sec/batch)
2017-05-09 00:41:10.236704: step 2870, loss = 1.23 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:41:11.546577: step 2880, loss = 1.11 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:41:12.842141: step 2890, loss = 1.13 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:41:14.223238: step 2900, loss = 1.25 (926.8 examples/sec; 0.138 sec/batch)
2017-05-09 00:41:15.429541: step 2910, loss = 1.36 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-09 00:41:16.737050: step 2920, loss = 1.19 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:41:18.045557: step 2930, loss = 1.29 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:41:19.318887: step 2940, loss = 1.25 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 00:41:20.591515: step 2950, loss = 1.19 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:41:21.854880: step 2960, loss = 1.13 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 00:41:23.147487: step 2970, loss = 1.20 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:41:24.449768: step 2980, loss = 1.20 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:41:25.711216: step 2990, loss = 1.13 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-09 00:41:27.088482: step 3000, loss = 1.08 (929.4 examples/sec; 0.138 sec/batch)
2017-05-09 00:41:28.268840: step 3010, loss = 1.10 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-09 00:41:29.554525: step 3020, loss = 1.10 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:41:30.824897: step 3030, loss = 1.03 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:41:32.114119: step 3040, loss = 1.20 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:41:33.403574: step 3050, loss = 1.12 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:41:34.680834: step 3060, loss = 1.21 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:41:35.949089: step 3070, loss = 1.08 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-09 00:41:37.260302: step 3080, loss = 1.21 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:41:38.569024: step 3090, loss = 1.80 (978.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:41:39.985791: step 3100, loss = 1.47 (903.5 examples/sec; 0.142 sec/batch)
2017-05-09 00:41:41.219983: step 3110, loss = 1.10 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-09 00:41:42.527171: step 3120, loss = 1.22 (979.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:41:43.796942: step 3130, loss = 1.23 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:41:45.110171: step 3140, loss = 0.99 (974.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:41:46.419171: step 3150, loss = 1.03 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:41:47.710175: step 3160, loss = 1.06 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:41:49.018298: step 3170, loss = 1.17 (978.5 examples/sec; 0.131 sec/batch)
2017-05-09 00:41:50.299749: step 3180, loss = 1.10 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:41:51.579457: step 3190, loss = 1.10 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:41:52.944695: step 3200, loss = 1.17 (937.6 examples/sec; 0.137 sec/batch)
2017-05-09 00:41:54.114842: step 3210, loss = 1.08 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-09 00:41:55.421202: step 3220, loss = 1.17 (979.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:41:56.705871: step 3230, loss = 1.09 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:41:58.004720: step 3240, loss = 0.92 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:41:59.309705: step 3250, loss = 1.24 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:42:00.629634: step 3260, loss = 1.30 (969.8 examples/sec; 0.132 sec/batch)
2017-05-09 00:42:01.946821: step 3270, loss = 1.03 (971.8 examples/sec; 0.132 sec/batch)
2017-05-09 00:42:03.271591: step 3280, loss = 1.04 (966.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:42:04.577419: step 3290, loss = 1.35 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:42:05.952275: step 3300, loss = 1.29 (931.0 examples/sec; 0.137 sec/batch)
2017-05-09 00:42:07.155743: step 3310, loss = 1.23 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-09 00:42:08.444007: step 3320, loss = 1.12 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:42:09.721836: step 3330, loss = 1.05 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:42:11.004109: step 3340, loss = 1.26 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:42:12.308349: step 3350, loss = 1.18 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:42:13.598900: step 3360, loss = 0.94 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:42:14.889409: step 3370, loss = 1.02 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:42:16.206092: step 3380, loss = 1.30 (972.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:42:17.487641: step 3390, loss = 1.41 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:42:18.858621: step 3400, loss = 1.08 (933.6 examples/sec; 0.137 sec/batch)
2017-05-09 00:42:20.029251: step 3410, loss = 1.12 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-09 00:42:21.329852: step 3420, loss = 1.16 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:42:22.624748: step 3430, loss = 1.11 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:42:23.900011: step 3440, loss = 1.18 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:42:25.225153: step 3450, loss = 1.00 (965.9 examples/sec; 0.133 sec/batch)
2017-05-09 00:42:26.507428: step 3460, loss = 1.09 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:42:27.806276: step 3470, loss = 1.15 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:42:29.151354: step 3480, loss = 1.04 (951.6 examples/sec; 0.135 sec/batch)
2017-05-09 00:42:30.467366: step 3490, loss = 1.14 (972.6 examples/sec; 0.132 sec/batch)
2017-05-09 00:42:31.845399: step 3500, loss = 1.14 (928.9 examples/sec; 0.138 sec/batch)
2017-05-09 00:42:33.004198: step 3510, loss = 1.11 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-09 00:42:34.279571: step 3520, loss = 1.20 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:42:35.566664: step 3530, loss = 0.98 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:42:36.855724: step 3540, loss = 1.11 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:42:38.148773: step 3550, loss = 1.12 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:42:39.498514: step 3560, loss = 1.15 (948.3 examples/sec; 0.135 sec/batch)
2017-05-09 00:42:40.871460: step 3570, loss = 1.18 (932.3 examples/sec; 0.137 sec/batch)
2017-05-09 00:42:42.147163: step 3580, loss = 1.16 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:42:43.454236: step 3590, loss = 1.17 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:42:44.915951: step 3600, loss = 0.92 (875.7 examples/sec; 0.146 sec/batch)
2017-05-09 00:42:46.145355: step 3610, loss = 1.32 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-09 00:42:47.540146: step 3620, loss = 1.59 (917.7 examples/sec; 0.139 sec/batch)
2017-05-09 00:42:48.884679: step 3630, loss = 1.11 (952.0 examples/sec; 0.134 sec/batch)
2017-05-09 00:42:50.163155: step 3640, loss = 1.02 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:42:51.486994: step 3650, loss = 1.24 (966.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:42:52.817178: step 3660, loss = 1.12 (962.3 examples/sec; 0.133 sec/batch)
2017-05-09 00:42:54.205146: step 3670, loss = 1.14 (922.2 examples/sec; 0.139 sec/batch)
2017-05-09 00:42:55.540417: step 3680, loss = 1.09 (958.6 examples/sec; 0.134 sec/batch)
2017-05-09 00:42:56.866490: step 3690, loss = 1.11 (965.3 examples/sec; 0.133 sec/batch)
2017-05-09 00:42:58.269119: step 3700, loss = 1.01 (912.6 examples/sec; 0.140 sec/batch)
2017-05-09 00:42:59.471641: step 3710, loss = 1.06 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-09 00:43:00.788607: step 3720, loss = 1.05 (971.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:43:02.135757: step 3730, loss = 1.17 (950.2 examples/sec; 0.135 sec/batch)
2017-05-09 00:43:03.477495: step 3740, loss = 1.25 (954.0 examples/sec; 0.134 sec/batch)
2017-05-09 00:43:04.782998: step 3750, loss = 1.12 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 00:43:06.099928: step 3760, loss = 1.23 (972.0 examples/sec; 0.132 sec/batch)
2017-05-09 00:43:07.441673: step 3770, loss = 1.36 (954.0 examples/sec; 0.134 sec/batch)
2017-05-09 00:43:08.800976: step 3780, loss = 1.23 (941.7 examples/sec; 0.136 sec/batch)
2017-05-09 00:43:10.225453: step 3790, loss = 0.90 (898.6 examples/sec; 0.142 sec/batch)
2017-05-09 00:43:11.676293: step 3800, loss = 0.99 (882.2 examples/sec; 0.145 sec/batch)
2017-05-09 00:43:12.956299: step 3810, loss = 1.04 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:43:14.303609: step 3820, loss = 1.07 (950.0 examples/sec; 0.135 sec/batch)
2017-05-09 00:43:15.638458: step 3830, loss = 1.02 (958.9 examples/sec; 0.133 sec/batch)
2017-05-09 00:43:16.944889: step 3840, loss = 1.34 (979.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:43:18.275002: step 3850, loss = 0.99 (962.3 examples/sec; 0.133 sec/batch)
2017-05-09 00:43:19.628510: step 3860, loss = 0.87 (945.7 examples/sec; 0.135 sec/batch)
2017-05-09 00:43:20.965850: step 3870, loss = 1.20 (957.1 examples/sec; 0.134 sec/batch)
2017-05-09 00:43:22.279468: step 3880, loss = 1.19 (974.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:43:23.598791: step 3890, loss = 1.27 (970.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:43:25.050379: step 3900, loss = 1.19 (881.8 examples/sec; 0.145 sec/batch)
2017-05-09 00:43:26.324708: step 3910, loss = 0.96 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:43:27.683045: step 3920, loss = 1.11 (942.3 examples/sec; 0.136 sec/batch)
2017-05-09 00:43:29.009845: step 3930, loss = 1.13 (964.7 examples/sec; 0.133 sec/batch)
2017-05-09 00:43:30.358203: step 3940, loss = 1.00 (949.3 examples/sec; 0.135 sec/batch)
2017-05-09 00:43:31.730829: step 3950, loss = 0.99 (932.5 examples/sec; 0.137 sec/batch)
2017-05-09 00:43:33.070408: step 3960, loss = 1.08 (955.5 examples/sec; 0.134 sec/batch)
2017-05-09 00:43:34.424405: step 3970, loss = 0.99 (945.3 examples/sec; 0.135 sec/batch)
2017-05-09 00:43:35.745374: step 3980, loss = 1.37 (969.0 examples/sec; 0.132 sec/batch)
2017-05-09 00:43:37.080722: step 3990, loss = 1.24 (958.6 examples/sec; 0.134 sec/batch)
2017-05-09 00:43:38.549896: step 4000, loss = 1.24 (871.2 examples/sec; 0.147 sec/batch)
2017-05-09 00:43:39.766303: step 4010, loss = 1.10 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-09 00:43:41.090707: step 4020, loss = 1.11 (966.5 examples/sec; 0.132 sec/batch)
2017-05-09 00:43:42.364897: step 4030, loss = 1.00 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:43:43.632963: step 4040, loss = 0.79 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:43:44.917388: step 4050, loss = 1.03 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:43:46.203307: step 4060, loss = 0.90 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:43:47.498055: step 4070, loss = 1.07 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:43:48.782383: step 4080, loss = 1.17 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:43:50.057780: step 4090, loss = 1.09 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:43:51.552738: step 4100, loss = 1.23 (856.2 examples/sec; 0.149 sec/batch)
2017-05-09 00:43:52.804400: step 4110, loss = 0.88 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-09 00:43:54.138990: step 4120, loss = 0.92 (959.1 examples/sec; 0.133 sec/batch)
2017-05-09 00:43:55.478987: step 4130, loss = 0.84 (955.2 examples/sec; 0.134 sec/batch)
2017-05-09 00:43:56.839200: step 4140, loss = 1.08 (941.0 examples/sec; 0.136 sec/batch)
2017-05-09 00:43:58.167893: step 4150, loss = 1.07 (963.4 examples/sec; 0.133 sec/batch)
2017-05-09 00:43:59.484562: step 4160, loss = 0.98 (972.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:44:00.859523: step 4170, loss = 1.35 (930.9 examples/sec; 0.137 sec/batch)
2017-05-09 00:44:02.202867: step 4180, loss = 0.94 (952.8 examples/sec; 0.134 sec/batch)
2017-05-09 00:44:03.549522: step 4190, loss = 1.52 (950.5 examples/sec; 0.135 sec/batch)
2017-05-09 00:44:05.035504: step 4200, loss = 1.08 (861.4 examples/sec; 0.149 sec/batch)
2017-05-09 00:44:06.309413: step 4210, loss = 1.08 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:44:07.597643: step 4220, loss = 1.11 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:44:08.866306: step 4230, loss = 1.19 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 00:44:10.133779: step 4240, loss = 1.08 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 00:44:11.431091: step 4250, loss = 1.05 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:44:12.737303: step 4260, loss = 1.19 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 00:44:14.006236: step 4270, loss = 1.06 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 00:44:15.265978: step 4280, loss = 1.08 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-09 00:44:16.541993: step 4290, loss = 0.98 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:44:17.916519: step 4300, loss = 1.10 (931.2 examples/sec; 0.137 sec/batch)
2017-05-09 00:44:19.137960: step 4310, loss = 1.13 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-09 00:44:20.409348: step 4320, loss = 1.62 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:44:21.709900: step 4330, loss = 0.99 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:44:23.016185: step 4340, loss = 1.14 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 00:44:24.321838: step 4350, loss = 1.23 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:44:25.634256: step 4360, loss = 1.08 (975.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:44:26.932084: step 4370, loss = 0.84 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:44:28.261802: step 4380, loss = 1.24 (962.6 examples/sec; 0.133 sec/batch)
2017-05-09 00:44:29.548922: step 4390, loss = 1.02 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:44:30.933566: step 4400, loss = 0.85 (924.4 examples/sec; 0.138 sec/batch)
2017-05-09 00:44:32.108666: step 4410, loss = 1.00 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-09 00:44:33.384362: step 4420, loss = 1.11 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:44:34.669119: step 4430, loss = 1.22 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:44:35.973488: step 4440, loss = 1.04 (981.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:44:37.255919: step 4450, loss = 1.24 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:44:38.536313: step 4460, loss = 1.01 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:44:39.812657: step 4470, loss = 1.16 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:44:41.106432: step 4480, loss = 0.80 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:44:42.374927: step 4490, loss = 0.94 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:44:43.754000: step 4500, loss = 1.10 (928.2 examples/sec; 0.138 sec/batch)
2017-05-09 00:44:44.936717: step 4510, loss = 0.97 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-09 00:44:46.228468: step 4520, loss = 0.89 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:44:47.503554: step 4530, loss = 1.04 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:44:48.789428: step 4540, loss = 1.05 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:44:50.075998: step 4550, loss = 0.84 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:44:51.341009: step 4560, loss = 0.86 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:44:52.612913: step 4570, loss = 1.33 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:44:53.901561: step 4580, loss = 1.13 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:44:55.202654: step 4590, loss = 1.04 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:44:56.583967: step 4600, loss = 1.15 (926.7 examples/sec; 0.138 sec/batch)
2017-05-09 00:44:57.743422: step 4610, loss = 1.23 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-09 00:44:59.029004: step 4620, loss = 1.04 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:45:00.287833: step 4630, loss = 1.08 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-09 00:45:01.566540: step 4640, loss = 0.91 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:45:02.869939: step 4650, loss = 0.95 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:45:04.139789: step 4660, loss = 0.95 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:45:05.426265: step 4670, loss = 1.29 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:45:06.715647: step 4680, loss = 1.17 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:45:07.985028: step 4690, loss = 0.94 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:45:09.358395: step 4700, loss = 0.89 (932.0 examples/sec; 0.137 sec/batch)
2017-05-09 00:45:10.536548: step 4710, loss = 1.01 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-09 00:45:11.815260: step 4720, loss = 1.11 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:45:13.078004: step 4730, loss = 1.18 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-09 00:45:14.333354: step 4740, loss = 1.17 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-09 00:45:15.627408: step 4750, loss = 1.23 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:45:16.906568: step 4760, loss = 1.02 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:45:18.197904: step 4770, loss = 1.13 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:45:19.486303: step 4780, loss = 1.08 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:45:20.802559: step 4790, loss = 1.08 (972.5 examples/sec; 0.132 sec/batch)
2017-05-09 00:45:22.173943: step 4800, loss = 1.08 (933.4 examples/sec; 0.137 sec/batch)
2017-05-09 00:45:23.387182: step 4810, loss = 1.07 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-09 00:45:24.682155: step 4820, loss = 1.15 (988.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:45:25.995810: step 4830, loss = 0.97 (974.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:45:27.321120: step 4840, loss = 1.01 (965.8 examples/sec; 0.133 sec/batch)
2017-05-09 00:45:28.623451: step 4850, loss = 0.92 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:45:29.876892: step 4860, loss = 1.22 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-09 00:45:31.162665: step 4870, loss = 1.22 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:45:32.489867: step 4880, loss = 1.21 (964.4 examples/sec; 0.133 sec/batch)
2017-05-09 00:45:33.766163: step 4890, loss = 0.84 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:45:35.143766: step 4900, loss = 0.90 (929.1 examples/sec; 0.138 sec/batch)
2017-05-09 00:45:36.313368: step 4910, loss = 1.15 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-09 00:45:37.573307: step 4920, loss = 1.05 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-09 00:45:38.840640: step 4930, loss = 0.93 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:45:40.155062: step 4940, loss = 1.15 (973.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:45:41.452703: step 4950, loss = 1.16 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:45:42.752489: step 4960, loss = 1.16 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:45:44.055284: step 4970, loss = 1.07 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:45:45.361004: step 4980, loss = 1.22 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:45:46.627533: step 4990, loss = 1.20 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:45:48.000697: step 5000, loss = 0.97 (932.2 examples/sec; 0.137 sec/batch)
2017-05-09 00:45:49.182214: step 5010, loss = 1.22 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-09 00:45:50.458433: step 5020, loss = 1.01 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:45:51.756968: step 5030, loss = 0.91 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:45:53.045121: step 5040, loss = 1.05 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:45:54.336535: step 5050, loss = 1.16 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:45:55.615201: step 5060, loss = 1.11 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:45:56.913751: step 5070, loss = 1.13 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:45:58.224951: step 5080, loss = 1.07 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:45:59.530229: step 5090, loss = 1.05 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 00:46:00.907403: step 5100, loss = 0.99 (929.4 examples/sec; 0.138 sec/batch)
2017-05-09 00:46:02.086953: step 5110, loss = 0.93 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-09 00:46:03.371129: step 5120, loss = 0.91 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:46:04.642221: step 5130, loss = 1.06 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:46:05.917243: step 5140, loss = 1.40 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:46:07.212798: step 5150, loss = 0.93 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:46:08.494010: step 5160, loss = 0.98 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:46:09.778532: step 5170, loss = 0.92 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:46:11.065022: step 5180, loss = 0.92 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:12.384724: step 5190, loss = 1.26 (969.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:46:13.764547: step 5200, loss = 1.14 (927.7 examples/sec; 0.138 sec/batch)
2017-05-09 00:46:14.962423: step 5210, loss = 0.99 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-09 00:46:16.254994: step 5220, loss = 0.78 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:17.543123: step 5230, loss = 1.04 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:18.875441: step 5240, loss = 1.02 (960.7 examples/sec; 0.133 sec/batch)
2017-05-09 00:46:20.177444: step 5250, loss = 1.16 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:46:21.461976: step 5260, loss = 1.05 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:46:22.785742: step 5270, loss = 1.17 (966.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:46:24.084877: step 5280, loss = 0.99 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:46:25.358256: step 5290, loss = 1.04 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 00:46:26.740416: step 5300, loss = 1.01 (926.1 examples/sec; 0.138 sec/batch)
2017-05-09 00:46:27.911018: step 5310, loss = 0.91 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-09 00:46:29.197562: step 5320, loss = 1.15 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:30.456756: step 5330, loss = 1.14 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-09 00:46:31.737105: step 5340, loss = 1.20 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:46:33.012024: step 5350, loss = 1.14 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:46:34.258468: step 5360, loss = 1.09 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-09 00:46:35.547554: step 5370, loss = 1.06 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:36.822478: step 5380, loss = 0.87 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:46:38.090348: step 5390, loss = 0.93 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:46:39.468457: step 5400, loss = 1.03 (928.8 examples/sec; 0.138 sec/batch)
2017-05-09 00:46:40.641981: step 5410, loss = 1.12 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-09 00:46:41.949825: step 5420, loss = 1.01 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:46:43.235500: step 5430, loss = 1.03 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:44.513759: step 5440, loss = 1.31 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:46:45.790706: step 5450, loss = 0.94 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:46:47.064953: step 5460, loss = 0.89 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:46:48.346426: step 5470, loss = 1.07 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:46:49.639515: step 5480, loss = 0.95 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:50.910362: step 5490, loss = 0.92 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 00:46:52.285518: step 5500, loss = 0.91 (930.8 examples/sec; 0.138 sec/batch)
2017-05-09 00:46:53.466700: step 5510, loss = 1.03 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-09 00:46:54.753627: step 5520, loss = 1.30 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:56.032859: step 5530, loss = 0.76 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:46:57.291431: step 5540, loss = 1.12 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-09 00:46:58.574860: step 5550, loss = 0.93 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:46:59.850616: step 5560, loss = 0.93 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:47:01.150870: step 5570, loss = 0.91 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:47:02.449115: step 5580, loss = 1.09 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:47:03.745259: step 5590, loss = 0.95 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:47:05.151509: step 5600, loss = 1.33 (910.2 examples/sec; 0.141 sec/batch)
2017-05-09 00:47:06.298225: step 5610, loss = 1.05 (1116.2 examples/sec; 0.115 sec/batch)
2017-05-09 00:47:07.583955: step 5620, loss = 1.13 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:47:08.867288: step 5630, loss = 0.98 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:47:10.144409: step 5640, loss = 0.83 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:47:11.405303: step 5650, loss = 0.98 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-09 00:47:12.729423: step 5660, loss = 0.91 (966.7 examples/sec; 0.132 sec/batch)
2017-05-09 00:47:14.002256: step 5670, loss = 0.81 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:47:15.311607: step 5680, loss = 1.02 (977.6 examples/sec; 0.131 sec/batch)
2017-05-09 00:47:16.675404: step 5690, loss = 1.20 (938.6 examples/sec; 0.136 sec/batch)
2017-05-09 00:47:18.091170: step 5700, loss = 1.15 (904.1 examples/sec; 0.142 sec/batch)
2017-05-09 00:47:19.330837: step 5710, loss = 0.74 (1032.5 examples/sec; 0.124 sec/batch)
2017-05-09 00:47:20.684209: step 5720, loss = 0.98 (945.8 examples/sec; 0.135 sec/batch)
2017-05-09 00:47:22.090620: step 5730, loss = 0.96 (910.1 examples/sec; 0.141 sec/batch)
2017-05-09 00:47:23.394611: step 5740, loss = 1.11 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:47:24.745888: step 5750, loss = 0.85 (947.3 examples/sec; 0.135 sec/batch)
2017-05-09 00:47:26.054457: step 5760, loss = 0.84 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:47:27.381280: step 5770, loss = 1.03 (964.7 examples/sec; 0.133 sec/batch)
2017-05-09 00:47:28.708867: step 5780, loss = 1.10 (964.2 examples/sec; 0.133 sec/batch)
2017-05-09 00:47:30.094442: step 5790, loss = 1.02 (923.8 examples/sec; 0.139 sec/batch)
2017-05-09 00:47:31.525408: step 5800, loss = 1.11 (894.5 examples/sec; 0.143 sec/batch)
2017-05-09 00:47:32.693919: step 5810, loss = 1.04 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-09 00:47:33.961391: step 5820, loss = 1.00 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 00:47:35.248598: step 5830, loss = 1.07 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:47:36.499387: step 5840, loss = 1.22 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-09 00:47:37.757759: step 5850, loss = 1.04 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-09 00:47:39.082540: step 5860, loss = 1.18 (966.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:47:40.394016: step 5870, loss = 0.89 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:47:41.722372: step 5880, loss = 1.00 (963.6 examples/sec; 0.133 sec/batch)
2017-05-09 00:47:43.030967: step 5890, loss = 1.18 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:47:44.434550: step 5900, loss = 1.43 (911.9 examples/sec; 0.140 sec/batch)
2017-05-09 00:47:45.619249: step 5910, loss = 1.26 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-09 00:47:46.879854: step 5920, loss = 1.28 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-09 00:47:48.159840: step 5930, loss = 0.97 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:47:49.445583: step 5940, loss = 1.13 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:47:50.716185: step 5950, loss = 0.87 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:47:52.004810: step 5960, loss = 0.96 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:47:53.275098: step 5970, loss = 1.02 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:47:54.560553: step 5980, loss = 1.06 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:47:55.883989: step 5990, loss = 1.32 (967.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:47:57.257164: step 6000, loss = 1.04 (932.1 examples/sec; 0.137 sec/batch)
2017-05-09 00:47:58.443466: step 6010, loss = 0.93 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-09 00:47:59.716959: step 6020, loss = 1.04 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:48:01.027988: step 6030, loss = 1.25 (976.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:48:02.337632: step 6040, loss = 1.02 (977.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:48:03.601186: step 6050, loss = 1.23 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-09 00:48:04.899197: step 6060, loss = 0.98 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:48:06.196119: step 6070, loss = 1.09 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:48:07.471473: step 6080, loss = 1.01 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:48:08.784609: step 6090, loss = 0.92 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:48:10.180915: step 6100, loss = 1.02 (916.7 examples/sec; 0.140 sec/batch)
2017-05-09 00:48:11.367785: step 6110, loss = 1.20 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-09 00:48:12.644006: step 6120, loss = 0.92 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:48:13.919927: step 6130, loss = 0.98 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:48:15.219870: step 6140, loss = 0.86 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:48:16.506480: step 6150, loss = 0.99 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:48:17.776116: step 6160, loss = 1.16 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 00:48:19.125205: step 6170, loss = 0.84 (948.8 examples/sec; 0.135 sec/batch)
2017-05-09 00:48:20.444199: step 6180, loss = 1.11 (970.4 examples/sec; 0.132 sec/batch)
2017-05-09 00:48:21.772835: step 6190, loss = 1.09 (963.4 examples/sec; 0.133 sec/batch)
2017-05-09 00:48:23.208664: step 6200, loss = 0.91 (891.5 examples/sec; 0.144 sec/batch)
2017-05-09 00:48:24.432621: step 6210, loss = 0.88 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-09 00:48:25.779087: step 6220, loss = 1.02 (950.6 examples/sec; 0.135 sec/batch)
2017-05-09 00:48:27.161307: step 6230, loss = 1.08 (926.0 examples/sec; 0.138 sec/batch)
2017-05-09 00:48:28.433604: step 6240, loss = 1.18 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:48:29.749511: step 6250, loss = 1.05 (972.7 examples/sec; 0.132 sec/batch)
2017-05-09 00:48:31.070600: step 6260, loss = 0.98 (968.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:48:32.400178: step 6270, loss = 0.93 (962.7 examples/sec; 0.133 sec/batch)
2017-05-09 00:48:33.735069: step 6280, loss = 1.31 (958.9 examples/sec; 0.133 sec/batch)
2017-05-09 00:48:35.041223: step 6290, loss = 1.21 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:48:36.488650: step 6300, loss = 0.71 (884.3 examples/sec; 0.145 sec/batch)
2017-05-09 00:48:37.697656: step 6310, loss = 0.94 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-09 00:48:39.013924: step 6320, loss = 0.97 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 00:48:40.380801: step 6330, loss = 1.14 (936.4 examples/sec; 0.137 sec/batch)
2017-05-09 00:48:41.725064: step 6340, loss = 0.80 (952.2 examples/sec; 0.134 sec/batch)
2017-05-09 00:48:43.062092: step 6350, loss = 0.88 (957.4 examples/sec; 0.134 sec/batch)
2017-05-09 00:48:44.379691: step 6360, loss = 1.12 (971.5 examples/sec; 0.132 sec/batch)
2017-05-09 00:48:45.728795: step 6370, loss = 1.11 (948.8 examples/sec; 0.135 sec/batch)
2017-05-09 00:48:47.135574: step 6380, loss = 0.99 (909.9 examples/sec; 0.141 sec/batch)
2017-05-09 00:48:48.483162: step 6390, loss = 1.19 (949.9 examples/sec; 0.135 sec/batch)
2017-05-09 00:48:49.867258: step 6400, loss = 1.06 (924.8 examples/sec; 0.138 sec/batch)
2017-05-09 00:48:51.124525: step 6410, loss = 1.22 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-09 00:48:52.460061: step 6420, loss = 1.05 (958.4 examples/sec; 0.134 sec/batch)
2017-05-09 00:48:53.845486: step 6430, loss = 0.99 (923.9 examples/sec; 0.139 sec/batch)
2017-05-09 00:48:55.197426: step 6440, loss = 1.13 (946.8 examples/sec; 0.135 sec/batch)
2017-05-09 00:48:56.518165: step 6450, loss = 0.88 (969.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:48:57.850042: step 6460, loss = 0.92 (961.1 examples/sec; 0.133 sec/batch)
2017-05-09 00:48:59.193453: step 6470, loss = 0.98 (952.8 examples/sec; 0.134 sec/batch)
2017-05-09 00:49:00.556251: step 6480, loss = 0.93 (939.2 examples/sec; 0.136 sec/batch)
2017-05-09 00:49:01.904333: step 6490, loss = 0.91 (949.5 examples/sec; 0.135 sec/batch)
2017-05-09 00:49:03.321018: step 6500, loss = 0.93 (903.5 examples/sec; 0.142 sec/batch)
2017-05-09 00:49:04.574184: step 6510, loss = 1.47 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-09 00:49:05.889592: step 6520, loss = 0.95 (973.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:49:07.183830: step 6530, loss = 0.91 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:08.504289: step 6540, loss = 0.99 (969.4 examples/sec; 0.132 sec/batch)
2017-05-09 00:49:09.829618: step 6550, loss = 1.08 (965.8 examples/sec; 0.133 sec/batch)
2017-05-09 00:49:11.191310: step 6560, loss = 1.07 (940.0 examples/sec; 0.136 sec/batch)
2017-05-09 00:49:12.511838: step 6570, loss = 1.19 (969.3 examples/sec; 0.132 sec/batch)
2017-05-09 00:49:13.866999: step 6580, loss = 0.97 (944.5 examples/sec; 0.136 sec/batch)
2017-05-09 00:49:15.229149: step 6590, loss = 1.04 (939.7 examples/sec; 0.136 sec/batch)
2017-05-09 00:49:16.681099: step 6600, loss = 1.03 (881.6 examples/sec; 0.145 sec/batch)
2017-05-09 00:49:17.907421: step 6610, loss = 0.94 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-09 00:49:19.197041: step 6620, loss = 0.96 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:20.471859: step 6630, loss = 0.99 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:49:21.757932: step 6640, loss = 0.94 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:23.038667: step 6650, loss = 0.87 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:49:24.311027: step 6660, loss = 0.89 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:49:25.589134: step 6670, loss = 1.09 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:49:26.878221: step 6680, loss = 1.05 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:28.152373: step 6690, loss = 0.95 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:49:29.533338: step 6700, loss = 1.02 (926.9 examples/sec; 0.138 sec/batch)
2017-05-09 00:49:30.693600: step 6710, loss = 0.86 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-09 00:49:31.975230: step 6720, loss = 1.03 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:49:33.278410: step 6730, loss = 0.90 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:49:34.588890: step 6740, loss = 1.04 (976.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:49:35.907524: step 6750, loss = 1.29 (970.7 examples/sec; 0.132 sec/batch)
2017-05-09 00:49:37.199740: step 6760, loss = 1.27 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:38.463193: step 6770, loss = 1.05 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-09 00:49:39.743291: step 6780, loss = 0.92 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:49:41.028902: step 6790, loss = 0.95 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:42.403173: step 6800, loss = 1.24 (931.4 examples/sec; 0.137 sec/batch)
2017-05-09 00:49:43.601055: step 6810, loss = 0.97 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-09 00:49:44.895646: step 6820, loss = 0.98 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:46.185074: step 6830, loss = 1.14 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:47.470384: step 6840, loss = 0.79 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:48.751494: step 6850, loss = 1.07 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:49:50.018732: step 6860, loss = 0.98 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:49:51.303693: step 6870, loss = 1.01 (996.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:49:52.588847: step 6880, loss = 1.05 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:53.849865: step 6890, loss = 1.21 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-09 00:49:55.235566: step 6900, loss = 1.02 (923.7 examples/sec; 0.139 sec/batch)
2017-05-09 00:49:56.413448: step 6910, loss = 0.94 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-09 00:49:57.680136: step 6920, loss = 1.22 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:49:58.982188: step 6930, loss = 1.20 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:00.277383: step 6940, loss = 0.84 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:01.555549: step 6950, loss = 0.84 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:02.846892: step 6960, loss = 0.90 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:50:04.126363: step 6970, loss = 0.88 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:05.409241: step 6980, loss = 0.98 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:06.707744: step 6990, loss = 1.23 (985.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:08.082523: step 7000, loss = 1.13 (931.1 examples/sec; 0.137 sec/batch)
2017-05-09 00:50:09.259751: step 7010, loss = 0.89 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-09 00:50:10.537277: step 7020, loss = 0.91 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:11.818129: step 7030, loss = 0.91 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:13.128412: step 7040, loss = 1.24 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 00:50:14.429379: step 7050, loss = 1.22 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:15.740274: step 7060, loss = 0.98 (976.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:50:17.047470: step 7070, loss = 0.90 (979.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:50:18.335887: step 7080, loss = 1.06 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:50:19.616625: step 7090, loss = 1.00 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:20.991832: step 7100, loss = 1.07 (930.8 examples/sec; 0.138 sec/batch)
2017-05-09 00:50:22.160046: step 7110, loss = 0.88 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-09 00:50:23.459742: step 7120, loss = 0.96 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:24.789188: step 7130, loss = 1.14 (962.8 examples/sec; 0.133 sec/batch)
2017-05-09 00:50:26.084370: step 7140, loss = 1.19 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:27.386806: step 7150, loss = 0.93 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:28.682893: step 7160, loss = 0.96 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:29.982810: step 7170, loss = 0.81 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:31.278697: step 7180, loss = 1.04 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:32.582729: step 7190, loss = 0.83 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:33.930039: step 7200, loss = 0.85 (950.0 examples/sec; 0.135 sec/batch)
2017-05-09 00:50:35.123308: step 7210, loss = 1.17 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-09 00:50:36.403969: step 7220, loss = 0.98 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:37.702978: step 7230, loss = 0.90 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:38.975423: step 7240, loss = 0.96 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 00:50:40.258496: step 7250, loss = 0.89 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:41.539530: step 7260, loss = 0.95 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:42.815019: step 7270, loss = 1.09 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:44.092023: step 7280, loss = 1.02 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:45.371112: step 7290, loss = 0.92 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:46.735586: step 7300, loss = 1.12 (938.1 examples/sec; 0.136 sec/batch)
2017-05-09 00:50:47.936985: step 7310, loss = 1.07 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-09 00:50:49.230953: step 7320, loss = 0.91 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:50:50.495965: step 7330, loss = 1.14 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:50:51.781152: step 7340, loss = 1.19 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:50:53.061226: step 7350, loss = 0.91 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:54.358646: step 7360, loss = 0.96 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:55.675186: step 7370, loss = 0.82 (972.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:50:56.991501: step 7380, loss = 0.93 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 00:50:58.286998: step 7390, loss = 0.95 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:59.688624: step 7400, loss = 1.29 (913.2 examples/sec; 0.140 sec/batch)
2017-05-09 00:51:00.897710: step 7410, loss = 0.85 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-09 00:51:02.180021: step 7420, loss = 0.80 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:03.446216: step 7430, loss = 1.13 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-09 00:51:04.712997: step 7440, loss = 0.97 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:51:05.998311: step 7450, loss = 0.86 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:51:07.280033: step 7460, loss = 1.00 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:08.548472: step 7470, loss = 0.93 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:51:09.859928: step 7480, loss = 1.34 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:51:11.151937: step 7490, loss = 1.08 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:51:12.544751: step 7500, loss = 0.95 (919.0 examples/sec; 0.139 sec/batch)
2017-05-09 00:51:13.760442: step 7510, loss = 0.75 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-09 00:51:15.052640: step 7520, loss = 0.93 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:51:16.333094: step 7530, loss = 0.84 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:17.614330: step 7540, loss = 0.95 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:18.901535: step 7550, loss = 1.21 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:51:20.233569: step 7560, loss = 1.11 (960.9 examples/sec; 0.133 sec/batch)
2017-05-09 00:51:21.499674: step 7570, loss = 1.02 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:51:22.813410: step 7580, loss = 0.74 (974.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:51:24.130782: step 7590, loss = 1.06 (971.6 examples/sec; 0.132 sec/batch)
2017-05-09 00:51:25.508214: step 7600, loss = 0.99 (929.3 examples/sec; 0.138 sec/batch)
2017-05-09 00:51:26.695829: step 7610, loss = 1.16 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-09 00:51:27.971343: step 7620, loss = 1.07 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:29.252256: step 7630, loss = 1.20 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:30.536780: step 7640, loss = 0.97 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:31.804371: step 7650, loss = 1.01 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:51:33.087166: step 7660, loss = 1.09 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:34.363615: step 7670, loss = 0.95 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:35.635828: step 7680, loss = 0.80 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:51:36.929920: step 7690, loss = 1.12 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:51:38.304889: step 7700, loss = 1.29 (930.9 examples/sec; 0.137 sec/batch)
2017-05-09 00:51:39.480673: step 7710, loss = 1.03 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-09 00:51:40.772676: step 7720, loss = 0.93 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:51:42.090015: step 7730, loss = 1.12 (971.7 examples/sec; 0.132 sec/batch)
2017-05-09 00:51:43.382132: step 7740, loss = 1.18 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:51:44.674704: step 7750, loss = 1.06 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:51:45.958864: step 7760, loss = 0.90 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:47.242881: step 7770, loss = 0.93 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:48.522914: step 7780, loss = 0.94 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:49.799747: step 7790, loss = 0.85 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:51.194078: step 7800, loss = 0.96 (918.0 examples/sec; 0.139 sec/batch)
2017-05-09 00:51:52.389800: step 7810, loss = 0.95 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-09 00:51:53.701377: step 7820, loss = 1.06 (975.9 examples/sec; 0.131 sec/batch)
2017-05-09 00:51:55.012669: step 7830, loss = 0.87 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:51:56.326978: step 7840, loss = 1.09 (973.9 examples/sec; 0.131 sec/batch)
2017-05-09 00:51:57.639980: step 7850, loss = 0.96 (975.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:51:58.920438: step 7860, loss = 0.96 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:52:00.210743: step 7870, loss = 0.91 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:52:01.506543: step 7880, loss = 0.80 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:52:02.768454: step 7890, loss = 0.89 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-09 00:52:04.145755: step 7900, loss = 0.93 (929.4 examples/sec; 0.138 sec/batch)
2017-05-09 00:52:05.327971: step 7910, loss = 0.92 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-09 00:52:06.594520: step 7920, loss = 0.84 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:52:07.876162: step 7930, loss = 1.06 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:52:09.172926: step 7940, loss = 0.85 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:52:10.458126: step 7950, loss = 1.02 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:52:11.783710: step 7960, loss = 1.09 (965.6 examples/sec; 0.133 sec/batch)
2017-05-09 00:52:13.089495: step 7970, loss = 1.06 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:52:14.389573: step 7980, loss = 1.20 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:52:15.706023: step 7990, loss = 0.97 (972.3 examples/sec; 0.132 sec/batch)
2017-05-09 00:52:17.104843: step 8000, loss = 0.97 (915.0 examples/sec; 0.140 sec/batch)
2017-05-09 00:52:18.315852: step 8010, loss = 0.85 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-09 00:52:19.624523: step 8020, loss = 1.08 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:52:20.928695: step 8030, loss = 1.07 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:52:22.199861: step 8040, loss = 0.87 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 00:52:23.469740: step 8050, loss = 0.83 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:52:24.739363: step 8060, loss = 0.89 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 00:52:25.998787: step 8070, loss = 0.87 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-09 00:52:27.287647: step 8080, loss = 0.76 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:52:28.567128: step 8090, loss = 1.05 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:52:29.926507: step 8100, loss = 0.71 (941.6 examples/sec; 0.136 sec/batch)
2017-05-09 00:52:31.182519: step 8110, loss = 1.19 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-09 00:52:32.514249: step 8120, loss = 0.88 (961.2 examples/sec; 0.133 sec/batch)
2017-05-09 00:52:33.870226: step 8130, loss = 0.94 (944.0 examples/sec; 0.136 sec/batch)
2017-05-09 00:52:35.177294: step 8140, loss = 1.16 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:52:36.456813: step 8150, loss = 0.86 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:52:37.722221: step 8160, loss = 0.87 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:52:38.979997: step 8170, loss = 0.83 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-09 00:52:40.263048: step 8180, loss = 0.80 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:52:41.522460: step 8190, loss = 0.93 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-09 00:52:42.863347: step 8200, loss = 1.02 (954.6 examples/sec; 0.134 sec/batch)
2017-05-09 00:52:44.046089: step 8210, loss = 0.97 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-09 00:52:45.323348: step 8220, loss = 1.18 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:52:46.615883: step 8230, loss = 1.02 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:52:47.899447: step 8240, loss = 0.81 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:52:49.170196: step 8250, loss = 0.95 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 00:52:50.464760: step 8260, loss = 0.78 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:52:51.822085: step 8270, loss = 0.92 (943.0 examples/sec; 0.136 sec/batch)
2017-05-09 00:52:53.205771: step 8280, loss = 0.98 (925.1 examples/sec; 0.138 sec/batch)
2017-05-09 00:52:54.529307: step 8290, loss = 0.69 (967.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:52:55.961952: step 8300, loss = 1.12 (893.4 examples/sec; 0.143 sec/batch)
2017-05-09 00:52:57.213626: step 8310, loss = 1.16 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-09 00:52:58.557645: step 8320, loss = 1.30 (952.4 examples/sec; 0.134 sec/batch)
2017-05-09 00:52:59.901389: step 8330, loss = 0.91 (952.6 examples/sec; 0.134 sec/batch)
2017-05-09 00:53:01.176116: step 8340, loss = 0.89 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:53:02.474350: step 8350, loss = 1.05 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:53:03.757663: step 8360, loss = 0.86 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:53:05.112820: step 8370, loss = 0.93 (944.5 examples/sec; 0.136 sec/batch)
2017-05-09 00:53:06.483975: step 8380, loss = 0.97 (933.5 examples/sec; 0.137 sec/batch)
2017-05-09 00:53:07.816179: step 8390, loss = 0.93 (960.8 examples/sec; 0.133 sec/batch)
2017-05-09 00:53:09.310361: step 8400, loss = 0.87 (856.7 examples/sec; 0.149 sec/batch)
2017-05-09 00:53:10.560480: step 8410, loss = 0.94 (1023.9 examples/sec; 0.125 sec/batch)
2017-05-09 00:53:11.899299: step 8420, loss = 1.02 (956.1 examples/sec; 0.134 sec/batch)
2017-05-09 00:53:13.313078: step 8430, loss = 0.99 (905.4 examples/sec; 0.141 sec/batch)
2017-05-09 00:53:14.644807: step 8440, loss = 0.92 (961.2 examples/sec; 0.133 sec/batch)
2017-05-09 00:53:15.978092: step 8450, loss = 1.00 (960.0 examples/sec; 0.133 sec/batch)
2017-05-09 00:53:17.322146: step 8460, loss = 0.99 (952.3 examples/sec; 0.134 sec/batch)
2017-05-09 00:53:18.671721: step 8470, loss = 1.02 (948.4 examples/sec; 0.135 sec/batch)
2017-05-09 00:53:19.995190: step 8480, loss = 0.96 (967.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:53:21.306111: step 8490, loss = 0.96 (976.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:53:22.695408: step 8500, loss = 1.07 (921.3 examples/sec; 0.139 sec/batch)
2017-05-09 00:53:23.910573: step 8510, loss = 0.88 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-09 00:53:25.215189: step 8520, loss = 0.88 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:53:26.491804: step 8530, loss = 0.96 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:53:27.777537: step 8540, loss = 1.00 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:53:29.065111: step 8550, loss = 0.82 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:53:30.346411: step 8560, loss = 0.94 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:53:31.626146: step 8570, loss = 0.90 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:53:32.923868: step 8580, loss = 0.89 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:53:34.177249: step 8590, loss = 1.01 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-09 00:53:35.591195: step 8600, loss = 1.15 (905.3 examples/sec; 0.141 sec/batch)
2017-05-09 00:53:36.799734: step 8610, loss = 0.92 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-09 00:53:38.058673: step 8620, loss = 1.06 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-09 00:53:39.364274: step 8630, loss = 0.98 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:53:40.644966: step 8640, loss = 0.97 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:53:41.935805: step 8650, loss = 1.01 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:53:43.239393: step 8660, loss = 0.84 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:53:44.571280: step 8670, loss = 0.93 (961.0 examples/sec; 0.133 sec/batch)
2017-05-09 00:53:45.873373: step 8680, loss = 0.96 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:53:47.125358: step 8690, loss = 0.94 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-09 00:53:48.500366: step 8700, loss = 0.92 (930.9 examples/sec; 0.138 sec/batch)
2017-05-09 00:53:49.655873: step 8710, loss = 0.98 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-09 00:53:50.932872: step 8720, loss = 0.94 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:53:52.214595: step 8730, loss = 0.95 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:53:53.488756: step 8740, loss = 0.89 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:53:54.766248: step 8750, loss = 0.82 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:53:56.055464: step 8760, loss = 0.77 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:53:57.349731: step 8770, loss = 1.05 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:53:58.620220: step 8780, loss = 0.91 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:53:59.893521: step 8790, loss = 1.22 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 00:54:01.284704: step 8800, loss = 1.05 (920.1 examples/sec; 0.139 sec/batch)
2017-05-09 00:54:02.463304: step 8810, loss = 0.92 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-09 00:54:03.753856: step 8820, loss = 0.85 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:54:05.039876: step 8830, loss = 0.89 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:54:06.314212: step 8840, loss = 0.97 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:54:07.624836: step 8850, loss = 1.33 (976.6 examples/sec; 0.131 sec/batch)
2017-05-09 00:54:08.925683: step 8860, loss = 0.92 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:54:10.239862: step 8870, loss = 0.96 (974.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:54:11.553173: step 8880, loss = 1.18 (974.6 examples/sec; 0.131 sec/batch)
2017-05-09 00:54:12.837398: step 8890, loss = 0.88 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:54:14.199050: step 8900, loss = 0.91 (940.0 examples/sec; 0.136 sec/batch)
2017-05-09 00:54:15.417632: step 8910, loss = 1.08 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-09 00:54:16.715966: step 8920, loss = 0.95 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:54:17.994969: step 8930, loss = 1.15 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:54:19.291216: step 8940, loss = 1.08 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:54:20.596833: step 8950, loss = 1.09 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:54:21.909216: step 8960, loss = 1.02 (975.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:54:23.202500: step 8970, loss = 0.92 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:54:24.468808: step 8980, loss = 1.09 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:54:25.739297: step 8990, loss = 0.81 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:54:27.115220: step 9000, loss = 1.01 (930.3 examples/sec; 0.138 sec/batch)
2017-05-09 00:54:28.289785: step 9010, loss = 0.83 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-09 00:54:29.569183: step 9020, loss = 0.78 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:54:30.902205: step 9030, loss = 1.12 (960.2 examples/sec; 0.133 sec/batch)
2017-05-09 00:54:32.206112: step 9040, loss = 0.94 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:54:33.548987: step 9050, loss = 0.77 (953.2 examples/sec; 0.134 sec/batch)
2017-05-09 00:54:34.872452: step 9060, loss = 0.90 (967.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:54:36.226546: step 9070, loss = 0.85 (945.3 examples/sec; 0.135 sec/batch)
2017-05-09 00:54:37.540555: step 9080, loss = 0.94 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:54:38.894272: step 9090, loss = 0.92 (945.5 examples/sec; 0.135 sec/batch)
2017-05-09 00:54:40.332408: step 9100, loss = 1.09 (890.0 examples/sec; 0.144 sec/batch)
2017-05-09 00:54:41.553228: step 9110, loss = 0.97 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-09 00:54:42.922413: step 9120, loss = 0.71 (934.9 examples/sec; 0.137 sec/batch)
2017-05-09 00:54:44.237611: step 9130, loss = 0.90 (973.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:54:45.587285: step 9140, loss = 1.07 (948.4 examples/sec; 0.135 sec/batch)
2017-05-09 00:54:46.889002: step 9150, loss = 1.21 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:54:48.216120: step 9160, loss = 0.87 (964.5 examples/sec; 0.133 sec/batch)
2017-05-09 00:54:49.491836: step 9170, loss = 0.93 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:54:50.790038: step 9180, loss = 0.81 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:54:52.084217: step 9190, loss = 1.07 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:54:53.478197: step 9200, loss = 0.97 (918.2 examples/sec; 0.139 sec/batch)
2017-05-09 00:54:54.672765: step 9210, loss = 0.88 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-09 00:54:55.965137: step 9220, loss = 1.15 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:54:57.261997: step 9230, loss = 1.14 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:54:58.539682: step 9240, loss = 1.05 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:54:59.843629: step 9250, loss = 0.89 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:55:01.143548: step 9260, loss = 1.11 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:55:02.478054: step 9270, loss = 0.88 (959.2 examples/sec; 0.133 sec/batch)
2017-05-09 00:55:03.759096: step 9280, loss = 0.80 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:55:05.054080: step 9290, loss = 1.02 (988.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:55:06.458621: step 9300, loss = 1.13 (911.3 examples/sec; 0.140 sec/batch)
2017-05-09 00:55:07.667635: step 9310, loss = 0.81 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-09 00:55:08.946025: step 9320, loss = 1.10 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:55:10.240945: step 9330, loss = 0.96 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:55:11.502172: step 9340, loss = 0.89 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-09 00:55:12.800051: step 9350, loss = 1.24 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:55:14.104365: step 9360, loss = 0.88 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:55:15.392967: step 9370, loss = 0.97 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:55:16.687526: step 9380, loss = 0.88 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:55:17.957403: step 9390, loss = 0.93 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:55:19.320879: step 9400, loss = 0.80 (938.8 examples/sec; 0.136 sec/batch)
2017-05-09 00:55:20.504037: step 9410, loss = 0.98 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-09 00:55:21.784867: step 9420, loss = 0.98 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:55:23.071584: step 9430, loss = 0.98 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:55:24.369594: step 9440, loss = 1.01 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:55:25.668939: step 9450, loss = 0.93 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:55:26.993489: step 9460, loss = 0.99 (966.4 examples/sec; 0.132 sec/batch)
2017-05-09 00:55:28.298189: step 9470, loss = 0.97 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:55:29.586666: step 9480, loss = 1.12 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:55:30.865670: step 9490, loss = 0.89 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:55:32.252283: step 9500, loss = 0.87 (923.1 examples/sec; 0.139 sec/batch)
2017-05-09 00:55:33.432510: step 9510, loss = 1.00 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-09 00:55:34.748105: step 9520, loss = 1.10 (972.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:55:36.048044: step 9530, loss = 0.91 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:55:37.330566: step 9540, loss = 0.88 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:55:38.621278: step 9550, loss = 0.93 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:55:39.897730: step 9560, loss = 0.90 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:55:41.175069: step 9570, loss = 1.01 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:55:42.443448: step 9580, loss = 0.87 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-09 00:55:43.715785: step 9590, loss = 0.93 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:55:45.106975: step 9600, loss = 0.98 (920.1 examples/sec; 0.139 sec/batch)
2017-05-09 00:55:46.308856: step 9610, loss = 0.93 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-09 00:55:47.606285: step 9620, loss = 0.84 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:55:48.945342: step 9630, loss = 1.06 (955.9 examples/sec; 0.134 sec/batch)
2017-05-09 00:55:50.215165: step 9640, loss = 0.85 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:55:51.508787: step 9650, loss = 1.04 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:55:52.812269: step 9660, loss = 0.96 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:55:54.102811: step 9670, loss = 0.95 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:55:55.372942: step 9680, loss = 0.98 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:55:56.688374: step 9690, loss = 1.02 (973.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:55:58.075543: step 9700, loss = 0.85 (922.7 examples/sec; 0.139 sec/batch)
2017-05-09 00:55:59.269783: step 9710, loss = 1.11 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-09 00:56:00.554436: step 9720, loss = 0.90 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:01.817913: step 9730, loss = 0.92 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-09 00:56:03.109281: step 9740, loss = 0.86 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:56:04.437587: step 9750, loss = 1.11 (963.6 examples/sec; 0.133 sec/batch)
2017-05-09 00:56:05.713480: step 9760, loss = 0.92 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:07.002588: step 9770, loss = 0.77 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:56:08.299468: step 9780, loss = 0.82 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:56:09.562104: step 9790, loss = 0.91 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-09 00:56:10.948127: step 9800, loss = 1.02 (923.5 examples/sec; 0.139 sec/batch)
2017-05-09 00:56:12.171794: step 9810, loss = 0.79 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-09 00:56:13.445593: step 9820, loss = 1.03 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 00:56:14.734737: step 9830, loss = 0.84 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:56:16.030885: step 9840, loss = 0.85 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:56:17.348966: step 9850, loss = 0.97 (971.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:56:18.634352: step 9860, loss = 1.12 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:56:19.937498: step 9870, loss = 0.82 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:56:21.213661: step 9880, loss = 0.89 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:22.510521: step 9890, loss = 1.03 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:56:23.889155: step 9900, loss = 0.95 (928.5 examples/sec; 0.138 sec/batch)
2017-05-09 00:56:25.083026: step 9910, loss = 0.93 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-09 00:56:26.360899: step 9920, loss = 1.21 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:27.655099: step 9930, loss = 0.85 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:56:28.941343: step 9940, loss = 0.93 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:56:30.215654: step 9950, loss = 0.82 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:56:31.502037: step 9960, loss = 0.91 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:56:32.788126: step 9970, loss = 1.09 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:56:34.067960: step 9980, loss = 1.05 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:35.346163: step 9990, loss = 0.98 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:36.749520: step 10000, loss = 0.94 (912.1 examples/sec; 0.140 sec/batch)
2017-05-09 00:56:37.939505: step 10010, loss = 0.96 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-09 00:56:39.239597: step 10020, loss = 1.16 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:56:40.546095: step 10030, loss = 1.19 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:56:41.786406: step 10040, loss = 0.96 (1032.0 examples/sec; 0.124 sec/batch)
2017-05-09 00:56:43.080620: step 10050, loss = 1.01 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:56:44.410683: step 10060, loss = 1.11 (962.4 examples/sec; 0.133 sec/batch)
2017-05-09 00:56:45.696282: step 10070, loss = 1.05 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:56:46.984477: step 10080, loss = 1.20 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:56:48.291883: step 10090, loss = 1.01 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:56:49.653680: step 10100, loss = 0.84 (939.9 examples/sec; 0.136 sec/batch)
2017-05-09 00:56:50.849161: step 10110, loss = 0.99 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-09 00:56:52.171627: step 10120, loss = 0.82 (967.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:56:53.469343: step 10130, loss = 1.03 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:56:54.757997: step 10140, loss = 0.91 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:56:56.054258: step 10150, loss = 0.79 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:56:57.337206: step 10160, loss = 0.88 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:58.583709: step 10170, loss = 0.78 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-09 00:56:59.877252: step 10180, loss = 0.93 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:57:01.169186: step 10190, loss = 0.98 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:57:02.551070: step 10200, loss = 0.79 (926.3 examples/sec; 0.138 sec/batch)
2017-05-09 00:57:03.738416: step 10210, loss = 0.79 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-09 00:57:05.037069: step 10220, loss = 1.02 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:57:06.316427: step 10230, loss = 1.02 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:57:07.601098: step 10240, loss = 0.99 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:57:08.894500: step 10250, loss = 0.98 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:57:10.157669: step 10260, loss = 1.10 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-09 00:57:11.471686: step 10270, loss = 1.17 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:57:12.767387: step 10280, loss = 0.86 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:57:14.075612: step 10290, loss = 0.99 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:57:15.473350: step 10300, loss = 0.83 (915.8 examples/sec; 0.140 sec/batch)
2017-05-09 00:57:16.661941: step 10310, loss = 0.78 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-09 00:57:17.926638: step 10320, loss = 1.03 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 00:57:19.203004: step 10330, loss = 1.18 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:57:20.468111: step 10340, loss = 0.94 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:57:21.736937: step 10350, loss = 0.92 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:57:23.019147: step 10360, loss = 1.23 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:57:24.318016: step 10370, loss = 0.92 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:57:25.587680: step 10380, loss = 0.95 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:57:26.890201: step 10390, loss = 0.91 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:57:28.297421: step 10400, loss = 0.75 (909.6 examples/sec; 0.141 sec/batch)
2017-05-09 00:57:29.503578: step 10410, loss = 0.77 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-09 00:57:30.781729: step 10420, loss = 0.88 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:57:32.091039: step 10430, loss = 0.88 (977.6 examples/sec; 0.131 sec/batch)
2017-05-09 00:57:33.390130: step 10440, loss = 0.98 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:57:34.658509: step 10450, loss = 1.08 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-09 00:57:35.926335: step 10460, loss = 0.77 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:57:37.213728: step 10470, loss = 1.12 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:57:38.490294: step 10480, loss = 0.84 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:57:39.764516: step 10490, loss = 0.82 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:57:41.193443: step 10500, loss = 0.98 (895.8 examples/sec; 0.143 sec/batch)
2017-05-09 00:57:42.433899: step 10510, loss = 1.02 (1031.9 examples/sec; 0.124 sec/batch)
2017-05-09 00:57:43.752996: step 10520, loss = 0.83 (970.4 examples/sec; 0.132 sec/batch)
2017-05-09 00:57:45.107019: step 10530, loss = 1.00 (945.3 examples/sec; 0.135 sec/batch)
2017-05-09 00:57:46.484069: step 10540, loss = 0.93 (929.5 examples/sec; 0.138 sec/batch)
2017-05-09 00:57:47.797293: step 10550, loss = 0.84 (974.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:57:49.138511: step 10560, loss = 0.78 (954.4 examples/sec; 0.134 sec/batch)
2017-05-09 00:57:50.476263: step 10570, loss = 0.95 (956.8 examples/sec; 0.134 sec/batch)
2017-05-09 00:57:51.788836: step 10580, loss = 0.90 (975.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:57:53.109134: step 10590, loss = 0.90 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 00:57:54.627040: step 10600, loss = 0.86 (843.3 examples/sec; 0.152 sec/batch)
2017-05-09 00:57:55.872511: step 10610, loss = 0.88 (1027.7 examples/sec; 0.125 sec/batch)
2017-05-09 00:57:57.153462: step 10620, loss = 0.91 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:57:58.512213: step 10630, loss = 0.93 (942.0 examples/sec; 0.136 sec/batch)
2017-05-09 00:57:59.841762: step 10640, loss = 0.88 (962.7 examples/sec; 0.133 sec/batch)
2017-05-09 00:58:01.160269: step 10650, loss = 0.80 (970.8 examples/sec; 0.132 sec/batch)
2017-05-09 00:58:02.507172: step 10660, loss = 0.87 (950.3 examples/sec; 0.135 sec/batch)
2017-05-09 00:58:03.782878: step 10670, loss = 1.04 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:05.111219: step 10680, loss = 1.38 (963.6 examples/sec; 0.133 sec/batch)
2017-05-09 00:58:06.364255: step 10690, loss = 0.97 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-09 00:58:07.725019: step 10700, loss = 0.98 (940.6 examples/sec; 0.136 sec/batch)
2017-05-09 00:58:08.922520: step 10710, loss = 0.91 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-09 00:58:10.217525: step 10720, loss = 1.10 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:58:11.533674: step 10730, loss = 1.11 (972.5 examples/sec; 0.132 sec/batch)
2017-05-09 00:58:12.819510: step 10740, loss = 0.90 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:58:14.107831: step 10750, loss = 0.95 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:58:15.419135: step 10760, loss = 1.05 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:58:16.698180: step 10770, loss = 0.70 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:17.964896: step 10780, loss = 0.93 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:58:19.256222: step 10790, loss = 0.78 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:58:20.641424: step 10800, loss = 0.88 (924.1 examples/sec; 0.139 sec/batch)
2017-05-09 00:58:21.817219: step 10810, loss = 1.00 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-09 00:58:23.089738: step 10820, loss = 0.80 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 00:58:24.384175: step 10830, loss = 0.92 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:58:25.665165: step 10840, loss = 0.74 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:26.939189: step 10850, loss = 0.92 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-09 00:58:28.219076: step 10860, loss = 0.86 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:29.538305: step 10870, loss = 1.05 (970.3 examples/sec; 0.132 sec/batch)
2017-05-09 00:58:30.798167: step 10880, loss = 0.79 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-09 00:58:32.078167: step 10890, loss = 0.96 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:33.467540: step 10900, loss = 0.94 (921.3 examples/sec; 0.139 sec/batch)
2017-05-09 00:58:34.635814: step 10910, loss = 0.88 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-09 00:58:35.939504: step 10920, loss = 1.01 (981.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:58:37.254138: step 10930, loss = 0.98 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:58:38.543264: step 10940, loss = 0.99 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:58:39.840653: step 10950, loss = 0.98 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:58:41.140011: step 10960, loss = 0.97 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:58:42.440713: step 10970, loss = 1.21 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:58:43.729293: step 10980, loss = 0.97 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:58:45.038286: step 10990, loss = 1.15 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:58:46.423522: step 11000, loss = 1.10 (924.0 examples/sec; 0.139 sec/batch)
2017-05-09 00:58:47.607973: step 11010, loss = 0.80 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-09 00:58:48.905480: step 11020, loss = 1.11 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:58:50.194590: step 11030, loss = 1.13 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:58:51.515399: step 11040, loss = 1.08 (969.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:58:52.804379: step 11050, loss = 0.85 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:58:54.084772: step 11060, loss = 0.87 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:55.378726: step 11070, loss = 0.84 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:58:56.650093: step 11080, loss = 0.89 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:58:57.930213: step 11090, loss = 0.93 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:59.296108: step 11100, loss = 1.04 (937.1 examples/sec; 0.137 sec/batch)
2017-05-09 00:59:00.510431: step 11110, loss = 1.00 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-09 00:59:01.797917: step 11120, loss = 0.92 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:59:03.101083: step 11130, loss = 0.85 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:04.423719: step 11140, loss = 1.06 (967.8 examples/sec; 0.132 sec/batch)
2017-05-09 00:59:05.700038: step 11150, loss = 0.94 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:59:07.027786: step 11160, loss = 0.93 (964.0 examples/sec; 0.133 sec/batch)
2017-05-09 00:59:08.312902: step 11170, loss = 1.11 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:59:09.617789: step 11180, loss = 0.93 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:10.928516: step 11190, loss = 0.96 (976.6 examples/sec; 0.131 sec/batch)
2017-05-09 00:59:12.311699: step 11200, loss = 1.05 (925.4 examples/sec; 0.138 sec/batch)
2017-05-09 00:59:13.518397: step 11210, loss = 0.90 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-09 00:59:14.818829: step 11220, loss = 0.99 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:16.137358: step 11230, loss = 0.99 (970.8 examples/sec; 0.132 sec/batch)
2017-05-09 00:59:17.436462: step 11240, loss = 0.99 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:18.750270: step 11250, loss = 0.83 (974.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:59:20.036507: step 11260, loss = 0.93 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:59:21.330157: step 11270, loss = 0.94 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:59:22.633410: step 11280, loss = 1.14 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:23.911814: step 11290, loss = 0.98 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:59:25.328638: step 11300, loss = 1.03 (903.4 examples/sec; 0.142 sec/batch)
2017-05-09 00:59:26.508424: step 11310, loss = 0.75 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-09 00:59:27.778813: step 11320, loss = 0.79 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:59:29.056409: step 11330, loss = 0.83 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:59:30.339553: step 11340, loss = 0.71 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:59:31.641981: step 11350, loss = 0.88 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:32.912076: step 11360, loss = 0.94 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:59:34.218034: step 11370, loss = 0.99 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:59:35.514473: step 11380, loss = 0.93 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:36.806207: step 11390, loss = 0.84 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:59:38.175924: step 11400, loss = 1.09 (934.5 examples/sec; 0.137 sec/batch)
2017-05-09 00:59:39.395921: step 11410, loss = 1.16 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-09 00:59:40.680516: step 11420, loss = 1.00 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:59:41.966841: step 11430, loss = 0.96 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:59:43.263081: step 11440, loss = 1.06 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:44.529853: step 11450, loss = 0.97 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:59:45.841181: step 11460, loss = 0.99 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:59:47.150232: step 11470, loss = 0.88 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:59:48.446086: step 11480, loss = 0.75 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:49.734693: step 11490, loss = 0.74 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:59:51.139244: step 11500, loss = 0.79 (911.3 examples/sec; 0.140 sec/batch)
2017-05-09 00:59:52.326347: step 11510, loss = 0.91 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-09 00:59:53.598988: step 11520, loss = 1.00 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:59:54.880044: step 11530, loss = 1.05 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:59:56.204444: step 11540, loss = 0.92 (966.5 examples/sec; 0.132 sec/batch)
2017-05-09 00:59:57.489983: step 11550, loss = 0.98 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:59:58.807661: step 11560, loss = 1.20 (971.4 examples/sec; 0.132 sec/batch)
2017-05-09 01:00:00.099867: step 11570, loss = 0.80 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:00:01.399211: step 11580, loss = 0.87 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:00:02.700080: step 11590, loss = 1.05 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:00:04.084411: step 11600, loss = 0.95 (924.6 examples/sec; 0.138 sec/batch)
2017-05-09 01:00:05.254132: step 11610, loss = 1.11 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-09 01:00:06.512666: step 11620, loss = 1.00 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:00:07.798509: step 11630, loss = 0.89 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:00:09.084035: step 11640, loss = 1.13 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:00:10.358728: step 11650, loss = 0.86 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:00:11.642730: step 11660, loss = 0.95 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:00:12.923992: step 11670, loss = 0.84 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:00:14.220664: step 11680, loss = 0.93 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:00:15.545549: step 11690, loss = 0.97 (966.1 examples/sec; 0.132 sec/batch)
2017-05-09 01:00:16.924691: step 11700, loss = 1.08 (928.1 examples/sec; 0.138 sec/batch)
2017-05-09 01:00:18.215841: step 11710, loss = 1.10 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:00:19.548483: step 11720, loss = 0.92 (960.5 examples/sec; 0.133 sec/batch)
2017-05-09 01:00:20.888771: step 11730, loss = 0.81 (955.0 examples/sec; 0.134 sec/batch)
2017-05-09 01:00:22.212612: step 11740, loss = 0.89 (966.9 examples/sec; 0.132 sec/batch)
2017-05-09 01:00:23.561820: step 11750, loss = 0.96 (948.7 examples/sec; 0.135 sec/batch)
2017-05-09 01:00:24.913545: step 11760, loss = 0.89 (946.9 examples/sec; 0.135 sec/batch)
2017-05-09 01:00:26.262503: step 11770, loss = 0.84 (948.9 examples/sec; 0.135 sec/batch)
2017-05-09 01:00:27.583181: step 11780, loss = 0.79 (969.2 examples/sec; 0.132 sec/batch)
2017-05-09 01:00:28.931892: step 11790, loss = 1.14 (949.1 examples/sec; 0.135 sec/batch)
2017-05-09 01:00:30.310676: step 11800, loss = 0.94 (928.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:00:31.492487: step 11810, loss = 0.95 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-09 01:00:32.820380: step 11820, loss = 0.83 (963.9 examples/sec; 0.133 sec/batch)
2017-05-09 01:00:34.134416: step 11830, loss = 0.82 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 01:00:35.486161: step 11840, loss = 0.91 (946.9 examples/sec; 0.135 sec/batch)
2017-05-09 01:00:36.806386: step 11850, loss = 0.87 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 01:00:38.167007: step 11860, loss = 0.95 (940.7 examples/sec; 0.136 sec/batch)
2017-05-09 01:00:39.506807: step 11870, loss = 0.88 (955.4 examples/sec; 0.134 sec/batch)
2017-05-09 01:00:40.782424: step 11880, loss = 0.88 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:00:42.099727: step 11890, loss = 0.94 (971.7 examples/sec; 0.132 sec/batch)
2017-05-09 01:00:43.508466: step 11900, loss = 1.01 (908.6 examples/sec; 0.141 sec/batch)
2017-05-09 01:00:44.743793: step 11910, loss = 0.82 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-09 01:00:46.070540: step 11920, loss = 0.87 (964.8 examples/sec; 0.133 sec/batch)
2017-05-09 01:00:47.400917: step 11930, loss = 0.92 (962.1 examples/sec; 0.133 sec/batch)
2017-05-09 01:00:48.743530: step 11940, loss = 1.05 (953.4 examples/sec; 0.134 sec/batch)
2017-05-09 01:00:50.119168: step 11950, loss = 1.05 (930.5 examples/sec; 0.138 sec/batch)
2017-05-09 01:00:51.458701: step 11960, loss = 0.76 (955.6 examples/sec; 0.134 sec/batch)
2017-05-09 01:00:52.805904: step 11970, loss = 0.85 (950.1 examples/sec; 0.135 sec/batch)
2017-05-09 01:00:54.153383: step 11980, loss = 0.80 (949.9 examples/sec; 0.135 sec/batch)
2017-05-09 01:00:55.505350: step 11990, loss = 1.05 (946.8 examples/sec; 0.135 sec/batch)
2017-05-09 01:00:56.975460: step 12000, loss = 0.87 (870.7 examples/sec; 0.147 sec/batch)
2017-05-09 01:00:58.195827: step 12010, loss = 0.90 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-09 01:00:59.466983: step 12020, loss = 0.84 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:01:00.748417: step 12030, loss = 0.88 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:02.045129: step 12040, loss = 0.97 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:01:03.320602: step 12050, loss = 0.93 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:04.596466: step 12060, loss = 0.86 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:05.872708: step 12070, loss = 0.80 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:07.169094: step 12080, loss = 0.85 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:01:08.493636: step 12090, loss = 1.17 (966.4 examples/sec; 0.132 sec/batch)
2017-05-09 01:01:09.884984: step 12100, loss = 0.96 (920.0 examples/sec; 0.139 sec/batch)
2017-05-09 01:01:11.093304: step 12110, loss = 1.03 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-09 01:01:12.381177: step 12120, loss = 1.02 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:13.660759: step 12130, loss = 0.81 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:14.938270: step 12140, loss = 0.85 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:16.238671: step 12150, loss = 1.04 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:01:17.529200: step 12160, loss = 0.84 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:18.823947: step 12170, loss = 1.05 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:20.138859: step 12180, loss = 0.98 (973.4 examples/sec; 0.131 sec/batch)
2017-05-09 01:01:21.445786: step 12190, loss = 1.00 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 01:01:22.839143: step 12200, loss = 0.98 (918.6 examples/sec; 0.139 sec/batch)
2017-05-09 01:01:24.039951: step 12210, loss = 0.90 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-09 01:01:25.329847: step 12220, loss = 0.88 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:26.604407: step 12230, loss = 1.24 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:01:27.876297: step 12240, loss = 0.97 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:01:29.195632: step 12250, loss = 0.90 (970.2 examples/sec; 0.132 sec/batch)
2017-05-09 01:01:30.505284: step 12260, loss = 1.13 (977.4 examples/sec; 0.131 sec/batch)
2017-05-09 01:01:31.795644: step 12270, loss = 0.98 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:33.078264: step 12280, loss = 0.95 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:34.346611: step 12290, loss = 0.95 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:01:35.734920: step 12300, loss = 0.99 (922.0 examples/sec; 0.139 sec/batch)
2017-05-09 01:01:36.905582: step 12310, loss = 0.83 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-09 01:01:38.179110: step 12320, loss = 0.97 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:01:39.483027: step 12330, loss = 0.90 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:01:40.798163: step 12340, loss = 1.00 (973.3 examples/sec; 0.132 sec/batch)
2017-05-09 01:01:42.087231: step 12350, loss = 1.13 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:43.376762: step 12360, loss = 0.97 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:44.676703: step 12370, loss = 0.78 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:01:45.974049: step 12380, loss = 0.89 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:01:47.264025: step 12390, loss = 0.93 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:48.634951: step 12400, loss = 0.87 (933.7 examples/sec; 0.137 sec/batch)
2017-05-09 01:01:49.825347: step 12410, loss = 0.80 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-09 01:01:51.116611: step 12420, loss = 1.12 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:52.453999: step 12430, loss = 0.69 (957.1 examples/sec; 0.134 sec/batch)
2017-05-09 01:01:53.744515: step 12440, loss = 1.01 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:55.056362: step 12450, loss = 0.99 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:01:56.340612: step 12460, loss = 0.94 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:57.609293: step 12470, loss = 1.06 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:01:58.890978: step 12480, loss = 0.75 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:02:00.165865: step 12490, loss = 0.77 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:02:01.540864: step 12500, loss = 0.86 (930.9 examples/sec; 0.138 sec/batch)
2017-05-09 01:02:02.733299: step 12510, loss = 1.03 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-09 01:02:04.044816: step 12520, loss = 0.92 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 01:02:05.368946: step 12530, loss = 0.86 (966.7 examples/sec; 0.132 sec/batch)
2017-05-09 01:02:06.654120: step 12540, loss = 0.94 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:02:07.927813: step 12550, loss = 0.87 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:02:09.207913: step 12560, loss = 0.90 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:02:10.526819: step 12570, loss = 0.85 (970.5 examples/sec; 0.132 sec/batch)
2017-05-09 01:02:11.821009: step 12580, loss = 0.83 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:02:13.096664: step 12590, loss = 0.93 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:02:14.479017: step 12600, loss = 0.97 (925.9 examples/sec; 0.138 sec/batch)
2017-05-09 01:02:15.671966: step 12610, loss = 0.97 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-09 01:02:16.979577: step 12620, loss = 0.82 (978.9 examples/sec; 0.131 sec/batch)
2017-05-09 01:02:18.286186: step 12630, loss = 1.10 (979.6 examples/sec; 0.131 sec/batch)
2017-05-09 01:02:19.575983: step 12640, loss = 1.04 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:02:20.865428: step 12650, loss = 1.05 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:02:22.148795: step 12660, loss = 1.17 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:02:23.460499: step 12670, loss = 1.13 (975.8 examples/sec; 0.131 sec/batch)
2017-05-09 01:02:24.777777: step 12680, loss = 0.88 (971.7 examples/sec; 0.132 sec/batch)
2017-05-09 01:02:26.065506: step 12690, loss = 0.94 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:02:27.447339: step 12700, loss = 0.84 (926.3 examples/sec; 0.138 sec/batch)
2017-05-09 01:02:28.659537: step 12710, loss = 0.93 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-09 01:02:29.952087: step 12720, loss = 0.89 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:02:31.249274: step 12730, loss = 0.96 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:02:32.550399: step 12740, loss = 0.91 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:02:33.836393: step 12750, loss = 0.81 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:02:35.137879: step 12760, loss = 0.95 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:02:36.422904: step 12770, loss = 0.80 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:02:37.693773: step 12780, loss = 0.98 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:02:38.979474: step 12790, loss = 1.08 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:02:40.357288: step 12800, loss = 0.80 (929.0 examples/sec; 0.138 sec/batch)
2017-05-09 01:02:41.532246: step 12810, loss = 0.99 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-09 01:02:42.796252: step 12820, loss = 0.79 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:02:44.075976: step 12830, loss = 0.91 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:02:45.393674: step 12840, loss = 1.12 (971.4 examples/sec; 0.132 sec/batch)
2017-05-09 01:02:46.695965: step 12850, loss = 0.98 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:02:47.979952: step 12860, loss = 0.84 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:02:49.256375: step 12870, loss = 0.90 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:02:50.524409: step 12880, loss = 1.01 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:02:51.817865: step 12890, loss = 0.82 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:02:53.210614: step 12900, loss = 0.76 (919.0 examples/sec; 0.139 sec/batch)
2017-05-09 01:02:54.389234: step 12910, loss = 0.79 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-09 01:02:55.709485: step 12920, loss = 1.03 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 01:02:57.004545: step 12930, loss = 0.84 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:02:58.286097: step 12940, loss = 0.75 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:02:59.574056: step 12950, loss = 1.03 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:00.844953: step 12960, loss = 0.81 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:03:02.127388: step 12970, loss = 0.87 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:03.419443: step 12980, loss = 0.97 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:04.702014: step 12990, loss = 1.00 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:06.097663: step 13000, loss = 0.99 (917.1 examples/sec; 0.140 sec/batch)
2017-05-09 01:03:07.276146: step 13010, loss = 1.05 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-09 01:03:08.591437: step 13020, loss = 0.94 (973.2 examples/sec; 0.132 sec/batch)
2017-05-09 01:03:09.878533: step 13030, loss = 0.92 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:11.184830: step 13040, loss = 1.04 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 01:03:12.463573: step 13050, loss = 0.70 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:13.722455: step 13060, loss = 0.76 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:03:15.006693: step 13070, loss = 0.96 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:16.292191: step 13080, loss = 0.91 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:17.559308: step 13090, loss = 0.87 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:03:18.951312: step 13100, loss = 0.82 (919.5 examples/sec; 0.139 sec/batch)
2017-05-09 01:03:20.132281: step 13110, loss = 0.99 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-09 01:03:21.415957: step 13120, loss = 1.23 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:22.703419: step 13130, loss = 1.23 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:24.031668: step 13140, loss = 0.82 (963.7 examples/sec; 0.133 sec/batch)
2017-05-09 01:03:25.319732: step 13150, loss = 0.85 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:26.613017: step 13160, loss = 1.10 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:27.885260: step 13170, loss = 0.95 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:03:29.172196: step 13180, loss = 0.96 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:30.465586: step 13190, loss = 0.88 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:31.839427: step 13200, loss = 0.91 (931.7 examples/sec; 0.137 sec/batch)
2017-05-09 01:03:33.026386: step 13210, loss = 1.02 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-09 01:03:34.314835: step 13220, loss = 0.75 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:35.635042: step 13230, loss = 0.71 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 01:03:36.938842: step 13240, loss = 1.11 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:03:38.204013: step 13250, loss = 0.96 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:03:39.505074: step 13260, loss = 0.83 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:03:40.797583: step 13270, loss = 0.85 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:42.090584: step 13280, loss = 0.90 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:43.403276: step 13290, loss = 0.78 (975.1 examples/sec; 0.131 sec/batch)
2017-05-09 01:03:44.771909: step 13300, loss = 1.13 (935.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:03:45.938814: step 13310, loss = 0.74 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-09 01:03:47.230645: step 13320, loss = 0.99 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:48.497556: step 13330, loss = 0.94 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:03:49.798832: step 13340, loss = 1.04 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:03:51.076105: step 13350, loss = 1.00 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:52.344491: step 13360, loss = 0.95 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:03:53.648624: step 13370, loss = 0.93 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:03:54.925688: step 13380, loss = 0.88 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:56.209889: step 13390, loss = 0.99 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:57.577052: step 13400, loss = 0.86 (936.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:03:58.774800: step 13410, loss = 0.89 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-09 01:04:00.053492: step 13420, loss = 0.80 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:01.331781: step 13430, loss = 0.79 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:02.609844: step 13440, loss = 1.03 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:03.923457: step 13450, loss = 0.99 (974.4 examples/sec; 0.131 sec/batch)
2017-05-09 01:04:05.206128: step 13460, loss = 0.85 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:06.477518: step 13470, loss = 0.81 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:04:07.750003: step 13480, loss = 1.17 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:04:09.025057: step 13490, loss = 0.85 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:10.381550: step 13500, loss = 0.91 (943.6 examples/sec; 0.136 sec/batch)
2017-05-09 01:04:11.573242: step 13510, loss = 0.86 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-09 01:04:12.847464: step 13520, loss = 1.03 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:04:14.142557: step 13530, loss = 0.93 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:04:15.422608: step 13540, loss = 0.92 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:16.724741: step 13550, loss = 0.88 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:04:18.028342: step 13560, loss = 0.90 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:04:19.281937: step 13570, loss = 0.82 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:04:20.599505: step 13580, loss = 0.85 (971.5 examples/sec; 0.132 sec/batch)
2017-05-09 01:04:21.886776: step 13590, loss = 0.85 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:04:23.263120: step 13600, loss = 1.08 (930.0 examples/sec; 0.138 sec/batch)
2017-05-09 01:04:24.468593: step 13610, loss = 1.11 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-09 01:04:25.771690: step 13620, loss = 0.91 (982.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:04:27.073227: step 13630, loss = 0.80 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:04:28.353414: step 13640, loss = 0.95 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:29.652020: step 13650, loss = 0.92 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:04:30.908425: step 13660, loss = 0.81 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:04:32.195909: step 13670, loss = 0.92 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:04:33.500140: step 13680, loss = 0.76 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:04:34.756832: step 13690, loss = 0.77 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:04:36.142633: step 13700, loss = 0.92 (923.7 examples/sec; 0.139 sec/batch)
2017-05-09 01:04:37.341714: step 13710, loss = 0.91 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-09 01:04:38.655389: step 13720, loss = 0.73 (974.4 examples/sec; 0.131 sec/batch)
2017-05-09 01:04:39.933742: step 13730, loss = 0.98 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:41.250037: step 13740, loss = 0.98 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 01:04:42.513087: step 13750, loss = 0.75 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:04:43.799281: step 13760, loss = 0.83 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:04:45.074308: step 13770, loss = 1.07 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:46.337643: step 13780, loss = 0.89 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:04:47.625212: step 13790, loss = 0.65 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:04:48.995766: step 13800, loss = 1.01 (933.9 examples/sec; 0.137 sec/batch)
2017-05-09 01:04:50.160995: step 13810, loss = 0.79 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-09 01:04:51.453606: step 13820, loss = 1.14 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:04:52.746134: step 13830, loss = 0.83 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:04:54.027571: step 13840, loss = 0.87 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:55.300986: step 13850, loss = 1.02 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:04:56.592218: step 13860, loss = 0.89 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:04:57.872811: step 13870, loss = 0.84 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:59.197111: step 13880, loss = 1.03 (966.6 examples/sec; 0.132 sec/batch)
2017-05-09 01:05:00.469756: step 13890, loss = 0.78 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:01.858564: step 13900, loss = 1.01 (921.7 examples/sec; 0.139 sec/batch)
2017-05-09 01:05:03.071234: step 13910, loss = 0.95 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-09 01:05:04.362696: step 13920, loss = 0.93 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:05:05.633968: step 13930, loss = 0.77 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:06.905303: step 13940, loss = 0.81 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:08.188621: step 13950, loss = 0.86 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:05:09.467225: step 13960, loss = 0.84 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:05:10.767965: step 13970, loss = 0.91 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:05:12.099462: step 13980, loss = 0.81 (961.3 examples/sec; 0.133 sec/batch)
2017-05-09 01:05:13.411574: step 13990, loss = 0.93 (975.5 examples/sec; 0.131 sec/batch)
2017-05-09 01:05:14.776525: step 14000, loss = 0.97 (937.8 examples/sec; 0.136 sec/batch)
2017-05-09 01:05:15.959766: step 14010, loss = 0.87 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-09 01:05:17.233384: step 14020, loss = 0.76 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:18.497955: step 14030, loss = 0.91 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:05:19.794873: step 14040, loss = 0.94 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:05:21.099342: step 14050, loss = 0.96 (981.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:05:22.363047: step 14060, loss = 0.83 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:05:23.648670: step 14070, loss = 0.80 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:05:24.933367: step 14080, loss = 0.85 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:05:26.226560: step 14090, loss = 1.03 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:05:27.635070: step 14100, loss = 1.12 (908.8 examples/sec; 0.141 sec/batch)
2017-05-09 01:05:28.827713: step 14110, loss = 0.87 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-09 01:05:30.108975: step 14120, loss = 0.80 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:05:31.390073: step 14130, loss = 0.78 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:05:32.688626: step 14140, loss = 0.86 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:05:33.962821: step 14150, loss = 0.86 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:35.250338: step 14160, loss = 0.84 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:05:36.517538: step 14170, loss = 1.08 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:37.789140: step 14180, loss = 0.84 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:39.070806: step 14190, loss = 0.90 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:05:40.460341: step 14200, loss = 0.82 (921.2 examples/sec; 0.139 sec/batch)
2017-05-09 01:05:41.702263: step 14210, loss = 1.01 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-09 01:05:42.998372: step 14220, loss = 1.00 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:05:44.287204: step 14230, loss = 1.06 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:05:45.582693: step 14240, loss = 0.82 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:05:46.870459: step 14250, loss = 0.71 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:05:48.182162: step 14260, loss = 0.69 (975.8 examples/sec; 0.131 sec/batch)
2017-05-09 01:05:49.487874: step 14270, loss = 1.05 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 01:05:50.776707: step 14280, loss = 0.88 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:05:52.053748: step 14290, loss = 1.05 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:05:53.433538: step 14300, loss = 0.78 (927.7 examples/sec; 0.138 sec/batch)
2017-05-09 01:05:54.626361: step 14310, loss = 0.87 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-09 01:05:55.891884: step 14320, loss = 0.93 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:57.184029: step 14330, loss = 0.97 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:05:58.488784: step 14340, loss = 1.02 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:05:59.768213: step 14350, loss = 0.89 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:01.032514: step 14360, loss = 0.89 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:06:02.288077: step 14370, loss = 0.98 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:06:03.553098: step 14380, loss = 0.93 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:04.836585: step 14390, loss = 0.75 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:06.216133: step 14400, loss = 0.99 (927.8 examples/sec; 0.138 sec/batch)
2017-05-09 01:06:07.398733: step 14410, loss = 0.76 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-09 01:06:08.668917: step 14420, loss = 1.15 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:09.927655: step 14430, loss = 0.90 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:06:11.211941: step 14440, loss = 0.90 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:12.483455: step 14450, loss = 0.95 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:13.800930: step 14460, loss = 0.90 (971.6 examples/sec; 0.132 sec/batch)
2017-05-09 01:06:15.105766: step 14470, loss = 0.94 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:06:16.390564: step 14480, loss = 0.83 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:17.666225: step 14490, loss = 0.80 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:19.058049: step 14500, loss = 0.99 (919.7 examples/sec; 0.139 sec/batch)
2017-05-09 01:06:20.223830: step 14510, loss = 0.77 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-09 01:06:21.504208: step 14520, loss = 0.93 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:22.769640: step 14530, loss = 0.89 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:24.039305: step 14540, loss = 0.83 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:25.321025: step 14550, loss = 0.83 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:26.601388: step 14560, loss = 0.73 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:27.892884: step 14570, loss = 0.84 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:06:29.151732: step 14580, loss = 0.99 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:06:30.436198: step 14590, loss = 0.82 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:31.819405: step 14600, loss = 0.80 (925.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:06:33.038651: step 14610, loss = 1.06 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-09 01:06:34.321538: step 14620, loss = 1.01 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:35.614777: step 14630, loss = 1.01 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:06:36.899624: step 14640, loss = 0.88 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:38.168395: step 14650, loss = 0.86 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:39.474364: step 14660, loss = 0.95 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 01:06:40.768652: step 14670, loss = 1.05 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:06:42.037891: step 14680, loss = 0.99 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:43.319967: step 14690, loss = 0.82 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:44.714877: step 14700, loss = 1.04 (917.6 examples/sec; 0.139 sec/batch)
2017-05-09 01:06:45.913946: step 14710, loss = 0.91 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-09 01:06:47.217741: step 14720, loss = 0.87 (981.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:06:48.522261: step 14730, loss = 0.98 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:06:49.807055: step 14740, loss = 0.91 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:51.098285: step 14750, loss = 1.00 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:06:52.384260: step 14760, loss = 0.86 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:06:53.668743: step 14770, loss = 1.23 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:54.980609: step 14780, loss = 1.08 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:06:56.265294: step 14790, loss = 0.86 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:57.653715: step 14800, loss = 0.81 (921.9 examples/sec; 0.139 sec/batch)
2017-05-09 01:06:58.845188: step 14810, loss = 0.83 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-09 01:07:00.115890: step 14820, loss = 0.75 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:01.377651: step 14830, loss = 0.84 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:07:02.645194: step 14840, loss = 0.94 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:03.917287: step 14850, loss = 1.07 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:05.198453: step 14860, loss = 0.80 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:06.478306: step 14870, loss = 0.80 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:07.773678: step 14880, loss = 0.98 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:07:09.081071: step 14890, loss = 0.82 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 01:07:10.487397: step 14900, loss = 1.03 (910.2 examples/sec; 0.141 sec/batch)
2017-05-09 01:07:11.663523: step 14910, loss = 0.90 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-09 01:07:12.957088: step 14920, loss = 0.98 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:07:14.221898: step 14930, loss = 0.94 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:07:15.500849: step 14940, loss = 0.72 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:16.786940: step 14950, loss = 0.94 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:07:18.088653: step 14960, loss = 0.79 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:07:19.389406: step 14970, loss = 0.82 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:07:20.664146: step 14980, loss = 0.88 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:21.940628: step 14990, loss = 0.82 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:23.332796: step 15000, loss = 0.95 (919.4 examples/sec; 0.139 sec/batch)
2017-05-09 01:07:24.499353: step 15010, loss = 0.76 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-09 01:07:25.777168: step 15020, loss = 0.89 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:27.038275: step 15030, loss = 0.72 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:07:28.310445: step 15040, loss = 0.93 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:29.563393: step 15050, loss = 0.98 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-09 01:07:30.829894: step 15060, loss = 0.77 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:32.097006: step 15070, loss = 0.80 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:33.400862: step 15080, loss = 0.92 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:07:34.679125: step 15090, loss = 0.85 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:36.057829: step 15100, loss = 0.90 (928.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:07:37.213812: step 15110, loss = 0.90 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-09 01:07:38.492858: step 15120, loss = 0.94 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:39.788922: step 15130, loss = 0.72 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:07:41.061793: step 15140, loss = 1.06 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:42.328101: step 15150, loss = 0.77 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:43.598681: step 15160, loss = 0.84 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:44.889528: step 15170, loss = 0.88 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:07:46.158456: step 15180, loss = 0.89 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:47.423924: step 15190, loss = 0.80 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:48.812769: step 15200, loss = 0.87 (921.6 examples/sec; 0.139 sec/batch)
2017-05-09 01:07:49.957505: step 15210, loss = 0.95 (1118.2 examples/sec; 0.114 sec/batch)
2017-05-09 01:07:51.226473: step 15220, loss = 0.68 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:52.509228: step 15230, loss = 1.17 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:53.769092: step 15240, loss = 0.87 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:07:55.039934: step 15250, loss = 0.88 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:56.319504: step 15260, loss = 0.86 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:57.602571: step 15270, loss = 0.86 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:58.876368: step 15280, loss = 0.72 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:08:00.141460: step 15290, loss = 1.06 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:08:01.510237: step 15300, loss = 1.01 (935.1 examples/sec; 0.137 sec/batch)
2017-05-09 01:08:02.689461: step 15310, loss = 1.00 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-09 01:08:03.978703: step 15320, loss = 0.95 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:08:05.260649: step 15330, loss = 1.00 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:06.518946: step 15340, loss = 0.78 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:08:07.788057: step 15350, loss = 0.87 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:08:09.055354: step 15360, loss = 0.91 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:08:10.304590: step 15370, loss = 0.81 (1024.6 examples/sec; 0.125 sec/batch)
2017-05-09 01:08:11.584091: step 15380, loss = 0.86 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:12.846972: step 15390, loss = 0.82 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:08:14.197993: step 15400, loss = 0.91 (947.4 examples/sec; 0.135 sec/batch)
2017-05-09 01:08:15.400549: step 15410, loss = 0.88 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-09 01:08:16.687265: step 15420, loss = 1.03 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:08:17.979028: step 15430, loss = 0.98 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:08:19.274655: step 15440, loss = 0.92 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:08:20.549145: step 15450, loss = 1.14 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:08:21.818087: step 15460, loss = 0.98 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:08:23.104019: step 15470, loss = 0.96 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:08:24.400723: step 15480, loss = 0.99 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:08:25.698933: step 15490, loss = 1.00 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:08:27.100701: step 15500, loss = 1.17 (913.1 examples/sec; 0.140 sec/batch)
2017-05-09 01:08:28.289584: step 15510, loss = 0.96 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-09 01:08:29.589033: step 15520, loss = 0.90 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:08:30.868053: step 15530, loss = 0.83 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:32.145419: step 15540, loss = 0.88 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:33.422477: step 15550, loss = 0.92 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:34.705035: step 15560, loss = 0.91 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:36.007431: step 15570, loss = 0.75 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:08:37.336950: step 15580, loss = 0.92 (962.8 examples/sec; 0.133 sec/batch)
2017-05-09 01:08:38.621071: step 15590, loss = 1.07 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:39.995851: step 15600, loss = 0.87 (931.1 examples/sec; 0.137 sec/batch)
2017-05-09 01:08:41.200203: step 15610, loss = 0.71 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-09 01:08:42.492058: step 15620, loss = 0.92 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:08:43.778343: step 15630, loss = 1.10 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:08:45.057788: step 15640, loss = 1.07 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:46.321043: step 15650, loss = 0.89 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:08:47.601987: step 15660, loss = 0.99 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:48.879855: step 15670, loss = 0.85 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:50.139897: step 15680, loss = 0.83 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:08:51.388297: step 15690, loss = 0.85 (1025.3 examples/sec; 0.125 sec/batch)
2017-05-09 01:08:52.771995: step 15700, loss = 0.98 (925.1 examples/sec; 0.138 sec/batch)
2017-05-09 01:08:53.956093: step 15710, loss = 0.76 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-09 01:08:55.221770: step 15720, loss = 0.71 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:08:56.516783: step 15730, loss = 0.84 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:08:57.810375: step 15740, loss = 0.93 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:08:59.082588: step 15750, loss = 0.86 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:09:00.387710: step 15760, loss = 1.07 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 01:09:01.673035: step 15770, loss = 1.05 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:09:02.975498: step 15780, loss = 1.00 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:09:04.256520: step 15790, loss = 0.94 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:05.639782: step 15800, loss = 1.04 (925.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:09:06.847191: step 15810, loss = 0.87 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-09 01:09:08.149044: step 15820, loss = 0.89 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:09:09.433735: step 15830, loss = 1.09 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:10.690578: step 15840, loss = 0.78 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:09:11.962037: step 15850, loss = 0.71 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:09:13.253750: step 15860, loss = 1.08 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:09:14.556550: step 15870, loss = 0.90 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:09:15.847993: step 15880, loss = 0.80 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:09:17.136727: step 15890, loss = 0.98 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:09:18.502076: step 15900, loss = 0.92 (937.5 examples/sec; 0.137 sec/batch)
2017-05-09 01:09:19.696385: step 15910, loss = 0.90 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-09 01:09:21.016596: step 15920, loss = 0.89 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 01:09:22.295466: step 15930, loss = 0.78 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:23.568365: step 15940, loss = 0.96 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:09:24.843794: step 15950, loss = 0.77 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:26.110183: step 15960, loss = 0.69 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:09:27.370485: step 15970, loss = 1.04 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:09:28.648834: step 15980, loss = 1.00 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:29.919675: step 15990, loss = 0.84 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:09:31.292410: step 16000, loss = 0.88 (932.4 examples/sec; 0.137 sec/batch)
2017-05-09 01:09:32.470636: step 16010, loss = 0.88 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-09 01:09:33.739039: step 16020, loss = 0.78 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:09:35.028153: step 16030, loss = 0.83 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:09:36.296602: step 16040, loss = 0.77 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:09:37.555929: step 16050, loss = 0.74 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:09:38.838449: step 16060, loss = 0.83 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:40.090902: step 16070, loss = 0.86 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:09:41.353387: step 16080, loss = 0.77 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:09:42.621636: step 16090, loss = 0.94 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:09:43.986736: step 16100, loss = 1.11 (937.7 examples/sec; 0.137 sec/batch)
2017-05-09 01:09:45.150921: step 16110, loss = 0.93 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-09 01:09:46.399212: step 16120, loss = 0.82 (1025.4 examples/sec; 0.125 sec/batch)
2017-05-09 01:09:47.662768: step 16130, loss = 0.80 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:09:48.943990: step 16140, loss = 0.95 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:50.224682: step 16150, loss = 0.85 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:51.492112: step 16160, loss = 0.88 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:09:52.772198: step 16170, loss = 0.95 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:54.020493: step 16180, loss = 0.74 (1025.5 examples/sec; 0.125 sec/batch)
2017-05-09 01:09:55.298134: step 16190, loss = 0.89 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:56.676777: step 16200, loss = 0.75 (928.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:09:57.840837: step 16210, loss = 0.83 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-09 01:09:59.158873: step 16220, loss = 0.85 (971.1 examples/sec; 0.132 sec/batch)
2017-05-09 01:10:00.507265: step 16230, loss = 0.83 (949.3 examples/sec; 0.135 sec/batch)
2017-05-09 01:10:01.789757: step 16240, loss = 0.90 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:10:03.152228: step 16250, loss = 0.84 (939.5 examples/sec; 0.136 sec/batch)
2017-05-09 01:10:04.560348: step 16260, loss = 1.18 (909.0 examples/sec; 0.141 sec/batch)
2017-05-09 01:10:05.939424: step 16270, loss = 0.89 (928.2 examples/sec; 0.138 sec/batch)
2017-05-09 01:10:07.348298: step 16280, loss = 0.94 (908.5 examples/sec; 0.141 sec/batch)
2017-05-09 01:10:08.620612: step 16290, loss = 0.98 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:10:10.001516: step 16300, loss = 0.91 (926.9 examples/sec; 0.138 sec/batch)
2017-05-09 01:10:11.213030: step 16310, loss = 0.93 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-09 01:10:12.516874: step 16320, loss = 0.79 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:10:13.801027: step 16330, loss = 0.89 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:10:15.089467: step 16340, loss = 0.94 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:10:16.380183: step 16350, loss = 0.98 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:10:17.680379: step 16360, loss = 0.98 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:10:18.946200: step 16370, loss = 0.97 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:10:20.295058: step 16380, loss = 0.82 (948.9 examples/sec; 0.135 sec/batch)
2017-05-09 01:10:21.587446: step 16390, loss = 0.88 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:10:22.949231: step 16400, loss = 1.01 (939.9 examples/sec; 0.136 sec/batch)
2017-05-09 01:10:24.232192: step 16410, loss = 0.81 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:10:25.614769: step 16420, loss = 0.96 (925.8 examples/sec; 0.138 sec/batch)
2017-05-09 01:10:27.009708: step 16430, loss = 0.90 (917.6 examples/sec; 0.139 sec/batch)
2017-05-09 01:10:28.380771: step 16440, loss = 0.71 (933.6 examples/sec; 0.137 sec/batch)
2017-05-09 01:10:29.785891: step 16450, loss = 0.95 (911.0 examples/sec; 0.141 sec/batch)
2017-05-09 01:10:31.156082: step 16460, loss = 0.89 (934.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:10:32.530361: step 16470, loss = 0.84 (931.4 examples/sec; 0.137 sec/batch)
2017-05-09 01:10:33.932586: step 16480, loss = 0.97 (912.8 examples/sec; 0.140 sec/batch)
2017-05-09 01:10:35.326275: step 16490, loss = 0.96 (918.4 examples/sec; 0.139 sec/batch)
2017-05-09 01:10:36.788280: step 16500, loss = 0.68 (875.5 examples/sec; 0.146 sec/batch)
2017-05-09 01:10:38.077298: step 16510, loss = 1.04 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:10:39.471030: step 16520, loss = 0.95 (918.4 examples/sec; 0.139 sec/batch)
2017-05-09 01:10:40.883498: step 16530, loss = 0.94 (906.2 examples/sec; 0.141 sec/batch)
2017-05-09 01:10:42.323172: step 16540, loss = 0.85 (889.1 examples/sec; 0.144 sec/batch)
2017-05-09 01:10:43.630425: step 16550, loss = 1.07 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 01:10:44.923059: step 16560, loss = 0.80 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:10:46.195560: step 16570, loss = 0.78 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:10:47.488817: step 16580, loss = 0.83 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:10:48.796730: step 16590, loss = 0.91 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:10:50.163987: step 16600, loss = 1.11 (936.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:10:51.370023: step 16610, loss = 0.78 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-09 01:10:52.665564: step 16620, loss = 0.92 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:10:53.951424: step 16630, loss = 0.89 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:10:55.230850: step 16640, loss = 0.88 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:10:56.492070: step 16650, loss = 0.83 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:10:57.753180: step 16660, loss = 0.70 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:10:59.029929: step 16670, loss = 0.89 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:11:00.293456: step 16680, loss = 0.80 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:01.574551: step 16690, loss = 1.08 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:11:02.952388: step 16700, loss = 0.91 (929.0 examples/sec; 0.138 sec/batch)
2017-05-09 01:11:04.142861: step 16710, loss = 0.68 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-09 01:11:05.414268: step 16720, loss = 0.93 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:06.675226: step 16730, loss = 0.74 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:07.948299: step 16740, loss = 0.92 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:09.215849: step 16750, loss = 1.04 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:10.483113: step 16760, loss = 0.81 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:11.785056: step 16770, loss = 0.92 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:11:13.085556: step 16780, loss = 1.17 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:11:14.341309: step 16790, loss = 0.97 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:15.719205: step 16800, loss = 0.82 (929.0 examples/sec; 0.138 sec/batch)
2017-05-09 01:11:16.924489: step 16810, loss = 0.93 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-09 01:11:18.192772: step 16820, loss = 0.82 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:19.448656: step 16830, loss = 0.96 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:20.722085: step 16840, loss = 0.82 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:21.991553: step 16850, loss = 0.67 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:23.255612: step 16860, loss = 0.90 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:24.519834: step 16870, loss = 0.97 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:25.814220: step 16880, loss = 0.85 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:11:27.071661: step 16890, loss = 0.93 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:28.426087: step 16900, loss = 0.82 (945.0 examples/sec; 0.135 sec/batch)
2017-05-09 01:11:29.618820: step 16910, loss = 0.77 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-09 01:11:30.884515: step 16920, loss = 0.91 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:32.160518: step 16930, loss = 0.85 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:11:33.428911: step 16940, loss = 1.08 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:34.678682: step 16950, loss = 0.91 (1024.2 examples/sec; 0.125 sec/batch)
2017-05-09 01:11:35.950569: step 16960, loss = 1.00 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:37.239264: step 16970, loss = 0.93 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:11:38.504113: step 16980, loss = 0.82 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:39.783630: step 16990, loss = 0.91 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:11:41.150506: step 17000, loss = 1.08 (936.4 examples/sec; 0.137 sec/batch)
2017-05-09 01:11:42.311458: step 17010, loss = 0.82 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-09 01:11:43.587521: step 17020, loss = 0.98 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:11:44.846544: step 17030, loss = 0.68 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:46.115335: step 17040, loss = 1.06 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:47.426050: step 17050, loss = 0.83 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 01:11:48.734890: step 17060, loss = 0.99 (978.0 examples/sec; 0.131 sec/batch)
2017-05-09 01:11:49.998243: step 17070, loss = 0.86 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:51.247074: step 17080, loss = 0.85 (1025.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:11:52.530956: step 17090, loss = 0.77 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:11:53.886015: step 17100, loss = 0.81 (944.6 examples/sec; 0.136 sec/batch)
2017-05-09 01:11:55.042446: step 17110, loss = 0.94 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-09 01:11:56.319448: step 17120, loss = 1.00 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:11:57.627046: step 17130, loss = 0.85 (978.9 examples/sec; 0.131 sec/batch)
2017-05-09 01:11:58.931495: step 17140, loss = 0.82 (981.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:12:00.224580: step 17150, loss = 0.97 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:01.509759: step 17160, loss = 0.85 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:02.756605: step 17170, loss = 0.90 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-09 01:12:04.063910: step 17180, loss = 0.78 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 01:12:05.348577: step 17190, loss = 1.05 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:12:06.731812: step 17200, loss = 1.06 (925.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:12:07.936588: step 17210, loss = 0.85 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-09 01:12:09.215083: step 17220, loss = 0.93 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:12:10.512025: step 17230, loss = 0.89 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:12:11.799093: step 17240, loss = 0.90 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:13.064043: step 17250, loss = 0.93 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:12:14.342220: step 17260, loss = 1.05 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:12:15.621578: step 17270, loss = 0.88 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:12:16.876617: step 17280, loss = 0.79 (1019.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:12:18.146213: step 17290, loss = 0.78 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:12:19.512544: step 17300, loss = 0.94 (936.8 examples/sec; 0.137 sec/batch)
2017-05-09 01:12:20.667158: step 17310, loss = 0.85 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-09 01:12:21.932499: step 17320, loss = 0.88 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:12:23.223607: step 17330, loss = 0.99 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:24.479970: step 17340, loss = 0.78 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:12:25.742179: step 17350, loss = 0.93 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:12:27.033732: step 17360, loss = 0.74 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:28.318370: step 17370, loss = 0.81 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:12:29.611241: step 17380, loss = 0.83 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:30.890725: step 17390, loss = 0.96 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:12:32.299739: step 17400, loss = 0.90 (908.4 examples/sec; 0.141 sec/batch)
2017-05-09 01:12:33.483547: step 17410, loss = 1.06 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-09 01:12:34.755945: step 17420, loss = 0.90 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:12:36.016681: step 17430, loss = 0.88 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:12:37.321437: step 17440, loss = 0.95 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:12:38.576679: step 17450, loss = 0.83 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:12:39.867408: step 17460, loss = 0.94 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:41.179774: step 17470, loss = 0.86 (975.3 examples/sec; 0.131 sec/batch)
2017-05-09 01:12:42.474631: step 17480, loss = 1.03 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:43.778145: step 17490, loss = 1.12 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:12:45.125026: step 17500, loss = 0.89 (950.3 examples/sec; 0.135 sec/batch)
2017-05-09 01:12:46.302054: step 17510, loss = 1.03 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-09 01:12:47.606608: step 17520, loss = 0.70 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:12:48.906078: step 17530, loss = 0.78 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:12:50.198455: step 17540, loss = 0.86 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:51.458567: step 17550, loss = 0.69 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:12:52.746830: step 17560, loss = 0.81 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:54.011101: step 17570, loss = 0.63 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:12:55.278349: step 17580, loss = 0.70 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:12:56.554817: step 17590, loss = 0.82 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:12:57.910952: step 17600, loss = 0.82 (943.9 examples/sec; 0.136 sec/batch)
2017-05-09 01:12:59.115931: step 17610, loss = 0.81 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-09 01:13:00.415253: step 17620, loss = 0.98 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:13:01.721353: step 17630, loss = 1.22 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 01:13:03.005392: step 17640, loss = 1.16 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:13:04.299254: step 17650, loss = 0.88 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:13:05.578449: step 17660, loss = 1.06 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:13:06.844631: step 17670, loss = 0.86 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:08.147092: step 17680, loss = 0.86 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:13:09.450117: step 17690, loss = 0.85 (982.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:13:10.818861: step 17700, loss = 0.95 (935.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:13:11.993702: step 17710, loss = 0.96 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-09 01:13:13.260309: step 17720, loss = 0.79 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:14.524194: step 17730, loss = 0.96 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:13:15.791276: step 17740, loss = 0.94 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:17.054706: step 17750, loss = 0.92 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:13:18.338875: step 17760, loss = 0.77 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:13:19.614336: step 17770, loss = 0.93 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:13:20.890859: step 17780, loss = 1.07 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:13:22.128009: step 17790, loss = 0.95 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-09 01:13:23.510190: step 17800, loss = 0.73 (926.1 examples/sec; 0.138 sec/batch)
2017-05-09 01:13:24.669168: step 17810, loss = 0.74 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-09 01:13:25.941208: step 17820, loss = 0.86 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:27.205657: step 17830, loss = 0.86 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:13:28.498023: step 17840, loss = 0.83 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:13:29.775041: step 17850, loss = 1.06 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:13:31.060126: step 17860, loss = 0.96 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:13:32.343223: step 17870, loss = 0.95 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:13:33.598421: step 17880, loss = 0.85 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:13:34.857662: step 17890, loss = 0.85 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:13:36.229092: step 17900, loss = 0.81 (933.3 examples/sec; 0.137 sec/batch)
2017-05-09 01:13:37.400298: step 17910, loss = 0.83 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-09 01:13:38.657778: step 17920, loss = 0.85 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:13:39.948172: step 17930, loss = 0.76 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:13:41.247967: step 17940, loss = 1.00 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:13:42.531952: step 17950, loss = 0.83 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:13:43.804990: step 17960, loss = 0.89 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:45.081475: step 17970, loss = 0.80 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:13:46.352900: step 17980, loss = 0.82 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:47.674941: step 17990, loss = 1.06 (968.2 examples/sec; 0.132 sec/batch)
2017-05-09 01:13:49.033373: step 18000, loss = 0.90 (942.3 examples/sec; 0.136 sec/batch)
2017-05-09 01:13:50.247301: step 18010, loss = 1.10 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-09 01:13:51.545603: step 18020, loss = 0.81 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:13:52.818814: step 18030, loss = 0.86 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:54.125049: step 18040, loss = 1.01 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 01:13:55.389735: step 18050, loss = 0.95 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:13:56.660638: step 18060, loss = 0.64 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:57.954028: step 18070, loss = 1.06 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:13:59.251685: step 18080, loss = 1.05 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:14:00.563611: step 18090, loss = 0.66 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:14:01.940095: step 18100, loss = 0.82 (929.9 examples/sec; 0.138 sec/batch)
2017-05-09 01:14:03.130580: step 18110, loss = 0.88 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-09 01:14:04.422457: step 18120, loss = 0.87 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:05.714043: step 18130, loss = 1.01 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:07.006414: step 18140, loss = 0.89 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:08.304779: step 18150, loss = 0.85 (985.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:14:09.581278: step 18160, loss = 0.97 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:10.879272: step 18170, loss = 0.86 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:14:12.165104: step 18180, loss = 1.00 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:13.408369: step 18190, loss = 0.87 (1029.5 examples/sec; 0.124 sec/batch)
2017-05-09 01:14:14.770889: step 18200, loss = 0.86 (939.4 examples/sec; 0.136 sec/batch)
2017-05-09 01:14:15.940410: step 18210, loss = 0.84 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-09 01:14:17.218642: step 18220, loss = 0.78 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:18.465019: step 18230, loss = 0.75 (1027.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:14:19.729672: step 18240, loss = 0.91 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:14:21.028470: step 18250, loss = 1.08 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:14:22.302739: step 18260, loss = 0.96 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:14:23.578242: step 18270, loss = 0.76 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:24.856043: step 18280, loss = 1.06 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:26.144369: step 18290, loss = 0.89 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:27.546436: step 18300, loss = 0.82 (912.9 examples/sec; 0.140 sec/batch)
2017-05-09 01:14:28.717834: step 18310, loss = 0.95 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-09 01:14:30.027063: step 18320, loss = 0.92 (977.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:14:31.324582: step 18330, loss = 0.94 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:14:32.602794: step 18340, loss = 0.80 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:33.885097: step 18350, loss = 0.86 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:35.165392: step 18360, loss = 0.84 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:36.451804: step 18370, loss = 0.81 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:37.713708: step 18380, loss = 0.97 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:14:38.969869: step 18390, loss = 0.94 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:14:40.326776: step 18400, loss = 0.88 (943.3 examples/sec; 0.136 sec/batch)
2017-05-09 01:14:41.530669: step 18410, loss = 0.86 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-09 01:14:42.816481: step 18420, loss = 0.84 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:44.063300: step 18430, loss = 0.72 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-09 01:14:45.342882: step 18440, loss = 0.92 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:46.598374: step 18450, loss = 0.92 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:14:47.899191: step 18460, loss = 1.10 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:14:49.169473: step 18470, loss = 0.93 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:14:50.448570: step 18480, loss = 0.94 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:51.730479: step 18490, loss = 0.79 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:53.107443: step 18500, loss = 1.02 (929.6 examples/sec; 0.138 sec/batch)
2017-05-09 01:14:54.261547: step 18510, loss = 0.69 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-09 01:14:55.528180: step 18520, loss = 0.82 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:14:56.804651: step 18530, loss = 1.01 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:58.054451: step 18540, loss = 0.95 (1024.2 examples/sec; 0.125 sec/batch)
2017-05-09 01:14:59.361310: step 18550, loss = 0.96 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 01:15:00.641552: step 18560, loss = 0.99 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:01.912343: step 18570, loss = 0.81 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:15:03.209018: step 18580, loss = 0.83 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:15:04.503673: step 18590, loss = 0.93 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:15:05.864101: step 18600, loss = 1.04 (940.9 examples/sec; 0.136 sec/batch)
2017-05-09 01:15:07.055385: step 18610, loss = 0.85 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-09 01:15:08.326109: step 18620, loss = 1.14 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:15:09.583969: step 18630, loss = 0.96 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:10.854720: step 18640, loss = 1.07 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:15:12.132554: step 18650, loss = 0.76 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:13.390900: step 18660, loss = 0.90 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:14.640718: step 18670, loss = 0.79 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-09 01:15:15.943631: step 18680, loss = 1.02 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:15:17.264207: step 18690, loss = 0.90 (969.3 examples/sec; 0.132 sec/batch)
2017-05-09 01:15:18.634275: step 18700, loss = 0.80 (934.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:15:19.835979: step 18710, loss = 0.97 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-09 01:15:21.088699: step 18720, loss = 0.82 (1021.8 examples/sec; 0.125 sec/batch)
2017-05-09 01:15:22.358061: step 18730, loss = 1.13 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:15:23.673157: step 18740, loss = 0.61 (973.3 examples/sec; 0.132 sec/batch)
2017-05-09 01:15:24.964442: step 18750, loss = 0.92 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:15:26.254723: step 18760, loss = 0.94 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:15:27.567063: step 18770, loss = 0.81 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 01:15:28.848622: step 18780, loss = 1.08 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:30.110001: step 18790, loss = 0.82 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:31.466103: step 18800, loss = 0.98 (943.9 examples/sec; 0.136 sec/batch)
2017-05-09 01:15:32.635979: step 18810, loss = 0.83 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-09 01:15:33.892542: step 18820, loss = 0.90 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:35.168302: step 18830, loss = 0.93 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:36.445099: step 18840, loss = 0.87 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:37.704400: step 18850, loss = 0.72 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:38.978603: step 18860, loss = 0.81 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:15:40.262982: step 18870, loss = 0.87 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:41.515521: step 18880, loss = 0.74 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-09 01:15:42.779792: step 18890, loss = 0.89 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:44.152464: step 18900, loss = 0.91 (932.5 examples/sec; 0.137 sec/batch)
2017-05-09 01:15:45.335282: step 18910, loss = 0.82 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-09 01:15:46.600087: step 18920, loss = 0.84 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:47.861198: step 18930, loss = 0.82 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:49.138958: step 18940, loss = 0.93 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:50.399713: step 18950, loss = 0.88 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:51.685374: step 18960, loss = 0.89 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:15:52.970018: step 18970, loss = 0.86 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:54.233384: step 18980, loss = 1.04 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:55.494892: step 18990, loss = 0.88 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:56.863375: step 19000, loss = 0.82 (935.3 examples/sec; 0.137 sec/batch)
2017-05-09 01:15:58.037498: step 19010, loss = 1.00 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-09 01:15:59.326785: step 19020, loss = 1.09 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:16:00.615065: step 19030, loss = 0.89 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:16:01.881612: step 19040, loss = 0.97 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:16:03.155960: step 19050, loss = 0.82 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:16:04.410053: step 19060, loss = 0.88 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-09 01:16:05.664124: step 19070, loss = 0.85 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-09 01:16:06.958987: step 19080, loss = 0.90 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:16:08.232242: step 19090, loss = 0.95 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:16:09.593166: step 19100, loss = 0.87 (940.5 examples/sec; 0.136 sec/batch)
2017-05-09 01:16:10.790823: step 19110, loss = 0.93 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-09 01:16:12.090923: step 19120, loss = 0.88 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:16:13.369772: step 19130, loss = 1.00 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:14.630638: step 19140, loss = 0.84 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:16:15.894785: step 19150, loss = 1.00 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:16:17.165545: step 19160, loss = 0.73 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:16:18.439091: step 19170, loss = 0.70 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:16:19.717744: step 19180, loss = 0.93 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:21.022473: step 19190, loss = 1.05 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:16:22.408997: step 19200, loss = 0.86 (923.2 examples/sec; 0.139 sec/batch)
2017-05-09 01:16:23.593213: step 19210, loss = 0.90 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-09 01:16:24.896569: step 19220, loss = 0.89 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:16:26.158399: step 19230, loss = 0.90 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:16:27.418575: step 19240, loss = 0.86 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:16:28.720619: step 19250, loss = 0.84 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:16:29.978435: step 19260, loss = 0.83 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:16:31.250993: step 19270, loss = 1.16 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:16:32.508566: step 19280, loss = 0.83 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:16:33.777096: step 19290, loss = 0.77 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:16:35.142119: step 19300, loss = 0.72 (937.7 examples/sec; 0.137 sec/batch)
2017-05-09 01:16:36.316891: step 19310, loss = 0.88 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-09 01:16:37.592355: step 19320, loss = 0.87 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:38.872160: step 19330, loss = 0.90 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:40.154987: step 19340, loss = 1.04 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:41.435666: step 19350, loss = 1.02 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:42.707549: step 19360, loss = 0.92 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:16:43.984062: step 19370, loss = 0.89 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:45.282418: step 19380, loss = 1.12 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:16:46.547096: step 19390, loss = 1.24 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:16:47.906835: step 19400, loss = 0.97 (941.3 examples/sec; 0.136 sec/batch)
2017-05-09 01:16:49.092849: step 19410, loss = 0.80 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-09 01:16:50.346170: step 19420, loss = 0.83 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-09 01:16:51.602070: step 19430, loss = 0.67 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:16:52.867363: step 19440, loss = 0.99 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:16:54.146969: step 19450, loss = 0.74 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:55.415201: step 19460, loss = 0.75 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:16:56.691379: step 19470, loss = 0.81 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:57.951300: step 19480, loss = 0.75 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:16:59.230975: step 19490, loss = 0.98 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:17:00.572061: step 19500, loss = 0.84 (954.5 examples/sec; 0.134 sec/batch)
2017-05-09 01:17:01.801715: step 19510, loss = 0.78 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-09 01:17:03.086815: step 19520, loss = 0.95 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:17:04.393542: step 19530, loss = 0.68 (979.5 examples/sec; 0.131 sec/batch)
2017-05-09 01:17:05.653163: step 19540, loss = 0.83 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:17:06.910017: step 19550, loss = 1.05 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:17:08.194754: step 19560, loss = 0.82 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:17:09.454974: step 19570, loss = 0.91 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:17:10.746624: step 19580, loss = 0.69 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:17:12.023038: step 19590, loss = 1.07 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:17:13.415086: step 19600, loss = 0.81 (919.5 examples/sec; 0.139 sec/batch)
2017-05-09 01:17:14.598520: step 19610, loss = 1.04 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-09 01:17:15.891068: step 19620, loss = 0.94 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:17:17.186826: step 19630, loss = 0.94 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:17:18.476431: step 19640, loss = 0.94 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:17:19.762106: step 19650, loss = 0.97 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:17:21.055272: step 19660, loss = 0.99 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:17:22.328343: step 19670, loss = 0.83 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:17:23.589183: step 19680, loss = 0.94 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:17:24.852511: step 19690, loss = 0.91 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:17:26.245667: step 19700, loss = 0.75 (918.8 examples/sec; 0.139 sec/batch)
2017-05-09 01:17:27.436957: step 19710, loss = 0.92 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-09 01:17:28.744383: step 19720, loss = 0.92 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 01:17:30.017293: step 19730, loss = 0.85 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:17:31.296220: step 19740, loss = 0.88 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:17:32.538164: step 19750, loss = 0.78 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-09 01:17:33.773606: step 19760, loss = 0.78 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-09 01:17:35.061001: step 19770, loss = 0.85 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:17:36.351419: step 19780, loss = 0.67 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:17:37.608246: step 19790, loss = 0.93 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:17:38.947766: step 19800, loss = 0.85 (955.6 examples/sec; 0.134 sec/batch)
2017-05-09 01:17:40.134373: step 19810, loss = 0.88 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-09 01:17:41.411904: step 19820, loss = 0.97 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:17:42.681595: step 19830, loss = 0.76 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:17:43.956158: step 19840, loss = 0.79 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:17:45.238415: step 19850, loss = 0.78 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:17:46.510455: step 19860, loss = 0.78 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:17:47.765992: step 19870, loss = 0.94 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:17:49.051973: step 19880, loss = 0.87 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:17:50.362057: step 19890, loss = 1.10 (977.0 examples/sec; 0.131 sec/batch)
2017-05-09 01:17:51.729943: step 19900, loss = 0.95 (935.7 examples/sec; 0.137 sec/batch)
2017-05-09 01:17:52.948590: step 19910, loss = 1.12 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-09 01:17:54.221028: step 19920, loss = 1.02 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:17:55.528088: step 19930, loss = 1.09 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 01:17:56.806278: step 19940, loss = 0.95 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:17:58.099002: step 19950, loss = 0.85 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:17:59.344220: step 19960, loss = 1.01 (1027.9 examples/sec; 0.125 sec/batch)
2017-05-09 01:18:00.607064: step 19970, loss = 0.80 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:18:01.873896: step 19980, loss = 1.03 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:18:03.119860: step 19990, loss = 0.85 (1027.3 examples/sec; 0.125 sec/batch)
2017-05-09 01:18:04.502089: step 20000, loss = 0.86 (926.0 examples/sec; 0.138 sec/batch)
2017-05-09 01:18:05.675245: step 20010, loss = 0.86 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-09 01:18:06.940442: step 20020, loss = 0.73 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:18:08.200097: step 20030, loss = 1.04 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:18:09.481048: step 20040, loss = 0.78 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:18:10.766448: step 20050, loss = 0.94 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:18:12.058112: step 20060, loss = 1.15 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:18:13.352802: step 20070, loss = 0.87 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:18:14.627130: step 20080, loss = 0.90 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:18:15.878392: step 20090, loss = 0.86 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:18:17.259947: step 20100, loss = 0.85 (926.5 examples/sec; 0.138 sec/batch)
2017-05-09 01:18:18.432256: step 20110, loss = 0.94 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-09 01:18:19.703465: step 20120, loss = 0.75 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:18:20.970121: step 20130, loss = 1.09 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:18:22.216868: step 20140, loss = 0.85 (1026.7 examples/sec; 0.125 sec/batch)
2017-05-09 01:18:23.497192: step 20150, loss = 0.87 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:18:24.764225: step 20160, loss = 0.86 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:18:26.011995: step 20170, loss = 0.84 (1025.8 examples/sec; 0.125 sec/batch)
2017-05-09 01:18:27.294439: step 20180, loss = 0.78 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:18:28.562445: step 20190, loss = 1.03 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:18:29.925833: step 20200, loss = 0.95 (938.8 examples/sec; 0.136 sec/batch)
2017-05-09 01:18:31.075262: step 20210, loss = 0.81 (1113.6 examples/sec; 0.115 sec/batch)
2017-05-09 01:18:32.359724: step 20220, loss = 0.83 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:18:33.639427: step 20230, loss = 1.04 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:18:34.923483: step 20240, loss = 0.87 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:18:36.183806: step 20250, loss = 0.85 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:18:37.454560: step 20260, loss = 0.92 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:18:38.711651: step 20270, loss = 0.82 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:18:39.985258: step 20280, loss = 0.85 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:18:41.245581: step 20290, loss = 0.77 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:18:42.620550: step 20300, loss = 0.88 (930.9 examples/sec; 0.137 sec/batch)
2017-05-09 01:18:43.803825: step 20310, loss = 1.00 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-09 01:18:45.091163: step 20320, loss = 0.94 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:18:46.341468: step 20330, loss = 0.83 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-09 01:18:47.635138: step 20340, loss = 0.82 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:18:48.881810: step 20350, loss = 0.74 (1026.7 examples/sec; 0.125 sec/batch)
2017-05-09 01:18:50.137235: step 20360, loss = 0.75 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:18:51.395664: step 20370, loss = 0.78 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:18:52.681063: step 20380, loss = 0.80 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:18:53.968361: step 20390, loss = 0.95 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:18:55.325198: step 20400, loss = 0.83 (943.4 examples/sec; 0.136 sec/batch)
2017-05-09 01:18:56.505777: step 20410, loss = 0.76 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-09 01:18:57.785480: step 20420, loss = 0.77 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:18:59.050656: step 20430, loss = 1.09 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:19:00.330732: step 20440, loss = 0.86 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:01.576999: step 20450, loss = 0.77 (1027.1 examples/sec; 0.125 sec/batch)
2017-05-09 01:19:02.850058: step 20460, loss = 0.79 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:19:04.111907: step 20470, loss = 0.75 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:19:05.406535: step 20480, loss = 0.98 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:19:06.680653: step 20490, loss = 0.82 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:19:08.046010: step 20500, loss = 0.87 (937.5 examples/sec; 0.137 sec/batch)
2017-05-09 01:19:09.220644: step 20510, loss = 0.89 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-09 01:19:10.478581: step 20520, loss = 0.75 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:19:11.752420: step 20530, loss = 0.87 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:19:13.001659: step 20540, loss = 0.97 (1024.6 examples/sec; 0.125 sec/batch)
2017-05-09 01:19:14.302036: step 20550, loss = 0.88 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:19:15.604028: step 20560, loss = 0.90 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:19:16.907442: step 20570, loss = 0.82 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:19:18.164302: step 20580, loss = 1.10 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:19:19.419378: step 20590, loss = 0.89 (1019.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:19:20.815644: step 20600, loss = 1.00 (916.7 examples/sec; 0.140 sec/batch)
2017-05-09 01:19:21.988195: step 20610, loss = 0.93 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-09 01:19:23.255146: step 20620, loss = 0.79 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:19:24.525407: step 20630, loss = 1.07 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:19:25.826954: step 20640, loss = 0.82 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:19:27.102459: step 20650, loss = 0.89 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:28.385631: step 20660, loss = 0.72 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:29.671741: step 20670, loss = 0.85 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:19:30.954185: step 20680, loss = 0.89 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:32.248176: step 20690, loss = 0.93 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:19:33.616377: step 20700, loss = 1.06 (935.5 examples/sec; 0.137 sec/batch)
2017-05-09 01:19:34.790864: step 20710, loss = 0.96 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-09 01:19:36.058749: step 20720, loss = 0.80 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:19:37.318581: step 20730, loss = 0.99 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:19:38.584125: step 20740, loss = 0.92 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:19:39.866735: step 20750, loss = 0.89 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:41.159009: step 20760, loss = 0.94 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:19:42.442018: step 20770, loss = 0.93 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:43.699626: step 20780, loss = 1.04 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:19:44.993817: step 20790, loss = 0.94 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:19:46.341854: step 20800, loss = 0.75 (949.5 examples/sec; 0.135 sec/batch)
2017-05-09 01:19:47.516997: step 20810, loss = 0.80 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-09 01:19:48.768260: step 20820, loss = 0.82 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:19:50.022079: step 20830, loss = 0.89 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-09 01:19:51.306928: step 20840, loss = 0.88 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:52.579109: step 20850, loss = 0.80 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:19:53.853434: step 20860, loss = 0.99 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:19:55.133635: step 20870, loss = 0.93 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:56.394598: step 20880, loss = 0.85 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:19:57.641987: step 20890, loss = 0.87 (1026.1 examples/sec; 0.125 sec/batch)
2017-05-09 01:19:59.032705: step 20900, loss = 0.94 (920.4 examples/sec; 0.139 sec/batch)
2017-05-09 01:20:00.203800: step 20910, loss = 0.87 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-09 01:20:01.465645: step 20920, loss = 0.82 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:20:02.730668: step 20930, loss = 1.04 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:03.988613: step 20940, loss = 0.81 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:20:05.238081: step 20950, loss = 0.84 (1024.4 examples/sec; 0.125 sec/batch)
2017-05-09 01:20:06.506733: step 20960, loss = 0.72 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:07.780106: step 20970, loss = 0.84 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:09.057227: step 20980, loss = 0.79 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:20:10.311531: step 20990, loss = 0.96 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-09 01:20:11.675011: step 21000, loss = 1.01 (938.8 examples/sec; 0.136 sec/batch)
2017-05-09 01:20:12.847595: step 21010, loss = 0.71 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-09 01:20:14.097976: step 21020, loss = 0.96 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-09 01:20:15.344086: step 21030, loss = 0.90 (1027.2 examples/sec; 0.125 sec/batch)
2017-05-09 01:20:16.621076: step 21040, loss = 0.74 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:20:17.866232: step 21050, loss = 1.00 (1028.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:20:19.149575: step 21060, loss = 0.81 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:20:20.421968: step 21070, loss = 0.79 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:21.682136: step 21080, loss = 0.85 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:20:22.956160: step 21090, loss = 0.83 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:24.327293: step 21100, loss = 0.94 (933.5 examples/sec; 0.137 sec/batch)
2017-05-09 01:20:25.553862: step 21110, loss = 1.02 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-09 01:20:26.807099: step 21120, loss = 1.13 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-09 01:20:28.106464: step 21130, loss = 0.73 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:20:29.368061: step 21140, loss = 0.95 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:20:30.607595: step 21150, loss = 0.79 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-09 01:20:31.874997: step 21160, loss = 0.97 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:33.139777: step 21170, loss = 0.80 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:20:34.407962: step 21180, loss = 0.81 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:35.657953: step 21190, loss = 0.90 (1024.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:20:37.028551: step 21200, loss = 0.95 (933.9 examples/sec; 0.137 sec/batch)
2017-05-09 01:20:38.187583: step 21210, loss = 0.82 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-09 01:20:39.451584: step 21220, loss = 0.71 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:20:40.706420: step 21230, loss = 0.78 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-09 01:20:41.997500: step 21240, loss = 1.01 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:20:43.283269: step 21250, loss = 1.12 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:20:44.550906: step 21260, loss = 1.00 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:45.804933: step 21270, loss = 1.00 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-09 01:20:47.074991: step 21280, loss = 0.82 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:48.349154: step 21290, loss = 0.90 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:49.712081: step 21300, loss = 0.96 (939.1 examples/sec; 0.136 sec/batch)
2017-05-09 01:20:50.888654: step 21310, loss = 0.75 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-09 01:20:52.153899: step 21320, loss = 0.92 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:53.454364: step 21330, loss = 0.90 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:20:54.713061: step 21340, loss = 0.81 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:20:55.980351: step 21350, loss = 0.77 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:57.230111: step 21360, loss = 1.02 (1024.2 examples/sec; 0.125 sec/batch)
2017-05-09 01:20:58.518453: step 21370, loss = 0.94 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:20:59.809884: step 21380, loss = 0.84 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:21:01.075770: step 21390, loss = 0.97 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:02.429817: step 21400, loss = 0.97 (945.3 examples/sec; 0.135 sec/batch)
2017-05-09 01:21:03.624493: step 21410, loss = 1.08 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-09 01:21:04.935076: step 21420, loss = 0.98 (976.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:21:06.196103: step 21430, loss = 0.90 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:21:07.474568: step 21440, loss = 0.92 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:08.745117: step 21450, loss = 0.80 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:10.008806: step 21460, loss = 0.75 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:21:11.266899: step 21470, loss = 0.99 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:21:12.532269: step 21480, loss = 0.77 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:13.783078: step 21490, loss = 0.85 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-09 01:21:15.160532: step 21500, loss = 1.05 (929.2 examples/sec; 0.138 sec/batch)
2017-05-09 01:21:16.371419: step 21510, loss = 1.34 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-09 01:21:17.636779: step 21520, loss = 0.93 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:18.907698: step 21530, loss = 0.86 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:20.187787: step 21540, loss = 0.94 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:21.441996: step 21550, loss = 0.91 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-09 01:21:22.688804: step 21560, loss = 0.80 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-09 01:21:23.961727: step 21570, loss = 0.80 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:25.232933: step 21580, loss = 0.92 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:26.498862: step 21590, loss = 1.02 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:27.864782: step 21600, loss = 0.94 (937.1 examples/sec; 0.137 sec/batch)
2017-05-09 01:21:29.053133: step 21610, loss = 0.79 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-09 01:21:30.330451: step 21620, loss = 0.78 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:31.589822: step 21630, loss = 0.84 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:21:32.857470: step 21640, loss = 0.70 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:34.112535: step 21650, loss = 0.77 (1019.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:21:35.398929: step 21660, loss = 0.95 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:21:36.670013: step 21670, loss = 0.90 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:37.950163: step 21680, loss = 0.78 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:39.228100: step 21690, loss = 0.77 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:40.587920: step 21700, loss = 0.97 (941.3 examples/sec; 0.136 sec/batch)
2017-05-09 01:21:41.748081: step 21710, loss = 0.90 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-09 01:21:43.045548: step 21720, loss = 0.77 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:21:44.320402: step 21730, loss = 0.93 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:45.603743: step 21740, loss = 1.09 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:46.893227: step 21750, loss = 0.85 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:21:48.154051: step 21760, loss = 1.03 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:21:49.417906: step 21770, loss = 0.83 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:21:50.684799: step 21780, loss = 0.74 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:51.958193: step 21790, loss = 0.86 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:53.323988: step 21800, loss = 1.15 (937.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:21:54.472034: step 21810, loss = 0.90 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-09 01:21:55.736660: step 21820, loss = 0.85 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:21:56.996115: step 21830, loss = 0.79 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:21:58.274771: step 21840, loss = 0.88 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:59.539150: step 21850, loss = 0.79 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:22:00.791833: step 21860, loss = 0.77 (1021.8 examples/sec; 0.125 sec/batch)
2017-05-09 01:22:02.072525: step 21870, loss = 0.92 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:22:03.332822: step 21880, loss = 0.83 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:22:04.610513: step 21890, loss = 0.92 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:22:05.973656: step 21900, loss = 0.95 (939.0 examples/sec; 0.136 sec/batch)
2017-05-09 01:22:07.145717: step 21910, loss = 0.83 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-09 01:22:08.415916: step 21920, loss = 0.75 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:22:09.681237: step 21930, loss = 0.82 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:22:10.941664: step 21940, loss = 0.84 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:22:12.231122: step 21950, loss = 0.92 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:22:13.506072: step 21960, loss = 1.00 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:22:14.782008: step 21970, loss = 0.94 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:22:16.072659: step 21980, loss = 1.05 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:22:17.317747: step 21990, loss = 0.74 (1028.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:22:18.683106: step 22000, loss = 0.76 (937.5 examples/sec; 0.137 sec/batch)
2017-05-09 01:22:19.827610: step 22010, loss = 0.84 (1118.4 examples/sec; 0.114 sec/batch)
2017-05-09 01:22:21.100010: step 22020, loss = 0.86 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:22:22.369701: step 22030, loss = 0.97 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:22:23.651074: step 22040, loss = 1.17 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:22:24.924067: step 22050, loss = 0.83 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:22:26.218995: step 22060, loss = 1.06 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:22:27.497037: step 22070, loss = 1.14 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:22:28.746674: step 22080, loss = 0.75 (1024.3 examples/sec; 0.125 sec/batch)
2017-05-09 01:22:30.054081: step 22090, loss = 0.81 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 01:22:31.431651: step 22100, loss = 1.01 (929.2 examples/sec; 0.138 sec/batch)
2017-05-09 01:22:32.639951: step 22110, loss = 0.87 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-09 01:22:33.899774: step 22120, loss = 0.99 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:22:35.163424: step 22130, loss = 0.82 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:22:36.478894: step 22140, loss = 0.93 (973.0 examples/sec; 0.132 sec/batch)
2017-05-09 01:22:37.738568: step 22150, loss = 0.85 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:22:39.005484: step 22160, loss = 0.84 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:22:40.268255: step 22170, loss = 0.81 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:22:41.532539: step 22180, loss = 0.81 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:22:42.795552: step 22190, loss = 0.78 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:22:44.165193: step 22200, loss = 0.77 (934.6 examples/sec; 0.137 sec/batch)
2017-05-09 01:22:45.339710: step 22210, loss = 0.95 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-09 01:22:46.606000: step 22220, loss = 0.94 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:22:47.868870: step 22230, loss = 0.80 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:22:49.143657: step 22240, loss = 0.74 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:22:50.458114: step 22250, loss = 0.90 (973.8 examples/sec; 0.131 sec/batch)
2017-05-09 01:22:51.725720: step 22260, loss = 0.82 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:22:52.996700: step 22270, loss = 0.86 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:22:54.249186: step 22280, loss = 0.93 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:22:55.537259: step 22290, loss = 0.79 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:22:56.904485: step 22300, loss = 0.81 (936.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:22:58.072410: step 22310, loss = 0.88 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-09 01:22:59.368943: step 22320, loss = 1.24 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:23:00.651655: step 22330, loss = 0.88 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:23:01.937237: step 22340, loss = 0.80 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:23:03.231702: step 22350, loss = 0.84 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:23:04.510919: step 22360, loss = 0.95 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:23:05.774795: step 22370, loss = 0.89 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:07.048471: step 22380, loss = 1.02 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:23:08.316410: step 22390, loss = 0.78 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:23:09.657461: step 22400, loss = 0.95 (954.5 examples/sec; 0.134 sec/batch)
2017-05-09 01:23:10.819354: step 22410, loss = 0.89 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-09 01:23:12.092226: step 22420, loss = 1.06 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:23:13.373037: step 22430, loss = 0.80 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:23:14.636740: step 22440, loss = 0.90 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:15.894850: step 22450, loss = 0.84 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:17.165911: step 22460, loss = 0.81 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:23:18.425860: step 22470, loss = 0.86 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:19.683173: step 22480, loss = 0.85 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:20.938704: step 22490, loss = 0.70 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:22.289098: step 22500, loss = 0.85 (947.9 examples/sec; 0.135 sec/batch)
2017-05-09 01:23:23.457211: step 22510, loss = 0.79 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-09 01:23:24.733428: step 22520, loss = 0.91 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:23:25.986092: step 22530, loss = 0.91 (1021.8 examples/sec; 0.125 sec/batch)
2017-05-09 01:23:27.271487: step 22540, loss = 1.09 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:23:28.554273: step 22550, loss = 0.85 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:23:29.823417: step 22560, loss = 0.94 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:23:31.075811: step 22570, loss = 0.82 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:23:32.346789: step 22580, loss = 0.89 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:23:33.632260: step 22590, loss = 0.67 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:23:35.018070: step 22600, loss = 0.91 (923.6 examples/sec; 0.139 sec/batch)
2017-05-09 01:23:36.228623: step 22610, loss = 0.92 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-09 01:23:37.509021: step 22620, loss = 0.79 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:23:38.800346: step 22630, loss = 0.97 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:23:40.091208: step 22640, loss = 0.90 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:23:41.344528: step 22650, loss = 0.94 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-09 01:23:42.624577: step 22660, loss = 0.87 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:23:43.894615: step 22670, loss = 0.83 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:23:45.170057: step 22680, loss = 0.74 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:23:46.422393: step 22690, loss = 0.78 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-09 01:23:47.778717: step 22700, loss = 0.83 (943.7 examples/sec; 0.136 sec/batch)
2017-05-09 01:23:48.952422: step 22710, loss = 1.07 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-09 01:23:50.224571: step 22720, loss = 0.93 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:23:51.489015: step 22730, loss = 0.73 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:52.783144: step 22740, loss = 0.83 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:23:54.012300: step 22750, loss = 0.87 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-09 01:23:55.283516: step 22760, loss = 0.69 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:23:56.554215: step 22770, loss = 0.76 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:23:57.810523: step 22780, loss = 0.82 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:59.096570: step 22790, loss = 0.77 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:24:00.455612: step 22800, loss = 0.98 (941.8 examples/sec; 0.136 sec/batch)
2017-05-09 01:24:01.637989: step 22810, loss = 0.81 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-09 01:24:02.936463: step 22820, loss = 0.91 (985.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:24:04.197106: step 22830, loss = 0.90 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:24:05.471200: step 22840, loss = 0.83 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:24:06.721534: step 22850, loss = 0.97 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-09 01:24:08.000905: step 22860, loss = 0.94 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:24:09.301134: step 22870, loss = 0.82 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:24:10.574596: step 22880, loss = 0.98 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:24:11.852559: step 22890, loss = 0.84 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:24:13.244547: step 22900, loss = 0.98 (919.6 examples/sec; 0.139 sec/batch)
2017-05-09 01:24:14.424673: step 22910, loss = 0.83 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-09 01:24:15.715996: step 22920, loss = 0.94 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:24:16.994766: step 22930, loss = 0.79 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:24:18.290779: step 22940, loss = 1.04 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:24:19.591255: step 22950, loss = 0.88 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:24:20.881404: step 22960, loss = 1.00 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:24:22.156876: step 22970, loss = 0.94 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:24:23.445785: step 22980, loss = 0.79 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:24:24.722138: step 22990, loss = 0.77 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:24:26.079139: step 23000, loss = 0.92 (943.3 examples/sec; 0.136 sec/batch)
2017-05-09 01:24:27.238920: step 23010, loss = 0.79 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-09 01:24:28.496391: step 23020, loss = 0.99 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:24:29.771379: step 23030, loss = 0.69 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:24:31.056052: step 23040, loss = 0.95 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:24:32.321144: step 23050, loss = 0.72 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:24:33.569123: step 23060, loss = 0.91 (1025.7 examples/sec; 0.125 sec/batch)
2017-05-09 01:24:34.851792: step 23070, loss = 0.80 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:24:36.120767: step 23080, loss = 0.84 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:24:37.390497: step 23090, loss = 0.74 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:24:38.744893: step 23100, loss = 0.87 (945.1 examples/sec; 0.135 sec/batch)
2017-05-09 01:24:39.917541: step 23110, loss = 0.80 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-09 01:24:41.181299: step 23120, loss = 0.80 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:24:42.439499: step 23130, loss = 0.89 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:24:43.718291: step 23140, loss = 0.75 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:24:44.983502: step 23150, loss = 0.90 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:24:46.234286: step 23160, loss = 0.84 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-09 01:24:47.493902: step 23170, loss = 0.94 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:24:48.791833: step 23180, loss = 0.87 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:24:50.058424: step 23190, loss = 0.75 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:24:51.406278: step 23200, loss = 1.00 (949.7 examples/sec; 0.135 sec/batch)
2017-05-09 01:24:52.596076: step 23210, loss = 0.76 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-09 01:24:53.881548: step 23220, loss = 0.93 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:24:55.171488: step 23230, loss = 1.08 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:24:56.470254: step 23240, loss = 0.74 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:24:57.765215: step 23250, loss = 0.90 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:24:59.071153: step 23260, loss = 0.93 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 01:25:00.332454: step 23270, loss = 0.98 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:25:01.590521: step 23280, loss = 0.81 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:25:02.883882: step 23290, loss = 0.75 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:25:04.238981: step 23300, loss = 0.79 (944.6 examples/sec; 0.136 sec/batch)
2017-05-09 01:25:05.412552: step 23310, loss = 0.78 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-09 01:25:06.685351: step 23320, loss = 0.73 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:25:07.960807: step 23330, loss = 1.15 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:25:09.235792: step 23340, loss = 0.74 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:25:10.516588: step 23350, loss = 0.83 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:25:11.780237: step 23360, loss = 0.79 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:25:13.063975: step 23370, loss = 1.10 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:25:14.318962: step 23380, loss = 0.84 (1019.9 examples/sec; 0.125 sec/batch)
2017-05-09 01:25:15.615069: step 23390, loss = 0.95 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:25:17.006671: step 23400, loss = 0.93 (919.8 examples/sec; 0.139 sec/batch)
2017-05-09 01:25:18.193008: step 23410, loss = 1.08 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-09 01:25:19.502879: step 23420, loss = 0.83 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 01:25:20.795097: step 23430, loss = 0.93 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:25:22.082671: step 23440, loss = 0.78 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:25:23.350595: step 23450, loss = 0.71 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:25:24.629406: step 23460, loss = 1.06 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:25:25.865113: step 23470, loss = 1.01 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-09 01:25:27.174102: step 23480, loss = 0.99 (977.9 examples/sec; 0.131 sec/batch)
2017-05-09 01:25:28.474624: step 23490, loss = 0.96 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:25:29.838414: step 23500, loss = 0.69 (938.6 examples/sec; 0.136 sec/batch)
2017-05-09 01:25:30.989520: step 23510, loss = 1.06 (1112.0 examples/sec; 0.115 sec/batch)
2017-05-09 01:25:32.272745: step 23520, loss = 1.11 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:25:33.524784: step 23530, loss = 0.83 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-09 01:25:34.777913: step 23540, loss = 0.75 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-09 01:25:36.072788: step 23550, loss = 0.67 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:25:37.319186: step 23560, loss = 0.93 (1027.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:25:38.574452: step 23570, loss = 0.91 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:25:39.845522: step 23580, loss = 0.81 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:25:41.104280: step 23590, loss = 0.76 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:25:42.468246: step 23600, loss = 0.82 (938.4 examples/sec; 0.136 sec/batch)
2017-05-09 01:25:43.644215: step 23610, loss = 0.85 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-09 01:25:44.910219: step 23620, loss = 0.80 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:25:46.173554: step 23630, loss = 0.79 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:25:47.447478: step 23640, loss = 0.99 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:25:48.704864: step 23650, loss = 0.97 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:25:49.975655: step 23660, loss = 0.80 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:25:51.268292: step 23670, loss = 0.75 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:25:52.528144: step 23680, loss = 0.91 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:25:53.795779: step 23690, loss = 1.13 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:25:55.146577: step 23700, loss = 0.78 (947.6 examples/sec; 0.135 sec/batch)
2017-05-09 01:25:56.307063: step 23710, loss = 0.86 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-09 01:25:57.574672: step 23720, loss = 0.84 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:25:58.841674: step 23730, loss = 0.80 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:00.105541: step 23740, loss = 0.87 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:26:01.390058: step 23750, loss = 0.85 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:26:02.668649: step 23760, loss = 0.81 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:26:03.950638: step 23770, loss = 0.99 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:26:05.204000: step 23780, loss = 0.94 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-09 01:26:06.465263: step 23790, loss = 0.98 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:26:07.829080: step 23800, loss = 0.80 (938.5 examples/sec; 0.136 sec/batch)
2017-05-09 01:26:09.002918: step 23810, loss = 0.93 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-09 01:26:10.261991: step 23820, loss = 0.98 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:26:11.559649: step 23830, loss = 0.92 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:26:12.841812: step 23840, loss = 0.63 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:26:14.116588: step 23850, loss = 0.81 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:15.367049: step 23860, loss = 0.69 (1023.6 examples/sec; 0.125 sec/batch)
2017-05-09 01:26:16.646596: step 23870, loss = 0.84 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:26:17.916084: step 23880, loss = 0.74 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:19.186166: step 23890, loss = 0.94 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:20.546952: step 23900, loss = 0.71 (940.6 examples/sec; 0.136 sec/batch)
2017-05-09 01:26:21.711965: step 23910, loss = 0.90 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-09 01:26:22.970047: step 23920, loss = 0.96 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:26:24.245381: step 23930, loss = 0.80 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:26:25.516169: step 23940, loss = 0.89 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:26.773309: step 23950, loss = 0.76 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:26:28.047465: step 23960, loss = 0.80 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:29.294306: step 23970, loss = 0.70 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-09 01:26:30.593391: step 23980, loss = 0.86 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:26:31.866895: step 23990, loss = 0.79 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:33.250997: step 24000, loss = 0.95 (924.8 examples/sec; 0.138 sec/batch)
2017-05-09 01:26:34.429965: step 24010, loss = 0.82 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-09 01:26:35.702594: step 24020, loss = 0.90 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:37.016945: step 24030, loss = 1.06 (973.9 examples/sec; 0.131 sec/batch)
2017-05-09 01:26:38.297557: step 24040, loss = 0.86 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:26:39.567181: step 24050, loss = 0.82 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:40.830806: step 24060, loss = 1.04 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:26:42.092094: step 24070, loss = 1.00 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:26:43.378879: step 24080, loss = 0.82 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:26:44.664337: step 24090, loss = 0.80 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:26:46.049712: step 24100, loss = 1.19 (923.9 examples/sec; 0.139 sec/batch)
2017-05-09 01:26:47.209259: step 24110, loss = 1.01 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-09 01:26:48.469598: step 24120, loss = 0.85 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:26:49.724626: step 24130, loss = 0.86 (1019.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:26:51.007163: step 24140, loss = 0.99 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:26:52.295222: step 24150, loss = 0.80 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:26:53.581287: step 24160, loss = 0.90 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:26:54.884238: step 24170, loss = 0.82 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:26:56.155730: step 24180, loss = 0.88 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:57.419629: step 24190, loss = 0.82 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:26:58.775769: step 24200, loss = 1.01 (943.9 examples/sec; 0.136 sec/batch)
2017-05-09 01:26:59.954671: step 24210, loss = 0.84 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-09 01:27:01.213601: step 24220, loss = 0.76 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:27:02.458592: step 24230, loss = 0.78 (1028.1 examples/sec; 0.124 sec/batch)
2017-05-09 01:27:03.732068: step 24240, loss = 0.89 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:05.019183: step 24250, loss = 0.96 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:27:06.271747: step 24260, loss = 0.91 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-09 01:27:07.552754: step 24270, loss = 0.55 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:08.829888: step 24280, loss = 0.85 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:10.072908: step 24290, loss = 0.85 (1029.7 examples/sec; 0.124 sec/batch)
2017-05-09 01:27:11.415164: step 24300, loss = 0.94 (953.6 examples/sec; 0.134 sec/batch)
2017-05-09 01:27:12.621313: step 24310, loss = 0.81 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-09 01:27:13.886617: step 24320, loss = 0.93 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:15.154828: step 24330, loss = 0.95 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:16.414250: step 24340, loss = 0.89 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:27:17.677659: step 24350, loss = 0.82 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:27:18.929114: step 24360, loss = 0.75 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-09 01:27:20.198945: step 24370, loss = 0.89 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:21.471874: step 24380, loss = 0.81 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:22.722794: step 24390, loss = 0.86 (1023.2 examples/sec; 0.125 sec/batch)
2017-05-09 01:27:24.081950: step 24400, loss = 0.72 (941.8 examples/sec; 0.136 sec/batch)
2017-05-09 01:27:25.248599: step 24410, loss = 0.79 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-09 01:27:26.510525: step 24420, loss = 0.72 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:27:27.803924: step 24430, loss = 0.86 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:27:29.059729: step 24440, loss = 1.02 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:27:30.367630: step 24450, loss = 1.07 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:27:31.658999: step 24460, loss = 0.84 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:27:32.941206: step 24470, loss = 1.14 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:34.193878: step 24480, loss = 0.84 (1021.8 examples/sec; 0.125 sec/batch)
2017-05-09 01:27:35.466173: step 24490, loss = 0.90 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:36.841874: step 24500, loss = 0.90 (930.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:27:38.007545: step 24510, loss = 0.68 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-09 01:27:39.283800: step 24520, loss = 0.98 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:40.540601: step 24530, loss = 0.79 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:27:41.812830: step 24540, loss = 0.97 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:43.061532: step 24550, loss = 0.82 (1025.1 examples/sec; 0.125 sec/batch)
2017-05-09 01:27:44.347773: step 24560, loss = 0.71 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:27:45.633019: step 24570, loss = 0.98 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:27:46.896453: step 24580, loss = 0.88 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:27:48.166956: step 24590, loss = 0.85 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:49.519015: step 24600, loss = 0.79 (946.7 examples/sec; 0.135 sec/batch)
2017-05-09 01:27:50.680582: step 24610, loss = 0.81 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-09 01:27:51.944395: step 24620, loss = 0.95 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:27:53.214845: step 24630, loss = 0.84 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:54.488792: step 24640, loss = 0.96 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:55.764468: step 24650, loss = 0.83 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:57.036494: step 24660, loss = 0.80 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:58.297794: step 24670, loss = 1.00 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:27:59.564132: step 24680, loss = 0.84 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:28:00.838946: step 24690, loss = 0.81 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:28:02.225373: step 24700, loss = 0.89 (923.2 examples/sec; 0.139 sec/batch)
2017-05-09 01:28:03.395705: step 24710, loss = 0.86 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-09 01:28:04.662228: step 24720, loss = 0.81 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:28:05.918685: step 24730, loss = 0.85 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:07.181266: step 24740, loss = 0.78 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:08.461507: step 24750, loss = 0.81 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:09.734074: step 24760, loss = 0.74 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:28:10.994439: step 24770, loss = 0.82 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:12.269949: step 24780, loss = 0.85 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:13.542601: step 24790, loss = 0.92 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:28:14.924941: step 24800, loss = 0.82 (926.0 examples/sec; 0.138 sec/batch)
2017-05-09 01:28:16.109424: step 24810, loss = 0.85 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-09 01:28:17.389229: step 24820, loss = 0.88 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:18.655501: step 24830, loss = 0.91 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:28:19.963885: step 24840, loss = 0.90 (978.3 examples/sec; 0.131 sec/batch)
2017-05-09 01:28:21.245901: step 24850, loss = 0.72 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:22.510615: step 24860, loss = 0.83 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:23.775587: step 24870, loss = 0.85 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:25.058956: step 24880, loss = 1.01 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:26.319842: step 24890, loss = 0.85 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:27.680180: step 24900, loss = 0.74 (940.9 examples/sec; 0.136 sec/batch)
2017-05-09 01:28:28.862816: step 24910, loss = 1.00 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-09 01:28:30.122487: step 24920, loss = 0.71 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:31.383486: step 24930, loss = 0.79 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:32.662440: step 24940, loss = 0.75 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:33.914806: step 24950, loss = 0.94 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-09 01:28:35.197722: step 24960, loss = 0.97 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:36.458537: step 24970, loss = 0.83 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:37.740094: step 24980, loss = 0.92 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:39.030196: step 24990, loss = 0.94 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:28:40.403919: step 25000, loss = 0.94 (931.8 examples/sec; 0.137 sec/batch)
2017-05-09 01:28:41.573668: step 25010, loss = 0.67 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-09 01:28:42.860086: step 25020, loss = 0.83 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:28:44.161304: step 25030, loss = 1.24 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:28:45.474061: step 25040, loss = 0.82 (975.0 examples/sec; 0.131 sec/batch)
2017-05-09 01:28:46.750802: step 25050, loss = 0.88 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:48.032856: step 25060, loss = 0.87 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:49.293186: step 25070, loss = 0.86 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:50.553261: step 25080, loss = 0.80 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:51.811124: step 25090, loss = 0.89 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:53.171362: step 25100, loss = 0.94 (941.0 examples/sec; 0.136 sec/batch)
2017-05-09 01:28:54.342345: step 25110, loss = 0.74 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-09 01:28:55.626166: step 25120, loss = 0.84 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:56.901490: step 25130, loss = 0.90 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:58.148689: step 25140, loss = 0.78 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-09 01:28:59.409711: step 25150, loss = 0.80 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:29:00.649717: step 25160, loss = 0.83 (1032.3 examples/sec; 0.124 sec/batch)
2017-05-09 01:29:01.922024: step 25170, loss = 0.90 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:29:03.216747: step 25180, loss = 0.89 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:29:04.505668: step 25190, loss = 0.92 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:29:05.859896: step 25200, loss = 0.77 (945.2 examples/sec; 0.135 sec/batch)
2017-05-09 01:29:07.015968: step 25210, loss = 0.84 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-09 01:29:08.303577: step 25220, loss = 0.86 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:29:09.565794: step 25230, loss = 0.85 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:29:10.823352: step 25240, loss = 0.88 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:29:12.088841: step 25250, loss = 0.79 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:29:13.361269: step 25260, loss = 0.91 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:29:14.623376: step 25270, loss = 0.73 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:29:15.883306: step 25280, loss = 0.72 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:29:17.159482: step 25290, loss = 0.96 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:29:18.533615: step 25300, loss = 0.79 (931.5 examples/sec; 0.137 sec/batch)
2017-05-09 01:29:19.706672: step 25310, loss = 1.14 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-09 01:29:21.011267: step 25320, loss = 0.87 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:29:22.255457: step 25330, loss = 0.79 (1028.8 examples/sec; 0.124 sec/batch)
2017-05-09 01:29:23.547547: step 25340, loss = 1.09 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:29:24.836392: step 25350, loss = 0.86 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:29:26.121552: step 25360, loss = 0.80 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:29:27.426962: step 25370, loss = 0.88 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 01:29:28.690133: step 25380, loss = 0.92 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:29:29.954795: step 25390, loss = 0.80 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:29:31.331386: step 25400, loss = 0.90 (929.8 examples/sec; 0.138 sec/batch)
2017-05-09 01:29:32.488560: step 25410, loss = 0.98 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-09 01:29:33.749299: step 25420, loss = 0.80 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:29:35.016826: step 25430, loss = 0.81 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:29:36.286527: step 25440, loss = 0.94 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:29:37.601280: step 25450, loss = 0.66 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 01:29:38.882423: step 25460, loss = 0.84 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:29:40.203238: step 25470, loss = 0.97 (969.1 examples/sec; 0.132 sec/batch)
2017-05-09 01:29:41.477706: step 25480, loss = 0.86 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:29:42.754918: step 25490, loss = 0.66 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:29:44.140936: step 25500, loss = 0.70 (923.5 examples/sec; 0.139 sec/batch)
2017-05-09 01:29:45.319505: step 25510, loss = 0.97 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-09 01:29:46.596210: step 25520, loss = 0.75 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:29:47.874577: step 25530, loss = 1.02 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:29:49.148817: step 25540, loss = 0.81 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:29:50.416742: step 25550, loss = 0.75 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:29:51.673226: step 25560, loss = 0.96 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:29:52.972923: step 25570, loss = 0.72 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:29:54.260948: step 25580, loss = 0.85 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:29:55.554815: step 25590, loss = 0.88 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:29:56.965747: step 25600, loss = 0.96 (907.2 examples/sec; 0.141 sec/batch)
2017-05-09 01:29:58.176884: step 25610, loss = 0.83 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-09 01:29:59.471067: step 25620, loss = 0.89 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:30:00.746208: step 25630, loss = 0.93 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:02.022012: step 25640, loss = 1.15 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:03.312042: step 25650, loss = 0.84 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:30:04.616896: step 25660, loss = 0.88 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:30:05.892030: step 25670, loss = 0.84 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:07.192128: step 25680, loss = 0.79 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:30:08.496252: step 25690, loss = 0.74 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:30:09.850796: step 25700, loss = 0.73 (945.0 examples/sec; 0.135 sec/batch)
2017-05-09 01:30:11.036406: step 25710, loss = 0.89 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-09 01:30:12.329450: step 25720, loss = 0.89 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:30:13.584688: step 25730, loss = 0.78 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:30:14.846123: step 25740, loss = 0.83 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:30:16.114258: step 25750, loss = 0.68 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:30:17.378771: step 25760, loss = 0.91 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:30:18.653168: step 25770, loss = 0.88 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:30:19.919579: step 25780, loss = 0.79 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:30:21.212239: step 25790, loss = 0.97 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:30:22.568261: step 25800, loss = 0.83 (943.9 examples/sec; 0.136 sec/batch)
2017-05-09 01:30:23.755377: step 25810, loss = 0.90 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-09 01:30:25.020685: step 25820, loss = 0.77 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:30:26.298120: step 25830, loss = 0.78 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:27.570627: step 25840, loss = 1.02 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:30:28.860483: step 25850, loss = 0.70 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:30:30.155980: step 25860, loss = 0.95 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:30:31.455948: step 25870, loss = 0.85 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:30:32.755736: step 25880, loss = 0.84 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:30:34.015769: step 25890, loss = 0.93 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:30:35.383725: step 25900, loss = 1.07 (935.7 examples/sec; 0.137 sec/batch)
2017-05-09 01:30:36.528159: step 25910, loss = 0.79 (1118.5 examples/sec; 0.114 sec/batch)
2017-05-09 01:30:37.783092: step 25920, loss = 0.79 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:30:39.048255: step 25930, loss = 0.90 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:30:40.320043: step 25940, loss = 0.83 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:30:41.607439: step 25950, loss = 0.95 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:30:42.883524: step 25960, loss = 1.00 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:44.168148: step 25970, loss = 0.95 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:45.423986: step 25980, loss = 0.74 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:30:46.682470: step 25990, loss = 0.80 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:30:48.033395: step 26000, loss = 0.71 (947.5 examples/sec; 0.135 sec/batch)
2017-05-09 01:30:49.213930: step 26010, loss = 1.01 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-09 01:30:50.476111: step 26020, loss = 0.87 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:30:51.743523: step 26030, loss = 0.89 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:30:53.010907: step 26040, loss = 0.81 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:30:54.265919: step 26050, loss = 0.89 (1019.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:30:55.546448: step 26060, loss = 0.74 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:56.840017: step 26070, loss = 0.83 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:30:58.107275: step 26080, loss = 0.87 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:30:59.361908: step 26090, loss = 0.66 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-09 01:31:00.729381: step 26100, loss = 0.92 (936.0 examples/sec; 0.137 sec/batch)
2017-05-09 01:31:01.899457: step 26110, loss = 0.80 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-09 01:31:03.177768: step 26120, loss = 0.84 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:04.455374: step 26130, loss = 0.94 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:05.725016: step 26140, loss = 0.88 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:06.988292: step 26150, loss = 0.77 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:31:08.255480: step 26160, loss = 0.94 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:09.521792: step 26170, loss = 0.87 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:10.796410: step 26180, loss = 0.77 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:12.071983: step 26190, loss = 0.98 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:13.439181: step 26200, loss = 0.81 (936.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:31:14.599914: step 26210, loss = 0.79 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-09 01:31:15.852782: step 26220, loss = 0.79 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-09 01:31:17.118451: step 26230, loss = 0.77 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:18.389026: step 26240, loss = 0.82 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:19.661260: step 26250, loss = 0.85 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:20.924456: step 26260, loss = 0.87 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:31:22.172062: step 26270, loss = 0.91 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:31:23.421445: step 26280, loss = 0.91 (1024.5 examples/sec; 0.125 sec/batch)
2017-05-09 01:31:24.707509: step 26290, loss = 0.84 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:31:26.075241: step 26300, loss = 0.84 (935.9 examples/sec; 0.137 sec/batch)
2017-05-09 01:31:27.255299: step 26310, loss = 0.84 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-09 01:31:28.507399: step 26320, loss = 0.81 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-09 01:31:29.752618: step 26330, loss = 1.00 (1027.9 examples/sec; 0.125 sec/batch)
2017-05-09 01:31:31.025295: step 26340, loss = 0.81 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:32.321545: step 26350, loss = 0.93 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:31:33.609589: step 26360, loss = 0.93 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:31:34.860734: step 26370, loss = 0.86 (1023.1 examples/sec; 0.125 sec/batch)
2017-05-09 01:31:36.153011: step 26380, loss = 1.12 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:31:37.403138: step 26390, loss = 0.82 (1023.9 examples/sec; 0.125 sec/batch)
2017-05-09 01:31:38.782431: step 26400, loss = 0.88 (928.0 examples/sec; 0.138 sec/batch)
2017-05-09 01:31:39.963207: step 26410, loss = 0.85 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-09 01:31:41.221382: step 26420, loss = 0.80 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:31:42.508260: step 26430, loss = 1.03 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:31:43.775927: step 26440, loss = 0.68 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:45.054810: step 26450, loss = 0.92 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:46.321482: step 26460, loss = 0.75 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:47.588161: step 26470, loss = 0.92 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:48.864921: step 26480, loss = 0.63 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:50.126442: step 26490, loss = 0.93 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:31:51.483170: step 26500, loss = 0.71 (943.4 examples/sec; 0.136 sec/batch)
2017-05-09 01:31:52.660122: step 26510, loss = 0.73 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-09 01:31:53.933998: step 26520, loss = 0.82 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:55.217166: step 26530, loss = 0.86 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:56.490808: step 26540, loss = 0.80 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:57.766676: step 26550, loss = 0.83 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:59.057755: step 26560, loss = 0.87 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:32:00.321923: step 26570, loss = 0.91 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:32:01.588906: step 26580, loss = 0.82 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:32:02.847351: step 26590, loss = 0.78 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:32:04.221765: step 26600, loss = 0.94 (931.3 examples/sec; 0.137 sec/batch)
2017-05-09 01:32:05.412037: step 26610, loss = 0.82 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-09 01:32:06.706842: step 26620, loss = 0.91 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:32:08.012258: step 26630, loss = 0.94 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 01:32:09.283989: step 26640, loss = 0.83 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:32:10.575674: step 26650, loss = 0.83 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:32:11.851859: step 26660, loss = 0.89 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:32:13.125785: step 26670, loss = 0.79 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:32:14.423422: step 26680, loss = 0.89 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:32:15.705112: step 26690, loss = 0.80 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:32:17.066491: step 26700, loss = 0.94 (940.2 examples/sec; 0.136 sec/batch)
2017-05-09 01:32:18.239885: step 26710, loss = 0.64 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-09 01:32:19.510687: step 26720, loss = 0.81 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:32:20.779870: step 26730, loss = 0.73 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:32:22.026260: step 26740, loss = 1.03 (1027.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:32:23.317790: step 26750, loss = 0.86 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:32:24.605086: step 26760, loss = 1.06 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:32:25.908422: step 26770, loss = 0.86 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:32:27.225289: step 26780, loss = 0.73 (972.0 examples/sec; 0.132 sec/batch)
2017-05-09 01:32:28.480199: step 26790, loss = 1.05 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:32:29.823960: step 26800, loss = 0.83 (952.5 examples/sec; 0.134 sec/batch)
2017-05-09 01:32:30.989623: step 26810, loss = 0.68 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-09 01:32:32.246049: step 26820, loss = 0.93 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:32:33.499353: step 26830, loss = 0.91 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-09 01:32:34.777060: step 26840, loss = 0.78 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:32:36.047578: step 26850, loss = 0.69 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:32:37.339003: step 26860, loss = 0.81 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:32:38.599595: step 26870, loss = 0.71 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:32:39.854804: step 26880, loss = 0.87 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:32:41.126526: step 26890, loss = 0.85 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:32:42.483748: step 26900, loss = 0.76 (943.1 examples/sec; 0.136 sec/batch)
2017-05-09 01:32:43.650734: step 26910, loss = 1.00 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-09 01:32:44.912422: step 26920, loss = 0.89 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:32:46.163172: step 26930, loss = 0.78 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-09 01:32:47.472155: step 26940, loss = 0.98 (977.9 examples/sec; 0.131 sec/batch)
2017-05-09 01:32:48.751112: step 26950, loss = 0.91 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:32:50.030918: step 26960, loss = 0.86 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:32:51.321182: step 26970, loss = 0.86 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:32:52.601269: step 26980, loss = 1.00 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:32:53.864133: step 26990, loss = 0.81 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:32:55.249525: step 27000, loss = 0.94 (923.9 examples/sec; 0.139 sec/batch)
2017-05-09 01:32:56.441817: step 27010, loss = 0.81 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-09 01:32:57.686848: step 27020, loss = 0.77 (1028.1 examples/sec; 0.125 sec/batch)
2017-05-09 01:32:58.958524: step 27030, loss = 0.72 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:00.248123: step 27040, loss = 0.80 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:33:01.533148: step 27050, loss = 0.95 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:33:02.820179: step 27060, loss = 0.95 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:33:04.112766: step 27070, loss = 0.90 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:33:05.380720: step 27080, loss = 0.80 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:06.651425: step 27090, loss = 0.82 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:08.010430: step 27100, loss = 0.79 (941.9 examples/sec; 0.136 sec/batch)
2017-05-09 01:33:09.175705: step 27110, loss = 0.81 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-09 01:33:10.444300: step 27120, loss = 1.01 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:11.719645: step 27130, loss = 0.69 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:13.000104: step 27140, loss = 0.86 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:14.245899: step 27150, loss = 0.88 (1027.5 examples/sec; 0.125 sec/batch)
2017-05-09 01:33:15.507423: step 27160, loss = 0.73 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:33:16.780918: step 27170, loss = 0.83 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:18.015641: step 27180, loss = 1.00 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-09 01:33:19.301264: step 27190, loss = 0.69 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:33:20.674393: step 27200, loss = 0.86 (932.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:33:21.854873: step 27210, loss = 0.92 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-09 01:33:23.153544: step 27220, loss = 0.89 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:33:24.416852: step 27230, loss = 0.87 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:33:25.675310: step 27240, loss = 0.90 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:33:26.935364: step 27250, loss = 0.82 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:33:28.205436: step 27260, loss = 0.83 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:29.471869: step 27270, loss = 0.89 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:30.728980: step 27280, loss = 0.89 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:33:32.002704: step 27290, loss = 0.95 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:33.392240: step 27300, loss = 0.89 (921.2 examples/sec; 0.139 sec/batch)
2017-05-09 01:33:34.566814: step 27310, loss = 0.83 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-09 01:33:35.838828: step 27320, loss = 0.83 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:37.083425: step 27330, loss = 0.74 (1028.5 examples/sec; 0.124 sec/batch)
2017-05-09 01:33:38.343640: step 27340, loss = 0.79 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:33:39.631218: step 27350, loss = 0.81 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:33:40.896611: step 27360, loss = 0.81 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:42.187482: step 27370, loss = 0.89 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:33:43.456808: step 27380, loss = 0.98 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:44.738388: step 27390, loss = 0.70 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:46.116215: step 27400, loss = 0.96 (929.0 examples/sec; 0.138 sec/batch)
2017-05-09 01:33:47.304054: step 27410, loss = 0.71 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-09 01:33:48.571117: step 27420, loss = 0.90 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:49.849861: step 27430, loss = 0.92 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:51.133899: step 27440, loss = 0.69 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:52.412500: step 27450, loss = 0.73 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:53.660784: step 27460, loss = 0.81 (1025.4 examples/sec; 0.125 sec/batch)
2017-05-09 01:33:54.927135: step 27470, loss = 0.77 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:56.205379: step 27480, loss = 0.93 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:57.483391: step 27490, loss = 0.91 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:58.846827: step 27500, loss = 0.65 (938.8 examples/sec; 0.136 sec/batch)
2017-05-09 01:34:00.021452: step 27510, loss = 0.81 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-09 01:34:01.270724: step 27520, loss = 1.00 (1024.6 examples/sec; 0.125 sec/batch)
2017-05-09 01:34:02.513078: step 27530, loss = 0.89 (1030.3 examples/sec; 0.124 sec/batch)
2017-05-09 01:34:03.801471: step 27540, loss = 1.11 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:34:05.097733: step 27550, loss = 0.80 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:34:06.390212: step 27560, loss = 1.08 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:34:07.639215: step 27570, loss = 0.86 (1024.8 examples/sec; 0.125 sec/batch)
2017-05-09 01:34:08.921595: step 27580, loss = 0.81 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:34:10.179617: step 27590, loss = 0.76 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:34:11.571214: step 27600, loss = 0.74 (919.8 examples/sec; 0.139 sec/batch)
2017-05-09 01:34:12.775112: step 27610, loss = 0.76 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-09 01:34:14.067498: step 27620, loss = 0.75 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:34:15.345114: step 27630, loss = 0.84 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:34:16.640352: step 27640, loss = 0.65 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:34:17.926176: step 27650, loss = 0.81 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:34:19.225450: step 27660, loss = 0.96 (985.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:34:20.493659: step 27670, loss = 0.74 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:34:21.780921: step 27680, loss = 0.75 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:34:23.090429: step 27690, loss = 1.02 (977.5 examples/sec; 0.131 sec/batch)
2017-05-09 01:34:24.472456: step 27700, loss = 0.85 (926.2 examples/sec; 0.138 sec/batch)
2017-05-09 01:34:25.646478: step 27710, loss = 0.76 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-09 01:34:26.911294: step 27720, loss = 0.75 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:34:28.191784: step 27730, loss = 0.70 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:34:29.537375: step 27740, loss = 0.86 (951.2 examples/sec; 0.135 sec/batch)
2017-05-09 01:34:30.832858: step 27750, loss = 0.79 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:34:32.157289: step 27760, loss = 0.80 (966.4 examples/sec; 0.132 sec/batch)
2017-05-09 01:34:33.516493: step 27770, loss = 0.97 (941.7 examples/sec; 0.136 sec/batch)
2017-05-09 01:34:34.906666: step 27780, loss = 0.68 (920.8 examples/sec; 0.139 sec/batch)
2017-05-09 01:34:36.291551: step 27790, loss = 0.80 (924.3 examples/sec; 0.138 sec/batch)
2017-05-09 01:34:37.764721: step 27800, loss = 0.88 (868.9 examples/sec; 0.147 sec/batch)
2017-05-09 01:34:39.047155: step 27810, loss = 0.86 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:34:40.431865: step 27820, loss = 0.86 (924.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:34:41.802063: step 27830, loss = 0.93 (934.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:34:43.201857: step 27840, loss = 0.86 (914.4 examples/sec; 0.140 sec/batch)
2017-05-09 01:34:44.596957: step 27850, loss = 0.87 (917.5 examples/sec; 0.140 sec/batch)
2017-05-09 01:34:45.964067: step 27860, loss = 0.93 (936.3 examples/sec; 0.137 sec/batch)
2017-05-09 01:34:47.363634: step 27870, loss = 0.79 (914.6 examples/sec; 0.140 sec/batch)
2017-05-09 01:34:48.751130: step 27880, loss = 0.66 (922.5 examples/sec; 0.139 sec/batch)
2017-05-09 01:34:50.109570: step 27890, loss = 0.98 (942.3 examples/sec; 0.136 sec/batch)
2017-05-09 01:34:51.587693: step 27900, loss = 0.99 (866.0 examples/sec; 0.148 sec/batch)
2017-05-09 01:34:52.880848: step 27910, loss = 0.85 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:34:54.282060: step 27920, loss = 1.09 (913.5 examples/sec; 0.140 sec/batch)
2017-05-09 01:34:55.631091: step 27930, loss = 0.87 (948.8 examples/sec; 0.135 sec/batch)
2017-05-09 01:34:56.928637: step 27940, loss = 0.79 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:34:58.207998: step 27950, loss = 0.75 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:34:59.529800: step 27960, loss = 0.87 (968.4 examples/sec; 0.132 sec/batch)
2017-05-09 01:35:00.813809: step 27970, loss = 0.88 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:35:02.117397: step 27980, loss = 0.96 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:35:03.373090: step 27990, loss = 0.79 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:35:04.736648: step 28000, loss = 0.89 (938.7 examples/sec; 0.136 sec/batch)
2017-05-09 01:35:05.911174: step 28010, loss = 0.78 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-09 01:35:07.166522: step 28020, loss = 1.07 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:35:08.446914: step 28030, loss = 1.03 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:35:09.705291: step 28040, loss = 0.79 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:35:10.982361: step 28050, loss = 0.88 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:35:12.246679: step 28060, loss = 0.75 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:35:13.499210: step 28070, loss = 0.83 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:35:14.762590: step 28080, loss = 0.83 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:35:16.048097: step 28090, loss = 0.94 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:35:17.441979: step 28100, loss = 0.93 (918.3 examples/sec; 0.139 sec/batch)
2017-05-09 01:35:18.659162: step 28110, loss = 0.82 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-09 01:35:19.998055: step 28120, loss = 0.95 (956.0 examples/sec; 0.134 sec/batch)
2017-05-09 01:35:21.310181: step 28130, loss = 0.74 (975.5 examples/sec; 0.131 sec/batch)
2017-05-09 01:35:22.634285: step 28140, loss = 0.91 (966.7 examples/sec; 0.132 sec/batch)
2017-05-09 01:35:23.946721: step 28150, loss = 0.94 (975.3 examples/sec; 0.131 sec/batch)
2017-05-09 01:35:25.245591: step 28160, loss = 0.76 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:35:26.544890: step 28170, loss = 0.81 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:35:27.878476: step 28180, loss = 0.71 (959.8 examples/sec; 0.133 sec/batch)
2017-05-09 01:35:29.212945: step 28190, loss = 1.12 (959.2 examples/sec; 0.133 sec/batch)
2017-05-09 01:35:30.619732: step 28200, loss = 0.85 (909.9 examples/sec; 0.141 sec/batch)
2017-05-09 01:35:31.828055: step 28210, loss = 0.81 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-09 01:35:33.144821: step 28220, loss = 0.76 (972.1 examples/sec; 0.132 sec/batch)
2017-05-09 01:35:34.459990: step 28230, loss = 0.84 (973.3 examples/sec; 0.132 sec/batch)
2017-05-09 01:35:35.771122: step 28240, loss = 0.99 (976.3 examples/sec; 0.131 sec/batch)
2017-05-09 01:35:37.034358: step 28250, loss = 0.78 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:35:38.319693: step 28260, loss = 0.69 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:35:39.613957: step 28270, loss = 1.13 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:35:40.889093: step 28280, loss = 0.96 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:35:42.189417: step 28290, loss = 0.90 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:35:43.588970: step 28300, loss = 0.82 (914.6 examples/sec; 0.140 sec/batch)
2017-05-09 01:35:44.788665: step 28310, loss = 0.92 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-09 01:35:46.075416: step 28320, loss = 0.84 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:35:47.369879: step 28330, loss = 0.84 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:35:48.643354: step 28340, loss = 0.92 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:35:49.958297: step 28350, loss = 0.75 (973.4 examples/sec; 0.131 sec/batch)
2017-05-09 01:35:51.241195: step 28360, loss = 0.74 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:35:52.574980: step 28370, loss = 0.79 (959.7 examples/sec; 0.133 sec/batch)
2017-05-09 01:35:53.887859: step 28380, loss = 0.80 (975.0 examples/sec; 0.131 sec/batch)
2017-05-09 01:35:55.195310: step 28390, loss = 0.89 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 01:35:56.581288: step 28400, loss = 0.80 (923.5 examples/sec; 0.139 sec/batch)
2017-05-09 01:35:57.726855: step 28410, loss = 1.06 (1117.4 examples/sec; 0.115 sec/batch)
2017-05-09 01:35:58.991614: step 28420, loss = 0.92 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:36:00.264838: step 28430, loss = 0.81 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:36:01.581644: step 28440, loss = 0.81 (972.0 examples/sec; 0.132 sec/batch)
2017-05-09 01:36:02.859957: step 28450, loss = 0.80 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:36:04.137936: step 28460, loss = 0.90 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:36:05.407507: step 28470, loss = 0.77 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:36:06.681382: step 28480, loss = 0.62 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:36:07.942227: step 28490, loss = 0.82 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:36:09.286899: step 28500, loss = 0.71 (951.9 examples/sec; 0.134 sec/batch)
2017-05-09 01:36:10.557621: step 28510, loss = 0.80 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:36:11.997379: step 28520, loss = 0.78 (889.0 examples/sec; 0.144 sec/batch)
2017-05-09 01:36:13.456792: step 28530, loss = 0.75 (877.1 examples/sec; 0.146 sec/batch)
2017-05-09 01:36:14.919043: step 28540, loss = 0.72 (875.4 examples/sec; 0.146 sec/batch)
2017-05-09 01:36:16.401916: step 28550, loss = 1.17 (863.2 examples/sec; 0.148 sec/batch)
2017-05-09 01:36:17.835507: step 28560, loss = 0.77 (892.9 examples/sec; 0.143 sec/batch)
2017-05-09 01:36:19.298947: step 28570, loss = 0.96 (874.6 examples/sec; 0.146 sec/batch)
2017-05-09 01:36:20.760309: step 28580, loss = 0.88 (875.9 examples/sec; 0.146 sec/batch)
2017-05-09 01:36:22.267165: step 28590, loss = 0.72 (849.5 examples/sec; 0.151 sec/batch)
2017-05-09 01:36:23.826565: step 28600, loss = 0.77 (820.8 examples/sec; 0.156 sec/batch)
2017-05-09 01:36:25.242344: step 28610, loss = 0.95 (904.1 examples/sec; 0.142 sec/batch)
2017-05-09 01:36:26.717655: step 28620, loss = 0.91 (867.6 examples/sec; 0.148 sec/batch)
2017-05-09 01:36:28.205657: step 28630, loss = 0.88 (860.2 examples/sec; 0.149 sec/batch)
2017-05-09 01:36:29.668619: step 28640, loss = 0.79 (874.9 examples/sec; 0.146 sec/batch)
2017-05-09 01:36:31.123839: step 28650, loss = 0.84 (879.6 examples/sec; 0.146 sec/batch)
2017-05-09 01:36:32.568113: step 28660, loss = 0.96 (886.3 examples/sec; 0.144 sec/batch)
2017-05-09 01:36:34.015637: step 28670, loss = 0.68 (884.3 examples/sec; 0.145 sec/batch)
2017-05-09 01:36:35.472770: step 28680, loss = 0.78 (878.4 examples/sec; 0.146 sec/batch)
2017-05-09 01:36:36.924868: step 28690, loss = 1.08 (881.5 examples/sec; 0.145 sec/batch)
2017-05-09 01:36:38.477289: step 28700, loss = 0.81 (824.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:36:39.805739: step 28710, loss = 0.85 (963.5 examples/sec; 0.133 sec/batch)
2017-05-09 01:36:41.262219: step 28720, loss = 0.85 (878.8 examples/sec; 0.146 sec/batch)
2017-05-09 01:36:42.690225: step 28730, loss = 0.92 (896.4 examples/sec; 0.143 sec/batch)
2017-05-09 01:36:44.173475: step 28740, loss = 0.91 (863.0 examples/sec; 0.148 sec/batch)
2017-05-09 01:36:45.641671: step 28750, loss = 0.72 (871.8 examples/sec; 0.147 sec/batch)
2017-05-09 01:36:47.090938: step 28760, loss = 1.10 (883.2 examples/sec; 0.145 sec/batch)
2017-05-09 01:36:48.561970: step 28770, loss = 0.87 (870.1 examples/sec; 0.147 sec/batch)
2017-05-09 01:36:50.024714: step 28780, loss = 0.90 (875.1 examples/sec; 0.146 sec/batch)
2017-05-09 01:36:51.543794: step 28790, loss = 0.84 (842.6 examples/sec; 0.152 sec/batch)
2017-05-09 01:36:53.128262: step 28800, loss = 1.01 (807.8 examples/sec; 0.158 sec/batch)
2017-05-09 01:36:54.531071: step 28810, loss = 1.07 (912.4 examples/sec; 0.140 sec/batch)
2017-05-09 01:36:55.986944: step 28820, loss = 0.87 (879.2 examples/sec; 0.146 sec/batch)
2017-05-09 01:36:57.477182: step 28830, loss = 0.77 (858.9 examples/sec; 0.149 sec/batch)
2017-05-09 01:36:58.956231: step 28840, loss = 1.08 (865.4 examples/sec; 0.148 sec/batch)
2017-05-09 01:37:00.443804: step 28850, loss = 0.97 (860.5 examples/sec; 0.149 sec/batch)
2017-05-09 01:37:01.951860: step 28860, loss = 1.02 (848.8 examples/sec; 0.151 sec/batch)
2017-05-09 01:37:03.413218: step 28870, loss = 0.81 (875.9 examples/sec; 0.146 sec/batch)
2017-05-09 01:37:04.922174: step 28880, loss = 0.93 (848.3 examples/sec; 0.151 sec/batch)
2017-05-09 01:37:06.437562: step 28890, loss = 0.83 (844.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:37:08.017829: step 28900, loss = 0.69 (810.0 examples/sec; 0.158 sec/batch)
2017-05-09 01:37:09.386171: step 28910, loss = 0.74 (935.4 examples/sec; 0.137 sec/batch)
2017-05-09 01:37:10.876103: step 28920, loss = 0.76 (859.1 examples/sec; 0.149 sec/batch)
2017-05-09 01:37:12.346973: step 28930, loss = 0.71 (870.2 examples/sec; 0.147 sec/batch)
2017-05-09 01:37:13.819291: step 28940, loss = 0.95 (869.4 examples/sec; 0.147 sec/batch)
2017-05-09 01:37:15.276716: step 28950, loss = 0.90 (878.3 examples/sec; 0.146 sec/batch)
2017-05-09 01:37:16.723824: step 28960, loss = 0.73 (884.5 examples/sec; 0.145 sec/batch)
2017-05-09 01:37:18.199087: step 28970, loss = 0.83 (867.6 examples/sec; 0.148 sec/batch)
2017-05-09 01:37:19.695314: step 28980, loss = 0.73 (855.5 examples/sec; 0.150 sec/batch)
2017-05-09 01:37:21.152048: step 28990, loss = 1.16 (878.7 examples/sec; 0.146 sec/batch)
2017-05-09 01:37:22.713671: step 29000, loss = 0.73 (819.7 examples/sec; 0.156 sec/batch)
2017-05-09 01:37:24.107819: step 29010, loss = 0.79 (918.1 examples/sec; 0.139 sec/batch)
2017-05-09 01:37:25.595800: step 29020, loss = 0.72 (860.2 examples/sec; 0.149 sec/batch)
2017-05-09 01:37:27.050009: step 29030, loss = 0.81 (880.2 examples/sec; 0.145 sec/batch)
2017-05-09 01:37:28.496169: step 29040, loss = 0.70 (885.1 examples/sec; 0.145 sec/batch)
2017-05-09 01:37:29.993208: step 29050, loss = 1.03 (855.0 examples/sec; 0.150 sec/batch)
2017-05-09 01:37:31.504058: step 29060, loss = 0.92 (847.2 examples/sec; 0.151 sec/batch)
2017-05-09 01:37:32.969904: step 29070, loss = 0.81 (873.2 examples/sec; 0.147 sec/batch)
2017-05-09 01:37:34.443438: step 29080, loss = 0.75 (868.7 examples/sec; 0.147 sec/batch)
2017-05-09 01:37:35.935573: step 29090, loss = 0.84 (857.8 examples/sec; 0.149 sec/batch)
2017-05-09 01:37:37.487062: step 29100, loss = 0.83 (825.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:37:38.826745: step 29110, loss = 1.15 (955.4 examples/sec; 0.134 sec/batch)
2017-05-09 01:37:40.270882: step 29120, loss = 0.85 (886.3 examples/sec; 0.144 sec/batch)
2017-05-09 01:37:41.715591: step 29130, loss = 0.86 (886.0 examples/sec; 0.144 sec/batch)
2017-05-09 01:37:43.139697: step 29140, loss = 0.82 (898.8 examples/sec; 0.142 sec/batch)
2017-05-09 01:37:44.595216: step 29150, loss = 0.94 (879.4 examples/sec; 0.146 sec/batch)
2017-05-09 01:37:46.066541: step 29160, loss = 0.95 (870.0 examples/sec; 0.147 sec/batch)
2017-05-09 01:37:47.524802: step 29170, loss = 0.81 (877.8 examples/sec; 0.146 sec/batch)
2017-05-09 01:37:48.972879: step 29180, loss = 0.85 (883.9 examples/sec; 0.145 sec/batch)
2017-05-09 01:37:50.457637: step 29190, loss = 0.83 (862.1 examples/sec; 0.148 sec/batch)
2017-05-09 01:37:52.005195: step 29200, loss = 0.75 (827.1 examples/sec; 0.155 sec/batch)
2017-05-09 01:37:53.367823: step 29210, loss = 0.84 (939.4 examples/sec; 0.136 sec/batch)
2017-05-09 01:37:54.827599: step 29220, loss = 0.70 (876.8 examples/sec; 0.146 sec/batch)
2017-05-09 01:37:56.284223: step 29230, loss = 0.86 (878.7 examples/sec; 0.146 sec/batch)
2017-05-09 01:37:57.763113: step 29240, loss = 0.88 (865.5 examples/sec; 0.148 sec/batch)
2017-05-09 01:37:59.259099: step 29250, loss = 0.77 (855.6 examples/sec; 0.150 sec/batch)
2017-05-09 01:38:00.729082: step 29260, loss = 0.79 (870.8 examples/sec; 0.147 sec/batch)
2017-05-09 01:38:02.204832: step 29270, loss = 0.80 (867.4 examples/sec; 0.148 sec/batch)
2017-05-09 01:38:03.654738: step 29280, loss = 0.99 (882.8 examples/sec; 0.145 sec/batch)
2017-05-09 01:38:05.129141: step 29290, loss = 0.91 (868.2 examples/sec; 0.147 sec/batch)
2017-05-09 01:38:06.668140: step 29300, loss = 0.77 (831.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:38:08.039024: step 29310, loss = 0.90 (933.7 examples/sec; 0.137 sec/batch)
2017-05-09 01:38:09.555958: step 29320, loss = 0.87 (843.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:38:11.046114: step 29330, loss = 0.86 (859.0 examples/sec; 0.149 sec/batch)
2017-05-09 01:38:12.539119: step 29340, loss = 0.81 (857.3 examples/sec; 0.149 sec/batch)
2017-05-09 01:38:14.068864: step 29350, loss = 0.81 (836.7 examples/sec; 0.153 sec/batch)
2017-05-09 01:38:15.553190: step 29360, loss = 0.87 (862.3 examples/sec; 0.148 sec/batch)
2017-05-09 01:38:17.026484: step 29370, loss = 0.96 (868.8 examples/sec; 0.147 sec/batch)
2017-05-09 01:38:18.510038: step 29380, loss = 0.83 (862.8 examples/sec; 0.148 sec/batch)
2017-05-09 01:38:19.995770: step 29390, loss = 0.87 (861.5 examples/sec; 0.149 sec/batch)
2017-05-09 01:38:21.536129: step 29400, loss = 0.73 (831.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:38:22.933714: step 29410, loss = 0.79 (915.9 examples/sec; 0.140 sec/batch)
2017-05-09 01:38:24.396403: step 29420, loss = 0.79 (875.1 examples/sec; 0.146 sec/batch)
2017-05-09 01:38:25.870432: step 29430, loss = 0.84 (868.4 examples/sec; 0.147 sec/batch)
2017-05-09 01:38:27.355984: step 29440, loss = 0.77 (861.6 examples/sec; 0.149 sec/batch)
2017-05-09 01:38:28.840162: step 29450, loss = 0.82 (862.4 examples/sec; 0.148 sec/batch)
2017-05-09 01:38:30.316499: step 29460, loss = 0.85 (867.0 examples/sec; 0.148 sec/batch)
2017-05-09 01:38:31.792937: step 29470, loss = 0.66 (866.9 examples/sec; 0.148 sec/batch)
2017-05-09 01:38:33.263937: step 29480, loss = 0.92 (870.2 examples/sec; 0.147 sec/batch)
2017-05-09 01:38:34.757488: step 29490, loss = 0.81 (857.0 examples/sec; 0.149 sec/batch)
2017-05-09 01:38:36.334440: step 29500, loss = 1.18 (811.7 examples/sec; 0.158 sec/batch)
2017-05-09 01:38:37.713377: step 29510, loss = 0.84 (928.3 examples/sec; 0.138 sec/batch)
2017-05-09 01:38:39.175929: step 29520, loss = 0.85 (875.2 examples/sec; 0.146 sec/batch)
2017-05-09 01:38:40.667463: step 29530, loss = 0.99 (858.2 examples/sec; 0.149 sec/batch)
2017-05-09 01:38:42.130678: step 29540, loss = 0.78 (874.8 examples/sec; 0.146 sec/batch)
2017-05-09 01:38:43.590655: step 29550, loss = 0.83 (876.7 examples/sec; 0.146 sec/batch)
2017-05-09 01:38:45.083993: step 29560, loss = 0.84 (857.1 examples/sec; 0.149 sec/batch)
2017-05-09 01:38:46.567597: step 29570, loss = 0.90 (862.8 examples/sec; 0.148 sec/batch)
2017-05-09 01:38:48.113031: step 29580, loss = 0.87 (828.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:38:49.583178: step 29590, loss = 0.77 (870.7 examples/sec; 0.147 sec/batch)
2017-05-09 01:38:51.157671: step 29600, loss = 0.82 (813.0 examples/sec; 0.157 sec/batch)
2017-05-09 01:38:52.519540: step 29610, loss = 0.79 (939.9 examples/sec; 0.136 sec/batch)
2017-05-09 01:38:53.983919: step 29620, loss = 0.70 (874.1 examples/sec; 0.146 sec/batch)
2017-05-09 01:38:55.454221: step 29630, loss = 0.91 (870.6 examples/sec; 0.147 sec/batch)
2017-05-09 01:38:56.911796: step 29640, loss = 0.83 (878.2 examples/sec; 0.146 sec/batch)
2017-05-09 01:38:58.367586: step 29650, loss = 0.74 (879.3 examples/sec; 0.146 sec/batch)
2017-05-09 01:38:59.848453: step 29660, loss = 0.76 (864.4 examples/sec; 0.148 sec/batch)
2017-05-09 01:39:01.322009: step 29670, loss = 0.78 (868.7 examples/sec; 0.147 sec/batch)
2017-05-09 01:39:02.799990: step 29680, loss = 0.68 (866.0 examples/sec; 0.148 sec/batch)
2017-05-09 01:39:04.248899: step 29690, loss = 0.75 (883.4 examples/sec; 0.145 sec/batch)
2017-05-09 01:39:05.814657: step 29700, loss = 0.85 (817.5 examples/sec; 0.157 sec/batch)
2017-05-09 01:39:07.185015: step 29710, loss = 0.87 (934.1 examples/sec; 0.137 sec/batch)
2017-05-09 01:39:08.703299: step 29720, loss = 0.79 (843.1 examples/sec; 0.152 sec/batch)
2017-05-09 01:39:10.158781: step 29730, loss = 0.81 (879.4 examples/sec; 0.146 sec/batch)
2017-05-09 01:39:11.632076: step 29740, loss = 0.81 (868.8 examples/sec; 0.147 sec/batch)
2017-05-09 01:39:13.098984: step 29750, loss = 0.86 (872.6 examples/sec; 0.147 sec/batch)
2017-05-09 01:39:14.582982: step 29760, loss = 0.83 (862.5 examples/sec; 0.148 sec/batch)
2017-05-09 01:39:16.074192: step 29770, loss = 0.78 (858.4 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:17.563521: step 29780, loss = 0.88 (859.4 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:19.001747: step 29790, loss = 0.96 (890.0 examples/sec; 0.144 sec/batch)
2017-05-09 01:39:20.574058: step 29800, loss = 0.79 (814.1 examples/sec; 0.157 sec/batch)
2017-05-09 01:39:21.974718: step 29810, loss = 0.93 (913.9 examples/sec; 0.140 sec/batch)
2017-05-09 01:39:23.486734: step 29820, loss = 1.06 (846.5 examples/sec; 0.151 sec/batch)
2017-05-09 01:39:25.013829: step 29830, loss = 0.92 (838.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:39:26.518388: step 29840, loss = 0.95 (850.7 examples/sec; 0.150 sec/batch)
2017-05-09 01:39:28.005607: step 29850, loss = 0.93 (860.7 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:29.494909: step 29860, loss = 0.86 (859.5 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:30.979574: step 29870, loss = 0.78 (862.1 examples/sec; 0.148 sec/batch)
2017-05-09 01:39:32.453576: step 29880, loss = 0.90 (868.4 examples/sec; 0.147 sec/batch)
2017-05-09 01:39:33.913950: step 29890, loss = 0.78 (876.5 examples/sec; 0.146 sec/batch)
2017-05-09 01:39:35.482208: step 29900, loss = 0.86 (816.2 examples/sec; 0.157 sec/batch)
2017-05-09 01:39:36.862430: step 29910, loss = 0.88 (927.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:39:38.329240: step 29920, loss = 0.76 (872.6 examples/sec; 0.147 sec/batch)
2017-05-09 01:39:39.820986: step 29930, loss = 0.80 (858.1 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:41.307454: step 29940, loss = 0.81 (861.1 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:42.794245: step 29950, loss = 0.90 (860.9 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:44.302482: step 29960, loss = 0.86 (848.7 examples/sec; 0.151 sec/batch)
2017-05-09 01:39:45.787796: step 29970, loss = 0.99 (861.8 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:47.301809: step 29980, loss = 1.13 (845.4 examples/sec; 0.151 sec/batch)
2017-05-09 01:39:48.790354: step 29990, loss = 0.97 (859.9 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:50.419845: step 30000, loss = 0.65 (785.5 examples/sec; 0.163 sec/batch)
2017-05-09 01:39:51.819174: step 30010, loss = 0.76 (914.7 examples/sec; 0.140 sec/batch)
2017-05-09 01:39:53.286990: step 30020, loss = 0.94 (872.0 examples/sec; 0.147 sec/batch)
2017-05-09 01:39:54.813901: step 30030, loss = 0.80 (838.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:39:56.277263: step 30040, loss = 0.80 (874.7 examples/sec; 0.146 sec/batch)
2017-05-09 01:39:57.755111: step 30050, loss = 0.77 (866.1 examples/sec; 0.148 sec/batch)
2017-05-09 01:39:59.247569: step 30060, loss = 0.78 (857.6 examples/sec; 0.149 sec/batch)
2017-05-09 01:40:00.729848: step 30070, loss = 0.79 (863.5 examples/sec; 0.148 sec/batch)
2017-05-09 01:40:02.202633: step 30080, loss = 0.80 (869.1 examples/sec; 0.147 sec/batch)
2017-05-09 01:40:03.691805: step 30090, loss = 0.86 (859.5 examples/sec; 0.149 sec/batch)
2017-05-09 01:40:05.253463: step 30100, loss = 0.87 (819.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:40:06.630878: step 30110, loss = 0.96 (929.3 examples/sec; 0.138 sec/batch)
2017-05-09 01:40:08.101831: step 30120, loss = 0.95 (870.2 examples/sec; 0.147 sec/batch)
2017-05-09 01:40:09.617916: step 30130, loss = 0.84 (844.3 examples/sec; 0.152 sec/batch)
2017-05-09 01:40:11.131207: step 30140, loss = 0.81 (845.8 examples/sec; 0.151 sec/batch)
2017-05-09 01:40:12.627903: step 30150, loss = 1.01 (855.2 examples/sec; 0.150 sec/batch)
2017-05-09 01:40:14.101964: step 30160, loss = 0.80 (868.3 examples/sec; 0.147 sec/batch)
2017-05-09 01:40:15.604620: step 30170, loss = 0.75 (851.8 examples/sec; 0.150 sec/batch)
2017-05-09 01:40:17.119027: step 30180, loss = 0.89 (845.2 examples/sec; 0.151 sec/batch)
2017-05-09 01:40:18.588924: step 30190, loss = 0.96 (870.8 examples/sec; 0.147 sec/batch)
2017-05-09 01:40:20.162523: step 30200, loss = 0.78 (813.4 examples/sec; 0.157 sec/batch)
2017-05-09 01:40:21.552456: step 30210, loss = 0.89 (920.9 examples/sec; 0.139 sec/batch)
2017-05-09 01:40:23.069148: step 30220, loss = 0.89 (843.9 examples/sec; 0.152 sec/batch)
2017-05-09 01:40:24.546776: step 30230, loss = 0.73 (866.2 examples/sec; 0.148 sec/batch)
2017-05-09 01:40:26.019101: step 30240, loss = 0.95 (869.4 examples/sec; 0.147 sec/batch)
2017-05-09 01:40:27.544110: step 30250, loss = 0.90 (839.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:40:29.048834: step 30260, loss = 0.84 (850.7 examples/sec; 0.150 sec/batch)
2017-05-09 01:40:30.516219: step 30270, loss = 0.90 (872.3 examples/sec; 0.147 sec/batch)
2017-05-09 01:40:32.030112: step 30280, loss = 0.77 (845.5 examples/sec; 0.151 sec/batch)
2017-05-09 01:40:33.487302: step 30290, loss = 0.64 (878.4 examples/sec; 0.146 sec/batch)
2017-05-09 01:40:35.091751: step 30300, loss = 0.92 (797.8 examples/sec; 0.160 sec/batch)
2017-05-09 01:40:36.482753: step 30310, loss = 0.69 (920.2 examples/sec; 0.139 sec/batch)
2017-05-09 01:40:37.960924: step 30320, loss = 0.80 (865.9 examples/sec; 0.148 sec/batch)
2017-05-09 01:40:39.449366: step 30330, loss = 0.87 (860.0 examples/sec; 0.149 sec/batch)
2017-05-09 01:40:40.993467: step 30340, loss = 0.95 (829.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:40:42.480395: step 30350, loss = 0.78 (860.8 examples/sec; 0.149 sec/batch)
2017-05-09 01:40:44.013375: step 30360, loss = 0.82 (835.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:40:45.504358: step 30370, loss = 0.81 (858.5 examples/sec; 0.149 sec/batch)
2017-05-09 01:40:47.039389: step 30380, loss = 1.14 (833.8 examples/sec; 0.154 sec/batch)
2017-05-09 01:40:48.561076: step 30390, loss = 0.91 (841.2 examples/sec; 0.152 sec/batch)
2017-05-09 01:40:50.171370: step 30400, loss = 0.66 (794.9 examples/sec; 0.161 sec/batch)
2017-05-09 01:40:51.556755: step 30410, loss = 1.03 (923.9 examples/sec; 0.139 sec/batch)
2017-05-09 01:40:53.057137: step 30420, loss = 1.02 (853.1 examples/sec; 0.150 sec/batch)
2017-05-09 01:40:54.546376: step 30430, loss = 0.92 (859.5 examples/sec; 0.149 sec/batch)
2017-05-09 01:40:56.040044: step 30440, loss = 0.98 (857.0 examples/sec; 0.149 sec/batch)
2017-05-09 01:40:57.521650: step 30450, loss = 0.81 (863.9 examples/sec; 0.148 sec/batch)
2017-05-09 01:40:59.016612: step 30460, loss = 0.75 (856.2 examples/sec; 0.149 sec/batch)
2017-05-09 01:41:00.524711: step 30470, loss = 0.97 (848.8 examples/sec; 0.151 sec/batch)
2017-05-09 01:41:02.022300: step 30480, loss = 0.87 (854.7 examples/sec; 0.150 sec/batch)
2017-05-09 01:41:03.498484: step 30490, loss = 0.90 (867.1 examples/sec; 0.148 sec/batch)
2017-05-09 01:41:05.095203: step 30500, loss = 0.72 (801.6 examples/sec; 0.160 sec/batch)
2017-05-09 01:41:06.474363: step 30510, loss = 0.82 (928.1 examples/sec; 0.138 sec/batch)
2017-05-09 01:41:08.000601: step 30520, loss = 0.72 (838.7 examples/sec; 0.153 sec/batch)
2017-05-09 01:41:09.527157: step 30530, loss = 0.81 (838.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:41:11.049362: step 30540, loss = 0.85 (840.9 examples/sec; 0.152 sec/batch)
2017-05-09 01:41:12.546709: step 30550, loss = 1.30 (854.8 examples/sec; 0.150 sec/batch)
2017-05-09 01:41:14.028846: step 30560, loss = 0.79 (863.6 examples/sec; 0.148 sec/batch)
2017-05-09 01:41:15.525986: step 30570, loss = 0.77 (855.0 examples/sec; 0.150 sec/batch)
2017-05-09 01:41:17.084465: step 30580, loss = 0.92 (821.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:41:18.558309: step 30590, loss = 0.85 (868.5 examples/sec; 0.147 sec/batch)
2017-05-09 01:41:20.163875: step 30600, loss = 0.96 (797.2 examples/sec; 0.161 sec/batch)
2017-05-09 01:41:21.571938: step 30610, loss = 0.79 (909.0 examples/sec; 0.141 sec/batch)
2017-05-09 01:41:23.077304: step 30620, loss = 0.80 (850.3 examples/sec; 0.151 sec/batch)
2017-05-09 01:41:24.578865: step 30630, loss = 0.85 (852.5 examples/sec; 0.150 sec/batch)
2017-05-09 01:41:26.124377: step 30640, loss = 0.74 (828.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:41:27.648666: step 30650, loss = 0.78 (839.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:41:29.173878: step 30660, loss = 0.88 (839.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:41:30.661175: step 30670, loss = 1.03 (860.6 examples/sec; 0.149 sec/batch)
2017-05-09 01:41:32.176111: step 30680, loss = 0.78 (844.9 examples/sec; 0.151 sec/batch)
2017-05-09 01:41:33.672409: step 30690, loss = 0.68 (855.4 examples/sec; 0.150 sec/batch)
2017-05-09 01:41:35.265242: step 30700, loss = 0.75 (803.6 examples/sec; 0.159 sec/batch)
2017-05-09 01:41:36.755267: step 30710, loss = 0.89 (859.1 examples/sec; 0.149 sec/batch)
2017-05-09 01:41:38.322115: step 30720, loss = 1.01 (816.9 examples/sec; 0.157 sec/batch)
2017-05-09 01:41:39.845008: step 30730, loss = 0.71 (840.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:41:41.358714: step 30740, loss = 0.77 (845.6 examples/sec; 0.151 sec/batch)
2017-05-09 01:41:42.850904: step 30750, loss = 0.67 (857.8 examples/sec; 0.149 sec/batch)
2017-05-09 01:41:44.349586: step 30760, loss = 0.89 (854.1 examples/sec; 0.150 sec/batch)
2017-05-09 01:41:45.813733: step 30770, loss = 0.76 (874.2 examples/sec; 0.146 sec/batch)
2017-05-09 01:41:47.319486: step 30780, loss = 0.94 (850.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:41:48.836759: step 30790, loss = 0.90 (843.6 examples/sec; 0.152 sec/batch)
2017-05-09 01:41:50.416933: step 30800, loss = 0.76 (810.0 examples/sec; 0.158 sec/batch)
2017-05-09 01:41:51.841596: step 30810, loss = 0.80 (898.5 examples/sec; 0.142 sec/batch)
2017-05-09 01:41:53.391781: step 30820, loss = 0.73 (825.7 examples/sec; 0.155 sec/batch)
2017-05-09 01:41:54.904367: step 30830, loss = 0.99 (846.3 examples/sec; 0.151 sec/batch)
2017-05-09 01:41:56.398961: step 30840, loss = 0.86 (856.4 examples/sec; 0.149 sec/batch)
2017-05-09 01:41:57.890022: step 30850, loss = 0.91 (858.5 examples/sec; 0.149 sec/batch)
2017-05-09 01:41:59.392352: step 30860, loss = 0.73 (852.0 examples/sec; 0.150 sec/batch)
2017-05-09 01:42:00.983220: step 30870, loss = 0.92 (804.6 examples/sec; 0.159 sec/batch)
2017-05-09 01:42:02.509018: step 30880, loss = 0.82 (838.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:42:04.041038: step 30890, loss = 0.76 (835.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:42:05.618446: step 30900, loss = 1.02 (811.5 examples/sec; 0.158 sec/batch)
2017-05-09 01:42:07.023589: step 30910, loss = 0.88 (910.9 examples/sec; 0.141 sec/batch)
2017-05-09 01:42:08.542121: step 30920, loss = 0.82 (842.9 examples/sec; 0.152 sec/batch)
2017-05-09 01:42:10.032559: step 30930, loss = 1.15 (858.8 examples/sec; 0.149 sec/batch)
2017-05-09 01:42:11.524894: step 30940, loss = 0.93 (857.7 examples/sec; 0.149 sec/batch)
2017-05-09 01:42:13.059438: step 30950, loss = 0.76 (834.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:42:14.586175: step 30960, loss = 1.07 (838.4 examples/sec; 0.153 sec/batch)
2017-05-09 01:42:16.074541: step 30970, loss = 0.84 (860.0 examples/sec; 0.149 sec/batch)
2017-05-09 01:42:17.573207: step 30980, loss = 0.85 (854.1 examples/sec; 0.150 sec/batch)
2017-05-09 01:42:19.101546: step 30990, loss = 0.94 (837.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:42:20.741700: step 31000, loss = 1.05 (780.4 examples/sec; 0.164 sec/batch)
2017-05-09 01:42:22.145173: step 31010, loss = 0.87 (912.0 examples/sec; 0.140 sec/batch)
2017-05-09 01:42:23.636381: step 31020, loss = 0.92 (858.4 examples/sec; 0.149 sec/batch)
2017-05-09 01:42:25.163997: step 31030, loss = 0.64 (837.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:42:26.688528: step 31040, loss = 0.88 (839.6 examples/sec; 0.152 sec/batch)
2017-05-09 01:42:28.200917: step 31050, loss = 0.93 (846.3 examples/sec; 0.151 sec/batch)
2017-05-09 01:42:29.724851: step 31060, loss = 0.71 (839.9 examples/sec; 0.152 sec/batch)
2017-05-09 01:42:31.246995: step 31070, loss = 0.97 (840.9 examples/sec; 0.152 sec/batch)
2017-05-09 01:42:32.825576: step 31080, loss = 0.78 (810.9 examples/sec; 0.158 sec/batch)
2017-05-09 01:42:34.365199: step 31090, loss = 0.76 (831.4 examples/sec; 0.154 sec/batch)
2017-05-09 01:42:35.955883: step 31100, loss = 0.94 (804.7 examples/sec; 0.159 sec/batch)
2017-05-09 01:42:37.422822: step 31110, loss = 0.78 (872.6 examples/sec; 0.147 sec/batch)
2017-05-09 01:42:38.926056: step 31120, loss = 0.80 (851.5 examples/sec; 0.150 sec/batch)
2017-05-09 01:42:40.467945: step 31130, loss = 0.94 (830.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:42:41.997831: step 31140, loss = 0.78 (836.7 examples/sec; 0.153 sec/batch)
2017-05-09 01:42:43.528815: step 31150, loss = 0.89 (836.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:42:45.061855: step 31160, loss = 0.79 (834.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:42:46.574355: step 31170, loss = 0.68 (846.3 examples/sec; 0.151 sec/batch)
2017-05-09 01:42:48.063264: step 31180, loss = 0.77 (859.7 examples/sec; 0.149 sec/batch)
2017-05-09 01:42:49.608130: step 31190, loss = 0.92 (828.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:42:51.287748: step 31200, loss = 0.85 (762.1 examples/sec; 0.168 sec/batch)
2017-05-09 01:42:52.726091: step 31210, loss = 0.73 (889.9 examples/sec; 0.144 sec/batch)
2017-05-09 01:42:54.277935: step 31220, loss = 0.93 (824.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:42:55.768025: step 31230, loss = 0.77 (859.0 examples/sec; 0.149 sec/batch)
2017-05-09 01:42:57.315807: step 31240, loss = 0.82 (827.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:42:58.823589: step 31250, loss = 0.71 (848.9 examples/sec; 0.151 sec/batch)
2017-05-09 01:43:00.361076: step 31260, loss = 0.90 (832.5 examples/sec; 0.154 sec/batch)
2017-05-09 01:43:01.889109: step 31270, loss = 0.83 (837.7 examples/sec; 0.153 sec/batch)
2017-05-09 01:43:03.433990: step 31280, loss = 0.87 (828.5 examples/sec; 0.154 sec/batch)
2017-05-09 01:43:04.950420: step 31290, loss = 0.66 (844.1 examples/sec; 0.152 sec/batch)
2017-05-09 01:43:06.572266: step 31300, loss = 0.71 (789.2 examples/sec; 0.162 sec/batch)
2017-05-09 01:43:08.019794: step 31310, loss = 0.79 (884.3 examples/sec; 0.145 sec/batch)
2017-05-09 01:43:09.554081: step 31320, loss = 0.68 (834.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:43:11.055138: step 31330, loss = 0.68 (852.7 examples/sec; 0.150 sec/batch)
2017-05-09 01:43:12.576931: step 31340, loss = 0.86 (841.1 examples/sec; 0.152 sec/batch)
2017-05-09 01:43:14.092580: step 31350, loss = 0.80 (844.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:43:15.638698: step 31360, loss = 0.70 (827.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:43:17.171405: step 31370, loss = 0.91 (835.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:43:18.706588: step 31380, loss = 1.06 (833.8 examples/sec; 0.154 sec/batch)
2017-05-09 01:43:20.235442: step 31390, loss = 0.77 (837.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:43:21.859229: step 31400, loss = 0.81 (788.3 examples/sec; 0.162 sec/batch)
2017-05-09 01:43:23.306050: step 31410, loss = 0.77 (884.7 examples/sec; 0.145 sec/batch)
2017-05-09 01:43:24.828295: step 31420, loss = 0.60 (840.9 examples/sec; 0.152 sec/batch)
2017-05-09 01:43:26.378515: step 31430, loss = 0.82 (825.7 examples/sec; 0.155 sec/batch)
2017-05-09 01:43:27.917856: step 31440, loss = 0.78 (831.5 examples/sec; 0.154 sec/batch)
2017-05-09 01:43:29.468205: step 31450, loss = 0.90 (825.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:43:31.034974: step 31460, loss = 0.71 (817.0 examples/sec; 0.157 sec/batch)
2017-05-09 01:43:32.591073: step 31470, loss = 0.68 (822.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:43:34.146283: step 31480, loss = 0.90 (823.0 examples/sec; 0.156 sec/batch)
2017-05-09 01:43:35.708130: step 31490, loss = 0.91 (819.5 examples/sec; 0.156 sec/batch)
2017-05-09 01:43:37.341491: step 31500, loss = 0.89 (783.7 examples/sec; 0.163 sec/batch)
2017-05-09 01:43:38.801751: step 31510, loss = 0.93 (876.6 examples/sec; 0.146 sec/batch)
2017-05-09 01:43:40.334532: step 31520, loss = 0.64 (835.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:43:41.853711: step 31530, loss = 0.72 (842.6 examples/sec; 0.152 sec/batch)
2017-05-09 01:43:43.388737: step 31540, loss = 0.78 (833.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:43:44.890775: step 31550, loss = 0.96 (852.2 examples/sec; 0.150 sec/batch)
2017-05-09 01:43:46.420370: step 31560, loss = 0.94 (836.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:43:47.928943: step 31570, loss = 0.91 (848.5 examples/sec; 0.151 sec/batch)
2017-05-09 01:43:49.479026: step 31580, loss = 0.79 (825.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:43:51.015018: step 31590, loss = 0.80 (833.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:43:52.628647: step 31600, loss = 0.80 (793.2 examples/sec; 0.161 sec/batch)
2017-05-09 01:43:54.078892: step 31610, loss = 0.98 (882.6 examples/sec; 0.145 sec/batch)
2017-05-09 01:43:55.635201: step 31620, loss = 0.86 (822.5 examples/sec; 0.156 sec/batch)
2017-05-09 01:43:57.162550: step 31630, loss = 0.76 (838.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:43:58.690761: step 31640, loss = 0.94 (837.6 examples/sec; 0.153 sec/batch)
2017-05-09 01:44:00.250210: step 31650, loss = 0.80 (820.8 examples/sec; 0.156 sec/batch)
2017-05-09 01:44:01.757983: step 31660, loss = 0.87 (848.9 examples/sec; 0.151 sec/batch)
2017-05-09 01:44:03.266505: step 31670, loss = 0.78 (848.5 examples/sec; 0.151 sec/batch)
2017-05-09 01:44:04.857733: step 31680, loss = 0.88 (804.4 examples/sec; 0.159 sec/batch)
2017-05-09 01:44:06.388067: step 31690, loss = 1.13 (836.4 examples/sec; 0.153 sec/batch)
2017-05-09 01:44:08.002366: step 31700, loss = 0.75 (792.9 examples/sec; 0.161 sec/batch)
2017-05-09 01:44:09.411017: step 31710, loss = 0.85 (908.7 examples/sec; 0.141 sec/batch)
2017-05-09 01:44:10.959799: step 31720, loss = 0.70 (826.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:44:12.461621: step 31730, loss = 0.81 (852.3 examples/sec; 0.150 sec/batch)
2017-05-09 01:44:13.985539: step 31740, loss = 0.78 (839.9 examples/sec; 0.152 sec/batch)
2017-05-09 01:44:15.533656: step 31750, loss = 0.80 (826.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:44:17.090207: step 31760, loss = 0.83 (822.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:44:18.630643: step 31770, loss = 0.86 (830.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:44:20.168790: step 31780, loss = 0.67 (832.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:44:21.701192: step 31790, loss = 0.86 (835.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:44:23.328002: step 31800, loss = 0.69 (786.8 examples/sec; 0.163 sec/batch)
2017-05-09 01:44:24.784594: step 31810, loss = 0.85 (878.8 examples/sec; 0.146 sec/batch)
2017-05-09 01:44:26.314965: step 31820, loss = 0.94 (836.4 examples/sec; 0.153 sec/batch)
2017-05-09 01:44:27.861578: step 31830, loss = 0.86 (827.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:44:29.375961: step 31840, loss = 0.92 (845.2 examples/sec; 0.151 sec/batch)
2017-05-09 01:44:30.908712: step 31850, loss = 0.72 (835.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:44:32.439695: step 31860, loss = 0.84 (836.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:44:33.983520: step 31870, loss = 0.81 (829.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:44:35.481633: step 31880, loss = 0.92 (854.4 examples/sec; 0.150 sec/batch)
2017-05-09 01:44:37.031425: step 31890, loss = 0.74 (825.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:44:38.683612: step 31900, loss = 0.71 (774.7 examples/sec; 0.165 sec/batch)
2017-05-09 01:44:40.113001: step 31910, loss = 0.93 (895.5 examples/sec; 0.143 sec/batch)
2017-05-09 01:44:41.647249: step 31920, loss = 0.80 (834.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:44:43.177138: step 31930, loss = 0.69 (836.7 examples/sec; 0.153 sec/batch)
2017-05-09 01:44:44.731251: step 31940, loss = 0.95 (823.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:44:46.306130: step 31950, loss = 0.83 (812.8 examples/sec; 0.157 sec/batch)
2017-05-09 01:44:47.905390: step 31960, loss = 0.79 (800.4 examples/sec; 0.160 sec/batch)
2017-05-09 01:44:49.468865: step 31970, loss = 0.85 (818.7 examples/sec; 0.156 sec/batch)
2017-05-09 01:44:51.011307: step 31980, loss = 0.94 (829.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:44:52.548604: step 31990, loss = 0.66 (832.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:44:54.143572: step 32000, loss = 0.75 (802.5 examples/sec; 0.159 sec/batch)
2017-05-09 01:44:55.611427: step 32010, loss = 1.50 (872.0 examples/sec; 0.147 sec/batch)
2017-05-09 01:44:57.191347: step 32020, loss = 1.02 (810.2 examples/sec; 0.158 sec/batch)
2017-05-09 01:44:58.683005: step 32030, loss = 0.95 (858.1 examples/sec; 0.149 sec/batch)
2017-05-09 01:45:00.259441: step 32040, loss = 0.93 (812.0 examples/sec; 0.158 sec/batch)
2017-05-09 01:45:01.832878: step 32050, loss = 0.69 (813.5 examples/sec; 0.157 sec/batch)
2017-05-09 01:45:03.371551: step 32060, loss = 0.80 (831.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:45:04.906857: step 32070, loss = 0.74 (833.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:45:06.461709: step 32080, loss = 0.72 (823.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:45:08.002141: step 32090, loss = 0.90 (830.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:45:09.607962: step 32100, loss = 0.71 (797.1 examples/sec; 0.161 sec/batch)
2017-05-09 01:45:11.074785: step 32110, loss = 0.86 (872.6 examples/sec; 0.147 sec/batch)
2017-05-09 01:45:12.608713: step 32120, loss = 0.79 (834.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:45:14.183099: step 32130, loss = 0.95 (813.0 examples/sec; 0.157 sec/batch)
2017-05-09 01:45:15.713210: step 32140, loss = 0.73 (836.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:45:17.292458: step 32150, loss = 0.83 (810.5 examples/sec; 0.158 sec/batch)
2017-05-09 01:45:18.833880: step 32160, loss = 0.69 (830.4 examples/sec; 0.154 sec/batch)
2017-05-09 01:45:20.382721: step 32170, loss = 0.76 (826.4 examples/sec; 0.155 sec/batch)
2017-05-09 01:45:21.911613: step 32180, loss = 0.77 (837.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:45:23.442148: step 32190, loss = 0.95 (836.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:45:25.111885: step 32200, loss = 0.85 (766.6 examples/sec; 0.167 sec/batch)
2017-05-09 01:45:26.548580: step 32210, loss = 0.85 (890.9 examples/sec; 0.144 sec/batch)
2017-05-09 01:45:28.095642: step 32220, loss = 0.82 (827.4 examples/sec; 0.155 sec/batch)
2017-05-09 01:45:29.626939: step 32230, loss = 0.77 (835.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:45:31.182409: step 32240, loss = 0.97 (822.9 examples/sec; 0.156 sec/batch)
2017-05-09 01:45:32.716652: step 32250, loss = 0.77 (834.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:45:34.258104: step 32260, loss = 0.90 (830.4 examples/sec; 0.154 sec/batch)
2017-05-09 01:45:35.844863: step 32270, loss = 0.76 (806.7 examples/sec; 0.159 sec/batch)
2017-05-09 01:45:37.409754: step 32280, loss = 0.91 (818.0 examples/sec; 0.156 sec/batch)
2017-05-09 01:45:38.949940: step 32290, loss = 0.75 (831.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:45:40.626095: step 32300, loss = 0.84 (763.7 examples/sec; 0.168 sec/batch)
2017-05-09 01:45:42.083420: step 32310, loss = 0.69 (878.3 examples/sec; 0.146 sec/batch)
2017-05-09 01:45:43.612911: step 32320, loss = 0.72 (836.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:45:45.157652: step 32330, loss = 0.73 (828.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:45:46.688783: step 32340, loss = 0.95 (836.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:45:48.273902: step 32350, loss = 0.70 (807.5 examples/sec; 0.159 sec/batch)
2017-05-09 01:45:49.824481: step 32360, loss = 1.11 (825.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:45:51.384746: step 32370, loss = 0.87 (820.4 examples/sec; 0.156 sec/batch)
2017-05-09 01:45:52.933488: step 32380, loss = 0.89 (826.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:45:54.485952: step 32390, loss = 0.79 (824.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:45:56.116736: step 32400, loss = 0.81 (784.9 examples/sec; 0.163 sec/batch)
2017-05-09 01:45:57.568488: step 32410, loss = 0.84 (881.7 examples/sec; 0.145 sec/batch)
2017-05-09 01:45:59.083150: step 32420, loss = 0.86 (845.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:46:00.628534: step 32430, loss = 0.77 (828.3 examples/sec; 0.155 sec/batch)
2017-05-09 01:46:02.136675: step 32440, loss = 0.81 (848.7 examples/sec; 0.151 sec/batch)
2017-05-09 01:46:03.679810: step 32450, loss = 0.67 (829.5 examples/sec; 0.154 sec/batch)
2017-05-09 01:46:05.205647: step 32460, loss = 0.83 (838.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:46:06.735099: step 32470, loss = 0.92 (836.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:46:08.290589: step 32480, loss = 0.70 (822.9 examples/sec; 0.156 sec/batch)
2017-05-09 01:46:09.819363: step 32490, loss = 0.85 (837.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:46:11.445931: step 32500, loss = 0.77 (786.9 examples/sec; 0.163 sec/batch)
2017-05-09 01:46:12.905110: step 32510, loss = 0.81 (877.2 examples/sec; 0.146 sec/batch)
2017-05-09 01:46:14.454638: step 32520, loss = 0.81 (826.1 examples/sec; 0.155 sec/batch)
2017-05-09 01:46:15.979887: step 32530, loss = 1.01 (839.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:46:17.517778: step 32540, loss = 0.82 (832.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:46:19.078464: step 32550, loss = 0.65 (820.2 examples/sec; 0.156 sec/batch)
2017-05-09 01:46:20.611700: step 32560, loss = 0.82 (834.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:46:22.141301: step 32570, loss = 0.78 (836.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:46:23.687384: step 32580, loss = 0.87 (827.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:46:25.289759: step 32590, loss = 0.72 (798.8 examples/sec; 0.160 sec/batch)
2017-05-09 01:46:26.918101: step 32600, loss = 0.85 (786.1 examples/sec; 0.163 sec/batch)
2017-05-09 01:46:28.397544: step 32610, loss = 0.93 (865.2 examples/sec; 0.148 sec/batch)
2017-05-09 01:46:29.920772: step 32620, loss = 0.82 (840.3 examples/sec; 0.152 sec/batch)
2017-05-09 01:46:31.462074: step 32630, loss = 0.71 (830.5 examples/sec; 0.154 sec/batch)
2017-05-09 01:46:32.993149: step 32640, loss = 0.98 (836.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:46:34.560664: step 32650, loss = 0.89 (816.6 examples/sec; 0.157 sec/batch)
2017-05-09 01:46:36.110834: step 32660, loss = 1.06 (825.7 examples/sec; 0.155 sec/batch)
2017-05-09 01:46:37.686697: step 32670, loss = 0.90 (812.3 examples/sec; 0.158 sec/batch)
2017-05-09 01:46:39.216482: step 32680, loss = 0.74 (836.7 examples/sec; 0.153 sec/batch)
2017-05-09 01:46:40.766811: step 32690, loss = 0.70 (825.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:46:42.378632: step 32700, loss = 0.83 (794.1 examples/sec; 0.161 sec/batch)
2017-05-09 01:46:43.863916: step 32710, loss = 0.75 (861.8 examples/sec; 0.149 sec/batch)
2017-05-09 01:46:45.403225: step 32720, loss = 0.80 (831.5 examples/sec; 0.154 sec/batch)
2017-05-09 01:46:46.962292: step 32730, loss = 0.87 (821.0 examples/sec; 0.156 sec/batch)
2017-05-09 01:46:48.534388: step 32740, loss = 0.72 (814.2 examples/sec; 0.157 sec/batch)
2017-05-09 01:46:50.076295: step 32750, loss = 0.77 (830.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:46:51.639249: step 32760, loss = 0.75 (819.0 examples/sec; 0.156 sec/batch)
2017-05-09 01:46:53.202844: step 32770, loss = 0.82 (818.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:46:54.752079: step 32780, loss = 1.16 (826.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:46:56.308328: step 32790, loss = 0.79 (822.5 examples/sec; 0.156 sec/batch)
2017-05-09 01:46:57.959579: step 32800, loss = 0.88 (775.2 examples/sec; 0.165 sec/batch)
2017-05-09 01:46:59.401826: step 32810, loss = 0.89 (887.5 examples/sec; 0.144 sec/batch)
2017-05-09 01:47:00.978399: step 32820, loss = 0.65 (811.9 examples/sec; 0.158 sec/batch)
2017-05-09 01:47:02.516835: step 32830, loss = 0.85 (832.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:47:04.138456: step 32840, loss = 0.89 (789.3 examples/sec; 0.162 sec/batch)
2017-05-09 01:47:05.661409: step 32850, loss = 0.80 (840.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:47:07.198628: step 32860, loss = 0.78 (832.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:47:08.738934: step 32870, loss = 0.85 (831.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:47:10.312135: step 32880, loss = 0.99 (813.6 examples/sec; 0.157 sec/batch)
2017-05-09 01:47:11.854044: step 32890, loss = 0.93 (830.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:47:13.508474: step 32900, loss = 0.85 (773.7 examples/sec; 0.165 sec/batch)
2017-05-09 01:47:14.975981: step 32910, loss = 0.80 (872.2 examples/sec; 0.147 sec/batch)
2017-05-09 01:47:16.557013: step 32920, loss = 0.92 (809.6 examples/sec; 0.158 sec/batch)
2017-05-09 01:47:18.129983: step 32930, loss = 0.98 (813.7 examples/sec; 0.157 sec/batch)
2017-05-09 01:47:19.692596: step 32940, loss = 0.73 (819.1 examples/sec; 0.156 sec/batch)
2017-05-09 01:47:21.250809: step 32950, loss = 0.76 (821.5 examples/sec; 0.156 sec/batch)
2017-05-09 01:47:22.798854: step 32960, loss = 0.75 (826.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:47:24.357309: step 32970, loss = 0.77 (821.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:47:25.887675: step 32980, loss = 0.89 (836.4 examples/sec; 0.153 sec/batch)
2017-05-09 01:47:27.429132: step 32990, loss = 0.63 (830.4 examples/sec; 0.154 sec/batch)
2017-05-09 01:47:29.097934: step 33000, loss = 0.80 (767.0 examples/sec; 0.167 sec/batch)
2017-05-09 01:47:30.594777: step 33010, loss = 0.69 (855.1 examples/sec; 0.150 sec/batch)
2017-05-09 01:47:32.121392: step 33020, loss = 0.80 (838.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:47:33.682812: step 33030, loss = 0.82 (819.8 examples/sec; 0.156 sec/batch)
2017-05-09 01:47:35.216001: step 33040, loss = 0.74 (834.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:47:36.753823: step 33050, loss = 0.72 (832.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:47:38.343500: step 33060, loss = 0.83 (805.2 examples/sec; 0.159 sec/batch)
2017-05-09 01:47:39.893048: step 33070, loss = 0.97 (826.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:47:41.440871: step 33080, loss = 0.85 (827.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:47:42.962640: step 33090, loss = 0.89 (841.1 examples/sec; 0.152 sec/batch)
2017-05-09 01:47:44.609660: step 33100, loss = 0.95 (777.2 examples/sec; 0.165 sec/batch)
2017-05-09 01:47:46.058667: step 33110, loss = 0.80 (883.4 examples/sec; 0.145 sec/batch)
2017-05-09 01:47:47.607716: step 33120, loss = 0.83 (826.3 examples/sec; 0.155 sec/batch)
2017-05-09 01:47:49.170403: step 33130, loss = 0.70 (819.1 examples/sec; 0.156 sec/batch)
2017-05-09 01:47:50.686592: step 33140, loss = 0.77 (844.2 examples/sec; 0.152 sec/batch)
2017-05-09 01:47:52.233033: step 33150, loss = 0.79 (827.7 examples/sec; 0.155 sec/batch)
2017-05-09 01:47:53.782237: step 33160, loss = 0.64 (826.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:47:55.325664: step 33170, loss = 0.89 (829.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:47:56.884135: step 33180, loss = 0.88 (821.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:47:58.422725: step 33190, loss = 0.75 (831.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:48:00.072173: step 33200, loss = 0.76 (776.0 examples/sec; 0.165 sec/batch)
2017-05-09 01:48:01.530206: step 33210, loss = 0.88 (877.9 examples/sec; 0.146 sec/batch)
2017-05-09 01:48:03.080938: step 33220, loss = 0.82 (825.4 examples/sec; 0.155 sec/batch)
2017-05-09 01:48:04.614762: step 33230, loss = 0.94 (834.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:48:06.188027: step 33240, loss = 0.78 (813.6 examples/sec; 0.157 sec/batch)
2017-05-09 01:48:07.733370: step 33250, loss = 0.94 (828.3 examples/sec; 0.155 sec/batch)
2017-05-09 01:48:09.278636: step 33260, loss = 0.76 (828.3 examples/sec; 0.155 sec/batch)
2017-05-09 01:48:10.842524: step 33270, loss = 0.68 (818.5 examples/sec; 0.156 sec/batch)
2017-05-09 01:48:12.401671: step 33280, loss = 0.70 (821.0 examples/sec; 0.156 sec/batch)
2017-05-09 01:48:13.976073: step 33290, loss = 0.86 (813.0 examples/sec; 0.157 sec/batch)
2017-05-09 01:48:15.593296: step 33300, loss = 0.74 (791.5 examples/sec; 0.162 sec/batch)
2017-05-09 01:48:17.066416: step 33310, loss = 1.00 (868.9 examples/sec; 0.147 sec/batch)
2017-05-09 01:48:18.647925: step 33320, loss = 0.96 (809.4 examples/sec; 0.158 sec/batch)
2017-05-09 01:48:20.228019: step 33330, loss = 0.95 (810.1 examples/sec; 0.158 sec/batch)
2017-05-09 01:48:21.811507: step 33340, loss = 0.90 (808.3 examples/sec; 0.158 sec/batch)
2017-05-09 01:48:23.346802: step 33350, loss = 0.90 (833.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:48:24.915042: step 33360, loss = 0.63 (816.2 examples/sec; 0.157 sec/batch)
2017-05-09 01:48:26.488894: step 33370, loss = 0.92 (813.3 examples/sec; 0.157 sec/batch)
2017-05-09 01:48:28.068855: step 33380, loss = 0.71 (810.1 examples/sec; 0.158 sec/batch)
2017-05-09 01:48:29.607187: step 33390, loss = 0.88 (832.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:48:31.253400: step 33400, loss = 0.66 (777.5 examples/sec; 0.165 sec/batch)
2017-05-09 01:48:32.701787: step 33410, loss = 0.80 (883.7 examples/sec; 0.145 sec/batch)
2017-05-09 01:48:34.228822: step 33420, loss = 0.75 (838.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:48:35.814988: step 33430, loss = 0.86 (807.0 examples/sec; 0.159 sec/batch)
2017-05-09 01:48:37.351307: step 33440, loss = 0.80 (833.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:48:38.934091: step 33450, loss = 0.84 (808.7 examples/sec; 0.158 sec/batch)
2017-05-09 01:48:40.504656: step 33460, loss = 0.85 (815.0 examples/sec; 0.157 sec/batch)
2017-05-09 01:48:42.090766: step 33470, loss = 0.91 (807.0 examples/sec; 0.159 sec/batch)
2017-05-09 01:48:43.663333: step 33480, loss = 0.79 (814.0 examples/sec; 0.157 sec/batch)
2017-05-09 01:48:45.197958: step 33490, loss = 0.83 (834.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:48:46.836393: step 33500, loss = 0.81 (781.2 examples/sec; 0.164 sec/batch)
2017-05-09 01:48:48.309639: step 33510, loss = 0.82 (868.8 examples/sec; 0.147 sec/batch)
2017-05-09 01:48:49.856578: step 33520, loss = 1.08 (827.4 examples/sec; 0.155 sec/batch)
2017-05-09 01:48:51.444808: step 33530, loss = 0.90 (805.9 examples/sec; 0.159 sec/batch)
2017-05-09 01:48:53.037053: step 33540, loss = 0.89 (803.9 examples/sec; 0.159 sec/batch)
2017-05-09 01:48:54.615231: step 33550, loss = 0.71 (811.1 examples/sec; 0.158 sec/batch)
2017-05-09 01:48:56.179205: step 33560, loss = 0.95 (818.4 examples/sec; 0.156 sec/batch)
2017-05-09 01:48:57.729229: step 33570, loss = 0.74 (825.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:48:59.276622: step 33580, loss = 0.79 (827.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:49:00.835596: step 33590, loss = 0.83 (821.1 examples/sec; 0.156 sec/batch)
2017-05-09 01:49:02.477026: step 33600, loss = 0.91 (779.8 examples/sec; 0.164 sec/batch)
2017-05-09 01:49:03.973262: step 33610, loss = 0.85 (855.5 examples/sec; 0.150 sec/batch)
2017-05-09 01:49:05.537902: step 33620, loss = 0.73 (818.1 examples/sec; 0.156 sec/batch)
2017-05-09 01:49:07.110845: step 33630, loss = 0.80 (813.8 examples/sec; 0.157 sec/batch)
2017-05-09 01:49:08.658394: step 33640, loss = 0.88 (827.1 examples/sec; 0.155 sec/batch)
2017-05-09 01:49:10.205942: step 33650, loss = 0.77 (827.1 examples/sec; 0.155 sec/batch)
2017-05-09 01:49:11.767619: step 33660, loss = 0.77 (819.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:49:13.334205: step 33670, loss = 0.92 (817.1 examples/sec; 0.157 sec/batch)
2017-05-09 01:49:14.868263: step 33680, loss = 0.87 (834.4 examples/sec; 0.153 sec/batch)
2017-05-09 01:49:16.445099: step 33690, loss = 0.81 (811.8 examples/sec; 0.158 sec/batch)
2017-05-09 01:49:18.085741: step 33700, loss = 0.70 (780.2 examples/sec; 0.164 sec/batch)
2017-05-09 01:49:19.556307: step 33710, loss = 0.67 (870.4 examples/sec; 0.147 sec/batch)
2017-05-09 01:49:21.085810: step 33720, loss = 0.78 (836.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:49:22.629211: step 33730, loss = 0.74 (829.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:49:24.181161: step 33740, loss = 0.77 (824.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:49:25.770161: step 33750, loss = 0.79 (805.5 examples/sec; 0.159 sec/batch)
2017-05-09 01:49:27.334385: step 33760, loss = 0.80 (818.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:49:28.900751: step 33770, loss = 1.09 (817.2 examples/sec; 0.157 sec/batch)
2017-05-09 01:49:30.481464: step 33780, loss = 0.77 (809.8 examples/sec; 0.158 sec/batch)
2017-05-09 01:49:32.054503: step 33790, loss = 0.90 (813.7 examples/sec; 0.157 sec/batch)
2017-05-09 01:49:33.733557: step 33800, loss = 0.92 (762.3 examples/sec; 0.168 sec/batch)
2017-05-09 01:49:35.261065: step 33810, loss = 0.82 (838.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:49:36.817866: step 33820, loss = 0.93 (822.2 examples/sec; 0.156 sec/batch)
2017-05-09 01:49:38.351159: step 33830, loss = 0.80 (834.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:49:39.925437: step 33840, loss = 0.90 (813.1 examples/sec; 0.157 sec/batch)
2017-05-09 01:49:41.465532: step 33850, loss = 0.75 (831.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:49:42.995112: step 33860, loss = 1.00 (836.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:49:44.539676: step 33870, loss = 1.01 (828.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:49:46.070284: step 33880, loss = 0.84 (836.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:49:47.620922: step 33890, loss = 0.87 (825.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:49:49.259765: step 33900, loss = 0.89 (781.0 examples/sec; 0.164 sec/batch)
2017-05-09 01:49:50.685244: step 33910, loss = 0.93 (897.9 examples/sec; 0.143 sec/batch)
2017-05-09 01:49:52.249291: step 33920, loss = 0.78 (818.4 examples/sec; 0.156 sec/batch)
2017-05-09 01:49:53.775450: step 33930, loss = 0.99 (838.7 examples/sec; 0.153 sec/batch)
2017-05-09 01:49:55.371550: step 33940, loss = 0.81 (802.0 examples/sec; 0.160 sec/batch)
2017-05-09 01:49:56.931662: step 33950, loss = 0.92 (820.5 examples/sec; 0.156 sec/batch)
2017-05-09 01:49:58.501244: step 33960, loss = 1.07 (815.5 examples/sec; 0.157 sec/batch)
2017-05-09 01:50:00.044898: step 33970, loss = 0.86 (829.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:50:01.626747: step 33980, loss = 0.82 (809.2 examples/sec; 0.158 sec/batch)
2017-05-09 01:50:03.198125: step 33990, loss = 0.96 (814.6 examples/sec; 0.157 sec/batch)
2017-05-09 01:50:04.827677: step 34000, loss = 1.18 (785.5 examples/sec; 0.163 sec/batch)
2017-05-09 01:50:06.264359: step 34010, loss = 0.73 (891.0 examples/sec; 0.144 sec/batch)
2017-05-09 01:50:07.799302: step 34020, loss = 0.70 (833.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:50:09.344688: step 34030, loss = 0.95 (828.3 examples/sec; 0.155 sec/batch)
2017-05-09 01:50:10.889254: step 34040, loss = 0.79 (828.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:50:12.432978: step 34050, loss = 0.97 (829.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:50:13.982890: step 34060, loss = 0.68 (825.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:50:15.514078: step 34070, loss = 0.79 (836.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:50:17.059928: step 34080, loss = 0.80 (828.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:50:18.598555: step 34090, loss = 0.84 (831.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:50:20.232877: step 34100, loss = 0.94 (783.2 examples/sec; 0.163 sec/batch)
2017-05-09 01:50:21.703632: step 34110, loss = 1.10 (870.3 examples/sec; 0.147 sec/batch)
2017-05-09 01:50:23.243908: step 34120, loss = 0.71 (831.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:50:24.764707: step 34130, loss = 0.82 (841.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:50:26.319028: step 34140, loss = 0.69 (823.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:50:27.837473: step 34150, loss = 0.84 (843.0 examples/sec; 0.152 sec/batch)
2017-05-09 01:50:29.356393: step 34160, loss = 0.82 (842.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:50:30.885489: step 34170, loss = 0.77 (837.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:50:32.415130: step 34180, loss = 0.81 (836.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:50:33.974914: step 34190, loss = 0.81 (820.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:50:35.616729: step 34200, loss = 0.92 (779.6 examples/sec; 0.164 sec/batch)
2017-05-09 01:50:37.018417: step 34210, loss = 0.71 (913.2 examples/sec; 0.140 sec/batch)
2017-05-09 01:50:38.538602: step 34220, loss = 0.88 (842.0 examples/sec; 0.152 sec/batch)
2017-05-09 01:50:40.056292: step 34230, loss = 0.97 (843.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:50:41.569164: step 34240, loss = 1.03 (846.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:50:43.144032: step 34250, loss = 0.82 (812.8 examples/sec; 0.157 sec/batch)
2017-05-09 01:50:44.693489: step 34260, loss = 0.77 (826.1 examples/sec; 0.155 sec/batch)
2017-05-09 01:50:46.215222: step 34270, loss = 0.78 (841.1 examples/sec; 0.152 sec/batch)
2017-05-09 01:50:47.730280: step 34280, loss = 0.80 (844.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:50:49.243977: step 34290, loss = 0.76 (845.6 examples/sec; 0.151 sec/batch)
2017-05-09 01:50:50.868386: step 34300, loss = 0.80 (788.0 examples/sec; 0.162 sec/batch)
2017-05-09 01:50:52.311877: step 34310, loss = 0.88 (886.7 examples/sec; 0.144 sec/batch)
2017-05-09 01:50:53.817033: step 34320, loss = 0.72 (850.4 examples/sec; 0.151 sec/batch)
2017-05-09 01:50:55.341393: step 34330, loss = 0.87 (839.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:50:56.850262: step 34340, loss = 0.82 (848.3 examples/sec; 0.151 sec/batch)
2017-05-09 01:50:58.347631: step 34350, loss = 0.87 (854.8 examples/sec; 0.150 sec/batch)
2017-05-09 01:50:59.851629: step 34360, loss = 0.83 (851.1 examples/sec; 0.150 sec/batch)
2017-05-09 01:51:01.370847: step 34370, loss = 0.91 (842.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:51:02.904420: step 34380, loss = 0.80 (834.7 examples/sec; 0.153 sec/batch)
2017-05-09 01:51:04.433742: step 34390, loss = 0.84 (837.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:51:06.049635: step 34400, loss = 0.80 (792.1 examples/sec; 0.162 sec/batch)
2017-05-09 01:51:07.479465: step 34410, loss = 0.78 (895.2 examples/sec; 0.143 sec/batch)
2017-05-09 01:51:09.008442: step 34420, loss = 0.90 (837.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:51:10.525538: step 34430, loss = 0.61 (843.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:51:12.079693: step 34440, loss = 0.97 (823.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:51:13.595567: step 34450, loss = 0.60 (844.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:51:15.108373: step 34460, loss = 0.67 (846.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:51:16.605077: step 34470, loss = 0.78 (855.2 examples/sec; 0.150 sec/batch)
2017-05-09 01:51:18.114419: step 34480, loss = 0.82 (848.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:51:19.652611: step 34490, loss = 0.66 (832.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:51:21.306589: step 34500, loss = 0.77 (773.9 examples/sec; 0.165 sec/batch)
2017-05-09 01:51:22.719755: step 34510, loss = 0.79 (905.8 examples/sec; 0.141 sec/batch)
2017-05-09 01:51:24.246107: step 34520, loss = 0.99 (838.6 examples/sec; 0.153 sec/batch)
2017-05-09 01:51:25.797174: step 34530, loss = 0.82 (825.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:51:27.339949: step 34540, loss = 0.87 (829.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:51:28.860960: step 34550, loss = 0.93 (841.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:51:30.361271: step 34560, loss = 0.74 (853.2 examples/sec; 0.150 sec/batch)
2017-05-09 01:51:31.893048: step 34570, loss = 0.80 (835.6 examples/sec; 0.153 sec/batch)
2017-05-09 01:51:33.445191: step 34580, loss = 0.86 (824.7 examples/sec; 0.155 sec/batch)
2017-05-09 01:51:34.985054: step 34590, loss = 0.93 (831.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:51:36.602464: step 34600, loss = 0.92 (791.4 examples/sec; 0.162 sec/batch)
2017-05-09 01:51:38.034724: step 34610, loss = 0.73 (893.7 examples/sec; 0.143 sec/batch)
2017-05-09 01:51:39.558128: step 34620, loss = 0.98 (840.2 examples/sec; 0.152 sec/batch)
2017-05-09 01:51:41.100614: step 34630, loss = 0.76 (829.8 examples/sec; 0.154 sec/batch)
2017-05-09 01:51:42.631632: step 34640, loss = 0.88 (836.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:51:44.179524: step 34650, loss = 0.99 (826.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:51:45.677777: step 34660, loss = 0.80 (854.3 examples/sec; 0.150 sec/batch)
2017-05-09 01:51:47.194006: step 34670, loss = 0.77 (844.2 examples/sec; 0.152 sec/batch)
2017-05-09 01:51:48.716570: step 34680, loss = 0.75 (840.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:51:50.251995: step 34690, loss = 0.60 (833.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:51:51.872145: step 34700, loss = 0.82 (790.1 examples/sec; 0.162 sec/batch)
2017-05-09 01:51:53.306404: step 34710, loss = 0.91 (892.4 examples/sec; 0.143 sec/batch)
2017-05-09 01:51:54.829258: step 34720, loss = 0.65 (840.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:51:56.383127: step 34730, loss = 0.73 (823.7 examples/sec; 0.155 sec/batch)
2017-05-09 01:51:57.929495: step 34740, loss = 0.80 (827.7 examples/sec; 0.155 sec/batch)
2017-05-09 01:51:59.481955: step 34750, loss = 1.04 (824.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:52:01.003414: step 34760, loss = 0.74 (841.3 examples/sec; 0.152 sec/batch)
2017-05-09 01:52:02.506052: step 34770, loss = 0.69 (851.8 examples/sec; 0.150 sec/batch)
2017-05-09 01:52:04.025465: step 34780, loss = 0.86 (842.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:52:05.542610: step 34790, loss = 0.80 (843.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:52:07.156468: step 34800, loss = 0.78 (793.1 examples/sec; 0.161 sec/batch)
2017-05-09 01:52:08.603952: step 34810, loss = 0.83 (884.3 examples/sec; 0.145 sec/batch)
2017-05-09 01:52:10.118560: step 34820, loss = 0.72 (845.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:52:11.654641: step 34830, loss = 0.68 (833.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:52:13.157588: step 34840, loss = 0.70 (851.7 examples/sec; 0.150 sec/batch)
2017-05-09 01:52:14.676682: step 34850, loss = 0.84 (842.6 examples/sec; 0.152 sec/batch)
2017-05-09 01:52:16.206283: step 34860, loss = 0.75 (836.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:52:17.743978: step 34870, loss = 0.79 (832.4 examples/sec; 0.154 sec/batch)
2017-05-09 01:52:19.292692: step 34880, loss = 0.77 (826.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:52:20.856939: step 34890, loss = 0.87 (818.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:52:22.483307: step 34900, loss = 0.82 (787.0 examples/sec; 0.163 sec/batch)
2017-05-09 01:52:23.969271: step 34910, loss = 1.04 (861.4 examples/sec; 0.149 sec/batch)
2017-05-09 01:52:25.516133: step 34920, loss = 0.87 (827.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:52:27.065935: step 34930, loss = 0.77 (825.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:52:28.618449: step 34940, loss = 0.75 (824.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:52:30.168456: step 34950, loss = 0.89 (825.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:52:31.735050: step 34960, loss = 0.94 (817.1 examples/sec; 0.157 sec/batch)
2017-05-09 01:52:33.285804: step 34970, loss = 1.08 (825.4 examples/sec; 0.155 sec/batch)
2017-05-09 01:52:34.894206: step 34980, loss = 0.71 (795.8 examples/sec; 0.161 sec/batch)
2017-05-09 01:52:36.468080: step 34990, loss = 0.75 (813.3 examples/sec; 0.157 sec/batch)
2017-05-09 01:52:38.126514: step 35000, loss = 0.81 (771.8 examples/sec; 0.166 sec/batch)
2017-05-09 01:52:39.608063: step 35010, loss = 0.86 (864.0 examples/sec; 0.148 sec/batch)
2017-05-09 01:52:41.164623: step 35020, loss = 0.86 (822.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:52:42.726497: step 35030, loss = 0.93 (819.5 examples/sec; 0.156 sec/batch)
2017-05-09 01:52:44.306029: step 35040, loss = 0.98 (810.4 examples/sec; 0.158 sec/batch)
2017-05-09 01:52:45.846459: step 35050, loss = 0.82 (830.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:52:47.439402: step 35060, loss = 0.85 (803.5 examples/sec; 0.159 sec/batch)
2017-05-09 01:52:48.986026: step 35070, loss = 0.79 (827.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:52:50.518563: step 35080, loss = 0.73 (835.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:52:52.069351: step 35090, loss = 0.77 (825.4 examples/sec; 0.155 sec/batch)
2017-05-09 01:52:53.725778: step 35100, loss = 0.94 (772.7 examples/sec; 0.166 sec/batch)
2017-05-09 01:52:55.217558: step 35110, loss = 0.83 (858.0 examples/sec; 0.149 sec/batch)
2017-05-09 01:52:56.781199: step 35120, loss = 0.67 (818.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:52:58.288910: step 35130, loss = 0.73 (849.0 examples/sec; 0.151 sec/batch)
2017-05-09 01:52:59.828024: step 35140, loss = 0.83 (831.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:53:01.342605: step 35150, loss = 0.81 (845.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:53:02.861951: step 35160, loss = 1.14 (842.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:53:04.397430: step 35170, loss = 1.06 (833.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:53:05.947143: step 35180, loss = 0.92 (826.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:53:07.485041: step 35190, loss = 0.78 (832.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:53:09.128704: step 35200, loss = 0.83 (778.8 examples/sec; 0.164 sec/batch)
2017-05-09 01:53:10.596300: step 35210, loss = 0.79 (872.2 examples/sec; 0.147 sec/batch)
2017-05-09 01:53:12.132210: step 35220, loss = 0.74 (833.4 examples/sec; 0.154 sec/batch)
2017-05-09 01:53:13.675650: step 35230, loss = 0.80 (829.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:53:15.262998: step 35240, loss = 1.00 (806.4 examples/sec; 0.159 sec/batch)
2017-05-09 01:53:16.828868: step 35250, loss = 0.79 (817.4 examples/sec; 0.157 sec/batch)
2017-05-09 01:53:18.350232: step 35260, loss = 0.83 (841.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:53:19.921442: step 35270, loss = 0.81 (814.7 examples/sec; 0.157 sec/batch)
2017-05-09 01:53:21.479907: step 35280, loss = 1.00 (821.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:53:23.008243: step 35290, loss = 0.64 (837.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:53:24.633878: step 35300, loss = 0.76 (787.4 examples/sec; 0.163 sec/batch)
2017-05-09 01:53:26.059349: step 35310, loss = 1.05 (898.0 examples/sec; 0.143 sec/batch)
2017-05-09 01:53:27.540710: step 35320, loss = 0.95 (864.1 examples/sec; 0.148 sec/batch)
2017-05-09 01:53:29.080682: step 35330, loss = 1.00 (831.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:53:30.613436: step 35340, loss = 0.85 (835.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:53:32.143192: step 35350, loss = 0.86 (836.7 examples/sec; 0.153 sec/batch)
2017-05-09 01:53:33.654155: step 35360, loss = 0.66 (847.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:53:35.162171: step 35370, loss = 0.82 (848.8 examples/sec; 0.151 sec/batch)
2017-05-09 01:53:36.665014: step 35380, loss = 0.78 (851.7 examples/sec; 0.150 sec/batch)
2017-05-09 01:53:38.182608: step 35390, loss = 0.68 (843.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:53:39.807674: step 35400, loss = 0.90 (787.7 examples/sec; 0.163 sec/batch)
2017-05-09 01:53:41.215468: step 35410, loss = 0.82 (909.2 examples/sec; 0.141 sec/batch)
2017-05-09 01:53:42.718018: step 35420, loss = 1.00 (851.9 examples/sec; 0.150 sec/batch)
2017-05-09 01:53:44.237109: step 35430, loss = 0.81 (842.6 examples/sec; 0.152 sec/batch)
2017-05-09 01:53:45.737417: step 35440, loss = 0.78 (853.2 examples/sec; 0.150 sec/batch)
2017-05-09 01:53:47.251675: step 35450, loss = 0.83 (845.3 examples/sec; 0.151 sec/batch)
2017-05-09 01:53:48.792443: step 35460, loss = 0.84 (830.8 examples/sec; 0.154 sec/batch)
2017-05-09 01:53:50.332271: step 35470, loss = 0.92 (831.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:53:51.848031: step 35480, loss = 0.77 (844.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:53:53.374105: step 35490, loss = 0.75 (838.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:53:54.993756: step 35500, loss = 0.84 (790.3 examples/sec; 0.162 sec/batch)
2017-05-09 01:53:56.446608: step 35510, loss = 0.77 (881.0 examples/sec; 0.145 sec/batch)
2017-05-09 01:53:57.962084: step 35520, loss = 0.82 (844.6 examples/sec; 0.152 sec/batch)
2017-05-09 01:53:59.474563: step 35530, loss = 1.05 (846.3 examples/sec; 0.151 sec/batch)
2017-05-09 01:54:00.990620: step 35540, loss = 0.70 (844.3 examples/sec; 0.152 sec/batch)
2017-05-09 01:54:02.526094: step 35550, loss = 0.97 (833.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:54:04.084003: step 35560, loss = 1.17 (821.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:54:05.680663: step 35570, loss = 0.75 (801.7 examples/sec; 0.160 sec/batch)
2017-05-09 01:54:07.256430: step 35580, loss = 0.76 (812.3 examples/sec; 0.158 sec/batch)
2017-05-09 01:54:08.810506: step 35590, loss = 0.76 (823.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:54:10.469309: step 35600, loss = 0.78 (771.6 examples/sec; 0.166 sec/batch)
2017-05-09 01:54:11.937854: step 35610, loss = 0.79 (871.6 examples/sec; 0.147 sec/batch)
2017-05-09 01:54:13.529584: step 35620, loss = 0.90 (804.2 examples/sec; 0.159 sec/batch)
2017-05-09 01:54:15.123631: step 35630, loss = 0.74 (803.0 examples/sec; 0.159 sec/batch)
2017-05-09 01:54:16.704277: step 35640, loss = 0.79 (809.8 examples/sec; 0.158 sec/batch)
2017-05-09 01:54:18.231545: step 35650, loss = 0.93 (838.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:54:19.820291: step 35660, loss = 1.05 (805.7 examples/sec; 0.159 sec/batch)
2017-05-09 01:54:21.382755: step 35670, loss = 0.78 (819.2 examples/sec; 0.156 sec/batch)
2017-05-09 01:54:22.931887: step 35680, loss = 0.77 (826.3 examples/sec; 0.155 sec/batch)
2017-05-09 01:54:24.504231: step 35690, loss = 0.70 (814.1 examples/sec; 0.157 sec/batch)
2017-05-09 01:54:26.136779: step 35700, loss = 0.74 (784.0 examples/sec; 0.163 sec/batch)
2017-05-09 01:54:27.597956: step 35710, loss = 0.73 (876.0 examples/sec; 0.146 sec/batch)
2017-05-09 01:54:29.157804: step 35720, loss = 0.81 (820.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:54:30.694956: step 35730, loss = 0.79 (832.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:54:32.226917: step 35740, loss = 0.83 (835.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:54:33.770619: step 35750, loss = 0.72 (829.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:54:35.325054: step 35760, loss = 0.86 (823.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:54:36.874862: step 35770, loss = 0.95 (825.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:54:38.427452: step 35780, loss = 0.92 (824.4 examples/sec; 0.155 sec/batch)
2017-05-09 01:54:39.967868: step 35790, loss = 1.02 (830.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:54:41.583984: step 35800, loss = 0.72 (792.0 examples/sec; 0.162 sec/batch)
2017-05-09 01:54:43.032111: step 35810, loss = 0.82 (883.9 examples/sec; 0.145 sec/batch)
2017-05-09 01:54:44.569387: step 35820, loss = 0.76 (832.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:54:46.102038: step 35830, loss = 1.09 (835.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:54:47.627456: step 35840, loss = 0.72 (839.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:54:49.188785: step 35850, loss = 0.75 (819.8 examples/sec; 0.156 sec/batch)
2017-05-09 01:54:50.729682: step 35860, loss = 0.79 (830.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:54:52.271091: step 35870, loss = 0.86 (830.4 examples/sec; 0.154 sec/batch)
2017-05-09 01:54:53.812820: step 35880, loss = 0.86 (830.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:54:55.337776: step 35890, loss = 0.71 (839.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:54:56.958413: step 35900, loss = 1.00 (789.8 examples/sec; 0.162 sec/batch)
2017-05-09 01:54:58.407340: step 35910, loss = 1.05 (883.4 examples/sec; 0.145 sec/batch)
2017-05-09 01:54:59.963723: step 35920, loss = 0.92 (822.4 examples/sec; 0.156 sec/batch)
2017-05-09 01:55:01.521198: step 35930, loss = 0.75 (821.8 examples/sec; 0.156 sec/batch)
2017-05-09 01:55:03.107778: step 35940, loss = 0.95 (806.8 examples/sec; 0.159 sec/batch)
2017-05-09 01:55:04.651839: step 35950, loss = 0.71 (829.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:55:06.168250: step 35960, loss = 0.66 (844.1 examples/sec; 0.152 sec/batch)
2017-05-09 01:55:07.725243: step 35970, loss = 0.89 (822.1 examples/sec; 0.156 sec/batch)
2017-05-09 01:55:09.284515: step 35980, loss = 0.87 (820.9 examples/sec; 0.156 sec/batch)
2017-05-09 01:55:10.817640: step 35990, loss = 0.93 (834.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:55:12.450733: step 36000, loss = 0.74 (783.8 examples/sec; 0.163 sec/batch)
2017-05-09 01:55:13.899430: step 36010, loss = 0.64 (883.6 examples/sec; 0.145 sec/batch)
2017-05-09 01:55:15.416731: step 36020, loss = 0.74 (843.6 examples/sec; 0.152 sec/batch)
2017-05-09 01:55:16.967051: step 36030, loss = 0.83 (825.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:55:18.530398: step 36040, loss = 0.77 (818.8 examples/sec; 0.156 sec/batch)
2017-05-09 01:55:20.067234: step 36050, loss = 0.87 (832.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:55:21.589659: step 36060, loss = 0.76 (840.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:55:23.147938: step 36070, loss = 0.75 (821.4 examples/sec; 0.156 sec/batch)
2017-05-09 01:55:24.684642: step 36080, loss = 0.76 (833.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:55:26.203512: step 36090, loss = 1.02 (842.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:55:27.806778: step 36100, loss = 0.84 (798.4 examples/sec; 0.160 sec/batch)
2017-05-09 01:55:29.245357: step 36110, loss = 0.78 (889.8 examples/sec; 0.144 sec/batch)
2017-05-09 01:55:30.772269: step 36120, loss = 0.79 (838.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:55:32.304611: step 36130, loss = 0.90 (835.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:55:33.833636: step 36140, loss = 0.68 (837.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:55:35.370004: step 36150, loss = 0.90 (833.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:55:36.898266: step 36160, loss = 0.80 (837.6 examples/sec; 0.153 sec/batch)
2017-05-09 01:55:38.426245: step 36170, loss = 0.94 (837.7 examples/sec; 0.153 sec/batch)
2017-05-09 01:55:39.960990: step 36180, loss = 0.94 (834.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:55:41.499629: step 36190, loss = 0.76 (831.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:55:43.128955: step 36200, loss = 0.83 (785.6 examples/sec; 0.163 sec/batch)
2017-05-09 01:55:44.572483: step 36210, loss = 0.91 (886.7 examples/sec; 0.144 sec/batch)
2017-05-09 01:55:46.117385: step 36220, loss = 0.73 (828.5 examples/sec; 0.154 sec/batch)
2017-05-09 01:55:47.684738: step 36230, loss = 0.86 (816.7 examples/sec; 0.157 sec/batch)
2017-05-09 01:55:49.233989: step 36240, loss = 0.82 (826.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:55:50.758917: step 36250, loss = 0.87 (839.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:55:52.298354: step 36260, loss = 0.79 (831.5 examples/sec; 0.154 sec/batch)
2017-05-09 01:55:53.783893: step 36270, loss = 0.85 (861.6 examples/sec; 0.149 sec/batch)
2017-05-09 01:55:55.306373: step 36280, loss = 0.59 (840.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:55:56.834471: step 36290, loss = 0.82 (837.7 examples/sec; 0.153 sec/batch)
2017-05-09 01:55:58.459400: step 36300, loss = 0.81 (787.7 examples/sec; 0.162 sec/batch)
2017-05-09 01:55:59.906970: step 36310, loss = 0.68 (884.2 examples/sec; 0.145 sec/batch)
2017-05-09 01:56:01.425243: step 36320, loss = 0.71 (843.1 examples/sec; 0.152 sec/batch)
2017-05-09 01:56:02.955843: step 36330, loss = 0.66 (836.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:04.473493: step 36340, loss = 0.72 (843.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:56:06.007275: step 36350, loss = 0.74 (834.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:07.506193: step 36360, loss = 0.78 (853.9 examples/sec; 0.150 sec/batch)
2017-05-09 01:56:09.036674: step 36370, loss = 0.69 (836.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:10.605973: step 36380, loss = 0.71 (815.7 examples/sec; 0.157 sec/batch)
2017-05-09 01:56:12.143502: step 36390, loss = 0.96 (832.5 examples/sec; 0.154 sec/batch)
2017-05-09 01:56:13.789635: step 36400, loss = 0.77 (777.6 examples/sec; 0.165 sec/batch)
2017-05-09 01:56:15.232203: step 36410, loss = 0.78 (887.3 examples/sec; 0.144 sec/batch)
2017-05-09 01:56:16.763666: step 36420, loss = 1.06 (835.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:18.267467: step 36430, loss = 0.87 (851.2 examples/sec; 0.150 sec/batch)
2017-05-09 01:56:19.792817: step 36440, loss = 0.83 (839.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:21.331953: step 36450, loss = 1.01 (831.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:56:22.854291: step 36460, loss = 1.02 (840.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:56:24.390856: step 36470, loss = 0.99 (833.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:56:25.896590: step 36480, loss = 0.76 (850.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:56:27.428931: step 36490, loss = 0.71 (835.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:29.066245: step 36500, loss = 0.73 (781.8 examples/sec; 0.164 sec/batch)
2017-05-09 01:56:30.476751: step 36510, loss = 0.83 (907.5 examples/sec; 0.141 sec/batch)
2017-05-09 01:56:32.000654: step 36520, loss = 0.89 (839.9 examples/sec; 0.152 sec/batch)
2017-05-09 01:56:33.514164: step 36530, loss = 0.87 (845.7 examples/sec; 0.151 sec/batch)
2017-05-09 01:56:35.021001: step 36540, loss = 0.91 (849.5 examples/sec; 0.151 sec/batch)
2017-05-09 01:56:36.519009: step 36550, loss = 0.73 (854.5 examples/sec; 0.150 sec/batch)
2017-05-09 01:56:38.018096: step 36560, loss = 0.77 (853.8 examples/sec; 0.150 sec/batch)
2017-05-09 01:56:39.537578: step 36570, loss = 0.71 (842.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:56:41.053479: step 36580, loss = 0.75 (844.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:56:42.562175: step 36590, loss = 0.70 (848.4 examples/sec; 0.151 sec/batch)
2017-05-09 01:56:44.157562: step 36600, loss = 0.70 (802.3 examples/sec; 0.160 sec/batch)
2017-05-09 01:56:45.585901: step 36610, loss = 0.82 (896.1 examples/sec; 0.143 sec/batch)
2017-05-09 01:56:47.127065: step 36620, loss = 0.74 (830.5 examples/sec; 0.154 sec/batch)
2017-05-09 01:56:48.684639: step 36630, loss = 0.78 (821.8 examples/sec; 0.156 sec/batch)
2017-05-09 01:56:50.223489: step 36640, loss = 0.74 (831.8 examples/sec; 0.154 sec/batch)
2017-05-09 01:56:51.782612: step 36650, loss = 0.96 (821.0 examples/sec; 0.156 sec/batch)
2017-05-09 01:56:53.298880: step 36660, loss = 0.73 (844.2 examples/sec; 0.152 sec/batch)
2017-05-09 01:56:54.872099: step 36670, loss = 0.70 (813.6 examples/sec; 0.157 sec/batch)
2017-05-09 01:56:56.393160: step 36680, loss = 0.80 (841.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:56:57.930115: step 36690, loss = 0.68 (832.8 examples/sec; 0.154 sec/batch)
2017-05-09 01:56:59.537164: step 36700, loss = 0.68 (796.5 examples/sec; 0.161 sec/batch)
2017-05-09 01:57:00.989004: step 36710, loss = 0.80 (881.6 examples/sec; 0.145 sec/batch)
2017-05-09 01:57:02.509861: step 36720, loss = 0.76 (841.6 examples/sec; 0.152 sec/batch)
2017-05-09 01:57:04.027287: step 36730, loss = 0.88 (843.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:57:05.558678: step 36740, loss = 0.89 (835.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:57:07.108391: step 36750, loss = 0.79 (826.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:57:08.660230: step 36760, loss = 0.75 (824.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:57:10.208025: step 36770, loss = 0.77 (827.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:57:11.745929: step 36780, loss = 0.61 (832.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:57:13.280874: step 36790, loss = 0.77 (833.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:57:14.896513: step 36800, loss = 0.84 (792.3 examples/sec; 0.162 sec/batch)
2017-05-09 01:57:16.356594: step 36810, loss = 0.84 (876.7 examples/sec; 0.146 sec/batch)
2017-05-09 01:57:17.879638: step 36820, loss = 0.86 (840.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:57:19.422927: step 36830, loss = 0.73 (829.4 examples/sec; 0.154 sec/batch)
2017-05-09 01:57:20.976112: step 36840, loss = 0.86 (824.1 examples/sec; 0.155 sec/batch)
2017-05-09 01:57:22.496794: step 36850, loss = 0.83 (841.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:57:24.019260: step 36860, loss = 0.78 (840.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:57:25.609716: step 36870, loss = 0.89 (804.8 examples/sec; 0.159 sec/batch)
2017-05-09 01:57:27.181182: step 36880, loss = 0.96 (814.5 examples/sec; 0.157 sec/batch)
2017-05-09 01:57:28.705978: step 36890, loss = 0.83 (839.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:57:30.324935: step 36900, loss = 0.82 (790.6 examples/sec; 0.162 sec/batch)
2017-05-09 01:57:31.798003: step 36910, loss = 0.82 (868.9 examples/sec; 0.147 sec/batch)
2017-05-09 01:57:33.322246: step 36920, loss = 0.85 (839.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:57:34.874016: step 36930, loss = 0.81 (824.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:57:36.442311: step 36940, loss = 0.93 (816.2 examples/sec; 0.157 sec/batch)
2017-05-09 01:57:37.997114: step 36950, loss = 0.83 (823.3 examples/sec; 0.155 sec/batch)
2017-05-09 01:57:39.552011: step 36960, loss = 0.85 (823.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:57:41.070645: step 36970, loss = 0.83 (842.9 examples/sec; 0.152 sec/batch)
2017-05-09 01:57:42.548225: step 36980, loss = 0.97 (866.3 examples/sec; 0.148 sec/batch)
2017-05-09 01:57:44.081792: step 36990, loss = 0.91 (834.7 examples/sec; 0.153 sec/batch)
2017-05-09 01:57:45.685188: step 37000, loss = 0.80 (798.3 examples/sec; 0.160 sec/batch)
2017-05-09 01:57:47.077859: step 37010, loss = 0.85 (919.1 examples/sec; 0.139 sec/batch)
2017-05-09 01:57:48.612740: step 37020, loss = 0.89 (833.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:57:50.121275: step 37030, loss = 0.80 (848.5 examples/sec; 0.151 sec/batch)
2017-05-09 01:57:51.652006: step 37040, loss = 0.75 (836.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:57:53.161035: step 37050, loss = 0.76 (848.2 examples/sec; 0.151 sec/batch)
2017-05-09 01:57:54.699900: step 37060, loss = 0.95 (831.8 examples/sec; 0.154 sec/batch)
2017-05-09 01:57:56.228073: step 37070, loss = 1.02 (837.6 examples/sec; 0.153 sec/batch)
2017-05-09 01:57:57.748564: step 37080, loss = 0.68 (841.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:57:59.242907: step 37090, loss = 0.82 (856.6 examples/sec; 0.149 sec/batch)
2017-05-09 01:58:00.855236: step 37100, loss = 0.75 (793.9 examples/sec; 0.161 sec/batch)
2017-05-09 01:58:02.284933: step 37110, loss = 0.86 (895.3 examples/sec; 0.143 sec/batch)
2017-05-09 01:58:03.834389: step 37120, loss = 0.87 (826.1 examples/sec; 0.155 sec/batch)
2017-05-09 01:58:05.396416: step 37130, loss = 0.80 (819.5 examples/sec; 0.156 sec/batch)
2017-05-09 01:58:06.918852: step 37140, loss = 0.75 (840.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:58:08.471081: step 37150, loss = 0.87 (824.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:58:09.991497: step 37160, loss = 0.91 (841.9 examples/sec; 0.152 sec/batch)
2017-05-09 01:58:11.513197: step 37170, loss = 0.71 (841.2 examples/sec; 0.152 sec/batch)
2017-05-09 01:58:13.079157: step 37180, loss = 0.81 (817.4 examples/sec; 0.157 sec/batch)
2017-05-09 01:58:14.615701: step 37190, loss = 0.92 (833.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:58:16.253950: step 37200, loss = 0.86 (781.3 examples/sec; 0.164 sec/batch)
2017-05-09 01:58:17.690084: step 37210, loss = 0.89 (891.3 examples/sec; 0.144 sec/batch)
2017-05-09 01:58:19.227198: step 37220, loss = 0.81 (832.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:58:20.729824: step 37230, loss = 0.79 (851.8 examples/sec; 0.150 sec/batch)
2017-05-09 01:58:22.265219: step 37240, loss = 0.73 (833.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:58:23.798756: step 37250, loss = 1.02 (834.7 examples/sec; 0.153 sec/batch)
2017-05-09 01:58:25.350883: step 37260, loss = 0.81 (824.7 examples/sec; 0.155 sec/batch)
2017-05-09 01:58:26.900833: step 37270, loss = 0.70 (825.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:58:28.440999: step 37280, loss = 0.78 (831.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:58:29.971850: step 37290, loss = 0.69 (836.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:58:31.592111: step 37300, loss = 0.68 (790.0 examples/sec; 0.162 sec/batch)
2017-05-09 01:58:33.010537: step 37310, loss = 0.89 (902.4 examples/sec; 0.142 sec/batch)
2017-05-09 01:58:34.564859: step 37320, loss = 0.79 (823.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:58:36.132379: step 37330, loss = 0.82 (816.6 examples/sec; 0.157 sec/batch)
2017-05-09 01:58:37.660067: step 37340, loss = 0.63 (837.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:58:39.175229: step 37350, loss = 0.83 (844.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:58:40.716987: step 37360, loss = 0.76 (830.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:58:42.278747: step 37370, loss = 0.72 (819.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:58:43.807545: step 37380, loss = 0.86 (837.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:58:45.355546: step 37390, loss = 0.95 (826.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:58:46.989030: step 37400, loss = 0.82 (783.6 examples/sec; 0.163 sec/batch)
2017-05-09 01:58:48.447659: step 37410, loss = 0.93 (877.5 examples/sec; 0.146 sec/batch)
2017-05-09 01:58:49.944717: step 37420, loss = 0.83 (855.0 examples/sec; 0.150 sec/batch)
2017-05-09 01:58:51.510045: step 37430, loss = 1.11 (817.7 examples/sec; 0.157 sec/batch)
2017-05-09 01:58:53.072417: step 37440, loss = 0.79 (819.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:58:54.622311: step 37450, loss = 0.99 (825.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:58:56.181173: step 37460, loss = 0.84 (821.1 examples/sec; 0.156 sec/batch)
2017-05-09 01:58:57.700286: step 37470, loss = 0.69 (842.6 examples/sec; 0.152 sec/batch)
2017-05-09 01:58:59.254182: step 37480, loss = 0.71 (823.7 examples/sec; 0.155 sec/batch)
2017-05-09 01:59:00.758542: step 37490, loss = 0.83 (850.9 examples/sec; 0.150 sec/batch)
2017-05-09 01:59:02.363835: step 37500, loss = 0.89 (797.4 examples/sec; 0.161 sec/batch)
2017-05-09 01:59:03.788979: step 37510, loss = 0.86 (898.2 examples/sec; 0.143 sec/batch)
2017-05-09 01:59:05.324011: step 37520, loss = 0.87 (833.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:59:06.849595: step 37530, loss = 0.73 (839.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:59:08.395746: step 37540, loss = 0.86 (827.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:59:09.918217: step 37550, loss = 0.75 (840.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:59:11.441773: step 37560, loss = 0.73 (840.1 examples/sec; 0.152 sec/batch)
2017-05-09 01:59:12.998662: step 37570, loss = 0.78 (822.1 examples/sec; 0.156 sec/batch)
2017-05-09 01:59:14.516240: step 37580, loss = 0.74 (843.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:59:16.039382: step 37590, loss = 0.71 (840.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:59:17.661564: step 37600, loss = 1.01 (789.1 examples/sec; 0.162 sec/batch)
2017-05-09 01:59:19.116894: step 37610, loss = 0.83 (879.5 examples/sec; 0.146 sec/batch)
2017-05-09 01:59:20.664789: step 37620, loss = 0.81 (826.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:59:22.209560: step 37630, loss = 0.66 (828.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:59:23.724089: step 37640, loss = 0.81 (845.2 examples/sec; 0.151 sec/batch)
2017-05-09 01:59:25.253573: step 37650, loss = 0.94 (836.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:59:26.762889: step 37660, loss = 0.87 (848.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:59:28.320152: step 37670, loss = 0.64 (821.9 examples/sec; 0.156 sec/batch)
2017-05-09 01:59:29.835122: step 37680, loss = 0.78 (844.9 examples/sec; 0.151 sec/batch)
2017-05-09 01:59:31.411714: step 37690, loss = 0.73 (811.9 examples/sec; 0.158 sec/batch)
2017-05-09 01:59:33.039280: step 37700, loss = 0.84 (786.4 examples/sec; 0.163 sec/batch)
2017-05-09 01:59:34.442908: step 37710, loss = 0.79 (911.9 examples/sec; 0.140 sec/batch)
2017-05-09 01:59:35.964556: step 37720, loss = 0.70 (841.2 examples/sec; 0.152 sec/batch)
2017-05-09 01:59:37.494756: step 37730, loss = 0.79 (836.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:59:39.040430: step 37740, loss = 0.81 (828.1 examples/sec; 0.155 sec/batch)
2017-05-09 01:59:40.562988: step 37750, loss = 0.77 (840.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:59:42.091253: step 37760, loss = 0.70 (837.6 examples/sec; 0.153 sec/batch)
2017-05-09 01:59:43.656523: step 37770, loss = 0.97 (817.8 examples/sec; 0.157 sec/batch)
2017-05-09 01:59:45.203681: step 37780, loss = 0.77 (827.3 examples/sec; 0.155 sec/batch)
2017-05-09 01:59:46.705348: step 37790, loss = 0.83 (852.4 examples/sec; 0.150 sec/batch)
2017-05-09 01:59:48.324079: step 37800, loss = 0.99 (790.7 examples/sec; 0.162 sec/batch)
2017-05-09 01:59:49.769515: step 37810, loss = 0.98 (885.5 examples/sec; 0.145 sec/batch)
2017-05-09 01:59:51.320020: step 37820, loss = 0.80 (825.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:59:52.854937: step 37830, loss = 0.82 (833.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:59:54.419167: step 37840, loss = 0.93 (818.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:59:55.981999: step 37850, loss = 0.83 (819.0 examples/sec; 0.156 sec/batch)
2017-05-09 01:59:57.533144: step 37860, loss = 0.89 (825.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:59:59.105542: step 37870, loss = 0.88 (814.0 examples/sec; 0.157 sec/batch)
2017-05-09 02:00:00.659355: step 37880, loss = 0.69 (823.8 examples/sec; 0.155 sec/batch)
2017-05-09 02:00:02.212347: step 37890, loss = 0.71 (824.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:00:03.836158: step 37900, loss = 0.89 (788.3 examples/sec; 0.162 sec/batch)
2017-05-09 02:00:05.291990: step 37910, loss = 0.66 (879.2 examples/sec; 0.146 sec/batch)
2017-05-09 02:00:06.865153: step 37920, loss = 0.85 (813.6 examples/sec; 0.157 sec/batch)
2017-05-09 02:00:08.425718: step 37930, loss = 0.95 (820.2 examples/sec; 0.156 sec/batch)
2017-05-09 02:00:09.973124: step 37940, loss = 1.01 (827.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:00:11.512931: step 37950, loss = 0.70 (831.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:00:13.069512: step 37960, loss = 0.64 (822.3 examples/sec; 0.156 sec/batch)
2017-05-09 02:00:14.628879: step 37970, loss = 1.16 (820.8 examples/sec; 0.156 sec/batch)
2017-05-09 02:00:16.213618: step 37980, loss = 0.73 (807.7 examples/sec; 0.158 sec/batch)
2017-05-09 02:00:17.745767: step 37990, loss = 0.80 (835.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:00:19.372215: step 38000, loss = 0.80 (787.0 examples/sec; 0.163 sec/batch)
2017-05-09 02:00:20.794149: step 38010, loss = 0.91 (900.2 examples/sec; 0.142 sec/batch)
2017-05-09 02:00:22.315827: step 38020, loss = 0.74 (841.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:00:23.864828: step 38030, loss = 0.85 (826.3 examples/sec; 0.155 sec/batch)
2017-05-09 02:00:25.388386: step 38040, loss = 0.81 (840.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:00:26.904449: step 38050, loss = 0.73 (844.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:00:28.447985: step 38060, loss = 0.77 (829.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:00:30.007855: step 38070, loss = 0.74 (820.6 examples/sec; 0.156 sec/batch)
2017-05-09 02:00:31.551821: step 38080, loss = 0.80 (829.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:00:33.085908: step 38090, loss = 0.92 (834.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:00:34.694046: step 38100, loss = 1.01 (796.0 examples/sec; 0.161 sec/batch)
2017-05-09 02:00:36.153746: step 38110, loss = 0.88 (876.9 examples/sec; 0.146 sec/batch)
2017-05-09 02:00:37.671678: step 38120, loss = 0.76 (843.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:00:39.172145: step 38130, loss = 0.75 (853.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:00:40.684798: step 38140, loss = 0.87 (846.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:00:42.237446: step 38150, loss = 1.05 (824.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:00:43.777201: step 38160, loss = 0.84 (831.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:00:45.339103: step 38170, loss = 1.03 (819.5 examples/sec; 0.156 sec/batch)
2017-05-09 02:00:46.897866: step 38180, loss = 0.74 (821.2 examples/sec; 0.156 sec/batch)
2017-05-09 02:00:48.414152: step 38190, loss = 0.82 (844.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:00:50.037166: step 38200, loss = 0.91 (788.7 examples/sec; 0.162 sec/batch)
2017-05-09 02:00:51.489692: step 38210, loss = 0.77 (881.2 examples/sec; 0.145 sec/batch)
2017-05-09 02:00:53.016981: step 38220, loss = 0.76 (838.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:00:54.545204: step 38230, loss = 0.96 (837.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:00:56.063054: step 38240, loss = 0.96 (843.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:00:57.605226: step 38250, loss = 0.92 (830.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:00:59.169733: step 38260, loss = 0.89 (818.1 examples/sec; 0.156 sec/batch)
2017-05-09 02:01:00.681538: step 38270, loss = 0.81 (846.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:01:02.208463: step 38280, loss = 0.86 (838.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:01:03.721496: step 38290, loss = 0.70 (846.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:01:05.359761: step 38300, loss = 0.83 (781.3 examples/sec; 0.164 sec/batch)
2017-05-09 02:01:06.819897: step 38310, loss = 1.11 (876.6 examples/sec; 0.146 sec/batch)
2017-05-09 02:01:08.352648: step 38320, loss = 0.83 (835.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:01:09.919768: step 38330, loss = 0.87 (816.8 examples/sec; 0.157 sec/batch)
2017-05-09 02:01:11.476075: step 38340, loss = 0.80 (822.4 examples/sec; 0.156 sec/batch)
2017-05-09 02:01:13.002198: step 38350, loss = 0.95 (838.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:01:14.512510: step 38360, loss = 0.75 (847.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:01:16.038737: step 38370, loss = 0.86 (838.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:01:17.585671: step 38380, loss = 0.89 (827.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:01:19.143739: step 38390, loss = 0.85 (821.5 examples/sec; 0.156 sec/batch)
2017-05-09 02:01:20.780576: step 38400, loss = 0.87 (782.0 examples/sec; 0.164 sec/batch)
2017-05-09 02:01:22.229673: step 38410, loss = 0.66 (883.3 examples/sec; 0.145 sec/batch)
2017-05-09 02:01:23.742509: step 38420, loss = 0.76 (846.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:01:25.246564: step 38430, loss = 0.91 (851.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:01:26.777094: step 38440, loss = 0.78 (836.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:01:28.306042: step 38450, loss = 0.89 (837.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:01:29.845299: step 38460, loss = 0.58 (831.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:01:31.358792: step 38470, loss = 0.70 (845.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:01:32.863273: step 38480, loss = 0.66 (850.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:01:34.375894: step 38490, loss = 0.72 (846.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:01:36.015918: step 38500, loss = 0.66 (780.5 examples/sec; 0.164 sec/batch)
2017-05-09 02:01:37.494774: step 38510, loss = 0.80 (865.5 examples/sec; 0.148 sec/batch)
2017-05-09 02:01:39.024691: step 38520, loss = 0.85 (836.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:01:40.561579: step 38530, loss = 0.75 (832.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:01:42.090566: step 38540, loss = 0.85 (837.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:01:43.646739: step 38550, loss = 0.86 (822.5 examples/sec; 0.156 sec/batch)
2017-05-09 02:01:45.203940: step 38560, loss = 0.83 (822.0 examples/sec; 0.156 sec/batch)
2017-05-09 02:01:46.735555: step 38570, loss = 0.65 (835.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:01:48.290473: step 38580, loss = 1.00 (823.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:01:49.797092: step 38590, loss = 0.86 (849.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:01:51.419119: step 38600, loss = 0.78 (789.1 examples/sec; 0.162 sec/batch)
2017-05-09 02:01:52.867773: step 38610, loss = 0.86 (883.6 examples/sec; 0.145 sec/batch)
2017-05-09 02:01:54.405458: step 38620, loss = 0.68 (832.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:01:55.970026: step 38630, loss = 0.79 (818.1 examples/sec; 0.156 sec/batch)
2017-05-09 02:01:57.465657: step 38640, loss = 0.78 (855.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:01:58.968668: step 38650, loss = 0.78 (851.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:00.498861: step 38660, loss = 0.96 (836.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:02:02.033443: step 38670, loss = 0.84 (834.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:02:03.565634: step 38680, loss = 0.90 (835.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:02:05.086246: step 38690, loss = 0.82 (841.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:02:06.663039: step 38700, loss = 0.72 (811.8 examples/sec; 0.158 sec/batch)
2017-05-09 02:02:08.062514: step 38710, loss = 0.90 (914.6 examples/sec; 0.140 sec/batch)
2017-05-09 02:02:09.580210: step 38720, loss = 0.86 (843.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:02:11.098251: step 38730, loss = 0.88 (843.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:02:12.595663: step 38740, loss = 0.77 (854.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:14.122495: step 38750, loss = 0.80 (838.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:02:15.613589: step 38760, loss = 0.87 (858.4 examples/sec; 0.149 sec/batch)
2017-05-09 02:02:17.121553: step 38770, loss = 0.68 (848.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:02:18.599413: step 38780, loss = 0.86 (866.1 examples/sec; 0.148 sec/batch)
2017-05-09 02:02:20.096715: step 38790, loss = 0.81 (854.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:21.639532: step 38800, loss = 0.72 (829.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:02:23.067210: step 38810, loss = 0.89 (896.6 examples/sec; 0.143 sec/batch)
2017-05-09 02:02:24.567034: step 38820, loss = 0.75 (853.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:26.086019: step 38830, loss = 0.85 (842.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:02:27.593697: step 38840, loss = 0.75 (849.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:02:29.104003: step 38850, loss = 0.82 (847.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:02:30.635433: step 38860, loss = 0.74 (835.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:02:32.129284: step 38870, loss = 0.71 (856.8 examples/sec; 0.149 sec/batch)
2017-05-09 02:02:33.634022: step 38880, loss = 0.84 (850.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:35.141028: step 38890, loss = 0.82 (849.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:02:36.744142: step 38900, loss = 0.82 (798.4 examples/sec; 0.160 sec/batch)
2017-05-09 02:02:38.171156: step 38910, loss = 0.82 (897.0 examples/sec; 0.143 sec/batch)
2017-05-09 02:02:39.648158: step 38920, loss = 0.76 (866.6 examples/sec; 0.148 sec/batch)
2017-05-09 02:02:41.149928: step 38930, loss = 0.79 (852.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:42.649405: step 38940, loss = 0.83 (853.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:44.137283: step 38950, loss = 0.82 (860.3 examples/sec; 0.149 sec/batch)
2017-05-09 02:02:45.651539: step 38960, loss = 0.76 (845.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:02:47.159601: step 38970, loss = 0.92 (848.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:02:48.659395: step 38980, loss = 0.75 (853.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:50.153797: step 38990, loss = 0.81 (856.5 examples/sec; 0.149 sec/batch)
2017-05-09 02:02:51.765128: step 39000, loss = 0.74 (794.4 examples/sec; 0.161 sec/batch)
2017-05-09 02:02:53.166195: step 39010, loss = 0.69 (913.6 examples/sec; 0.140 sec/batch)
2017-05-09 02:02:54.663437: step 39020, loss = 0.69 (854.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:56.175633: step 39030, loss = 0.88 (846.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:02:57.658674: step 39040, loss = 0.76 (863.1 examples/sec; 0.148 sec/batch)
2017-05-09 02:02:59.184030: step 39050, loss = 0.85 (839.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:03:00.686005: step 39060, loss = 0.71 (852.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:03:02.169305: step 39070, loss = 0.77 (862.9 examples/sec; 0.148 sec/batch)
2017-05-09 02:03:03.665285: step 39080, loss = 0.94 (855.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:03:05.145289: step 39090, loss = 0.78 (864.9 examples/sec; 0.148 sec/batch)
2017-05-09 02:03:06.732827: step 39100, loss = 0.85 (806.3 examples/sec; 0.159 sec/batch)
2017-05-09 02:03:08.170054: step 39110, loss = 0.72 (890.6 examples/sec; 0.144 sec/batch)
2017-05-09 02:03:09.666476: step 39120, loss = 0.92 (855.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:03:11.148394: step 39130, loss = 0.76 (863.7 examples/sec; 0.148 sec/batch)
2017-05-09 02:03:12.627711: step 39140, loss = 0.72 (865.3 examples/sec; 0.148 sec/batch)
2017-05-09 02:03:14.108487: step 39150, loss = 0.85 (864.4 examples/sec; 0.148 sec/batch)
2017-05-09 02:03:15.630755: step 39160, loss = 0.68 (840.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:03:17.140859: step 39170, loss = 0.81 (847.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:03:18.629984: step 39180, loss = 0.72 (859.6 examples/sec; 0.149 sec/batch)
2017-05-09 02:03:20.140499: step 39190, loss = 0.77 (847.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:03:21.711148: step 39200, loss = 0.93 (815.0 examples/sec; 0.157 sec/batch)
2017-05-09 02:03:23.107465: step 39210, loss = 0.76 (916.7 examples/sec; 0.140 sec/batch)
2017-05-09 02:03:24.598429: step 39220, loss = 0.77 (858.5 examples/sec; 0.149 sec/batch)
2017-05-09 02:03:26.115663: step 39230, loss = 0.83 (843.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:03:27.653110: step 39240, loss = 0.81 (832.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:03:29.195662: step 39250, loss = 0.73 (829.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:03:30.745122: step 39260, loss = 1.01 (826.1 examples/sec; 0.155 sec/batch)
2017-05-09 02:03:32.300617: step 39270, loss = 0.68 (822.9 examples/sec; 0.156 sec/batch)
2017-05-09 02:03:33.824892: step 39280, loss = 0.90 (839.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:03:35.359473: step 39290, loss = 0.83 (834.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:03:36.981897: step 39300, loss = 0.84 (788.9 examples/sec; 0.162 sec/batch)
2017-05-09 02:03:38.445728: step 39310, loss = 0.85 (874.4 examples/sec; 0.146 sec/batch)
2017-05-09 02:03:39.987712: step 39320, loss = 0.82 (830.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:03:41.519723: step 39330, loss = 0.74 (835.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:03:43.073340: step 39340, loss = 0.71 (823.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:03:44.599979: step 39350, loss = 0.81 (838.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:03:46.126251: step 39360, loss = 0.78 (838.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:03:47.637800: step 39370, loss = 0.76 (846.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:03:49.184464: step 39380, loss = 0.64 (827.6 examples/sec; 0.155 sec/batch)
2017-05-09 02:03:50.736231: step 39390, loss = 0.68 (824.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:03:52.355803: step 39400, loss = 0.66 (790.3 examples/sec; 0.162 sec/batch)
2017-05-09 02:03:53.781133: step 39410, loss = 0.69 (898.0 examples/sec; 0.143 sec/batch)
2017-05-09 02:03:55.306098: step 39420, loss = 0.80 (839.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:03:56.846417: step 39430, loss = 0.95 (831.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:03:58.410886: step 39440, loss = 0.97 (818.2 examples/sec; 0.156 sec/batch)
2017-05-09 02:03:59.940599: step 39450, loss = 0.89 (836.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:04:01.505142: step 39460, loss = 0.81 (818.1 examples/sec; 0.156 sec/batch)
2017-05-09 02:04:03.043976: step 39470, loss = 0.74 (831.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:04:04.591752: step 39480, loss = 0.85 (827.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:04:06.100422: step 39490, loss = 0.67 (848.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:04:07.751833: step 39500, loss = 0.86 (775.1 examples/sec; 0.165 sec/batch)
2017-05-09 02:04:09.233359: step 39510, loss = 0.83 (864.0 examples/sec; 0.148 sec/batch)
2017-05-09 02:04:10.763466: step 39520, loss = 0.78 (836.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:04:12.319911: step 39530, loss = 0.77 (822.4 examples/sec; 0.156 sec/batch)
2017-05-09 02:04:13.875574: step 39540, loss = 0.93 (822.8 examples/sec; 0.156 sec/batch)
2017-05-09 02:04:15.397678: step 39550, loss = 0.95 (840.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:04:16.913429: step 39560, loss = 0.69 (844.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:04:18.451568: step 39570, loss = 0.75 (832.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:04:19.965211: step 39580, loss = 0.79 (845.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:04:21.470673: step 39590, loss = 0.73 (850.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:04:23.088022: step 39600, loss = 0.93 (791.4 examples/sec; 0.162 sec/batch)
2017-05-09 02:04:24.527990: step 39610, loss = 0.84 (888.9 examples/sec; 0.144 sec/batch)
2017-05-09 02:04:26.080909: step 39620, loss = 0.89 (824.3 examples/sec; 0.155 sec/batch)
2017-05-09 02:04:27.631370: step 39630, loss = 0.99 (825.6 examples/sec; 0.155 sec/batch)
2017-05-09 02:04:29.173466: step 39640, loss = 0.80 (830.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:04:30.703140: step 39650, loss = 0.93 (836.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:04:32.239259: step 39660, loss = 0.88 (833.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:04:33.769476: step 39670, loss = 0.86 (836.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:04:35.270601: step 39680, loss = 0.78 (852.7 examples/sec; 0.150 sec/batch)
2017-05-09 02:04:36.800378: step 39690, loss = 0.77 (836.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:04:38.392398: step 39700, loss = 0.82 (804.0 examples/sec; 0.159 sec/batch)
2017-05-09 02:04:39.839862: step 39710, loss = 0.76 (884.3 examples/sec; 0.145 sec/batch)
2017-05-09 02:04:41.377900: step 39720, loss = 0.72 (832.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:04:42.899575: step 39730, loss = 0.58 (841.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:04:44.409640: step 39740, loss = 0.91 (847.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:04:45.929898: step 39750, loss = 0.86 (842.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:04:47.498889: step 39760, loss = 0.76 (815.8 examples/sec; 0.157 sec/batch)
2017-05-09 02:04:49.022003: step 39770, loss = 0.84 (840.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:04:50.532968: step 39780, loss = 0.87 (847.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:04:52.038313: step 39790, loss = 0.69 (850.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:04:53.655598: step 39800, loss = 0.84 (791.4 examples/sec; 0.162 sec/batch)
2017-05-09 02:04:55.070297: step 39810, loss = 0.86 (904.8 examples/sec; 0.141 sec/batch)
2017-05-09 02:04:56.588706: step 39820, loss = 0.87 (843.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:04:58.132366: step 39830, loss = 0.62 (829.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:04:59.707895: step 39840, loss = 0.97 (812.4 examples/sec; 0.158 sec/batch)
2017-05-09 02:05:01.218702: step 39850, loss = 0.70 (847.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:05:02.746615: step 39860, loss = 0.72 (837.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:05:04.270614: step 39870, loss = 0.73 (839.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:05:05.791184: step 39880, loss = 0.98 (841.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:05:07.305006: step 39890, loss = 0.80 (845.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:05:08.940877: step 39900, loss = 1.07 (782.5 examples/sec; 0.164 sec/batch)
2017-05-09 02:05:10.357692: step 39910, loss = 0.83 (903.4 examples/sec; 0.142 sec/batch)
2017-05-09 02:05:11.886392: step 39920, loss = 0.94 (837.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:05:13.399064: step 39930, loss = 0.95 (846.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:05:14.909600: step 39940, loss = 0.82 (847.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:05:16.433146: step 39950, loss = 0.82 (840.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:05:17.950224: step 39960, loss = 0.77 (843.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:05:19.520750: step 39970, loss = 0.85 (815.0 examples/sec; 0.157 sec/batch)
2017-05-09 02:05:21.062270: step 39980, loss = 0.90 (830.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:05:22.624486: step 39990, loss = 0.85 (819.3 examples/sec; 0.156 sec/batch)
2017-05-09 02:05:24.250416: step 40000, loss = 0.70 (787.2 examples/sec; 0.163 sec/batch)
2017-05-09 02:05:25.682126: step 40010, loss = 0.81 (894.0 examples/sec; 0.143 sec/batch)
2017-05-09 02:05:27.202537: step 40020, loss = 0.72 (841.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:05:28.759239: step 40030, loss = 0.86 (822.2 examples/sec; 0.156 sec/batch)
2017-05-09 02:05:30.263703: step 40040, loss = 0.98 (850.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:05:31.810555: step 40050, loss = 0.73 (827.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:05:33.353823: step 40060, loss = 0.87 (829.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:05:34.880053: step 40070, loss = 0.95 (838.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:05:36.398837: step 40080, loss = 0.81 (842.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:05:37.916675: step 40090, loss = 0.86 (843.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:05:39.530627: step 40100, loss = 0.79 (793.1 examples/sec; 0.161 sec/batch)
2017-05-09 02:05:40.939968: step 40110, loss = 0.88 (908.2 examples/sec; 0.141 sec/batch)
2017-05-09 02:05:42.476376: step 40120, loss = 0.67 (833.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:05:44.007524: step 40130, loss = 0.74 (836.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:05:45.504176: step 40140, loss = 0.80 (855.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:05:47.047208: step 40150, loss = 1.00 (829.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:05:48.585824: step 40160, loss = 0.90 (831.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:05:50.081120: step 40170, loss = 0.64 (856.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:05:51.658993: step 40180, loss = 0.78 (811.2 examples/sec; 0.158 sec/batch)
2017-05-09 02:05:53.172401: step 40190, loss = 0.91 (845.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:05:54.741911: step 40200, loss = 0.92 (815.5 examples/sec; 0.157 sec/batch)
2017-05-09 02:05:56.154400: step 40210, loss = 0.63 (906.2 examples/sec; 0.141 sec/batch)
2017-05-09 02:05:57.708875: step 40220, loss = 0.74 (823.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:05:59.235162: step 40230, loss = 0.74 (838.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:06:00.754245: step 40240, loss = 1.01 (842.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:06:02.310958: step 40250, loss = 0.94 (822.2 examples/sec; 0.156 sec/batch)
2017-05-09 02:06:03.837226: step 40260, loss = 0.87 (838.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:06:05.396149: step 40270, loss = 0.87 (821.1 examples/sec; 0.156 sec/batch)
2017-05-09 02:06:06.932253: step 40280, loss = 0.74 (833.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:06:08.482757: step 40290, loss = 0.92 (825.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:06:10.112259: step 40300, loss = 0.66 (785.5 examples/sec; 0.163 sec/batch)
2017-05-09 02:06:11.542566: step 40310, loss = 0.78 (894.9 examples/sec; 0.143 sec/batch)
2017-05-09 02:06:13.042151: step 40320, loss = 0.97 (853.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:06:14.573042: step 40330, loss = 0.76 (836.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:06:16.115175: step 40340, loss = 0.76 (830.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:06:17.624215: step 40350, loss = 0.79 (848.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:06:19.130369: step 40360, loss = 0.91 (849.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:06:20.699386: step 40370, loss = 0.87 (815.8 examples/sec; 0.157 sec/batch)
2017-05-09 02:06:22.229948: step 40380, loss = 0.85 (836.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:06:23.794692: step 40390, loss = 0.84 (818.0 examples/sec; 0.156 sec/batch)
2017-05-09 02:06:25.386691: step 40400, loss = 0.84 (804.0 examples/sec; 0.159 sec/batch)
2017-05-09 02:06:26.822770: step 40410, loss = 0.94 (891.3 examples/sec; 0.144 sec/batch)
2017-05-09 02:06:28.329098: step 40420, loss = 0.82 (849.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:06:29.883111: step 40430, loss = 0.88 (823.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:06:31.405574: step 40440, loss = 0.74 (840.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:06:32.925594: step 40450, loss = 0.81 (842.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:06:34.415233: step 40460, loss = 0.79 (859.3 examples/sec; 0.149 sec/batch)
2017-05-09 02:06:35.952463: step 40470, loss = 1.01 (832.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:06:37.476234: step 40480, loss = 0.79 (840.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:06:39.049063: step 40490, loss = 0.74 (813.8 examples/sec; 0.157 sec/batch)
2017-05-09 02:06:40.661397: step 40500, loss = 0.84 (793.9 examples/sec; 0.161 sec/batch)
2017-05-09 02:06:42.113849: step 40510, loss = 0.79 (881.3 examples/sec; 0.145 sec/batch)
2017-05-09 02:06:43.624214: step 40520, loss = 0.95 (847.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:06:45.183671: step 40530, loss = 0.99 (820.8 examples/sec; 0.156 sec/batch)
2017-05-09 02:06:46.716744: step 40540, loss = 0.74 (834.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:06:48.237521: step 40550, loss = 0.73 (841.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:06:49.746444: step 40560, loss = 0.86 (848.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:06:51.261626: step 40570, loss = 0.86 (844.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:06:52.778741: step 40580, loss = 0.69 (843.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:06:54.296818: step 40590, loss = 0.99 (843.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:06:55.936197: step 40600, loss = 0.68 (780.8 examples/sec; 0.164 sec/batch)
2017-05-09 02:06:57.393933: step 40610, loss = 0.76 (878.1 examples/sec; 0.146 sec/batch)
2017-05-09 02:06:58.923643: step 40620, loss = 0.68 (836.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:07:00.442798: step 40630, loss = 1.02 (842.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:07:01.999200: step 40640, loss = 0.95 (822.4 examples/sec; 0.156 sec/batch)
2017-05-09 02:07:03.516395: step 40650, loss = 0.76 (843.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:07:05.043214: step 40660, loss = 0.67 (838.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:07:06.562088: step 40670, loss = 0.79 (842.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:07:08.110193: step 40680, loss = 0.76 (826.8 examples/sec; 0.155 sec/batch)
2017-05-09 02:07:09.614048: step 40690, loss = 0.74 (851.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:07:11.216548: step 40700, loss = 0.71 (798.8 examples/sec; 0.160 sec/batch)
2017-05-09 02:07:12.641056: step 40710, loss = 1.05 (898.5 examples/sec; 0.142 sec/batch)
2017-05-09 02:07:14.143201: step 40720, loss = 0.73 (852.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:07:15.640784: step 40730, loss = 0.68 (854.7 examples/sec; 0.150 sec/batch)
2017-05-09 02:07:17.148043: step 40740, loss = 0.91 (849.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:07:18.645799: step 40750, loss = 0.76 (854.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:07:20.145739: step 40760, loss = 0.77 (853.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:07:21.675030: step 40770, loss = 0.97 (837.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:07:23.210589: step 40780, loss = 0.74 (833.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:07:24.729712: step 40790, loss = 0.76 (842.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:07:26.328421: step 40800, loss = 0.82 (800.7 examples/sec; 0.160 sec/batch)
2017-05-09 02:07:27.725645: step 40810, loss = 0.73 (916.1 examples/sec; 0.140 sec/batch)
2017-05-09 02:07:29.242783: step 40820, loss = 0.87 (843.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:07:30.727634: step 40830, loss = 0.75 (862.0 examples/sec; 0.148 sec/batch)
2017-05-09 02:07:32.248413: step 40840, loss = 0.74 (841.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:07:33.767562: step 40850, loss = 0.89 (842.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:07:35.271802: step 40860, loss = 0.79 (850.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:07:36.794116: step 40870, loss = 0.79 (840.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:07:38.330103: step 40880, loss = 0.84 (833.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:07:39.868757: step 40890, loss = 0.94 (831.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:07:41.442151: step 40900, loss = 0.81 (813.5 examples/sec; 0.157 sec/batch)
2017-05-09 02:07:42.880070: step 40910, loss = 0.75 (890.2 examples/sec; 0.144 sec/batch)
2017-05-09 02:07:44.394008: step 40920, loss = 0.83 (845.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:07:45.911707: step 40930, loss = 1.02 (843.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:07:47.449728: step 40940, loss = 0.76 (832.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:07:48.978204: step 40950, loss = 0.88 (837.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:07:50.474963: step 40960, loss = 0.63 (855.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:07:51.975543: step 40970, loss = 0.91 (853.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:07:53.471795: step 40980, loss = 0.88 (855.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:07:54.947027: step 40990, loss = 0.86 (867.7 examples/sec; 0.148 sec/batch)
2017-05-09 02:07:56.549302: step 41000, loss = 0.86 (798.9 examples/sec; 0.160 sec/batch)
2017-05-09 02:07:57.949422: step 41010, loss = 0.72 (914.2 examples/sec; 0.140 sec/batch)
2017-05-09 02:07:59.457769: step 41020, loss = 0.96 (848.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:08:00.939154: step 41030, loss = 0.65 (864.1 examples/sec; 0.148 sec/batch)
2017-05-09 02:08:02.434364: step 41040, loss = 0.96 (856.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:08:03.930425: step 41050, loss = 0.81 (855.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:08:05.465909: step 41060, loss = 0.74 (833.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:08:07.023388: step 41070, loss = 0.85 (821.8 examples/sec; 0.156 sec/batch)
2017-05-09 02:08:08.572301: step 41080, loss = 0.82 (826.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:08:10.082645: step 41090, loss = 0.88 (847.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:08:11.664053: step 41100, loss = 0.67 (809.4 examples/sec; 0.158 sec/batch)
2017-05-09 02:08:13.073076: step 41110, loss = 0.71 (908.4 examples/sec; 0.141 sec/batch)
2017-05-09 02:08:14.598720: step 41120, loss = 0.85 (839.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:08:16.095570: step 41130, loss = 0.86 (855.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:08:17.606974: step 41140, loss = 0.78 (846.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:08:19.110828: step 41150, loss = 0.82 (851.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:08:20.628802: step 41160, loss = 0.95 (843.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:08:22.155112: step 41170, loss = 0.79 (838.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:08:23.644754: step 41180, loss = 0.67 (859.3 examples/sec; 0.149 sec/batch)
2017-05-09 02:08:25.145453: step 41190, loss = 0.83 (852.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:08:26.748457: step 41200, loss = 0.94 (798.5 examples/sec; 0.160 sec/batch)
2017-05-09 02:08:28.221485: step 41210, loss = 0.93 (869.0 examples/sec; 0.147 sec/batch)
2017-05-09 02:08:29.734935: step 41220, loss = 0.78 (845.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:08:31.219960: step 41230, loss = 0.65 (861.9 examples/sec; 0.149 sec/batch)
2017-05-09 02:08:32.743526: step 41240, loss = 0.77 (840.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:08:34.270052: step 41250, loss = 0.90 (838.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:08:35.755857: step 41260, loss = 0.86 (861.5 examples/sec; 0.149 sec/batch)
2017-05-09 02:08:37.261472: step 41270, loss = 0.65 (850.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:08:38.763455: step 41280, loss = 0.77 (852.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:08:40.272352: step 41290, loss = 0.61 (848.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:08:41.914342: step 41300, loss = 0.69 (779.5 examples/sec; 0.164 sec/batch)
2017-05-09 02:08:43.336083: step 41310, loss = 0.66 (900.3 examples/sec; 0.142 sec/batch)
2017-05-09 02:08:44.829998: step 41320, loss = 0.81 (856.8 examples/sec; 0.149 sec/batch)
2017-05-09 02:08:46.343765: step 41330, loss = 0.73 (845.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:08:47.870374: step 41340, loss = 0.74 (838.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:08:49.384458: step 41350, loss = 0.96 (845.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:08:50.894571: step 41360, loss = 0.93 (847.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:08:52.438541: step 41370, loss = 0.69 (829.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:08:53.956272: step 41380, loss = 0.84 (843.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:08:55.517987: step 41390, loss = 0.85 (819.6 examples/sec; 0.156 sec/batch)
2017-05-09 02:08:57.149758: step 41400, loss = 0.75 (784.4 examples/sec; 0.163 sec/batch)
2017-05-09 02:08:58.561512: step 41410, loss = 0.72 (906.7 examples/sec; 0.141 sec/batch)
2017-05-09 02:09:00.106199: step 41420, loss = 0.90 (828.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:09:01.647548: step 41430, loss = 0.83 (830.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:09:03.191562: step 41440, loss = 0.81 (829.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:09:04.736742: step 41450, loss = 0.78 (828.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:09:06.219120: step 41460, loss = 0.90 (863.5 examples/sec; 0.148 sec/batch)
2017-05-09 02:09:07.758987: step 41470, loss = 0.96 (831.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:09:09.285582: step 41480, loss = 0.73 (838.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:09:10.772053: step 41490, loss = 0.74 (861.1 examples/sec; 0.149 sec/batch)
2017-05-09 02:09:12.407003: step 41500, loss = 0.84 (782.9 examples/sec; 0.163 sec/batch)
2017-05-09 02:09:13.866891: step 41510, loss = 0.85 (876.8 examples/sec; 0.146 sec/batch)
2017-05-09 02:09:15.402293: step 41520, loss = 0.79 (833.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:09:16.910261: step 41530, loss = 0.85 (848.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:09:18.457188: step 41540, loss = 0.78 (827.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:09:19.977823: step 41550, loss = 0.70 (841.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:09:21.497832: step 41560, loss = 0.75 (842.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:09:22.998830: step 41570, loss = 0.69 (852.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:09:24.520184: step 41580, loss = 0.88 (841.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:09:26.031670: step 41590, loss = 0.78 (846.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:09:27.627448: step 41600, loss = 0.73 (802.1 examples/sec; 0.160 sec/batch)
2017-05-09 02:09:29.055910: step 41610, loss = 0.66 (896.1 examples/sec; 0.143 sec/batch)
2017-05-09 02:09:30.582617: step 41620, loss = 0.71 (838.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:09:32.096668: step 41630, loss = 0.69 (845.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:09:33.615370: step 41640, loss = 0.78 (842.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:09:35.121167: step 41650, loss = 0.86 (850.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:09:36.646090: step 41660, loss = 0.83 (839.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:09:38.153372: step 41670, loss = 0.84 (849.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:09:39.656950: step 41680, loss = 0.85 (851.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:09:41.170986: step 41690, loss = 0.87 (845.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:09:42.747698: step 41700, loss = 1.09 (811.8 examples/sec; 0.158 sec/batch)
2017-05-09 02:09:44.212390: step 41710, loss = 0.80 (873.9 examples/sec; 0.146 sec/batch)
2017-05-09 02:09:45.736089: step 41720, loss = 0.81 (840.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:09:47.252885: step 41730, loss = 1.06 (843.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:09:48.783715: step 41740, loss = 0.74 (836.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:09:50.309683: step 41750, loss = 0.95 (838.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:09:51.844241: step 41760, loss = 0.79 (834.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:09:53.367586: step 41770, loss = 0.95 (840.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:09:54.883710: step 41780, loss = 1.06 (844.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:09:56.418484: step 41790, loss = 0.74 (834.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:09:57.992060: step 41800, loss = 0.97 (813.4 examples/sec; 0.157 sec/batch)
2017-05-09 02:09:59.433924: step 41810, loss = 0.82 (887.7 examples/sec; 0.144 sec/batch)
2017-05-09 02:10:00.976570: step 41820, loss = 0.70 (829.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:10:02.509392: step 41830, loss = 0.71 (835.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:10:04.078166: step 41840, loss = 0.62 (815.9 examples/sec; 0.157 sec/batch)
2017-05-09 02:10:05.593246: step 41850, loss = 0.67 (844.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:10:07.104989: step 41860, loss = 0.73 (846.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:10:08.644986: step 41870, loss = 0.75 (831.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:10:10.113022: step 41880, loss = 0.77 (871.9 examples/sec; 0.147 sec/batch)
2017-05-09 02:10:11.627585: step 41890, loss = 0.96 (845.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:10:13.217519: step 41900, loss = 1.07 (805.1 examples/sec; 0.159 sec/batch)
2017-05-09 02:10:14.634387: step 41910, loss = 0.79 (903.4 examples/sec; 0.142 sec/batch)
2017-05-09 02:10:16.137737: step 41920, loss = 0.73 (851.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:10:17.628514: step 41930, loss = 0.67 (858.6 examples/sec; 0.149 sec/batch)
2017-05-09 02:10:19.142125: step 41940, loss = 0.79 (845.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:10:20.654457: step 41950, loss = 0.75 (846.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:10:22.164145: step 41960, loss = 0.80 (847.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:10:23.699776: step 41970, loss = 0.85 (833.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:10:25.229955: step 41980, loss = 0.91 (836.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:10:26.704419: step 41990, loss = 0.80 (868.1 examples/sec; 0.147 sec/batch)
2017-05-09 02:10:28.294922: step 42000, loss = 0.89 (804.8 examples/sec; 0.159 sec/batch)
2017-05-09 02:10:29.692916: step 42010, loss = 0.76 (915.6 examples/sec; 0.140 sec/batch)
2017-05-09 02:10:31.190875: step 42020, loss = 0.85 (854.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:10:32.697099: step 42030, loss = 0.72 (849.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:10:34.175783: step 42040, loss = 0.79 (865.6 examples/sec; 0.148 sec/batch)
2017-05-09 02:10:35.676538: step 42050, loss = 0.83 (852.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:10:37.182349: step 42060, loss = 0.78 (850.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:10:38.666653: step 42070, loss = 0.73 (862.3 examples/sec; 0.148 sec/batch)
2017-05-09 02:10:40.179997: step 42080, loss = 0.67 (845.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:10:41.642060: step 42090, loss = 0.74 (875.5 examples/sec; 0.146 sec/batch)
2017-05-09 02:10:43.217854: step 42100, loss = 0.71 (812.3 examples/sec; 0.158 sec/batch)
2017-05-09 02:10:44.605165: step 42110, loss = 0.76 (922.6 examples/sec; 0.139 sec/batch)
2017-05-09 02:10:46.091905: step 42120, loss = 0.79 (860.9 examples/sec; 0.149 sec/batch)
2017-05-09 02:10:47.562764: step 42130, loss = 0.78 (870.2 examples/sec; 0.147 sec/batch)
2017-05-09 02:10:49.063258: step 42140, loss = 0.78 (853.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:10:50.588032: step 42150, loss = 0.84 (839.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:10:52.078031: step 42160, loss = 0.64 (859.1 examples/sec; 0.149 sec/batch)
2017-05-09 02:10:53.548031: step 42170, loss = 0.83 (870.7 examples/sec; 0.147 sec/batch)
2017-05-09 02:10:55.023820: step 42180, loss = 0.68 (867.3 examples/sec; 0.148 sec/batch)
2017-05-09 02:10:56.519625: step 42190, loss = 0.82 (855.7 examples/sec; 0.150 sec/batch)
2017-05-09 02:10:58.108535: step 42200, loss = 0.75 (805.6 examples/sec; 0.159 sec/batch)
2017-05-09 02:10:59.517841: step 42210, loss = 0.85 (908.2 examples/sec; 0.141 sec/batch)
2017-05-09 02:11:01.045605: step 42220, loss = 1.07 (837.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:11:02.535711: step 42230, loss = 0.80 (859.0 examples/sec; 0.149 sec/batch)
2017-05-09 02:11:04.037117: step 42240, loss = 0.72 (852.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:11:05.537933: step 42250, loss = 0.63 (852.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:11:07.026110: step 42260, loss = 0.77 (860.1 examples/sec; 0.149 sec/batch)
2017-05-09 02:11:08.545119: step 42270, loss = 0.83 (842.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:11:10.036537: step 42280, loss = 0.91 (858.2 examples/sec; 0.149 sec/batch)
2017-05-09 02:11:11.536203: step 42290, loss = 0.79 (853.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:11:13.142442: step 42300, loss = 0.75 (796.9 examples/sec; 0.161 sec/batch)
2017-05-09 02:11:14.535212: step 42310, loss = 0.78 (919.0 examples/sec; 0.139 sec/batch)
2017-05-09 02:11:16.029541: step 42320, loss = 0.76 (856.6 examples/sec; 0.149 sec/batch)
2017-05-09 02:11:17.522616: step 42330, loss = 0.82 (857.3 examples/sec; 0.149 sec/batch)
2017-05-09 02:11:19.027282: step 42340, loss = 0.78 (850.7 examples/sec; 0.150 sec/batch)
2017-05-09 02:11:20.520204: step 42350, loss = 0.66 (857.4 examples/sec; 0.149 sec/batch)
2017-05-09 02:11:22.011275: step 42360, loss = 1.00 (858.4 examples/sec; 0.149 sec/batch)
2017-05-09 02:11:23.495446: step 42370, loss = 0.84 (862.4 examples/sec; 0.148 sec/batch)
2017-05-09 02:11:25.004056: step 42380, loss = 0.98 (848.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:11:26.482807: step 42390, loss = 0.79 (865.6 examples/sec; 0.148 sec/batch)
2017-05-09 02:11:28.054089: step 42400, loss = 1.05 (814.6 examples/sec; 0.157 sec/batch)
2017-05-09 02:11:29.463057: step 42410, loss = 0.85 (908.5 examples/sec; 0.141 sec/batch)
2017-05-09 02:11:30.967233: step 42420, loss = 0.74 (851.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:11:32.464914: step 42430, loss = 0.80 (854.7 examples/sec; 0.150 sec/batch)
2017-05-09 02:11:33.968668: step 42440, loss = 0.80 (851.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:11:35.507699: step 42450, loss = 0.86 (831.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:11:36.996001: step 42460, loss = 0.71 (860.0 examples/sec; 0.149 sec/batch)
2017-05-09 02:11:38.498278: step 42470, loss = 0.96 (852.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:11:40.002164: step 42480, loss = 0.83 (851.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:11:41.494253: step 42490, loss = 0.70 (857.9 examples/sec; 0.149 sec/batch)
2017-05-09 02:11:43.116558: step 42500, loss = 0.81 (789.0 examples/sec; 0.162 sec/batch)
2017-05-09 02:11:44.531898: step 42510, loss = 0.72 (904.4 examples/sec; 0.142 sec/batch)
2017-05-09 02:11:46.020007: step 42520, loss = 0.70 (860.1 examples/sec; 0.149 sec/batch)
2017-05-09 02:11:47.532370: step 42530, loss = 0.78 (846.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:11:49.036707: step 42540, loss = 0.86 (850.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:11:50.547998: step 42550, loss = 0.63 (847.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:11:52.065941: step 42560, loss = 0.68 (843.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:11:53.583824: step 42570, loss = 0.88 (843.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:11:55.093377: step 42580, loss = 0.87 (847.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:11:56.618013: step 42590, loss = 0.86 (839.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:11:58.239093: step 42600, loss = 0.79 (789.6 examples/sec; 0.162 sec/batch)
2017-05-09 02:11:59.693471: step 42610, loss = 0.84 (880.1 examples/sec; 0.145 sec/batch)
2017-05-09 02:12:01.219566: step 42620, loss = 0.76 (838.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:12:02.708185: step 42630, loss = 0.68 (859.9 examples/sec; 0.149 sec/batch)
2017-05-09 02:12:04.190417: step 42640, loss = 0.76 (863.6 examples/sec; 0.148 sec/batch)
2017-05-09 02:12:05.688428: step 42650, loss = 0.85 (854.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:12:07.199647: step 42660, loss = 0.64 (847.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:12:08.716554: step 42670, loss = 0.83 (843.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:12:10.232019: step 42680, loss = 0.80 (844.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:12:11.771214: step 42690, loss = 0.88 (831.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:12:13.346790: step 42700, loss = 0.85 (812.4 examples/sec; 0.158 sec/batch)
2017-05-09 02:12:14.786304: step 42710, loss = 0.78 (889.2 examples/sec; 0.144 sec/batch)
2017-05-09 02:12:16.309096: step 42720, loss = 0.86 (840.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:12:17.800705: step 42730, loss = 0.86 (858.1 examples/sec; 0.149 sec/batch)
2017-05-09 02:12:19.341925: step 42740, loss = 0.94 (830.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:12:20.886527: step 42750, loss = 0.68 (828.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:12:22.421981: step 42760, loss = 0.76 (833.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:12:23.942877: step 42770, loss = 0.81 (841.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:12:25.449985: step 42780, loss = 0.72 (849.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:12:26.952042: step 42790, loss = 0.85 (852.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:12:28.558928: step 42800, loss = 0.77 (796.6 examples/sec; 0.161 sec/batch)
2017-05-09 02:12:30.027643: step 42810, loss = 0.98 (871.5 examples/sec; 0.147 sec/batch)
2017-05-09 02:12:31.554797: step 42820, loss = 0.86 (838.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:12:33.063849: step 42830, loss = 0.76 (848.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:12:34.572468: step 42840, loss = 0.68 (848.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:12:36.068559: step 42850, loss = 0.78 (855.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:12:37.598976: step 42860, loss = 0.85 (836.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:12:39.124781: step 42870, loss = 0.69 (838.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:12:40.622237: step 42880, loss = 0.94 (854.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:12:42.131867: step 42890, loss = 0.81 (847.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:12:43.760052: step 42900, loss = 0.68 (786.2 examples/sec; 0.163 sec/batch)
2017-05-09 02:12:45.174951: step 42910, loss = 0.78 (904.7 examples/sec; 0.141 sec/batch)
2017-05-09 02:12:46.689285: step 42920, loss = 0.75 (845.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:12:48.212813: step 42930, loss = 0.69 (840.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:12:49.728738: step 42940, loss = 0.61 (844.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:12:51.243351: step 42950, loss = 0.84 (845.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:12:52.761842: step 42960, loss = 0.77 (842.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:12:54.275385: step 42970, loss = 0.73 (845.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:12:55.790233: step 42980, loss = 0.71 (845.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:12:57.289984: step 42990, loss = 0.77 (853.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:12:58.879279: step 43000, loss = 0.66 (805.4 examples/sec; 0.159 sec/batch)
2017-05-09 02:13:00.332811: step 43010, loss = 0.79 (880.6 examples/sec; 0.145 sec/batch)
2017-05-09 02:13:01.850458: step 43020, loss = 0.92 (843.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:13:03.385753: step 43030, loss = 0.87 (833.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:13:04.904238: step 43040, loss = 0.86 (842.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:13:06.441760: step 43050, loss = 0.64 (832.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:13:07.957459: step 43060, loss = 0.86 (844.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:13:09.497017: step 43070, loss = 0.84 (831.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:13:11.013547: step 43080, loss = 0.76 (844.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:13:12.516201: step 43090, loss = 1.04 (851.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:13:14.118710: step 43100, loss = 0.80 (798.7 examples/sec; 0.160 sec/batch)
2017-05-09 02:13:15.538405: step 43110, loss = 0.78 (901.6 examples/sec; 0.142 sec/batch)
2017-05-09 02:13:17.058801: step 43120, loss = 0.73 (841.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:13:18.553910: step 43130, loss = 0.75 (856.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:13:20.058817: step 43140, loss = 0.86 (850.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:13:21.541639: step 43150, loss = 0.83 (863.2 examples/sec; 0.148 sec/batch)
2017-05-09 02:13:23.053096: step 43160, loss = 0.81 (846.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:13:24.566660: step 43170, loss = 0.73 (845.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:13:26.088041: step 43180, loss = 0.88 (841.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:13:27.583720: step 43190, loss = 0.64 (855.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:13:29.207289: step 43200, loss = 0.83 (788.4 examples/sec; 0.162 sec/batch)
2017-05-09 02:13:30.622424: step 43210, loss = 0.78 (904.5 examples/sec; 0.142 sec/batch)
2017-05-09 02:13:32.112745: step 43220, loss = 0.82 (858.9 examples/sec; 0.149 sec/batch)
2017-05-09 02:13:33.621764: step 43230, loss = 0.74 (848.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:13:35.124995: step 43240, loss = 0.74 (851.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:13:36.627634: step 43250, loss = 0.76 (851.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:13:38.123991: step 43260, loss = 0.75 (855.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:13:39.639889: step 43270, loss = 0.82 (844.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:13:41.191717: step 43280, loss = 0.75 (824.8 examples/sec; 0.155 sec/batch)
2017-05-09 02:13:42.688076: step 43290, loss = 0.68 (855.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:13:44.308357: step 43300, loss = 0.63 (790.0 examples/sec; 0.162 sec/batch)
2017-05-09 02:13:45.721995: step 43310, loss = 0.67 (905.4 examples/sec; 0.141 sec/batch)
2017-05-09 02:13:47.260742: step 43320, loss = 0.82 (831.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:13:48.762991: step 43330, loss = 0.79 (852.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:13:50.293185: step 43340, loss = 0.87 (836.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:13:51.807220: step 43350, loss = 0.80 (845.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:13:53.303897: step 43360, loss = 0.86 (855.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:13:54.805762: step 43370, loss = 0.98 (852.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:13:56.317996: step 43380, loss = 0.90 (846.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:13:57.787036: step 43390, loss = 1.22 (871.3 examples/sec; 0.147 sec/batch)
2017-05-09 02:13:59.365207: step 43400, loss = 0.83 (811.1 examples/sec; 0.158 sec/batch)
2017-05-09 02:14:00.766792: step 43410, loss = 0.90 (913.2 examples/sec; 0.140 sec/batch)
2017-05-09 02:14:02.320859: step 43420, loss = 0.80 (823.6 examples/sec; 0.155 sec/batch)
2017-05-09 02:14:03.857941: step 43430, loss = 0.78 (832.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:14:05.374025: step 43440, loss = 0.65 (844.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:14:06.875623: step 43450, loss = 0.71 (852.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:14:08.407569: step 43460, loss = 0.74 (835.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:14:09.974100: step 43470, loss = 0.72 (817.1 examples/sec; 0.157 sec/batch)
2017-05-09 02:14:11.470432: step 43480, loss = 0.82 (855.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:14:13.004816: step 43490, loss = 0.88 (834.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:14:14.635219: step 43500, loss = 0.77 (785.1 examples/sec; 0.163 sec/batch)
2017-05-09 02:14:16.058557: step 43510, loss = 0.93 (899.3 examples/sec; 0.142 sec/batch)
2017-05-09 02:14:17.600454: step 43520, loss = 0.70 (830.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:14:19.113176: step 43530, loss = 0.82 (846.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:14:20.603311: step 43540, loss = 0.87 (859.0 examples/sec; 0.149 sec/batch)
2017-05-09 02:14:22.124182: step 43550, loss = 0.57 (841.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:14:23.656787: step 43560, loss = 0.73 (835.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:14:25.143509: step 43570, loss = 0.92 (861.0 examples/sec; 0.149 sec/batch)
2017-05-09 02:14:26.640766: step 43580, loss = 0.70 (854.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:14:28.142729: step 43590, loss = 0.71 (852.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:14:29.736730: step 43600, loss = 0.66 (803.0 examples/sec; 0.159 sec/batch)
2017-05-09 02:14:31.140572: step 43610, loss = 0.61 (911.8 examples/sec; 0.140 sec/batch)
2017-05-09 02:14:32.659032: step 43620, loss = 0.74 (842.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:14:34.192741: step 43630, loss = 0.75 (834.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:14:35.721286: step 43640, loss = 0.74 (837.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:14:37.273024: step 43650, loss = 0.92 (824.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:14:38.757342: step 43660, loss = 0.61 (862.4 examples/sec; 0.148 sec/batch)
2017-05-09 02:14:40.278440: step 43670, loss = 0.69 (841.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:14:41.775860: step 43680, loss = 0.79 (854.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:14:43.279911: step 43690, loss = 0.75 (851.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:14:44.874265: step 43700, loss = 0.90 (802.8 examples/sec; 0.159 sec/batch)
2017-05-09 02:14:46.293147: step 43710, loss = 0.70 (902.1 examples/sec; 0.142 sec/batch)
2017-05-09 02:14:47.826053: step 43720, loss = 0.83 (835.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:14:49.335614: step 43730, loss = 0.81 (847.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:14:50.881857: step 43740, loss = 0.79 (827.8 examples/sec; 0.155 sec/batch)
2017-05-09 02:14:52.395741: step 43750, loss = 0.91 (845.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:14:53.901022: step 43760, loss = 0.88 (850.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:14:55.384800: step 43770, loss = 1.00 (862.7 examples/sec; 0.148 sec/batch)
2017-05-09 02:14:56.868339: step 43780, loss = 0.93 (862.8 examples/sec; 0.148 sec/batch)
2017-05-09 02:14:58.374615: step 43790, loss = 0.76 (849.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:14:59.990495: step 43800, loss = 0.83 (792.1 examples/sec; 0.162 sec/batch)
2017-05-09 02:15:01.385745: step 43810, loss = 0.65 (917.4 examples/sec; 0.140 sec/batch)
2017-05-09 02:15:02.928343: step 43820, loss = 0.75 (829.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:15:04.443137: step 43830, loss = 0.67 (845.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:15:05.934419: step 43840, loss = 0.73 (858.3 examples/sec; 0.149 sec/batch)
2017-05-09 02:15:07.450137: step 43850, loss = 0.61 (844.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:15:08.956391: step 43860, loss = 0.69 (849.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:15:10.449013: step 43870, loss = 0.92 (857.5 examples/sec; 0.149 sec/batch)
2017-05-09 02:15:11.968708: step 43880, loss = 0.81 (842.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:15:13.465301: step 43890, loss = 0.63 (855.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:15:15.081690: step 43900, loss = 0.91 (791.9 examples/sec; 0.162 sec/batch)
2017-05-09 02:15:16.517398: step 43910, loss = 0.90 (891.5 examples/sec; 0.144 sec/batch)
2017-05-09 02:15:18.026583: step 43920, loss = 0.80 (848.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:15:19.524494: step 43930, loss = 0.65 (854.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:15:21.030400: step 43940, loss = 0.89 (850.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:15:22.560017: step 43950, loss = 0.67 (836.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:15:24.066483: step 43960, loss = 0.86 (849.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:15:25.569331: step 43970, loss = 0.78 (851.7 examples/sec; 0.150 sec/batch)
2017-05-09 02:15:27.072828: step 43980, loss = 0.93 (851.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:15:28.575110: step 43990, loss = 0.72 (852.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:15:30.186211: step 44000, loss = 0.73 (794.5 examples/sec; 0.161 sec/batch)
2017-05-09 02:15:31.611694: step 44010, loss = 0.76 (897.9 examples/sec; 0.143 sec/batch)
2017-05-09 02:15:33.127277: step 44020, loss = 0.77 (844.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:15:34.619587: step 44030, loss = 0.71 (857.7 examples/sec; 0.149 sec/batch)
2017-05-09 02:15:36.137015: step 44040, loss = 0.75 (843.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:15:37.637904: step 44050, loss = 0.89 (852.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:15:39.119177: step 44060, loss = 0.79 (864.1 examples/sec; 0.148 sec/batch)
2017-05-09 02:15:40.627752: step 44070, loss = 0.67 (848.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:15:42.156832: step 44080, loss = 0.64 (837.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:15:43.672165: step 44090, loss = 0.75 (844.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:15:45.249299: step 44100, loss = 0.67 (811.6 examples/sec; 0.158 sec/batch)
2017-05-09 02:15:46.682070: step 44110, loss = 0.78 (893.4 examples/sec; 0.143 sec/batch)
2017-05-09 02:15:48.175442: step 44120, loss = 0.68 (857.1 examples/sec; 0.149 sec/batch)
2017-05-09 02:15:49.662456: step 44130, loss = 0.76 (860.8 examples/sec; 0.149 sec/batch)
2017-05-09 02:15:51.185124: step 44140, loss = 1.00 (840.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:15:52.683236: step 44150, loss = 0.81 (854.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:15:54.189300: step 44160, loss = 0.70 (849.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:15:55.664137: step 44170, loss = 0.85 (867.9 examples/sec; 0.147 sec/batch)
2017-05-09 02:15:57.125789: step 44180, loss = 0.67 (875.7 examples/sec; 0.146 sec/batch)
2017-05-09 02:15:58.624217: step 44190, loss = 0.88 (854.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:16:00.211677: step 44200, loss = 0.96 (806.3 examples/sec; 0.159 sec/batch)
2017-05-09 02:16:01.592002: step 44210, loss = 0.81 (927.3 examples/sec; 0.138 sec/batch)
2017-05-09 02:16:03.083085: step 44220, loss = 0.75 (858.4 examples/sec; 0.149 sec/batch)
2017-05-09 02:16:04.560824: step 44230, loss = 0.75 (866.2 examples/sec; 0.148 sec/batch)
2017-05-09 02:16:06.060615: step 44240, loss = 0.88 (853.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:16:07.584387: step 44250, loss = 0.80 (840.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:16:09.103095: step 44260, loss = 0.76 (842.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:16:10.616167: step 44270, loss = 0.98 (846.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:16:12.104973: step 44280, loss = 0.91 (859.7 examples/sec; 0.149 sec/batch)
2017-05-09 02:16:13.606947: step 44290, loss = 0.71 (852.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:16:15.234231: step 44300, loss = 0.84 (786.6 examples/sec; 0.163 sec/batch)
2017-05-09 02:16:16.622281: step 44310, loss = 0.94 (922.2 examples/sec; 0.139 sec/batch)
2017-05-09 02:16:18.117122: step 44320, loss = 0.68 (856.3 examples/sec; 0.149 sec/batch)
2017-05-09 02:16:19.617220: step 44330, loss = 0.73 (853.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:16:21.117371: step 44340, loss = 0.77 (853.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:16:22.634722: step 44350, loss = 0.79 (843.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:16:24.144444: step 44360, loss = 0.97 (847.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:16:25.653127: step 44370, loss = 0.82 (848.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:16:27.135545: step 44380, loss = 0.85 (863.5 examples/sec; 0.148 sec/batch)
2017-05-09 02:16:28.641100: step 44390, loss = 0.85 (850.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:16:30.222256: step 44400, loss = 0.69 (809.5 examples/sec; 0.158 sec/batch)
2017-05-09 02:16:31.620734: step 44410, loss = 0.70 (915.3 examples/sec; 0.140 sec/batch)
2017-05-09 02:16:33.130355: step 44420, loss = 0.96 (847.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:16:34.617244: step 44430, loss = 0.98 (860.9 examples/sec; 0.149 sec/batch)
2017-05-09 02:16:36.101596: step 44440, loss = 0.68 (862.3 examples/sec; 0.148 sec/batch)
2017-05-09 02:16:37.590405: step 44450, loss = 0.73 (859.8 examples/sec; 0.149 sec/batch)
2017-05-09 02:16:39.089238: step 44460, loss = 0.67 (854.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:16:40.610661: step 44470, loss = 0.87 (841.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:16:42.089875: step 44480, loss = 0.79 (865.3 examples/sec; 0.148 sec/batch)
2017-05-09 02:16:43.577287: step 44490, loss = 0.79 (860.5 examples/sec; 0.149 sec/batch)
2017-05-09 02:16:45.159988: step 44500, loss = 0.78 (808.7 examples/sec; 0.158 sec/batch)
2017-05-09 02:16:46.579104: step 44510, loss = 0.80 (902.0 examples/sec; 0.142 sec/batch)
2017-05-09 02:16:48.111174: step 44520, loss = 0.76 (835.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:16:49.643719: step 44530, loss = 0.75 (835.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:16:51.152614: step 44540, loss = 0.78 (848.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:16:52.630688: step 44550, loss = 0.73 (866.0 examples/sec; 0.148 sec/batch)
2017-05-09 02:16:54.117118: step 44560, loss = 0.60 (861.1 examples/sec; 0.149 sec/batch)
2017-05-09 02:16:55.615229: step 44570, loss = 0.82 (854.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:16:57.115192: step 44580, loss = 0.77 (853.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:16:58.601031: step 44590, loss = 0.84 (861.5 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:00.189285: step 44600, loss = 0.93 (805.9 examples/sec; 0.159 sec/batch)
2017-05-09 02:17:01.582582: step 44610, loss = 0.88 (918.7 examples/sec; 0.139 sec/batch)
2017-05-09 02:17:03.060552: step 44620, loss = 0.89 (866.1 examples/sec; 0.148 sec/batch)
2017-05-09 02:17:04.572100: step 44630, loss = 0.73 (846.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:17:06.065417: step 44640, loss = 0.85 (857.1 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:07.551184: step 44650, loss = 0.75 (861.5 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:09.062919: step 44660, loss = 0.78 (846.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:17:10.538297: step 44670, loss = 1.02 (867.6 examples/sec; 0.148 sec/batch)
2017-05-09 02:17:12.028507: step 44680, loss = 1.04 (858.9 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:13.522110: step 44690, loss = 0.66 (857.0 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:15.129955: step 44700, loss = 0.72 (796.1 examples/sec; 0.161 sec/batch)
2017-05-09 02:17:16.557687: step 44710, loss = 0.66 (896.5 examples/sec; 0.143 sec/batch)
2017-05-09 02:17:18.054095: step 44720, loss = 0.84 (855.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:17:19.577519: step 44730, loss = 0.91 (840.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:17:21.100546: step 44740, loss = 0.84 (840.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:17:22.583833: step 44750, loss = 0.81 (863.0 examples/sec; 0.148 sec/batch)
2017-05-09 02:17:24.112676: step 44760, loss = 0.88 (837.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:17:25.611862: step 44770, loss = 0.83 (853.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:17:27.079607: step 44780, loss = 0.67 (872.1 examples/sec; 0.147 sec/batch)
2017-05-09 02:17:28.589043: step 44790, loss = 0.99 (848.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:17:30.161182: step 44800, loss = 0.68 (814.2 examples/sec; 0.157 sec/batch)
2017-05-09 02:17:31.573435: step 44810, loss = 0.80 (906.4 examples/sec; 0.141 sec/batch)
2017-05-09 02:17:33.063095: step 44820, loss = 0.73 (859.3 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:34.547602: step 44830, loss = 0.75 (862.2 examples/sec; 0.148 sec/batch)
2017-05-09 02:17:36.019246: step 44840, loss = 0.75 (869.8 examples/sec; 0.147 sec/batch)
2017-05-09 02:17:37.528799: step 44850, loss = 0.69 (847.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:17:39.020018: step 44860, loss = 0.73 (858.4 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:40.521752: step 44870, loss = 0.68 (852.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:17:42.000706: step 44880, loss = 0.93 (865.5 examples/sec; 0.148 sec/batch)
2017-05-09 02:17:43.511249: step 44890, loss = 0.90 (847.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:17:45.085279: step 44900, loss = 0.78 (813.2 examples/sec; 0.157 sec/batch)
2017-05-09 02:17:46.481743: step 44910, loss = 0.71 (916.6 examples/sec; 0.140 sec/batch)
2017-05-09 02:17:47.971406: step 44920, loss = 0.64 (859.3 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:49.455014: step 44930, loss = 0.84 (862.8 examples/sec; 0.148 sec/batch)
2017-05-09 02:17:50.938061: step 44940, loss = 0.76 (863.1 examples/sec; 0.148 sec/batch)
2017-05-09 02:17:52.425485: step 44950, loss = 0.76 (860.6 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:53.928593: step 44960, loss = 0.75 (851.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:17:55.434012: step 44970, loss = 0.95 (850.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:17:56.906076: step 44980, loss = 0.91 (869.5 examples/sec; 0.147 sec/batch)
2017-05-09 02:17:58.394182: step 44990, loss = 0.77 (860.2 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:59.964118: step 45000, loss = 0.86 (815.3 examples/sec; 0.157 sec/batch)
2017-05-09 02:18:01.391243: step 45010, loss = 0.73 (896.9 examples/sec; 0.143 sec/batch)
2017-05-09 02:18:02.887906: step 45020, loss = 0.72 (855.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:18:04.353291: step 45030, loss = 0.90 (873.5 examples/sec; 0.147 sec/batch)
2017-05-09 02:18:05.861539: step 45040, loss = 0.75 (848.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:18:07.358261: step 45050, loss = 0.62 (855.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:18:08.922393: step 45060, loss = 0.88 (818.3 examples/sec; 0.156 sec/batch)
2017-05-09 02:18:10.416072: step 45070, loss = 0.68 (856.9 examples/sec; 0.149 sec/batch)
2017-05-09 02:18:11.924706: step 45080, loss = 0.73 (848.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:18:13.447247: step 45090, loss = 0.75 (840.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:18:15.049474: step 45100, loss = 0.74 (798.9 examples/sec; 0.160 sec/batch)
2017-05-09 02:18:16.466549: step 45110, loss = 0.78 (903.3 examples/sec; 0.142 sec/batch)
2017-05-09 02:18:17.970883: step 45120, loss = 0.92 (850.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:18:19.468767: step 45130, loss = 0.89 (854.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:18:20.986522: step 45140, loss = 0.88 (843.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:18:22.509587: step 45150, loss = 0.69 (840.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:18:24.007197: step 45160, loss = 0.89 (854.7 examples/sec; 0.150 sec/batch)
2017-05-09 02:18:25.511301: step 45170, loss = 0.68 (851.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:18:27.010984: step 45180, loss = 0.70 (853.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:18:28.514917: step 45190, loss = 0.81 (851.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:18:30.121813: step 45200, loss = 0.99 (796.6 examples/sec; 0.161 sec/batch)
2017-05-09 02:18:31.584690: step 45210, loss = 0.71 (875.0 examples/sec; 0.146 sec/batch)
2017-05-09 02:18:33.134934: step 45220, loss = 0.75 (825.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:18:34.666696: step 45230, loss = 0.92 (835.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:18:36.225698: step 45240, loss = 0.70 (821.0 examples/sec; 0.156 sec/batch)
2017-05-09 02:18:37.760066: step 45250, loss = 0.77 (834.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:18:39.274602: step 45260, loss = 0.67 (845.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:18:40.759475: step 45270, loss = 1.01 (862.0 examples/sec; 0.148 sec/batch)
2017-05-09 02:18:42.272498: step 45280, loss = 0.60 (846.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:18:43.781828: step 45290, loss = 0.82 (848.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:18:45.421490: step 45300, loss = 0.80 (780.6 examples/sec; 0.164 sec/batch)
2017-05-09 02:18:46.847711: step 45310, loss = 0.67 (897.5 examples/sec; 0.143 sec/batch)
2017-05-09 02:18:48.396521: step 45320, loss = 0.87 (826.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:18:49.925659: step 45330, loss = 0.93 (837.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:18:51.463998: step 45340, loss = 0.83 (831.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:18:52.992382: step 45350, loss = 0.80 (837.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:18:54.520927: step 45360, loss = 0.93 (837.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:18:56.082081: step 45370, loss = 0.87 (819.9 examples/sec; 0.156 sec/batch)
2017-05-09 02:18:57.622409: step 45380, loss = 0.72 (831.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:18:59.143883: step 45390, loss = 0.85 (841.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:19:00.797745: step 45400, loss = 0.78 (773.9 examples/sec; 0.165 sec/batch)
2017-05-09 02:19:02.208349: step 45410, loss = 0.60 (907.4 examples/sec; 0.141 sec/batch)
2017-05-09 02:19:03.722113: step 45420, loss = 0.93 (845.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:19:05.224286: step 45430, loss = 0.94 (852.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:19:06.717486: step 45440, loss = 1.19 (857.2 examples/sec; 0.149 sec/batch)
2017-05-09 02:19:08.210807: step 45450, loss = 0.78 (857.2 examples/sec; 0.149 sec/batch)
2017-05-09 02:19:09.749621: step 45460, loss = 0.69 (831.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:19:11.287209: step 45470, loss = 0.92 (832.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:19:12.877101: step 45480, loss = 0.89 (805.1 examples/sec; 0.159 sec/batch)
2017-05-09 02:19:14.440099: step 45490, loss = 0.69 (818.9 examples/sec; 0.156 sec/batch)
2017-05-09 02:19:16.098089: step 45500, loss = 0.74 (772.0 examples/sec; 0.166 sec/batch)
2017-05-09 02:19:17.529032: step 45510, loss = 0.78 (894.5 examples/sec; 0.143 sec/batch)
2017-05-09 02:19:19.060971: step 45520, loss = 0.92 (835.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:19:20.613998: step 45530, loss = 0.74 (824.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:19:22.133016: step 45540, loss = 0.86 (842.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:19:23.707520: step 45550, loss = 0.75 (813.0 examples/sec; 0.157 sec/batch)
2017-05-09 02:19:25.242420: step 45560, loss = 0.72 (833.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:19:26.812267: step 45570, loss = 0.88 (815.4 examples/sec; 0.157 sec/batch)
2017-05-09 02:19:28.350975: step 45580, loss = 0.79 (831.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:19:29.917048: step 45590, loss = 0.87 (817.3 examples/sec; 0.157 sec/batch)
2017-05-09 02:19:31.546855: step 45600, loss = 0.76 (785.4 examples/sec; 0.163 sec/batch)
2017-05-09 02:19:32.997586: step 45610, loss = 0.81 (882.3 examples/sec; 0.145 sec/batch)
2017-05-09 02:19:34.525260: step 45620, loss = 0.57 (837.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:19:36.050702: step 45630, loss = 0.91 (839.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:19:37.575024: step 45640, loss = 0.71 (839.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:19:39.108935: step 45650, loss = 0.71 (834.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:19:40.595344: step 45660, loss = 0.90 (861.2 examples/sec; 0.149 sec/batch)
2017-05-09 02:19:42.104019: step 45670, loss = 0.64 (848.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:19:43.611113: step 45680, loss = 0.81 (849.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:19:45.136687: step 45690, loss = 0.76 (839.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:19:46.749034: step 45700, loss = 0.88 (793.9 examples/sec; 0.161 sec/batch)
2017-05-09 02:19:48.185486: step 45710, loss = 0.75 (891.1 examples/sec; 0.144 sec/batch)
2017-05-09 02:19:49.689842: step 45720, loss = 0.80 (850.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:19:51.186544: step 45730, loss = 0.73 (855.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:19:52.679979: step 45740, loss = 0.78 (857.1 examples/sec; 0.149 sec/batch)
2017-05-09 02:19:54.171506: step 45750, loss = 0.73 (858.2 examples/sec; 0.149 sec/batch)
2017-05-09 02:19:55.660052: step 45760, loss = 0.74 (859.9 examples/sec; 0.149 sec/batch)
2017-05-09 02:19:57.178525: step 45770, loss = 0.66 (843.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:19:58.652095: step 45780, loss = 0.94 (868.6 examples/sec; 0.147 sec/batch)
2017-05-09 02:20:00.151666: step 45790, loss = 0.79 (853.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:20:01.725289: step 45800, loss = 0.74 (813.4 examples/sec; 0.157 sec/batch)
2017-05-09 02:20:03.147086: step 45810, loss = 0.87 (900.3 examples/sec; 0.142 sec/batch)
2017-05-09 02:20:04.635260: step 45820, loss = 0.71 (860.1 examples/sec; 0.149 sec/batch)
2017-05-09 02:20:06.155372: step 45830, loss = 0.79 (842.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:20:07.638627: step 45840, loss = 0.76 (863.0 examples/sec; 0.148 sec/batch)
2017-05-09 02:20:09.114943: step 45850, loss = 0.69 (867.0 examples/sec; 0.148 sec/batch)
2017-05-09 02:20:10.607938: step 45860, loss = 0.91 (857.3 examples/sec; 0.149 sec/batch)
2017-05-09 02:20:12.141253: step 45870, loss = 0.94 (834.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:20:13.687932: step 45880, loss = 0.93 (827.6 examples/sec; 0.155 sec/batch)
2017-05-09 02:20:15.215935: step 45890, loss = 0.80 (837.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:20:16.846356: step 45900, loss = 0.87 (785.1 examples/sec; 0.163 sec/batch)
2017-05-09 02:20:18.259278: step 45910, loss = 0.84 (905.9 examples/sec; 0.141 sec/batch)
2017-05-09 02:20:19.782142: step 45920, loss = 0.83 (840.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:20:21.361462: step 45930, loss = 0.89 (810.5 examples/sec; 0.158 sec/batch)
2017-05-09 02:20:22.926766: step 45940, loss = 0.67 (817.7 examples/sec; 0.157 sec/batch)
2017-05-09 02:20:24.464507: step 45950, loss = 0.99 (832.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:20:26.011360: step 45960, loss = 0.82 (827.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:20:27.562802: step 45970, loss = 0.74 (825.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:20:29.182726: step 45980, loss = 0.81 (790.2 examples/sec; 0.162 sec/batch)
2017-05-09 02:20:30.696466: step 45990, loss = 0.81 (845.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:20:32.315055: step 46000, loss = 0.81 (790.8 examples/sec; 0.162 sec/batch)
2017-05-09 02:20:33.794043: step 46010, loss = 0.88 (865.5 examples/sec; 0.148 sec/batch)
2017-05-09 02:20:35.349884: step 46020, loss = 0.65 (822.7 examples/sec; 0.156 sec/batch)
2017-05-09 02:20:36.902276: step 46030, loss = 0.84 (824.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:20:38.453794: step 46040, loss = 0.86 (825.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:20:40.032735: step 46050, loss = 0.95 (810.7 examples/sec; 0.158 sec/batch)
2017-05-09 02:20:41.578253: step 46060, loss = 0.83 (828.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:20:43.090522: step 46070, loss = 0.85 (846.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:20:44.630635: step 46080, loss = 0.75 (831.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:20:46.149835: step 46090, loss = 0.72 (842.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:20:47.762683: step 46100, loss = 0.70 (793.6 examples/sec; 0.161 sec/batch)
2017-05-09 02:20:49.241552: step 46110, loss = 0.72 (865.5 examples/sec; 0.148 sec/batch)
2017-05-09 02:20:50.800764: step 46120, loss = 0.73 (820.9 examples/sec; 0.156 sec/batch)
2017-05-09 02:20:52.344569: step 46130, loss = 0.79 (829.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:20:53.888229: step 46140, loss = 0.70 (829.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:20:55.447520: step 46150, loss = 0.72 (820.9 examples/sec; 0.156 sec/batch)
2017-05-09 02:20:56.975586: step 46160, loss = 0.64 (837.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:20:58.476582: step 46170, loss = 0.74 (852.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:21:00.021157: step 46180, loss = 0.92 (828.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:21:01.548053: step 46190, loss = 0.71 (838.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:21:03.194768: step 46200, loss = 0.75 (777.3 examples/sec; 0.165 sec/batch)
2017-05-09 02:21:04.617082: step 46210, loss = 0.68 (899.9 examples/sec; 0.142 sec/batch)
2017-05-09 02:21:06.137546: step 46220, loss = 0.78 (841.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:21:07.684468: step 46230, loss = 0.67 (827.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:21:09.212172: step 46240, loss = 0.71 (837.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:21:10.726868: step 46250, loss = 0.74 (845.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:21:12.278400: step 46260, loss = 0.80 (825.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:21:13.827304: step 46270, loss = 0.72 (826.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:21:15.368282: step 46280, loss = 0.82 (830.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:21:16.893226: step 46290, loss = 0.74 (839.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:21:18.537701: step 46300, loss = 0.83 (778.4 examples/sec; 0.164 sec/batch)
2017-05-09 02:21:19.983407: step 46310, loss = 0.89 (885.4 examples/sec; 0.145 sec/batch)
2017-05-09 02:21:21.518271: step 46320, loss = 0.74 (833.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:21:23.046253: step 46330, loss = 0.81 (837.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:21:24.586688: step 46340, loss = 0.58 (830.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:21:26.091437: step 46350, loss = 0.75 (850.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:21:27.605879: step 46360, loss = 0.87 (845.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:21:29.145415: step 46370, loss = 0.61 (831.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:21:30.657575: step 46380, loss = 0.85 (846.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:21:32.179057: step 46390, loss = 0.94 (841.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:21:33.793562: step 46400, loss = 0.83 (792.8 examples/sec; 0.161 sec/batch)
2017-05-09 02:21:35.205523: step 46410, loss = 0.86 (906.5 examples/sec; 0.141 sec/batch)
2017-05-09 02:21:36.708790: step 46420, loss = 0.77 (851.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:21:38.221564: step 46430, loss = 0.67 (846.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:21:39.776246: step 46440, loss = 0.90 (823.3 examples/sec; 0.155 sec/batch)
2017-05-09 02:21:41.324171: step 46450, loss = 0.79 (826.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:21:42.864822: step 46460, loss = 0.75 (830.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:21:44.393444: step 46470, loss = 0.84 (837.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:21:45.912126: step 46480, loss = 0.83 (842.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:21:47.468633: step 46490, loss = 0.73 (822.4 examples/sec; 0.156 sec/batch)
2017-05-09 02:21:49.096954: step 46500, loss = 0.57 (786.1 examples/sec; 0.163 sec/batch)
2017-05-09 02:21:50.514961: step 46510, loss = 0.78 (902.7 examples/sec; 0.142 sec/batch)
2017-05-09 02:21:52.056728: step 46520, loss = 0.82 (830.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:21:53.591621: step 46530, loss = 0.89 (833.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:21:55.137131: step 46540, loss = 0.69 (828.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:21:56.682810: step 46550, loss = 0.75 (828.1 examples/sec; 0.155 sec/batch)
2017-05-09 02:21:58.172542: step 46560, loss = 0.79 (859.2 examples/sec; 0.149 sec/batch)
2017-05-09 02:21:59.687929: step 46570, loss = 0.81 (844.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:22:01.243477: step 46580, loss = 0.79 (822.9 examples/sec; 0.156 sec/batch)
2017-05-09 02:22:02.765982: step 46590, loss = 0.85 (840.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:22:04.377205: step 46600, loss = 0.68 (794.4 examples/sec; 0.161 sec/batch)
2017-05-09 02:22:05.813898: step 46610, loss = 0.81 (890.9 examples/sec; 0.144 sec/batch)
2017-05-09 02:22:07.359973: step 46620, loss = 0.86 (827.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:22:08.905862: step 46630, loss = 0.86 (828.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:22:10.431668: step 46640, loss = 0.73 (838.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:22:11.967104: step 46650, loss = 0.89 (833.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:22:13.510519: step 46660, loss = 0.89 (829.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:22:15.035471: step 46670, loss = 0.70 (839.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:22:16.587252: step 46680, loss = 0.81 (824.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:22:18.133640: step 46690, loss = 0.87 (827.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:22:19.799543: step 46700, loss = 0.83 (768.4 examples/sec; 0.167 sec/batch)
2017-05-09 02:22:21.223916: step 46710, loss = 0.83 (898.6 examples/sec; 0.142 sec/batch)
2017-05-09 02:22:22.734212: step 46720, loss = 0.77 (847.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:22:24.260511: step 46730, loss = 0.88 (838.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:22:25.791917: step 46740, loss = 0.85 (835.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:22:27.350458: step 46750, loss = 0.92 (821.3 examples/sec; 0.156 sec/batch)
2017-05-09 02:22:28.867985: step 46760, loss = 0.63 (843.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:22:30.431714: step 46770, loss = 0.74 (818.6 examples/sec; 0.156 sec/batch)
2017-05-09 02:22:31.982461: step 46780, loss = 0.79 (825.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:22:33.520847: step 46790, loss = 0.87 (832.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:22:35.157214: step 46800, loss = 0.80 (782.2 examples/sec; 0.164 sec/batch)
2017-05-09 02:22:36.635741: step 46810, loss = 0.74 (865.7 examples/sec; 0.148 sec/batch)
2017-05-09 02:22:38.192226: step 46820, loss = 1.05 (822.4 examples/sec; 0.156 sec/batch)
2017-05-09 02:22:39.731533: step 46830, loss = 0.70 (831.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:22:41.274515: step 46840, loss = 0.65 (829.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:22:42.847457: step 46850, loss = 0.77 (813.8 examples/sec; 0.157 sec/batch)
2017-05-09 02:22:44.420903: step 46860, loss = 0.71 (813.5 examples/sec; 0.157 sec/batch)
2017-05-09 02:22:45.938206: step 46870, loss = 0.82 (843.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:22:47.465509: step 46880, loss = 0.78 (838.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:22:48.981045: step 46890, loss = 0.92 (844.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:22:50.596990: step 46900, loss = 0.79 (792.1 examples/sec; 0.162 sec/batch)
2017-05-09 02:22:52.032962: step 46910, loss = 0.77 (891.4 examples/sec; 0.144 sec/batch)
2017-05-09 02:22:53.515128: step 46920, loss = 0.61 (863.6 examples/sec; 0.148 sec/batch)
2017-05-09 02:22:55.046466: step 46930, loss = 0.73 (835.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:22:56.562117: step 46940, loss = 0.77 (844.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:22:58.086981: step 46950, loss = 0.90 (839.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:22:59.620663: step 46960, loss = 0.76 (834.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:23:01.137583: step 46970, loss = 0.86 (843.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:23:02.681619: step 46980, loss = 1.00 (829.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:23:04.182507: step 46990, loss = 0.85 (852.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:23:05.792587: step 47000, loss = 0.74 (795.0 examples/sec; 0.161 sec/batch)
2017-05-09 02:23:07.228217: step 47010, loss = 0.75 (891.6 examples/sec; 0.144 sec/batch)
2017-05-09 02:23:08.729098: step 47020, loss = 0.65 (852.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:23:10.253836: step 47030, loss = 0.68 (839.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:23:11.776771: step 47040, loss = 0.85 (840.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:23:13.326082: step 47050, loss = 0.79 (826.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:23:14.850610: step 47060, loss = 0.64 (839.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:23:16.380526: step 47070, loss = 0.74 (836.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:23:17.908697: step 47080, loss = 0.89 (837.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:23:19.454833: step 47090, loss = 0.87 (827.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:23:21.061672: step 47100, loss = 1.05 (796.6 examples/sec; 0.161 sec/batch)
2017-05-09 02:23:22.549840: step 47110, loss = 0.85 (860.1 examples/sec; 0.149 sec/batch)
2017-05-09 02:23:24.075610: step 47120, loss = 0.84 (838.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:23:25.573004: step 47130, loss = 0.82 (854.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:23:27.119587: step 47140, loss = 0.70 (827.6 examples/sec; 0.155 sec/batch)
2017-05-09 02:23:28.661010: step 47150, loss = 0.78 (830.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:23:30.170557: step 47160, loss = 0.80 (847.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:23:31.686427: step 47170, loss = 0.89 (844.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:23:33.235290: step 47180, loss = 0.84 (826.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:23:34.783982: step 47190, loss = 0.92 (826.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:23:36.402808: step 47200, loss = 0.74 (790.7 examples/sec; 0.162 sec/batch)
2017-05-09 02:23:37.824359: step 47210, loss = 0.85 (900.4 examples/sec; 0.142 sec/batch)
2017-05-09 02:23:39.349734: step 47220, loss = 0.81 (839.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:23:40.870662: step 47230, loss = 0.89 (841.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:23:42.416236: step 47240, loss = 0.97 (828.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:23:43.952841: step 47250, loss = 0.88 (833.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:23:45.474721: step 47260, loss = 0.69 (841.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:23:46.994138: step 47270, loss = 0.87 (842.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:23:48.532976: step 47280, loss = 0.79 (831.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:23:50.032439: step 47290, loss = 0.85 (853.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:23:51.660149: step 47300, loss = 0.78 (786.4 examples/sec; 0.163 sec/batch)
2017-05-09 02:23:53.135780: step 47310, loss = 0.73 (867.4 examples/sec; 0.148 sec/batch)
2017-05-09 02:23:54.660199: step 47320, loss = 0.77 (839.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:23:56.169801: step 47330, loss = 0.72 (847.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:23:57.709458: step 47340, loss = 0.96 (831.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:23:59.214292: step 47350, loss = 0.77 (850.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:24:00.759985: step 47360, loss = 0.78 (828.1 examples/sec; 0.155 sec/batch)
2017-05-09 02:24:02.291441: step 47370, loss = 0.66 (835.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:03.788994: step 47380, loss = 0.78 (854.7 examples/sec; 0.150 sec/batch)
2017-05-09 02:24:05.328482: step 47390, loss = 0.85 (831.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:24:06.948277: step 47400, loss = 0.70 (790.2 examples/sec; 0.162 sec/batch)
2017-05-09 02:24:08.385958: step 47410, loss = 1.05 (890.3 examples/sec; 0.144 sec/batch)
2017-05-09 02:24:09.914560: step 47420, loss = 0.77 (837.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:11.427227: step 47430, loss = 0.80 (846.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:24:12.956833: step 47440, loss = 0.79 (836.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:14.486911: step 47450, loss = 0.74 (836.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:16.017145: step 47460, loss = 0.91 (836.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:17.533024: step 47470, loss = 0.66 (844.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:24:19.054686: step 47480, loss = 0.80 (841.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:24:20.570655: step 47490, loss = 0.82 (844.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:24:22.203180: step 47500, loss = 0.70 (784.1 examples/sec; 0.163 sec/batch)
2017-05-09 02:24:23.655485: step 47510, loss = 0.93 (881.4 examples/sec; 0.145 sec/batch)
2017-05-09 02:24:25.208988: step 47520, loss = 0.80 (823.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:24:26.719119: step 47530, loss = 0.76 (847.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:24:28.271037: step 47540, loss = 0.93 (824.8 examples/sec; 0.155 sec/batch)
2017-05-09 02:24:29.786717: step 47550, loss = 0.58 (844.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:24:31.316206: step 47560, loss = 0.68 (836.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:32.836198: step 47570, loss = 0.83 (842.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:24:34.393358: step 47580, loss = 0.88 (822.0 examples/sec; 0.156 sec/batch)
2017-05-09 02:24:35.925442: step 47590, loss = 0.88 (835.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:37.522058: step 47600, loss = 0.69 (801.7 examples/sec; 0.160 sec/batch)
2017-05-09 02:24:38.964511: step 47610, loss = 1.03 (887.4 examples/sec; 0.144 sec/batch)
2017-05-09 02:24:40.503469: step 47620, loss = 0.74 (831.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:24:42.033306: step 47630, loss = 0.75 (836.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:43.607248: step 47640, loss = 0.80 (813.2 examples/sec; 0.157 sec/batch)
2017-05-09 02:24:45.159694: step 47650, loss = 0.76 (824.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:24:46.694834: step 47660, loss = 0.77 (833.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:24:48.246547: step 47670, loss = 0.97 (824.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:24:49.772058: step 47680, loss = 0.91 (839.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:51.322294: step 47690, loss = 0.72 (825.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:24:52.913742: step 47700, loss = 0.96 (804.3 examples/sec; 0.159 sec/batch)
2017-05-09 02:24:54.356619: step 47710, loss = 0.78 (887.1 examples/sec; 0.144 sec/batch)
2017-05-09 02:24:55.881774: step 47720, loss = 0.81 (839.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:57.415856: step 47730, loss = 0.88 (834.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:58.959608: step 47740, loss = 0.89 (829.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:25:00.508010: step 47750, loss = 0.78 (826.6 examples/sec; 0.155 sec/batch)
2017-05-09 02:25:02.108807: step 47760, loss = 0.78 (799.6 examples/sec; 0.160 sec/batch)
2017-05-09 02:25:03.653531: step 47770, loss = 0.81 (828.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:25:05.208265: step 47780, loss = 0.84 (823.3 examples/sec; 0.155 sec/batch)
2017-05-09 02:25:06.720355: step 47790, loss = 0.73 (846.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:25:08.344012: step 47800, loss = 0.77 (788.3 examples/sec; 0.162 sec/batch)
2017-05-09 02:25:09.777846: step 47810, loss = 0.84 (892.7 examples/sec; 0.143 sec/batch)
2017-05-09 02:25:11.331721: step 47820, loss = 0.81 (823.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:25:12.862231: step 47830, loss = 0.76 (836.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:25:14.391232: step 47840, loss = 0.65 (837.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:25:15.937799: step 47850, loss = 0.68 (827.6 examples/sec; 0.155 sec/batch)
2017-05-09 02:25:17.458997: step 47860, loss = 0.90 (841.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:25:19.002174: step 47870, loss = 0.79 (829.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:25:20.528694: step 47880, loss = 0.76 (838.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:25:22.074222: step 47890, loss = 0.89 (828.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:25:23.715855: step 47900, loss = 0.94 (779.7 examples/sec; 0.164 sec/batch)
2017-05-09 02:25:25.157443: step 47910, loss = 0.81 (887.9 examples/sec; 0.144 sec/batch)
2017-05-09 02:25:26.698116: step 47920, loss = 0.75 (830.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:25:28.236483: step 47930, loss = 0.80 (832.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:25:29.773013: step 47940, loss = 0.76 (833.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:25:31.339807: step 47950, loss = 1.01 (817.0 examples/sec; 0.157 sec/batch)
2017-05-09 02:25:32.851857: step 47960, loss = 0.84 (846.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:25:34.406642: step 47970, loss = 0.68 (823.3 examples/sec; 0.155 sec/batch)
2017-05-09 02:25:35.962244: step 47980, loss = 0.87 (822.8 examples/sec; 0.156 sec/batch)
2017-05-09 02:25:37.480172: step 47990, loss = 0.75 (843.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:25:39.130885: step 48000, loss = 0.56 (775.4 examples/sec; 0.165 sec/batch)
2017-05-09 02:25:40.568522: step 48010, loss = 0.68 (890.4 examples/sec; 0.144 sec/batch)
2017-05-09 02:25:42.094097: step 48020, loss = 0.60 (839.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:25:43.603989: step 48030, loss = 0.74 (847.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:25:45.111591: step 48040, loss = 0.74 (849.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:25:46.640027: step 48050, loss = 1.01 (837.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:25:48.190385: step 48060, loss = 0.92 (825.6 examples/sec; 0.155 sec/batch)
2017-05-09 02:25:49.707423: step 48070, loss = 0.84 (843.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:25:51.244530: step 48080, loss = 0.87 (832.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:25:52.780212: step 48090, loss = 0.59 (833.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:25:54.404531: step 48100, loss = 0.89 (788.0 examples/sec; 0.162 sec/batch)
2017-05-09 02:25:55.820662: step 48110, loss = 0.76 (903.9 examples/sec; 0.142 sec/batch)
2017-05-09 02:25:57.333581: step 48120, loss = 0.81 (846.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:25:58.831830: step 48130, loss = 0.73 (854.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:26:00.340986: step 48140, loss = 0.88 (848.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:26:01.877185: step 48150, loss = 0.64 (833.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:26:03.388516: step 48160, loss = 0.73 (846.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:26:04.919338: step 48170, loss = 0.74 (836.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:26:06.468997: step 48180, loss = 0.69 (826.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:26:08.002537: step 48190, loss = 0.80 (834.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:26:09.636204: step 48200, loss = 0.98 (783.5 examples/sec; 0.163 sec/batch)
2017-05-09 02:26:11.058821: step 48210, loss = 0.81 (899.8 examples/sec; 0.142 sec/batch)
2017-05-09 02:26:12.599863: step 48220, loss = 0.69 (830.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:26:14.143546: step 48230, loss = 0.87 (829.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:26:15.690303: step 48240, loss = 0.92 (827.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:26:17.231918: step 48250, loss = 0.95 (830.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:26:18.765291: step 48260, loss = 0.74 (834.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:26:20.318529: step 48270, loss = 0.87 (824.1 examples/sec; 0.155 sec/batch)
2017-05-09 02:26:21.859263: step 48280, loss = 0.70 (830.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:26:23.403525: step 48290, loss = 0.65 (828.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:26:25.054683: step 48300, loss = 0.86 (775.2 examples/sec; 0.165 sec/batch)
2017-05-09 02:26:26.525044: step 48310, loss = 0.64 (870.5 examples/sec; 0.147 sec/batch)
2017-05-09 02:26:28.037544: step 48320, loss = 0.76 (846.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:26:29.579026: step 48330, loss = 0.73 (830.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:26:31.118294: step 48340, loss = 0.75 (831.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:26:32.666047: step 48350, loss = 0.87 (827.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:26:34.183504: step 48360, loss = 0.89 (843.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:26:35.708449: step 48370, loss = 0.78 (839.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:26:37.252758: step 48380, loss = 0.92 (828.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:26:38.798756: step 48390, loss = 0.94 (827.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:26:40.435345: step 48400, loss = 0.86 (782.1 examples/sec; 0.164 sec/batch)
2017-05-09 02:26:41.935284: step 48410, loss = 0.66 (853.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:26:43.490521: step 48420, loss = 0.85 (823.0 examples/sec; 0.156 sec/batch)
2017-05-09 02:26:45.015362: step 48430, loss = 0.63 (839.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:26:46.539038: step 48440, loss = 0.65 (840.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:26:48.076891: step 48450, loss = 0.80 (832.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:26:49.612662: step 48460, loss = 0.75 (833.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:26:51.151908: step 48470, loss = 0.86 (831.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:26:52.712765: step 48480, loss = 0.70 (820.1 examples/sec; 0.156 sec/batch)
2017-05-09 02:26:54.234709: step 48490, loss = 0.79 (841.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:26:55.859577: step 48500, loss = 0.66 (787.8 examples/sec; 0.162 sec/batch)
2017-05-09 02:26:57.334055: step 48510, loss = 0.82 (868.1 examples/sec; 0.147 sec/batch)
2017-05-09 02:26:58.878496: step 48520, loss = 0.80 (828.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:27:00.436213: step 48530, loss = 0.66 (821.7 examples/sec; 0.156 sec/batch)
2017-05-09 02:27:01.972465: step 48540, loss = 0.91 (833.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:27:03.516548: step 48550, loss = 0.68 (829.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:27:05.055119: step 48560, loss = 0.89 (831.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:27:06.600388: step 48570, loss = 0.73 (828.3 examples/sec; 0.155 sec/batch)
2017-05-09 02:27:08.159256: step 48580, loss = 0.77 (821.1 examples/sec; 0.156 sec/batch)
2017-05-09 02:27:09.723756: step 48590, loss = 0.79 (818.2 examples/sec; 0.156 sec/batch)
2017-05-09 02:27:11.353885: step 48600, loss = 0.87 (785.2 examples/sec; 0.163 sec/batch)
2017-05-09 02:27:12.807436: step 48610, loss = 0.75 (880.6 examples/sec; 0.145 sec/batch)
2017-05-09 02:27:14.317699: step 48620, loss = 0.76 (847.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:27:15.867948: step 48630, loss = 0.84 (825.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:27:17.448647: step 48640, loss = 0.67 (809.8 examples/sec; 0.158 sec/batch)
2017-05-09 02:27:19.003721: step 48650, loss = 0.86 (823.1 examples/sec; 0.156 sec/batch)
2017-05-09 02:27:20.539876: step 48660, loss = 0.69 (833.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:27:22.070085: step 48670, loss = 0.78 (836.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:27:23.643714: step 48680, loss = 1.12 (813.4 examples/sec; 0.157 sec/batch)
2017-05-09 02:27:25.169549: step 48690, loss = 0.70 (838.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:27:26.797745: step 48700, loss = 0.59 (786.1 examples/sec; 0.163 sec/batch)
2017-05-09 02:27:28.286569: step 48710, loss = 0.93 (859.7 examples/sec; 0.149 sec/batch)
2017-05-09 02:27:29.837096: step 48720, loss = 0.85 (825.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:27:31.366138: step 48730, loss = 0.79 (837.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:27:32.909476: step 48740, loss = 0.80 (829.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:27:34.448980: step 48750, loss = 0.87 (831.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:27:35.997293: step 48760, loss = 0.75 (826.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:27:37.529448: step 48770, loss = 0.77 (835.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:27:39.083448: step 48780, loss = 0.73 (823.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:27:40.622540: step 48790, loss = 0.78 (831.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:27:42.241422: step 48800, loss = 0.92 (790.7 examples/sec; 0.162 sec/batch)
2017-05-09 02:27:43.698790: step 48810, loss = 0.81 (878.3 examples/sec; 0.146 sec/batch)
2017-05-09 02:27:45.222582: step 48820, loss = 0.74 (840.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:27:46.766619: step 48830, loss = 0.80 (829.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:27:48.314304: step 48840, loss = 0.81 (827.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:27:49.833443: step 48850, loss = 0.73 (842.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:27:51.381266: step 48860, loss = 0.81 (827.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:27:52.919730: step 48870, loss = 0.63 (832.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:27:54.461644: step 48880, loss = 0.97 (830.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:27:56.001071: step 48890, loss = 0.84 (831.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:27:57.639955: step 48900, loss = 0.88 (781.0 examples/sec; 0.164 sec/batch)
2017-05-09 02:27:59.088644: step 48910, loss = 0.86 (883.6 examples/sec; 0.145 sec/batch)
2017-05-09 02:28:00.653279: step 48920, loss = 0.69 (818.1 examples/sec; 0.156 sec/batch)
2017-05-09 02:28:02.261535: step 48930, loss = 0.66 (795.9 examples/sec; 0.161 sec/batch)
2017-05-09 02:28:03.837189: step 48940, loss = 0.79 (812.4 examples/sec; 0.158 sec/batch)
2017-05-09 02:28:05.391418: step 48950, loss = 1.07 (823.6 examples/sec; 0.155 sec/batch)
2017-05-09 02:28:06.942595: step 48960, loss = 0.69 (825.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:28:08.472805: step 48970, loss = 1.05 (836.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:28:10.011579: step 48980, loss = 0.68 (831.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:28:11.567340: step 48990, loss = 0.64 (822.8 examples/sec; 0.156 sec/batch)
2017-05-09 02:28:13.181800: step 49000, loss = 0.74 (792.8 examples/sec; 0.161 sec/batch)
2017-05-09 02:28:14.639819: step 49010, loss = 0.85 (877.9 examples/sec; 0.146 sec/batch)
2017-05-09 02:28:16.217444: step 49020, loss = 0.72 (811.3 examples/sec; 0.158 sec/batch)
2017-05-09 02:28:17.757017: step 49030, loss = 0.87 (831.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:28:19.287069: step 49040, loss = 0.72 (836.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:28:20.808489: step 49050, loss = 0.66 (841.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:28:22.346830: step 49060, loss = 0.79 (832.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:28:23.858486: step 49070, loss = 0.72 (846.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:28:25.391258: step 49080, loss = 0.71 (835.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:28:26.921804: step 49090, loss = 0.82 (836.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:28:28.566814: step 49100, loss = 0.76 (778.1 examples/sec; 0.165 sec/batch)
2017-05-09 02:28:29.998205: step 49110, loss = 0.99 (894.2 examples/sec; 0.143 sec/batch)
2017-05-09 02:28:31.519708: step 49120, loss = 0.91 (841.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:28:33.063684: step 49130, loss = 0.82 (829.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:28:34.591160: step 49140, loss = 0.81 (838.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:28:36.134938: step 49150, loss = 0.68 (829.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:28:37.678951: step 49160, loss = 0.98 (829.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:28:39.224148: step 49170, loss = 0.70 (828.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:28:40.766285: step 49180, loss = 0.88 (830.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:28:42.329570: step 49190, loss = 0.78 (818.8 examples/sec; 0.156 sec/batch)
2017-05-09 02:28:43.975524: step 49200, loss = 0.66 (777.7 examples/sec; 0.165 sec/batch)
2017-05-09 02:28:45.416779: step 49210, loss = 0.84 (888.1 examples/sec; 0.144 sec/batch)
2017-05-09 02:28:46.964615: step 49220, loss = 0.97 (827.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:28:48.523078: step 49230, loss = 0.90 (821.3 examples/sec; 0.156 sec/batch)
2017-05-09 02:28:50.048956: step 49240, loss = 0.99 (838.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:28:51.592798: step 49250, loss = 0.80 (829.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:28:53.124557: step 49260, loss = 0.85 (835.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:28:54.666237: step 49270, loss = 0.86 (830.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:28:56.204632: step 49280, loss = 0.86 (832.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:28:57.744018: step 49290, loss = 0.81 (831.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:28:59.375266: step 49300, loss = 0.77 (784.7 examples/sec; 0.163 sec/batch)
2017-05-09 02:29:00.805633: step 49310, loss = 0.78 (894.9 examples/sec; 0.143 sec/batch)
2017-05-09 02:29:02.346960: step 49320, loss = 0.84 (830.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:29:03.872272: step 49330, loss = 0.82 (839.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:29:05.428813: step 49340, loss = 0.83 (822.3 examples/sec; 0.156 sec/batch)
2017-05-09 02:29:06.982101: step 49350, loss = 0.77 (824.1 examples/sec; 0.155 sec/batch)
2017-05-09 02:29:08.526689: step 49360, loss = 0.72 (828.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:29:10.076498: step 49370, loss = 0.76 (825.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:29:11.609499: step 49380, loss = 0.78 (835.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:29:13.142615: step 49390, loss = 0.98 (834.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:29:14.758796: step 49400, loss = 0.81 (792.0 examples/sec; 0.162 sec/batch)
2017-05-09 02:29:16.191382: step 49410, loss = 0.68 (893.5 examples/sec; 0.143 sec/batch)
2017-05-09 02:29:17.701396: step 49420, loss = 0.86 (847.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:29:19.254692: step 49430, loss = 0.66 (824.1 examples/sec; 0.155 sec/batch)
2017-05-09 02:29:20.790570: step 49440, loss = 0.80 (833.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:29:22.340048: step 49450, loss = 0.65 (826.1 examples/sec; 0.155 sec/batch)
2017-05-09 02:29:23.868875: step 49460, loss = 0.96 (837.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:29:25.410069: step 49470, loss = 0.91 (830.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:29:26.954097: step 49480, loss = 0.68 (829.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:29:28.500986: step 49490, loss = 1.00 (827.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:29:30.110379: step 49500, loss = 0.70 (795.3 examples/sec; 0.161 sec/batch)
2017-05-09 02:29:31.557883: step 49510, loss = 0.75 (884.2 examples/sec; 0.145 sec/batch)
2017-05-09 02:29:33.085017: step 49520, loss = 0.94 (838.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:29:34.631900: step 49530, loss = 0.88 (827.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:29:36.152364: step 49540, loss = 0.96 (841.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:29:37.681546: step 49550, loss = 0.96 (837.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:29:39.219544: step 49560, loss = 0.83 (832.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:29:40.779122: step 49570, loss = 0.86 (820.7 examples/sec; 0.156 sec/batch)
2017-05-09 02:29:42.353077: step 49580, loss = 0.67 (813.2 examples/sec; 0.157 sec/batch)
2017-05-09 02:29:43.913005: step 49590, loss = 0.71 (820.6 examples/sec; 0.156 sec/batch)
2017-05-09 02:29:45.545989: step 49600, loss = 0.79 (783.8 examples/sec; 0.163 sec/batch)
2017-05-09 02:29:47.005201: step 49610, loss = 0.72 (877.2 examples/sec; 0.146 sec/batch)
2017-05-09 02:29:48.541696: step 49620, loss = 0.73 (833.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:29:50.081195: step 49630, loss = 0.72 (831.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:29:51.632650: step 49640, loss = 0.60 (825.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:29:53.178525: step 49650, loss = 0.62 (828.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:29:54.685290: step 49660, loss = 0.67 (849.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:29:56.216245: step 49670, loss = 0.86 (836.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:29:57.771489: step 49680, loss = 0.85 (823.0 examples/sec; 0.156 sec/batch)
2017-05-09 02:29:59.323281: step 49690, loss = 0.73 (824.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:30:00.955693: step 49700, loss = 1.15 (784.1 examples/sec; 0.163 sec/batch)
2017-05-09 02:30:02.436661: step 49710, loss = 0.64 (864.3 examples/sec; 0.148 sec/batch)
2017-05-09 02:30:03.973733: step 49720, loss = 0.72 (832.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:30:05.491914: step 49730, loss = 0.73 (843.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:30:07.019790: step 49740, loss = 0.87 (837.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:30:08.586781: step 49750, loss = 0.84 (816.8 examples/sec; 0.157 sec/batch)
2017-05-09 02:30:10.125012: step 49760, loss = 0.88 (832.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:30:11.654827: step 49770, loss = 0.71 (836.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:30:13.197679: step 49780, loss = 0.93 (829.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:30:14.759237: step 49790, loss = 0.92 (819.7 examples/sec; 0.156 sec/batch)
2017-05-09 02:30:16.392023: step 49800, loss = 0.89 (783.9 examples/sec; 0.163 sec/batch)
2017-05-09 02:30:17.827116: step 49810, loss = 0.63 (891.9 examples/sec; 0.144 sec/batch)
2017-05-09 02:30:19.402925: step 49820, loss = 0.84 (812.3 examples/sec; 0.158 sec/batch)
2017-05-09 02:30:20.924372: step 49830, loss = 0.65 (841.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:30:22.521501: step 49840, loss = 0.74 (801.4 examples/sec; 0.160 sec/batch)
2017-05-09 02:30:24.068489: step 49850, loss = 0.82 (827.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:30:25.623941: step 49860, loss = 0.92 (822.9 examples/sec; 0.156 sec/batch)
2017-05-09 02:30:27.216193: step 49870, loss = 0.62 (803.9 examples/sec; 0.159 sec/batch)
2017-05-09 02:30:28.781914: step 49880, loss = 0.65 (817.5 examples/sec; 0.157 sec/batch)
2017-05-09 02:30:30.351393: step 49890, loss = 0.70 (815.5 examples/sec; 0.157 sec/batch)
2017-05-09 02:30:32.003510: step 49900, loss = 0.72 (774.8 examples/sec; 0.165 sec/batch)
2017-05-09 02:30:33.497135: step 49910, loss = 0.82 (857.0 examples/sec; 0.149 sec/batch)
2017-05-09 02:30:35.034746: step 49920, loss = 0.89 (832.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:30:36.556069: step 49930, loss = 0.72 (841.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:30:38.078672: step 49940, loss = 0.76 (840.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:30:39.618945: step 49950, loss = 0.82 (831.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:30:41.171263: step 49960, loss = 0.89 (824.6 examples/sec; 0.155 sec/batch)
2017-05-09 02:30:42.699214: step 49970, loss = 0.76 (837.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:30:44.224473: step 49980, loss = 0.77 (839.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:30:45.780239: step 49990, loss = 0.69 (822.7 examples/sec; 0.156 sec/batch)
2017-05-09 02:30:47.399721: step 50000, loss = 0.73 (790.4 examples/sec; 0.162 sec/batch)
--- 6965.64756393 seconds ---
